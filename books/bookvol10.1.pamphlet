%%%% -*- Mode: LaTeX; -*-
\documentclass[dvips]{book}
\newcommand{\VolumeName}{Volume 10.1: Axiom Algebra: Theory}
\usepackage{amsmath}
\input{bookheader.tex}
\pagenumbering{arabic}
\mainmatter
\setcounter{chapter}{0} % Chapter 1
% NOTE THAT ALL CITED ARTICLE ARE BY AUTHOR PERMISSION
\chapter{Ordered Algebraic Structures, the GCD}

This chapter is used by permission from ``Elements of
Programming'' by Stepanov and McJones \cite{Step09}.

\section{Basic Algebraic Structures}

An element is called an {\sl identity element} of a binary operation
if, when combined with any other element as the first or second
argument, the operation returns the other element:

\hspace*{0.2cm}{\bf property}(T : {\sl Regular}, 
Op : {\sl BinaryOperation})\\
\hspace*{0.4cm}{\bf requires}(T = Domain(Op))\\
\hspace*{0.2cm}identity\_element : T $\times$ Op\\
\hspace*{0.4cm}($e$,op) $\mapsto$ ($\forall a \in$ T) = op($a,e$) = $a$

{\bf Lemma 5.1} An identity element is unique:

\hspace*{0.2cm}identity\_element($e$,op) $\land$
identity\_element($e^\prime$,op) $\Rightarrow$ $e$ = $e^\prime$

The empty string is the identity element of string concatenation. The
matrix 
$\left(\begin{array}{cc}1 & 0\\
0 & 1\end{array}\right)$
is the multiplicative identity of $2 \times 2$ matrices, while
$\left(\begin{array}{cc}0 & 0\\
0 & 0\end{array}\right)$
is their additive identity.

A transformation is called an {\sl inverse operation} of a binary
operation with respect to a given element (usually the identity
element of the binary operation) if it satisfies the following:

\hspace*{0.2cm}{\bf property}
(F : {\sl Transformation}, 
T : {\sl Regular}, 
Op : {\sl BinaryOperation})\\
\hspace*{0.8cm}{\bf requires}(Domain(F) = T = Domain(Op))\\
\hspace*{0.2cm}inverse\_operation : F $\times$ T $\times$ Op\\
\hspace*{0.8cm}($inv$,$e$,op) $\mapsto$ ($\forall a \in$ T)
op($a,inv(a)$) = op($inv(a),a$) = $e$

{\bf Lemma 5.2} $f(n)=n^3$ is the multiplicative inverse for the
multiplcation of non-zero remainders module 5.

A binary operation is {\sl commutative} if its results is the same
when its arguments are interchanged:

\hspace*{0.2cm}{\bf property}(Op : {\sl BinaryOperation})\\
\hspace*{0.2cm}commutative : Op\\
\hspace*{0.6cm}op $\mapsto$ ($\forall a,b \in$ Domain(Op)) op($a,b$) =
op($b,a$)

Composition of transformations is associative but not commutative.

A set with an associative operation is called a 
{\sl semigroup}. $+$ is always used to denoate an associative,
commutative operation, a type with $+$ is called an 
{\sl additive semigroup}:

{\sl AdditiveSemigroup}({\bf T}) {$\overset{\triangle}{=}$\\
\hspace*{0.8cm}{\sl Regular}({\bf T})\\
\hspace*{0.4cm}$\land +$ : 
{\bf T} $\times$ {\bf T} $\rightarrow$ {\bf T}\\
\hspace*{0.8cm}$\land$ associative($+$)\\
\hspace*{0.8cm}$\land$ commutative($+$)

Multiplication is sometimes not commutative. Consider, for example,
matrix multiplication.

$MultiplicativeSemigroup$({\bf T})$\overset{\triangle}{=}$\\
\hspace*{0.8cm}{\sl Regular}{{\bf T})\\
\hspace*{0.4cm}$\land$ : {\bf T}$\times${\bf T} $\rightarrow$ {\bf T}\\
\hspace*{0.4cm}$\land$ associative($\cdot$)

We use the following notation

\begin{center}
\begin{tabular}{c|c|c}
 & Specifications & C$++$\\
\hline\\
Multiplication & $\cdot$ & ${\bf *}$
\end{tabular}
\end{center}

A semigroup with an identity element is called a {\sl monoid}. The
additive identity element is denoted by 0, which leads to the
definition of an {\sl additive monoid}:

{\sl AdditiveMonoid}({\bf T}) $\overset{\triangle}{=}$\\
\hspace*{0.8cm}{\sl AdditiveSemigroup}({\bf T})\\
\hspace*{0.4cm}$\land$ 0 $\in$ {\bf T}\\
\hspace*{0.4cm}$\land$ identity\_element(0,$+$)

We use the following notation:

\begin{center}
\begin{tabular}{c|c|c}
 & Specifications & C$++$\\
\hline\\
Additive identity & 0 & ${\bf T(0)}$
\end{tabular}
\end{center}

Non-negative reals are an additive monoid, as are matrices with
natural numbers as their coefficients.

The multiplicative identity element is denoted by 1, which leads to
the definition of a {\sl multiplicative monoid}:

{\sl MultiplicativeMonoid}({\bf T}) $\overset{\triangle}{=}$\\
\hspace*{0.8cm}{\sl MultiplicativeSemigroup}({\bf T})\\
\hspace*{0.4cm}$\land$ 1 $\in$ {\bf T}\\
\hspace*{0.4cm}$\land$ identity\_element(1,$\cdot$)

We use the following notation:

\begin{center}
\begin{tabular}{c|c|c}
 & Specifications & C$++$\\
\hline\\
Multiplicatiev identity & 1 & ${\bf T(1)}$
\end{tabular}
\end{center}

Matrices with integer coefficients are a multiplicative monoid.

A monoid with an inverse operation is called a {\sl group}. If an
additive monoid has an inverse, it is denotd by unary $-$, and there
is a derived operation called {\sl subtraction}, denoted by binary
$-$. That leads to the definition of an {\sl additive group}:

{\sl AdditiveGroup}({\bf T}) $\overset{\triangle}{=}$\\
\hspace*{0.8cm}{\sl AdditiveMonoid}({\bf T})\\
\hspace*{0.4cm}$\land$ $-$ : {\bf T} $\rightarrow$ {\bf T}\\
\hspace*{0.4cm}$\land$ inverse\_operation(unary $-$,0,$+$)\\
\hspace*{0.4cm}$\land$ $-$ : {\bf T} $\times$ {\bf T}
$\rightarrow$ {\bf T}\\
\hspace*{0.8cm}$(a,b) \mapsto a + (-b)$

Matrices with integer coefficients are an additive group.

{\bf Lemma 5.3} In an additive group, $-0 = 0$.

Just as there is a concept of additive group, there is a corresponding
concept of {\sl multiplicative group}. In this concept the inverse is
called {\sl multiplicative inverse}, and there is a derived operation
called {\sl division}, denoted by binary $/$:

{\sl MultiplicativeGroup}({\bf T}) $\overset{\triangle}{=}$\\
\hspace*{0.8cm}{\sl MultiplicativeMonoid}({\bf T})\\
\hspace*{0.4cm}$\land$ multiplicative\_inverse : {\bf T} $\rightarrow$
{\bf T}\\
\hspace*{0.4cm}$\land$ inverse\_operation(multiplicative\_inverse,1,$\cdot$)\\
\hspace*{0.4cm}$\land$ $/$ : {\bf T} $\times$ {\bf T} $\rightarrow$ {\bf T}\\
\hspace*{0.8cm}$(a,b) \mapsto a \cdot$ multiplicative\_inverse($b$)

{\tt multiplicative\_inverse} is written as $x^{-1}$.

The set \{cos $\theta$ $+$ $i$ sin $\theta$\} of complex numbers on the unit
circle is a commutative multiplicative group. A unimodular group
${\tt GL}_n(\mathbb{Z})$ $(n\times n$ matrices with integer
coefficients with determinant equal to $\pm 1$) is a noncommutative
multiplicative group.

Two concepts can be combined on the same type with the help of axioms
connecting their operations. When both $+$ and $\cdot$ are present on
a type, they are interrelated with axioms defining a {\sl semiring}:

{\sl Semiring}({\bf T}) $\overset{\triangle}{=}$\\
\hspace*{0.8cm}{\sl AdditiveMonoid}({\bf T})\\
\hspace*{0.4cm}$\land$ {\sl MultiplicativeMonoid}({\bf T})\\
\hspace*{0.4cm}$\land$ $0 \ne 1$\\
\hspace*{0.4cm}$\land$ 
$(\forall a \in {\bf T }) 0 \cdot a = a \cdot 0 = 0$\\
\hspace*{0.4cm}$\land$ $(\forall a,b,c \in {\bf T})$\\
\hspace*{0.8cm}$a\cdot(b+c) = a\cdot b+a\cdot c$\\
\hspace*{0.8cm}$(b+c)\cdot a = b\cdot a+c\cdot a$

The axiom about multiplication by 0 is called the 
{\sl annihilation property}. The final axiom connecting $+$ and
$\cdot$ is called {\sl distributivity}.

Matrices with non-negative integer coefficients constitute a semiring.

{\sl CommutativeSemiring}({\bf T}) $\overset{\triangle}{=}$\\
\hspace*{0.8cm}{\sl Semiring}({\bf T})\\
\hspace*{0.4cm}$\land$ commutative($\cdot$)

Non-negative integers constitute a commutative semiring.

{\sl Ring}({\bf T}) $\overset{\triangle}{=}$\\
\hspace*{0.8cm}{\sl AdditiveGroup}({\bf T})\\
\hspace*{0.4cm}$\land$ Semiringt({\bf T})

Matrices with integer coefficients constitute a ring.

{\sl CommutativeRing}({\bf T}) $\overset{\triangle}{=}$\\
\hspace*{0.8cm}{\sl AdditiveGroup}({\bf T})\\
\hspace*{0.4cm}$\land$ CommutativeSemiring({\bf T})

Integers constitute a commutative ring; polynomials with integer
coefficients constitute a commutative ring.

A {\sl relational concept} is a concept defined on two types. 
{\sl Semimodule} is a relational concept that connnects an additive
monoid and a commutative ring:

{\sl Semimodule}({\bf T,S}) $\overset{\triangle}{=}$\\
\hspace*{0.8cm}{\sl AdditiveMonoid}({\bf T})\\
\hspace*{0.4cm}$\land$ {\sl CommutativeSemiring}(S)\\
\hspace*{0.4cm}$\land ~{\bf S}\times{\bf T} \rightarrow {\bf T}$\\
\hspace*{0.4cm}$\land ~(\forall \alpha,\beta \in S)
(\forall a,b \in T)$\\
\hspace*{1.0cm}$\alpha\cdot(\beta\cdot a)$  = 
$(\alpha\cdot\beta)\cdot a$\\
\hspace*{1.0cm}$(\alpha +\beta)\cdot a$  = 
$\alpha\cdot a + \beta\cdot a$\\
\hspace*{1.0cm}$\alpha\cdot(a+b)$  =  
$\alpha\cdot a + \alpha\cdot b$\\
\hspace*{1.0cm}$1\cdot a$  =  $a$

If {\sl Semimodule}({\bf T,S}), we say that {\bf T} is a semimodule
{\sl over} {\bf S}. We borrow terminology from vector spaces and call
elements of {\bf T} {\sl vectors} and elements of {\bf S} 
{\sl scalars}. For example, polynomials with non-negative integer
coefficients constitute a semimodule over non-negative integers.

{\bf Theorem 5.1} {\sl AdditiveMonoid}({\bf T}) $\Rightarrow$
{\sl Semimodule}({\bf T},$\mathbb{N}$), where scalar multiplication is
defined as $n\cdot x = x+\cdots +x \quad n$ times.

{\sl Proof}. It follows trivially from the definition of scalar
multiplication together with associativity and commutativity of the
monoid operation. For example,

\[\begin{array}{rcl}
n\cdot a+n\cdot b & = & (a+\cdots +a)+(b+\cdots + b)\\
                  & = & (a+b)+\cdots +(a+b)\\
                  & = & n\cdot (a+b)\\
\hfill\qed
\end{array}\]

Using {\tt power} allows us to implement multiplication by an integer
in $\log_2 n$ steps.

Strengthening the requirements by replacing the additive monoid with
an additive group and replacing the semiring with a ring transforms a
semimodule into a module:

{\sl Module}({\bf T,S}) $\overset{\triangle}{=}$\\
\hspace*{0.8cm}{\sl Semimodule}({\bf T,S})\\
\hspace*{0.4cm}$\land$ {\sl AdditiveGroup}(T)\\
\hspace*{0.4cm}$\land$ {\sl Ring}(S)

{\bf Lemma 5.4} Every additive group is a module over integers with an
appropriately defined scalar multiplication.

Computer types are often partial models of concepts. A model is called 
{\sl partial} when the operations satisfy the axioms where they are
defined but are not everywhere defined. For example, the result of
concatenation of strings may not be representable, because of memory
limitations, but concatenation is associative where it is defined.

\section{Ordered Algebraic Structures}

When a total ordering is defined on the elements of a structure in
such a way that the ordering is consistent with the structure's
algebraic properties, it is the {\sl natural total ordering} for the
structure: 

{\sl OrderedAdditiveSemigroup}({\bf T}) $\overset{\triangle}{=}$\\
\hspace*{0.8cm}{\sl AdditiveSemigroup}({\bf T})\\
\hspace*{0.4cm}$\land$ {\sl TotallyOrdered}(T)\\
\hspace*{0.4cm}$\land (\forall a,b,c \in T)~ a < b \Rightarrow
a+c < b+c$

{\sl OrderedAdditiveMonoid}({\bf T}) $\overset{\triangle}{=}$\\
\hspace*{0.8cm}{\sl OrderedAdditiveSemigroup}({\bf T})\\
\hspace*{0.4cm}$\land$ {\sl AdditiveMonoid}(T)\\

{\sl OrderedAdditiveGroup}({\bf T}) $\overset{\triangle}{=}$\\
\hspace*{0.8cm}{\sl OrderedAdditiveMonoid}({\bf T})\\
\hspace*{0.4cm}$\land$ {\sl AdditiveGroup}(T)\\

{\bf Lemma 5.5} In an ordered additive semigroup, 
$a < b \land c < d \Rightarrow a+c < b+d$

{\bf Lemma 5.6} In an ordered additive monoid viewed as a semimodule
over natural numbers, $a>0 \land n>0 \Rightarrow na > 0$.

{\bf Lemma 5.7} In an ordered additive group $a<b \Rightarrow -b<-a$.

Total ordering and negation allow us to define absolute value:

{\tt
template<typename T>\\
\hspace*{0.6cm}requires(OrderedAdditiveGroup(T))\\
T abs(const T\& a)\\
\{\\
\hspace*{0.6cm}if (a $<$ T(0)) return -a;\\
\hspace*{0.6cm}else\hspace{1.75cm} return \ a;\\
\}
}

The following lemma captures an important property of {\tt abs}.

{\bf Lemma 5.8} In an ordered additive group 
$a<0 \Rightarrow 0 < -a$.

We use the notation $\abs{a}$ for the absolute value of $a$.
Absolute value satisfies the following properties.

\[\begin{array}{rcl}
\abs{a-b}  & =           & \abs{b-a}\\
\abs{a+b}  & \le         & \abs{a}+\abs{b}\\
\abs{a-b}  & \ge         & \abs{a}-\abs{b}\\
\abs{a}=0  & \Rightarrow & a=0\\
a \ne 0    & \Rightarrow & \abs{a} > 0
\end{array}\]

\section{Remainder}

We saw that repeated addition in an additive monoid induces
multiplication by a non-negative integer. In an additive group, this
algorithm can be inverted, obtaining division by repeated subtraction
on elements of the form $a=nb$, where $b$ divides $a$. To extend this
to division with remainder for an arbitrary pair of elements, we need
ordering. The ordering allows the algorithm to terminate when it is no
longer possible to subtract. As we shall see, it also enables an
algorithm to take a logarithmic number of steps. The subtraction
operation does not need to be defined everywhere; it is sufficient to
have a partial subtraction called {\sl cancellation}, where $a-b$ is
only defined when $b$ does not exceed $a$:

{\sl CancellableMonoid}({\bf T}) $\overset{\triangle}{=}$\\
\hspace*{0.8cm}{\sl OrderedAdditiveMonoid}({\bf T})\\
\hspace*{0.4cm}$\land ~-:{\bf T}\times {\bf T} \rightarrow {\bf T}$\\
\hspace*{0.4cm}$\land ~(\forall a,b \in {\bf T})~b\le a \Rightarrow 
a-b$ is defined $\land~(a-b)+b=a$.

We write the axiom as $(a-b)+b=a$ instead of $(a+b)-b=a$ to avoid
overflow in partial models of {\sl CancellableMonoid}:

{\tt
template<typename T>\\
\hspace*{0.6cm}requires(CancellableMonoid(T))\\
T slow\_remainder(T a, T b)\\
\{\\
\hspace*{0.6cm}//$Precondition: a \ge 0 \land b>0$\\
\hspace*{0.6cm}while (b <= a) a = a - b;\\
\hspace*{0.6cm}return a;\\
\}
}

The concept {\sl CancellableMonoid} is not strong enough to prove
termination of {\tt slow\_remainder}. For example, {\tt slow\_remainder}
does not always terminate for polynomials with integer coefficients,
ordered lexicographically.

To ensure that the algorithm terminates, we need another property,
called the {\sl Axiom of Archimedes}\footnote{"...the excess by which
the greater of (two) unequal areas exceeds the less can, by being
added to itself, be made to exceed any given finite area"}

{\sl ArchimedeanMonoid}({\bf T}) $\overset{\triangle}{=}$\\
\hspace*{0.8cm}{\sl CancellableMonoid}({\bf T})\\
\hspace*{0.4cm}$\land ~(\forall a,b \in {\bf T})(a\ge 0\land b>0)
\Rightarrow {\tt slow\_remainder}(a,b)~{\tt terminates}$\\
\hspace*{0.4cm}$\land ~{\tt QuotientType}: ArchimedeanMonoid
\rightarrow Integer$

Observe that termination of an algorithm is a legitimate axiom; in
this case it is equivalent to
\[(\exists n \in {\tt QuotientType(T)})~a-n\cdot b < b\]

While the Axiom of Archimedes is usually given as ``there exists an
integer $n$ such that $a<n\cdot b$,'' our version works with partial
Archimedean monoids where $n\cdot b$ might overflow. The type function 
{\tt QuotientType} returns a type large enough to represent the number
of iterations performed by {\tt slow\_remainder}.

{\bf Lemma 5.10} The following are Archimedean monoids: integers,
rational numbers, binary fractions $\left\{\frac{n}{2^k}\right\}$,
ternary fractions $\left\{\frac{n}{3^k}\right\}$, and real numbers.

We can trivially adapt the code of {\tt slow\_remainder} to return the
quotient: 

{\tt
template<typename T>\\
\hspace*{0.6cm}requires(ArchimedeanMonoid(T))\\
QuotientType({\bf T}) slow\_quotient(T a, T b)\\
\{\\
\hspace*{0.6cm}//$Precondition: a \ge 0 \land b>0$\\
\hspace*{0.6cm}QuotientType({\bf T}) n(0);\\
\hspace*{0.6cm}while (b <= a) \{\\
\hspace*{1.0cm}a = a - b;\\
\hspace*{1.0cm}n = successor(n);\\
\hspace*{0.6cm}\}\\
\hspace*{0.6cm}return n;\\
\}\\
}

Repeated doubling leads to the logarithmic-complexity {\tt power}
algorithm. A related algorithm is possible for remainder\footnote{The
Egyptians used this algorithm to do division with remainder, as they
used the power algorithm to do mulitplication} Let us derive an
expression for the remainder $u$ from dividing $a$ by $b$ in terms of
the remainder $v$ by dividing $a$ by $2b$:
\[a=n(2b)+v\]

Since the remainder $v$ must be less than the divisor $2b$, it follows
that 
\[u=
\left\{
\begin{array}{ll}
v & {\tt if\ }v < b\\
v-b & {\tt if\ }v \ge b
\end{array}
\right.\]

This leads to the following recursive procedure:

{\tt
template<typename T>\\
\hspace*{0.6cm}requires(ArchimedeanMonoid(T))\\
T remainder\_recursive(T a, T b)\\
\{\\
\hspace*{0.6cm}//$Precondition: a \ge b > 0$\\
\hspace*{0.6cm}if (a - b >= b) \{\\
\hspace*{1.0cm}a = remainder\_recursive(a,b + b);\\
\hspace*{1.0cm}if (a < b) return a;\\
\hspace*{0.6cm}\}\\
\hspace*{0.6cm}return a - b;\\
\}\\
}

Testing $a-b\ge b$ rather than $a\ge b+b$ avoids overflow of $b_b$. 

{\tt
template<typename T>\\
\hspace*{0.6cm}requires(ArchimedeanMonoid(T))\\
T remainder\_nonnegative(T a, T b)\\
\{\\
\hspace*{0.6cm}//$Precondition: a \ge 0 \land b > 0$\\
\hspace*{0.6cm}if (a < b) return a;\\
\hspace*{0.6cm}return remainder\_recursive(a, b);\\
\}\\
}

Floyd and Knuth \cite{Floy90} give a constant-space algorithm for
remainder on Archimedean monoids that performs about 31\% more
operations than {\tt remainder\_nonnegative}, but when we can divide
by 2 an algorithm exists that does not increase the operation 
count\footnote{Dijkstra attributes this algorithm to N.G. deBruijn}
This is likely to be possible in many situations. For example, while
the general k-section of an angle by ruler and compass cannot be done,
the bisection is trivial.

{\sl HalvableMonoid}({\bf T}) $\overset{\triangle}{=}$\\
\hspace*{0.8cm}{\sl ArchimedeanMonoid}({\bf T})\\
\hspace*{0.4cm}$\land$ half : {\bf T} $\rightarrow$ {\bf T}\\
\hspace*{0.4cm}$\land ~(\forall a,b \in {\bf T})(b>0\land a=b+b)
\Rightarrow half(a)=b$\\

Oberve that {\tt half} needs to be defined on for ``even'' elements.

{\tt
template<typename T>\\
\hspace*{0.6cm}requires(HalvableMonoid(T))\\
T remainder\_nonnegative\_iterative(T a, T b)\\
\{\\
\hspace*{0.6cm}//$Precondition: a \ge 0 \land b > 0$\\
\hspace*{0.6cm}if (a < b) return a;\\
\hspace*{0.6cm}T c = largest\_doubling(a, b);\\
\hspace*{0.6cm}a = a - c;\\
\hspace*{0.6cm}while (c != b) \{\\
\hspace*{1.0cm}c = half(c);\\
\hspace*{1.0cm}if (c <= a) a = a - c;\\
\hspace*{0.6cm}\}\\
\hspace*{0.6cm}return a;\\
\}\\
}

where {\tt largest\_doubling} is defined by the following procedure:

{\tt
template<typename T>\\
\hspace*{0.6cm}requires(ArchimedeanMonoid(T))\\
T largest\_doubling(T a, T b)\\
\{\\
\hspace*{0.6cm}//$Precondition: a \ge b > 0$\\
\hspace*{0.6cm}while (b <= a - b ) b = b + b;\\
\hspace*{0.6cm}return b;\\
\}\\
}

The correctness of {\tt remainder\_nonnegative\_iterative} depends on
the following lemma.

{\bf Lemma 5.11} The result of doubling a positive element of a
halvable monoid $k$ times may be halved $k$ times.

We would only need {\tt remainder\_nonnegative} if we had an
Archimedean monoid that was not halvable. The examples we gave -- line
segments in Euclidean geometry, rational numbers, binary and ternary
fractions -- are all halvable.

\section{Greatest Common Divisor}

For $a \ge 0$ and $b > 0$ in an Archimedean monoid {\bf T}, we define 
{\sl divisibility} as follows:
\[b {\tt \ divides\ } a \Leftrightarrow
(\exists n \in {\tt QuotientType(T))}~ a = nb\]

{\bf Lemma 5.12} In an Archimedean monoid {\bf T} with positive 
$x,a,b$
\begin{itemize}
\item $b$ divides $a$ $\Leftrightarrow$ {\tt remainder\_nonnegative(a,b)}=0
\item $b$ divides $a$ $\Rightarrow$ $b \le a$
\item $a>b \land x$ divides $a \land x$ divides $b$ $\Rightarrow$
$x$ divides $(a-b)$
\item $x$ divides $a \land x$ divides $b \Rightarrow$ divides
{\tt remainder\_nonnegative(a,b)}
\end{itemize}

The {\sl greatest common divisor} of $a$ and $b$, denoted by gcd(a,b),
is a divisor of $a$ and $b$ that is divisible by any other common
divisor of $a$ and $b$\footnote{While this definition works for
Archimedean monoids, it does not depend on ordering and can be
extended to other structures with divisibility relations, such as
rings.} 

{\bf Lemma 5.13} In an Archimedean monoid, the following holds for
positive $x,a,b$:
\begin{itemize}
\item gcd is commutative
\item gcd is associative
\item $x$ divides $a$ $\land$ $x$ divides $b \Rightarrow$
$x\le gcd(a,b)$ 
\item gcd(a,b) is unique
\item gcd(a,a) = a
\item $a>b \Rightarrow$ gcd(a,b) = gcd(a-b,b)
\end{itemize}

The previous lemmas immediately imply that if the following algorithm
terminates, it returns the gcd of its arguments:\footnote{It is known
as Euclid's algorithm}

{\tt
template<typename T>\\
\hspace*{0.6cm}requires(ArchimedeanMonoid(T))\\
T subtractive\_gcd\_nonzero(T a, T b)\\
\{\\
\hspace*{0.6cm}//$Precondition: a>0 \land b>0$\\
\hspace*{0.6cm}while (true) \{\\
\hspace*{1.0cm}if (b < a)\hspace{0.95cm} a = a - b;\\
\hspace*{1.0cm}else if (a < b) b = b - a;\\
\hspace*{1.0cm}else\hspace{2.0cm}  return a;\\
\hspace*{0.6cm}\}\\
\}\\
}

{\bf Lemma 5.14} It always terminates for integers and rationals.

There are types for which it does not always terminate. In particular,
it does not always terminate for real numbers; specifically, it does
not terminate for input of $\sqrt{2}$ and 1. The proof of this fact
depends on the following two lemmas:

{\bf Lemma 5.15} gcd($\frac{a}{gcd(a,b)}$,$\frac{b}{gcd(a,b)}$) = 1

{\bf Lemma 5.16} If the square of an integer $n$ is even, $n$ is even.

{\bf Theorem 5.2} {\tt subtractive\_gcd\_nonzero}($\sqrt{2}$,1) does
not terminate.

{\sl Proof.} Suppose that 
{\tt subtractive\_gcd\_nonzero}($\sqrt{2}$,1) terminates, returning
$d$. Let m = $\frac{\sqrt{2}}{d}$ and n = $\frac{1}{d}$; by Lemma 5.15, 
$m$ and $n$ have no common factors greater than 1. $\frac{m}{n}$ =
$\frac{\sqrt{2}}{1}$ = $\sqrt{2}$, so $m^2=2n^2$; $m$ is even; for some
integer $u$, $m=2u$. $4u^2=2n^2$, so $n^2=2u^2$; $n$ is even. Both $m$
and $n$ are divisible by 2; a contradiction\footnote{The
incommentsurability of the side and the diagonal of a square was one
of the first mathematical proofs discovered by the Greeks. Aristotle
refers to it in {\sl Prior Analytics} I. 23 as the canonical example
of proof by contradiction ({\sl reductio ad absurdum})}. \qed

A {\sl Euclidean} monoid is an Archimedean monoid where 
{\tt subtractive\_gcd\_nonzero} always terminates:

{\sl EuclideanMonoid}({\bf T}) $\overset{\triangle}{=}$\\
\hspace*{0.8cm}{\sl ArchimedeanMonoid}({\bf T})\\
\hspace*{0.4cm}$\land$ $(\forall a,b \in T)(a>0 \land~b>0)
\Rightarrow$ {\tt subtractive\_gcd\_nonzero} terminates.

{\bf Lemma 5.17} Every Archimedean monoid with a smallest positive
element is Euclidean.

{\bf Lemma 5.18} The rational numbers are a Euclidean monoid.

It is straightforward to extend {\sl subtractive\_gcd\_nonzero} to the
case in which one of its arguments is zero, since any $b\ne 0$ divides
the zero of the monoid:

{\tt
template<typename T>\\
\hspace*{0.6cm}requires(EuclideanMonoid(T))\\
T subtractive\_gcd(T a, T b)\\
\{\\
\hspace*{0.6cm}//$Precondition: a\ge 0 \land b\ge 0 \land 
\lnot(a=0 \land b=0)\\$
\hspace*{0.6cm}while (true) \{\\
\hspace*{1.0cm}if (b == T(0)) return a;\\
\hspace*{1.0cm}while (b <= a) a = a - b;\\
\hspace*{1.0cm}if (a == T(0)) return b;\\
\hspace*{1.0cm}while (a <= b) b = b - a;\\
\hspace*{0.6cm}\}\\
\}\\
}

Each of the inner {\tt while} statements in 
{\tt subtractive\_gcd} is equivalent to a call of
{\tt slow\_remainder}. By using our logarithmic remainder algorithm,
we speed up the case when $a$ and $b$ are very different in magnitude
while relying only on primitive sabtraction on type {\bf T}:

{\tt
template<typename T>\\
\hspace*{0.6cm}requires(EuclideanMonoid(T))\\
T fast\_subtractive\_gcd(T a, T b)\\
\{\\
\hspace*{0.6cm}//$Precondition: a\ge 0 \land b\ge 0 \land 
\lnot(a=0 \land b=0)\\$
\hspace*{0.6cm}while (true) \{\\
\hspace*{1.0cm}if (b == T(0)) return a;\\
\hspace*{1.0cm}a = remainder\_nonnegative(a, b);\\
\hspace*{1.0cm}if (a == T(0)) return b;\\
\hspace*{1.0cm}b = remainder\_nonnegative(b, a);\\
\hspace*{0.6cm}\}\\
\}\\
}

The concept of Euclidean monoid gives us an abstract setting for the
original Euclid algorithm, which was based on repeated subtraction.

\section{Generalizing gcd}

We can use {\tt fast\_subtractive\_gcd} with integers because they
constitute a Eucliden monoid. For integers, we could also use the
same algorithm with the built-in remainder instead of
{\tt remainder\_nonnegative}. Furthermore, the algorithm works for
certain non-Archimedean domains, provided that they possess a suitable
remainder function. For example, the standard long-division algorithm
easily extends from decimal integers to polynomials over reals. Using
such a remainder, we can compute the gcd of two polynomials.

Abstract algebra introduces the notion of a Euclidean ring (also known
as a Euclidean domain) to accommodate such uses of the Euclid
algorithm. However, the requirements of semiring suffice:

{\sl EuclideanMonoid}({\bf T}) $\overset{\triangle}{=}$\\
\hspace*{0.8cm}{\sl CommutativeSemiring}({\bf T})\\
\hspace*{0.4cm}$\land$ {\tt NormType} : 
{\sl EuclideanSemiring} $\rightarrow$ {\sl Integer}\\
\hspace*{0.4cm}$\land$ w : {\bf T} $\rightarrow$ {\tt NormType({\bf T})}\\
\hspace*{0.4cm}$\land$ ($\forall$ a $\in$ T)~w(a)$ \ge 0$\\
\hspace*{0.4cm}$\land$ ($\forall$ a $\in$ T)~w(a)$ = 0 \Leftrightarrow a=0$\\
\hspace*{0.4cm}$\land$ ($\forall$ a,b $\in$ T)~b$\ne 0 \Rightarrow$ 
w$(a\cdot b) \ge $w(a)\\
\hspace*{0.4cm}$\land$ {\tt remainder} : T$\times$ T $\rightarrow$ T\\
\hspace*{0.4cm}$\land$ {\tt quotient} : T$\times$ T $\rightarrow$ T\\
\hspace*{0.4cm}$\land$ ($\forall$ a,b $\in$ T)~b $\ne 0 \Rightarrow $
a = {\tt quotient}(a,b)$\cdot$b+{\tt remainder}(a,b)\\
\hspace*{0.4cm}$\land$ ($\forall$ a,b $\in$ T)~b $\ne 0 \Rightarrow $
w({\tt remainder}(a,b)) $<$ w(b)

w is called the {\sl Euclidean function}.

{\bf Lemma 5.19} In a Euclidean semiring,
$a\cdot b = 0 \Rightarrow a = 0 \lor b = 0$.

{\tt
template<typename T>\\
\hspace*{0.6cm}requires(EuclideanMonoid(T))\\
T gcd(T a, T b)\\
\{\\
\hspace*{0.6cm}//$Precondition: \lnot(a = 0 \land b = 0)$\\
\hspace*{0.6cm}while (true) \{\\
\hspace*{1.0cm}if (b == T(0)) return a;\\
\hspace*{1.0cm}a = remainder(a, b);\\
\hspace*{1.0cm}if (a == T(0)) return b;\\
\hspace*{1.0cm}b = remainder(b, a);\\
\hspace*{0.6cm}\}\\
\}\\
}

Observe that instead of using {\tt remainder\_nonnegative}, we use the 
{\tt remainder} function defined by the type. The fact that {\tt w}
decreases with every application of {\tt remainder} ensures
termination.

{\bf Lemma 5.20} {\tt gcd} terminates on a Euclidean semiring.

In a Euclidean semiring, {\tt quotient} returns an element of the
semiring. This precludes its use in the original setting of Euclid:
determining the common measure of any two commensurable
quantities. For example,
\[{\tt gcd}(\frac{1}{2},\frac{3}{4}) = \frac{1}{4}\]

We can unify the original setting and the modern setting with the
concept {\sl Euclidean semimodule}, which allows {\tt quotient} to
return a different type and takes the termination of {\tt gcd} as an
axiom: 

{\sl EuclideanSemimodule}({\bf T}) $\overset{\triangle}{=}$\\
\hspace*{0.8cm}{\sl Semimodule}({\bf T})\\
\hspace*{0.4cm}$\land$ {\tt remainder} : T$\times$ T $\rightarrow$ T\\
\hspace*{0.4cm}$\land$ {\tt quotient} : T$\times$ T $\rightarrow$ T\\
\hspace*{0.4cm}$\land$ ($\forall$ a,b $\in$ T)~b $\ne 0 \Rightarrow $
a = {\tt quotient}(a,b)$\cdot$b+{\tt remainder}(a,b)\\
\hspace*{0.4cm}$\land$ ($\forall$ a,b $\in$ T)($a\ne 0 \lor b\ne 0$)
$\Rightarrow$ gcd(a,b) terminates

where {\tt gcd} is defined as

{\tt
template<typename T, typename S>\\
\hspace*{0.6cm}requires(EuclideanSemimoudle(T,S))\\
T gcd(T a, T b)\\
\{\\
\hspace*{0.6cm}//$Precondition: \lnot(a = 0 \land b = 0)$\\
\hspace*{0.6cm}while (true) \{\\
\hspace*{1.0cm}if (b == T(0)) return a;\\
\hspace*{1.0cm}a = remainder(a, b);\\
\hspace*{1.0cm}if (a == T(0)) return b;\\
\hspace*{1.0cm}b = remainder(b, a);\\
\hspace*{0.6cm}\}\\
\}\\
}

Since every commutative semiring is a semimodule over itself, this
algorithm can be used even when {\tt quotient} returns the same type,
as with polynomials over reals.

\section{Stein gcd}

In 1961 Josef Stein discovered a new gcd algorithm for integers that
is frequently faster than Euclid's algorithm. His algorithm depends on
these two familiar properties:
\[\begin{array}{rcl}
{\tt gcd}(a,b) & = & {\tt gcd}(b,a)\\
{\tt gcd}(a,a) & = & a
\end{array}\]
together with these additional properties that for all $a>b>0$:
\[\begin{array}{rcl}
{\tt gcd}(2a,2b)     & = & 2{\tt gcd}(a,b)\\
{\tt gcd}(2a,2b+1)   & = & {\tt gcd}(a,2b+1)\\
{\tt gcd}(2a+1,2b)   & = & {\tt gcd}(2a+1,b)\\
{\tt gcd}(2a+1,2b+1) & = & {\tt gcd}(2b+1,a-b)
\end{array}\]

While it might appear that Stein gcd depends on the binary
representation of integers, the intuition that 2 is the smallest prime
integer allows generalizing it to other domains by using smallest
primes in these domains; for example, the monomial $x$ for polynomials
or $1+i$ for Gaussian integers. Stein gcd could be used in rings that
are not Euclidean.

\section{Quotient}

The derivation of fast quotient and remainder exactly parallels our
earlier derivation of fast remainder. We derive an expression for the
quotient $m$ and remainder $u$ from dividing $a$ by $b$ in terms of
the quotient $n$ and remainder $v$ from dividing $a$ by $2b$:
\[ a = n(2b)+v\]

Since the remainder $v$ must be less than the divisor $2b$, it follows
that 
\[u=\left\{
\begin{array}{lr}
v & {\tt if\ }v < b\\
v-b & {\tt if\ }v \ge b
\end{array}\right.\]

and

\[m=\left\{
\begin{array}{lr}
2n & {\tt if\ }v < b\\
2n+1 & {\tt if\ }v \ge b
\end{array}\right.\]

This leads to the following code:

{\tt
template<typename T>\\
\hspace*{0.6cm}requires(ArchimedeanMonoid(T))\\
pair<QuotientType(T), T>\\
quotient\_remainder\_nonnegative(T a, T b)\\
\{\\
\hspace*{0.6cm}//$Precondition: a \ge 0 \land b > 0$\\
\hspace*{0.6cm}typedef QuotientType(T) N;\\
\hspace*{0.6cm}if (a < b) return pair<N, T>(N(0), a);\\
\hspace*{0.6cm}if (a - b < b) return pair <N, T>(N(1), a - b);\\
\hspace*{0.6cm}pair<N, T> q = quotient\_remainder\_nonnegative(a, b + b);\\
\hspace*{0.6cm}N m = twice(q.m0);\\
\hspace*{0.6cm}q = q.m1;\\
\hspace*{0.6cm}if (a < b) return pair<N, T>(m, a);\\
\hspace*{0.6cm}else\hspace*{1.1cm} return 
pair<N, T>(successor(m), a - b);\\
\}\\
}

When ``halving'' is available, we obtain the following:

{\tt
template<typename T>\\
\hspace*{0.6cm}requires(HalvableMonoid(T))\\
pair<QuotientType(T), T>\\
quotient\_remainder\_nonnegative\_iterative(T a, T b)\\
\{\\
\hspace*{0.6cm}//$Precondition: a \ge 0 \land b > 0$\\
\hspace*{0.6cm}typedef QuotientType(T) N;\\
\hspace*{0.6cm}if (a < b) return pair<N, T>(N(0), a);\\
\hspace*{0.6cm}T c = largest\_doubling(a, b);\\
\hspace*{0.6cm}a = a - c;\\
\hspace*{0.6cm}N n(1);\\
\hspace*{0.6cm}while (c != b) \{\\
\hspace*{1.0cm}n = twice(n);\\
\hspace*{1.0cm}c = half(c);\\
\hspace*{1.0cm}if (c <= a) \{\\
\hspace*{1.4cm}a = a - c;\\
\hspace*{1.4cm}n = successor(n);\\
\hspace*{1.0cm}\}\\
\hspace*{0.6cm}\}\\
\hspace*{0.6cm}return pair<N, T>(n, a);\\
\}\\
}

\section{Quotient and Remainder for Negative Quantities}

The definition of quotient and remainder used by many computer
processors and programming languages handles negative quantities
incorrectly. An extension of our definitions for an Archimedean monoid
to an Archimedian group {\bf T} must satisfy these properties, where 
$b\ne 0$:
\[\begin{array}{l}
a = {\tt quotient}(a,b)\cdot b + {\tt remainder}(a,b)\\
\abs{{\tt remainder}(a,b)} < \abs{b}\\
{\tt remainder}(a+b,b)={\tt remainder}(a-b,b) =
{\tt remainder}(a,b)
\end{array}\]

The final property is equivalent to the classical mathematical
definition of congruence\footnote{``If two numbers $a$ and $b$ have
the same remainder $r$ relative to the same modulus $k$ they will be
called {\sl congruent} relative to the modulus $k$ (following
Gauss)''} While books on number theory usually assume $b>0$, we can
consistenly extend {\tt remainder} to $b<0$. These requirements are
not satisfied by implementations that truncate quotient toward zero,
thus violating our third requirement. In addition to violating the
third requirement, truncation is an inferior way of rounding because
it sends twice as many values to zero as to any other integer, thus
leading to a nonuniform distribution.

Given a remainder procedure {\tt rem} and a quotient-remainder
procedure {\tt quo\_rem} satisfying our three requirements for
non-negative inputs, we can write adapter procedures that give
correct results for positive or negative inputs. These adapter
procedures will work on an {\sl Archimedian group}:

{\sl ArchimedeanGroup}({\bf T}) $\overset{\triangle}{=}$\\
\hspace*{0.9cm}{\sl ArchimedeanMonoid}({\bf T})\\
\hspace*{0.6cm}$\land$~{\sl AdditiveGroup}({\bf T})\\

{\tt
template<typename Op>\\
\hspace*{0.6cm}requires(BinaryOperation(Op) \&\&\\
\hspace*{1.2cm}ArchimedeanGroup(Domain(Op)))\\
Domain(Op) remainder(Domain(op) a, Domain(Op) b, Op rem)\\
\{\\
\hspace*{0.6cm}//$Precondition: b\ne 0$\\
\hspace*{0.6cm}typedef Domain(Op) T;\\
\hspace*{0.6cm}T r;\\
\hspace*{0.6cm}if (a < T(0))\\
\hspace*{1.0cm}if (b < T(0)) \{\\
\hspace*{1.4cm}r = -rem(-a, -b);\\
\hspace*{1.0cm}\} else \{\\
\hspace*{1.4cm}r = rem(-a, b);\\
\hspace*{1.4cm}if (r != T(0)) r = b - r;\\
\hspace*{1.0cm}\}\\
\hspace*{0.6cm}else\\
\hspace*{1.0cm}if (b < T(0)) \{\\
\hspace*{1.4cm}if r = rem(a, -b);\\
\hspace*{1.4cm}if (r != T(0)) r = b + r;\\
\hspace*{1.0cm}\} else \{\\
\hspace*{1.4cm}r = rem(a, b);\\
\hspace*{1.0cm}\}\\
\hspace*{0.6cm}return r;\\
\}\\
}

{\tt
template<typename F>\\
\hspace*{0.6cm}requires(HomogeneousFunction(F) \&\& Arity(F) == 2 \&\&\\
\hspace*{1.2cm}ArchimedeanGroup(Domain(F))) \&\& \\
\hspace*{1.2cm}Codomain(F) == pair<QuotientType(Domain(F)),\\
\hspace*{5.2cm}Domain(F)>)\\
pair<QuotientType(Domain(F)), Domain(F)>\\
quotient\_remainder(Domain(F) a, Domain(F) b, F quo\_rem)\\
\{\\
\hspace*{0.6cm}//$Precondition: b\ne 0$\\
\hspace*{0.6cm}typedef Domain(F) T;\\
\hspace*{0.6cm}pair<QuotientType(T), T> q\_r;\\
\hspace*{0.6cm}if (a < T(0)) \{\\
\hspace*{1.0cm}if (b < T(0)) \{\\
\hspace*{1.4cm}q\_r = quo\_rem(-a, -b);\\
\hspace*{1.4cm}q\_r.m1 = -q\_r.m1;\\
\hspace*{1.0cm}\} else \{\\
\hspace*{1.4cm}q\_r = quo\_rem(-a, b);\\
\hspace*{1.4cm}if (q\_r.m1 != T(0)) \{\\
\hspace*{1.8cm}q\_r.m1 = b - q\_r.m1;\\
\hspace*{1.8cm}q\_r.m0 = successor(q\_r.m0);\\
\hspace*{1.4cm}\}\\
\hspace*{1.4cm}q\_r.m0 = -q\_r.m0;\\
\hspace*{1.4cm}\}\\
\hspace*{0.6cm}\} else \{\\
\hspace*{1.4cm}if (b < T(0)) \{\\
\hspace*{1.8cm}q\_r = quo\_rem( a, -b);\\
\hspace*{1.8cm}if (q\_r.m1 != T(0)) \{\\
\hspace*{2.2cm}q\_r.m1 = b + q\_r.m1;\\
\hspace*{2.2cm}q\_r.m0 = successor(q\_r.m0);\\
\hspace*{1.8cm}\}\\
\hspace*{1.8cm}q\_r.m0 = -q\_r.m0;\\
\hspace*{1.0cm}\}\\
\hspace*{1.0cm}else\\
\hspace*{1.8cm}q\_r = quo\_rem( a, b);\\
\hspace*{0.6cm}\}\\
\hspace*{0.6cm}return q\_r;\\
\}\\
}

{\bf Lemma 5.21} {\tt remainder} and {\tt quotient\_remainder} satisfy
our requirements when their functional parameters satisfy the
requirements for positive arguments.

\section{Concepts and Their Models}

We have been using integer types without formally defining the
concept. Building on the ordered algebraic structures defined earlier
in this chapter, we can formalize our treatment of integers. First, we
define {\sl discrete Archimedean semiring}:

{\sl DiscreteArchimedeanSemiring}({\bf T}) $\overset{\triangle}{=}$\\
\hspace*{0.9cm}{\sl CommutativeSemiring}({\bf T})\\
\hspace*{0.6cm}$\land$~{\sl ArchimedeanMonoid}({\bf T})\\
\hspace*{0.6cm}$\land$~($\forall a,b,c \in$ {\bf T})~$a<b \land 0<c
\Rightarrow a\cdot c < b\cdot c$\\
\hspace*{0.6cm}$\land$~$\lnot$($\exists a \in$ {\bf T})~$0<a<1$

{\sl Discreteness} refers to the last property: There is no element
between 0 and 1.

A discrete Archimedean semiring might have negative elements. The
related concept that does not have negative elements is

{\sl NonnegativeDiscreteArchimedeanSemiring}({\bf T}) $\overset{\triangle}{=}$\\
\hspace*{0.9cm}{\sl DiscreteArchimedanSemiring}({\bf T})\\
\hspace*{0.6cm}$\land (\forall a \in$ {\bf T})~$0\le a$

A discrete Archimedean semiring lacks additive inverses; the related
concept with additive inverses is

{\sl DiscreteArchimedeanRing}({\bf T}) $\overset{\triangle}{=}$\\
\hspace*{0.9cm}{\sl DiscreteArchimedanSemiring}({\bf T})\\
\hspace*{0.6cm}$\land$ {\sl AdditiveGroup}({\bf T})

Two types {\bf T} and {\bf T}$^\prime$ are {\sl isomorphic} if it is
possible to write conversion functions from {\bf T} to 
{\bf T}$^\prime$ and from {\bf T}$^\prime$ to {\bf T} that preserve
the procedures and their axioms.

A concept is {\sl univalent} if any types satisfying it are
isomorphic. The concept {\sl NonnegativeDiscreteArchimedeanSemiring}
is univalent; types satisfying it are isomorphic to $\mathbb{N}$, the
natural numbers.\footnote{We follow Peano and include 0 in the natural
numbers} {\sl DiscreteArchimedeanRing} is univalent; types satisfying
it are isomorphic to $\mathbb{Z}$, the integers. As we have seen here,
adding axioms reduces the number of models of a concept, so that one
quickly reaches the point of univalency.

The chapter proceeds deductively, from more general to more specific
concepts, by adding more operations and axioms. The deductive approach
statically presents a taxonomy of concepts and affiliated theorems and
algorithms. The actual process of discovery proceeds inductively,
starting with concrete models, such as integers or reals, and then
removing operations and axioms to find the weakest concept to which
interesting algorithms apply.

When we define a concept, the independence and consistency of its
axioms must be verified, and its usefulness must be demonstrated.

A proposition is {\sl independent} from a set of axioms if there is a
model in which all the axioms are true, but the proposition is
false. For example, associativity and commutativity are independent:
String concatenation is associative but not commutative, while the
average of two values $(\frac{x+y}{2})$ is commutative but not
associative. A proposition is {\sl dependent} or {\sl provable} from a
set of axioms if it can be dervied from them.

A concept is {\sl consistent} if it has a model. Continuing our
example, addition of natural numbers is associative and commutative. A
concept is {\sl inconsistent} if both a proposition and its negation
can be derived from its axioms. In other words, to demonstrate
consistency, we construct a model; to demonstrate inconsistency, we
derive a contradiction.

A concept is {\sl useful} if there are useful algorithms for which
this is the most abstract setting. For example, parallel out-of-order
reduction applies to any associative, commutative operation.

\section{Computer Integer Types}

Computer instruction sets typically provide partial representations of
natural numbers and integers. For example, a 
{\sl bounded unsigned binary integer type} $U_n$, where 
$n=8,16,32,64,\ldots$ is an unsigned integer type capable of
representing a value in the interval $[0,2^n)$; a 
{\sl bounded signed binary integer type}, $S_n$, where
$n=8,16,32,64,\ldots$, is a signed integer type capable of
representing a value in the interval $[-2^{n-1},2^{n-1})$. Although
these types are bounded, typical computer instructions provide total
operations on them because the results are encoded as a tuple of
bounded values.

Instructions on bounded unsigned types with signatures like these
usually exist:
\[\begin{array}{rcl}
{\tt sum\_extended} & : & U_n\times U_n \times U_1
\rightarrow U_1\times U_n\\
{\tt difference\_extended} & : & U_n\times U_n \times U_1
\rightarrow U_1\times U_n\\
{\tt product\_extended} & : & U_n\times U_n 
\rightarrow U_{2n}\\
{\tt quotient\_remainder\_extended} & : & U_n\times U_n 
\rightarrow U_n\times U_n
\end{array}\]

Observer that $U_{2n}$ can be represented as $U_n\times U_n$ (a pair
of $U_n$). Programming languages that provide full access to these
hardware operations make it possible to write efficient and abstract
software components involving integer types.

\section{Conclusions}

We can combine algorithms and mathematica strutures into a seamless
whole by describing algorithms in abstract terms and adjusting
theories to fit algorithmic requirements. The mathematics and
algorithms in this chapter are abstract restatements of results that
are more than two thousand years old.

\chapter{Why Computer Algebra Systems Can't Solve Simple Equations}

This is a paper by Richard Fateman \cite{Fate96a}.

\section{Introduction}

Among the basic equations we might wish a computer to solve
symbolically is the inverse of the power function, solving 
$y=z^w$ for $z$. While many special cases, easily solved, abound, the
general question is fraught with implications: if this is so hard, how
can we expect success in other ventures? Having solved this, we can
naturally use it in a ``composition' of solution methods for
expressions of the form $y=f(z)^w$.

Can't we already do this? Is it not the case that the solution of
$y=z^{a+bi}$ is trivially $z=y^{1/(a+bi)}$?

Not so. If this were the case, then a plot of the function
$t(y) := y-(y^{1/(1+i)})^{1+i}$ would be indistinguishable from
$t(y)\equiv 0$. For many values, $t(y)$ is (allowing for round-off
error), zero. But if your computer system correctly computes with
values in the complex plane, then, (to pick two complex points from a
region described later), $t(-10000+4000i)$ is not zero, but about
$-9981+3993i$ and $t(-0.01+0.002i)$ is about $5.34-1.06i$. These
strange numbers are not the consequence of round-off error or some
other numerical phenomena. The alleged solution is just not
mathematically correct.

\section{How to solve $y=z^w$ for $z$}

We assume that $y$ and $w$ are complex-valued variables in general,
and the solution sought for $z$ is permitted to be complex as well. If
we know specific values for $y$ and $w$, we can simplify the question
and answer it. For example, $9=z^2$ has the solution set
$\{-3,3\}$. The equation $-9=z^2$ has the solution set
$\{-3i,3i\}$. The equation $3=z^{1/2}$ has the solution set $\{9\}$. 

But $-3=z^{1/2}$ has no solution for real or complex numbers, at least
given the conventional meaning for this power function: if 
$z:= r\cdot exp(i\theta)$ then $z^{1/2}$ must be 
$\sqrt{r}\cdot exp(i\theta/2)$ where the positive $\sqrt{r}$ of the
positive value $r$ is taken. There is no value for $r$ and for 
$\pi < \theta \le \pi/2$ to satisfy this equation. If you feel like
arguing this point, read the footnote\footnote{Even if you wish to
specify that $()^{1/2}$ means a set of two values, then the equation
still has no solution. If you think that a solution is $z=9$, observe
that $\{3\}\ne \{3,-3\}$. If you uniformly choose (somewhat perversly)
the negative square root, then $3=z^{1/2}$ has no solution. It appears
that a solution would entail being able to magically distinguish the
number 9 whose square-root is 3 from the number 9 whose square-root is
$-3$.}

To some extent we can try to limit the scope of the answer even though
we may not have specific values for $w$ and $y$. One way of furthering
this exploration is to inquire about the real and imaginary parts (or
perhaps the argument and magnitude) of $w$ and $y$. We've already seen
situations above where the solution set has zero, one, or more
distinct solutions, giving us some hint as to what to expect.

\section{A systemic attempt}

There are any number of ways of approaching this problem from a naive
complex-variables direction. We've tried quite a few, and believe this
is about as simple as it gets.

Let us define $y:=s\cdot exp(i\rho)$, $z:=r\cdot exp(i\theta)$ and
$w:=a+bi$. That's right, even though we have three complex variables,
we don't use the same representation for $w$ as for $y$ and $z$. This
is a matter of convenience; any alternative representation can be
changed to this.

Note: $s$ and $r$ are nonnegative real, $a$ and $b$ are real, $\rho$
and $\theta$ are in the half-open real interval $(-\pi,\pi]$. These
are all conventional restrictions to make the representations of
complex values canonical, and do not limit the ``values'' they can
assume\footnote{If $z=0$ we will say the $r=0$ and $\theta=0$ for
definiteness.} 

Then
\[z^w = exp((a+bi)\cdot (\log r+i\theta))\]
\[= exp(a \log r - b\theta + i(a\theta + b\log r))\]
So

(1)\hspace{4.0cm}$z^w=exp(a\log r- b\theta)\cdot exp(i\phi)$

where $\phi=b\log r + a\theta$. We do not assume that $\phi$ is in 
$(-\pi,\pi]$. Note however, that the first factor in equation (1) is
necessarily real because $r$ is non-negative and $a$, $b$ and $\theta$
are real.

Our object is for $z^w$, so expressed, to be equal to $y$:

(2)\hspace{5.0cm}$y=s\cdot exp(i\rho)$.

The magnitude and the argument (modulo $2\pi$) of the two expressions
must be equal, and so we are provided with 2 equations:

(3)\hspace{5.0cm}$s=exp(a\log r - b\theta)$

and

(4)\hspace{5.0cm}$\rho = b\log r + a\theta + 2n\pi$

(for some integer value $n$) which must be solved simultaneously for
$r$ and $\theta$. Solving for $r$, a real value, in (3) yields:

(5)\hspace{5.0cm}$r=exp((\log(s) + b\theta)/a)$.

Equation (5) should alert us to a possible problem at
$a=0$. Proceeding nevertheless, we substitute for $\log r$ (note, $r$
is non-negative) in (4) and get

\hspace{4.0cm}$\rho=b/a\cdot(\log s + b\theta)+a\theta+2n\pi$

The solutions to the latter are

(6)\hspace{4.0cm}$\theta=(-b\log s + a\rho + 2an\pi)/(a^2+b^2)$.

Now what remains is for $n$ to be chosen approapriately. Given a set
of values for $a$, $b$, $\rho$, and $s$ in equation (6) we can fid
some set of integer values for $n$, namely when
\[\frac{-\pi(a^2+b^2)+b\log s - a\rho}{2a\pi} < n \le
\frac{\pi(a^2+b^2)+b\log s - a\rho}{2a\pi}\]
which then imposes the condition the $\theta$ is in $(-\pi,pi]$. We
then use those values to get corresponding values for $r$ from
equation (5).

It would be nice if this were the end of it. Unfortunately, it is not
so simple.

\section{Branch Cuts}

The solutions of the previous section fall apart in various ways
because of singularities and the necessity of defining a branch cut in
the logarithm function. The branch cut is normally along the negative
real axis, and the values along the cut are pasted to the ``top''
part. In more detail, let us consider the situation.

1. For $a=b=0$, we are solving at a singular point, and the equation
degenerates to $y=z^0$. The only solution is when $y=1$, and then $z$
is arbitrary.

2. If $b=0$, $a\ne 0$ (the real exponent case), then the solution
exists for the simpler equations
\[r=exp((\log s)/a)\]
and
\[\theta=\rho/a\]

Since $-\pi < \theta \le \pi$, a solution can exist only when

(7)\hspace{5.0cm}$-a\pi < \rho \le a\pi$.

Thus there is no solution for $y=z^a$ unless $\rho$ (which is 
$\arg y$) abides by condition (7). Two examples: if $a=1/2$ then $y$
must be in the right half plane with 
$\rho \in (\pi/2,\pi/2]$. If $a=1/3$, then $y$ must be in a wedge in
the half-open interval with $\rho\in(\pi/3,\pi/3]$.

3. If $a=0$ (but $b\ne 0$) we must avoid the division in equation (5)
and go back to equations (3) and (4) giving us
\[\theta = -\log(s)/b\]
\[r=exp(\rho/b)\]

The restrictions: since $-\pi < \theta \le \pi$,
\[-b\pi < -\log(s) \le b\pi\]
or if $b$ is negative,
\[b\pi \le -\log(s) < -b\pi\]

Since exp is monotonic, we impose one of

(8)\hspace{5.0cm}$exp(-b\pi)\le s < exp(b\pi)$

or

($8^\prime$)\hspace{5.0cm}$exp(b\pi) < s \le exp(-b\pi)$.

Thus there is no solution for $y=z^{ib}$ unless $s$ (which is
$\abs{y}$), abides by condition (8) or ($8^\prime$). For example, 
$exp(\pi)=z^i$ has no solution, but $exp(-\pi)=z^i$ has
one. Geometrically, {\sl the acceptable values of y appear in an
annulus: the region between two concentric circles about the origin in
the complex plane of radii} $p=exp(-\abs{b}\pi)$ {\sl and}
$1/\rho$. We would draw a figure, except that this is not hard to
visualize. 

4. If $a\ne 0$ and $b\ne 0$, cnosider, once again from (6) that
$-\pi < \theta \le \pi$ and therefore

(9)\hspace{2cm}$-\pi (a^2+b^2) < -b\log s + a\rho + 2an\pi 
\le \pi (a^2+b^2)$

Consider the border curve of this region as determined by this
equation:
\[-b\log s + a\rho = \pm \pi (a^2+b^2-2an\pi)\].

The curve, in polar form is

(10)\hspace{5.0cm}$s=K exp((a/b)\cdot \rho)$

for the constants

\[K=exp(\pm\pi (a^2+b^2-2an)/b)\]

Equation (10) defines a spiral starting on the real axis 
$(\rho = -\pi)$ and ending on the real axis $(\rho=\pi)$ after one
revolution. The interior of the acceptable region includes one of
these spirals (the one with the $-\pi$) but not the other. It is as
though the annulus of the previous case were cut along the negative
real axis and distorted. The two curves are joined in two places by
segments of the negative real axis.

\section{An example of the complex case}

If we look at a particular instance of our equation, namely
\[y=z^{i+1}\]
we can easily but rather cavalierly solve it as
\[z=y^{1/(i+1)}=y^{1/2-i/2}\]

For this case, the two spirals are govened by
$K=exp(\pm\pi(2-2n))$. For $n=0$, the vaues of $K$ are
$\{535.492, 0.00186744\}$. The outer logarithmic spiral hits the
negative real axis at about $-12.392$ and the inner spiral hits the
negative real axis at about $-0.0432$. The acceptabe values for $y$
are between the spirals and the line connecting them on the negative
real axis (this line is joined upward to the figure).

Is $n=0$ acceptable? By equation (6), using $a=1$ and $b=1$, we
require $\theta = (-\log s + \rho)/2$ to be in $(-\pi,\pi]$ when $\rho$
is in $(-\pi,\pi]$. This is no problem since for every value of $\rho$
there is a satisfactorily corresponding $s$. There may be other $n$
possible, in which case there is a pair of spirals such as illustrated
below. The solution exists between them. We give two figures to show
the general shape; the first, two spirals, and then a close-up of them
near the origin.

\section{Conclusion}

In the case of solving equations, one would like to have as much of
the following information as possible: a description of the inverse
function's domain; the number of distinct solutions; a formula to
numerically evaluate each inverse.

If particular cases simplify, and the result can be expressed as lists
of equations, this seems most plausible to be useful.

If one cannot make determinations as to whether $w=0$, or if its real
part is 0, or whether various conditions hold on $y$, then some
alternatives must be considered.

\begin{enumerate}
\item We could ask the user, or inquire of some assumption ``knowledge
base'' in the system to try to determine appropriate information. Some
CAS designers (e.g. Maple) are opposed to halting the computation to
ask the user. Others (e.g. Macsyma) are not so shy. A reference to a
set of ``assumptions'' is possible in either of these systems.
\item We could give a symbolic {\tt If [...]} answer along the lines
of the construction above. This is rarely useful unless (a) it
collapses substantially as a consequence of arithmetic and/or logical
simplification, and (b) the CAS is able to continue computation with
``conditional'' expressions, including, for example, adding and
multiplyig them, inverting them (!), etc. This is a challenge, but in
some sense inevitable if the answer to the questions of domain are not
and cannot yet be ``known''. Although we've written this out, it does
not seem to warrant repetition.
\item We could assume that if one cannot prove (say) $a=0$, then it is
definitely the case that $a\ne 0$. Although some CAS use this ``closed
world assumption'' (that all true things are known, and anything that
is not known or provable is false), the consequences are potentially
dreadful.
\item We could defer the execution of the program that evaluates the
test conditions until the time that it is in fact needed to proceed
with a computation. At that time our computer program would insist
that any information that is indeed needed is provided. This so-called
lazy evaluation is rarly used in computer algebra systems with the
exception of some series computations in which terms are computed
``as needed''.
\item One could refuse to solve the equation as given.
\end{enumerate}

We recommend some version of 2, as being almost inevitable in cases
where ``symbolic'' parameters must be used, although in some
circumstances, approach 4 is workable. Most existing computer algebra
systems seem to use some version of 1 or 3.

{\center{\includegraphics{ps/v101Fate1.eps}}}
{\center{\includegraphics{ps/v101Fate2.eps}}}

\chapter{Interval Arithmetic}
Lambov \cite{Lamb06} defines a set of useful formulas for 
computing intervals using the IEEE-754 floating-point standard.

The first thing to note is that IEEE floating point defaults to 
{\bf round-to-nearest}. However, Lambov sets the rounding mode
to {\bf round to $-\infty$}. Computing lower bounds directly uses
the hardware floating point operations but computing upper bounds
he uses the identity
\[\Delta(x) = -\nabla(-x)\]
so that the upper bound of the pair of bounds is always negated.
That is,
\[ x = \left[\underline{x},\overline{x}\right] = 
\left<\underline{x},-\overline{x}\right> \]

Given that convention
\begin{itemize}
\item the sum of $x$ and $y$ is evaluated by
\[\left<\nabla(\underline{x} + \underline{y}),
  -\nabla(-\overline{x} - \overline{y})\right> \]
\item changing the sign of an interval $x$ is achieved by swapping
the two bounds, that is $\left<-\overline{x},\underline{x}\right>$
\item joining two intervals (that is, finding an interval containing
all numbers in both, or finding the minimum of the lower bounds
and the maximum of the higher bounds) is performed as
\[\left<min(\underline{x},\underline{y}),
  -min((-\overline{x}),(-\overline{y}))\right>\]
\end{itemize}

Lambov defines operations which, under the given rounding condition,
give the tightest bounds.

\section{Addition}
\[ x + y = \left[\underline{x}+\underline{y},\overline{x}+\overline{y}\right]
\subseteq \left<\nabla(\underline{x}+\underline{y})
-\nabla((-\overline{x})+(-\overline{y}))\right>\]
The negated sign of the higher bound ensures the proper direction
of the rounding.

\section{Sign Change}
\[ -x = \left[-\overline{x},-\underline{x}\right] =
\left<-\overline{x},\underline{x}\right> \]
This is a single swap of the two values. No rounding is performed.

\section{Subtraction}
\[ x-y = \left[\underline{x}-\overline{y},\overline{x}-\underline{y}\right]
\subseteq \left<\nabla(\underline{x}+(-\overline{y})),
-\nabla((-\overline{x})+\underline{y})\right>\]
Subtraction is implemented as $x+(-y)$.

\section{Multiplication}
\[ xy = \left[min(\underline{x}\underline{y},
                  \underline{x}\overline{y},
                  \overline{x}\underline{y},
                  \overline{x}\overline{y}),
              max(\underline{x}\underline{y},
                  \underline{x}\overline{y},
                  \overline{x}\underline{y},
                  \overline{x}\overline{y})\right]
\]
The rounding steps are part of the operation so all 8 multiplications
are required. Lambov notes that since
\[\Delta(\nabla(r)+\epsilon) \ge \Delta(r) \]
for $\epsilon$ being the smallest representable positive number, one
can do with 4 multiplications at the expense of some accuracy.

In Lambov's case he makes the observation that
\[
xy=\left\{
\begin{array}{l}
\left[min(\underline{x}\underline{y},\overline{x}\underline{y}),
      max(\overline{x}\overline{y},\underline{x}\overline{y})\right],
      {\rm\ if\ }0 \le \underline{x} \le \overline{x}\\
\left[min(\underline{x}\overline{y},\overline{x}\underline{y}),
      max(\overline{x}\overline{y},\underline{x}\underline{y})\right],
      {\rm\ if\ }\underline{x} < 0 \le \overline{x}\\
\left[min(\underline{x}\overline{y},\overline{x}\overline{y}),
      max(\overline{x}\underline{y},\underline{x}\underline{y})\right],
      {\rm\ if\ }\underline{x} \le \overline{x} < 0
\end{array}
\right.
\]
from which he derives the formula actually used
\[xy \subseteq \left<min(\nabla(a\underline{x}),\nabla(b(-\overline{x}))),
-min(\nabla(c(-\overline{x})),\nabla(d\underline{x}))\right>
\]
where 
\[
\begin{array}{rcl}
a & = & \left\{ 
\begin{array}{rl}
\underline{y} & {\rm if\ } 0 \le \underline{x}\\
-(-\overline{y}) & {\rm otherwise}
\end{array}\right.\\
b & = & \left\{ 
\begin{array}{rl}
-\underline{y} & {\rm if\ } (-\overline{x}) \le 0\\
(-\overline{y}) & {\rm otherwise}
\end{array}\right.\\
c & = & \left\{
\begin{array}{rl}
-(-\overline{y}) & {\rm if\ } (-\overline{x}) \le 0\\
\underline{y} & {\rm otherwise}
\end{array}\right.\\
d & = & \left\{
\begin{array}{rl}
(-\overline{y}) & {\rm if\ } 0 \le \underline{x}\\
-\underline{y} & {\rm otherwise}
\end{array}\right.
\end{array}
\]
which computes the rounded results of the original multiplication
formula but achieves better performance.

\section{Multiplication by a positive number}
If one of the numbers is known to be positive (e.g. a constant) then 
\[ {\rm if\ }x > 0 {\rm\ then\ } xy \equiv
\left[min(\underline{x}\underline{y},\overline{x}\underline{y}),
       max(\overline{x}\overline{y},\underline{x}\overline{y})
\right]
\]
This formula is faster than the general multiplication formula.

\section{Multiplication of Two Positive Numbers}

If both multiples are positive simply change the sign of the 
higher bound on one of the arguments prior to multiplication.
If one of the numbers is a constant this can be arranged to
skip the sign change.

\section{Division}
Division is an expensive operation.
\[\frac{x}{y} =
\left[
min\left(\frac{\underline{x}}{\underline{y}},
         \frac{\underline{y}}{\overline{y}},
         \frac{\overline{x}}{\underline{y}},
         \frac{\overline{x}}{\overline{y}}\right),
max\left(\frac{\underline{x}}{\underline{y}},
         \frac{\underline{x}}{\overline{y}},
         \frac{\overline{x}}{\underline{y}},
         \frac{\overline{x}}{\overline{y}}\right)
\right]\]
which is undefined if $0 \in y$. To speed up the computation Lambov
uses the identity
\[ \frac{x}{y} = x\frac{1}{y} \]

Lambov does a similar analysis to improve the overall efficiency.
\[\frac{x}{y} = \left\{
\begin{array}{rl}
\left[min(\frac{\underline{x}}{\underline{y}},
          \frac{\underline{x}}{\overline{y}}),
      max(\frac{\overline{x}}{\underline{y}},
          \frac{\overline{x}}{\overline{y}})\right], &
{\rm if\ }0 < \underline{y} \le \overline{y}\\
{\rm exception} & 
{\rm if\ }\underline{y} \le 0 \le \overline{y}\\
\left[min(\frac{\overline{x}}{\underline{y}},
          \frac{\overline{x}}{\overline{y}}),
      max(\frac{\underline{x}}{\underline{y}},
          \frac{\underline{x}}{\overline{y}})\right], &
{\rm if\ }\underline{y} \le \overline{y} \le 0
\end{array}
\right.
\]

The formula he uses is
\[\frac{x}{y} \subseteq
\left<min\left(\nabla\left(\frac{a}{\underline{y}}\right),
               \nabla\left(\frac{-a}{(-\overline{y})}\right)\right),
     -min\left(\nabla\left(\frac{-b}{(-\overline{y})}\right),
               \nabla\left(\frac{b}{\underline{y}}\right)\right)
\right>
\]
where
\[
\begin{array}{rcl}
a & = & \left\{
\begin{array}{rl}
\underline{x} & {\rm if\ }(-\overline{y}) \le 0\\
-(-\overline{x}) & {\rm otherwise}
\end{array}\right.\\
b & = & \left\{
\begin{array}{rl}
(-\overline{x}) & {\rm if\ }0 \le \underline{y}\\
-\underline{x} & {\rm otherwise}
\end{array}\right.
\end{array}
\]

\section{Reciprocal}
\[\frac{1}{x} = \left[\frac{1}{\overline{x}},\frac{1}{\underline{x}}\right]
\subseteq 
\left<\nabla\left(\frac{-1}{(-\overline{x})}\right),
      \nabla\left(\frac{-1}{\underline{x}}\right)
\right>
\]
which is undefined if $0 \in x$. Lambov implements this by checking for
zero, followed by division of $-1$ by the argument and swapping the
two components.   

\section{Absolute Value}

\[\abs{x} = 
\left[max(\underline{x},-\overline{x},0),
      max(-\underline{x},\overline{x})\right] =
\left<max(0,\underline{x},(-\overline{x})),
     -min(\underline{x},(-\overline{x}))\right>
\]

\section{Square}
\[x^2 = \abs{x}\abs{x}\]
using multiplication by positive numbers, mentioned above.

\section{Square Root}

\[\sqrt{x} = \left[\sqrt{\underline{x}},\sqrt{\overline{x}}\right]\]
which is defined if $0 \le \underline{x}$

Lambov notes that this formula has a rounding issue. He notes that
since
\[\Delta(r) \le -\nabla(-\epsilon - \nabla(r))\]
he uses the formula
\[\sqrt{x} \subseteq
\left\{
\begin{array}{l}
\left<\nabla\left(\sqrt{\underline{x}}\right),
     -\nabla\left(\sqrt{-(-\overline{x})}\right)\right>,
{\rm\ if\ }\nabla\left(\nabla\left(\sqrt{-(-\overline{x})}\right)\right)^2
= -(-\overline{x})\\
\left<\nabla\left(\sqrt{\underline{x}}\right),
      \nabla\left(\nabla\left(-\epsilon-\sqrt{-(-\overline{x})}\right)
      \right)\right>,
{\rm\ otherwise}
\end{array}
\right.
\]
where $\epsilon$ is the smallest representable positive number.

The first branch of this formula is only satisfied if the result of
$\sqrt{-(-\overline{x})}$ is exactly representable, in which case
\[\nabla\left(\sqrt{-(-\overline{x})}\right) =
  \nabla\left(\sqrt{-(-\overline{x})}\right)
\]
otherwise the second branch of the formula adjusts the high bound
to the next representable number. If tight bounds are not required
the second branch is always sufficient.

If the argument is entirely negative, the implementation will raise
an exception. If it contains a negative part, the implementation will
crop it to only its non-negative part to allow that computations
such as $\sqrt{0}$ ca be carried out in exact real arithmetic.

\chapter{Integration}

An {\sl elementary function}\cite{Bro98b}
\index{elementary function}
of a variable $x$ is a function that can
be obtained from the rational functions in $x$ by repeatedly adjoining
a finite number of nested logarithms, exponentials, and algebraic
numbers or functions. Since $\sqrt{-1}$ is elementary, the
trigonometric functions and their inverses are also elementary (when
they are rewritten using complex exponentials and logarithms) as well
as all the ``usual'' functions of calculus. For example,

\begin{equation}\label{Int1}
\sin(x+\tan(x^3-\sqrt{x^3-x+1}))
\end{equation}
is elementary when rewritten as
\[
\frac{\sqrt{-1}}{2}(e^{t-x\sqrt{-1}}-e^{x\sqrt{-1}-t})
{\rm\ where\ }
t=\frac{1-e^{2\sqrt{-1}(x^3-\sqrt{x^3-x+1})}}
{1+e^{2\sqrt{-1}(x^3-\sqrt{x^3-x+1})}}
\]
This tutorial describes recent algorithmic solutions to the {\sl
problem of integration in finite terms}: 
\index{integration in finite terms}
to decide in a finite number
of steps whether a given elementary funcction has an elementary
indefinite integral, and to compute it explicitly if it exists. While
this problem was studied extensively by Abel and Liouville during the
last century, the difficulties posed by algebraic functions caused
Hardy (1916) to state that ``there is reason to suppose that no such
method can be given''. This conjecture was eventually disproved by
Risch (1970), who described an algorithm for this problem in a series
of reports \cite{Ostr1845,Risc68,Risc69a,Risc69b,Risc70}. 
In the past 30 years, this procedure
has been repeatedly improved, extended and refined, yielding practical
algorithms that are now becoming standard and are implemented in most
of the major computer algebra systems. In this tutorial, we outline
the above algorithms for various classes of elementary functions,
starting with rational functions and progressively increasing the
class of functions up to general elementary functions. Proofs of
correctness of the algorithms presented here can be found in several
of the references, and are generally too long and too detailed to be
described in this tutorial.

{\bf Notations}: we write $x$ for the variable of integration, and $\prime$
for the derivation $d/dx$. $\mathbb{Z}$,$\mathbb{Q}$,$\mathbb{R}$,and
$\mathbb{C}$ denote respectively the integers, rational, real and
complex numbers. All fields are commutative and, except when mentioned
explicitly otherwise, have characteristic 0. If $K$ is a field, then
$\overline{K}$ denotes its algebraic closure. For a polynomial $p$, 
pp($p$) denotes the primitive
part of $p$, {\sl i. e.} $p$ divided by the gcd of its coefficients.
\section{Rational Functions}
By a {\sl rational function}, we mean a quotient of polynomials in the
integration variable $x$. This means that other functions can appear
in the integrand, provided they do not involve $x$, hence that the
coefficients of our polynomials in $x$ lie in an arbitrary field $K$
satisfying: $\forall{a} \in K,\ a^{\prime}=0$.

\subsection{The full partial-fraction algorithm}
This method, which dates back to Newton, Leibniz, and Bernoulli,
should not be used in practice, yet it remains the method found in
most calculus tests and is often taught. Its major drawback is the
factorization of the denominator of the integrand over the real or
complex numbers. We outline it because it provides the theoretical
foundations for all the subsequent algorithms. Let 
$f \in \mathbb{R}(x)$ be our integrand, and write 
$f=P+A/D$ where $P, A, D \in \mathbb{R}[x]$, gcd$(A,D)=1$, and
deg$(A) < $deg$(D)$. Let
\[
D=c\prod_{i=1}^n(x-a_i)^{e_i}\prod_{j=1}^m(x^2+b_jx+c_j)^{f_j}
\]
be the irreducible factorization of $D$ over $\mathbb{R}$, where $c$,
the $a_i$'s, $b_j$'s and $c_j$'s are in $\mathbb{R}$ and the $e_i$'s
and $f_j$'s are positive integers. Computing the partial fraction
decomposition of $f$, we get
\[
f=P+\sum_{i=1}^n\sum_{k=1}^{e_i}\frac{A_{ik}}{(x-a_i)^k}
+\sum_{j=1}^m\sum_{k=1}^{f_i}\frac{B_{jk}x+C_{jk}}{(x^2+b_jx+c_j)^k}
\]
where the $A_{ik}$'s, $B_{jk}$'s, and $C_{jk}$'s are in
$\mathbb{R}$. Hence,
\[
\int{f}=\int{P}+\sum_{i=1}^n\sum_{k=1}^{e_i}\int{\frac{A_{ik}}{(x-a_i)^k}}
+\sum_{j=1}^m\sum_{k=1}^{f_i}\int{\frac{B_{jk}x+C_{jk}}{(x^2+b_jx+c_j)^k}}
\]
Computing $\int{P}$ poses no problem (it will for any other class of
functions), and for the other terms we have
\begin{equation}\label{Int2}
\int{\frac{A_{ik}}{(x-a_i)^k}}=\left\{
\begin{array}{lc}
A_{ik}(x-a_i)^{1-k}/(1-k)&{\rm if\ } k > 1\\
A_{i1}\log(x-a_i)&{\rm if\ } k = 1\\
\end{array}
\right.
\end{equation}
and, noting that $b_j^2-4c_j < 0$ since $x^2+b_jx+c_j$ is irreducible
in $\mathbb{R}$[x].
\[
\int\frac{B_{j1}x+C_{j1}}{(x^2+b_jx+c_j)}=
\frac{B_{j1}}{2}\log(x^2+b_jx+c_j)
+\frac{2C_{j1}-b_jB_{j1}}{\sqrt{4c_j-b_j^2}}
arctan\left(\frac{2x+b_j}{\sqrt{4c_j-b_j^2}}\right)
\]
and for $k > 1$,
\[
\begin{array}{lcl}
\displaystyle
\int{\frac{B_{jk}x+C_{jk}}{(x^2+b_jx+c_j)^k}}&=&
\displaystyle\frac{(2C_{jk}-b_jB_{jk})x+b_jC_{jk}-2c_jB_{jk}}
{(k-1)(4c_j-b_j^2)(x^2+b_jx+c_j)^{k-1}}\\
&&\displaystyle+\int{\frac{(2k-3)(2C_{jk}-b_jB_{jk})}
{(k-1)(4c_j-b_j^2)(x^2+b_jx+c_j)^{k-1}}}\\
\end{array}
\]
This last formula is then used recursively until $k=1$.

An alternative is to factor $D$ linearly over $\mathbb{C}$:
$D=\prod_{i=1}^q(x-\alpha_i)^{e_i}$, and then use \ref{Int2} on each term of
\begin{equation}\label{Int3}
f=P+\sum_{i=1}^q\sum_{j=1}^{e_i}\frac{A_{ij}}{(x-\alpha_i)^j}
\end{equation}
Note that this alternative is applicable to coefficients in any field
$K$, if we factor $D$ linearly over its algebraic closure
$\overline{K}$, and is equivalent to expanding $f$ into its Laurent
series at all its finite poles, since that series at 
$x=\alpha_i \in \overline{K}$ is
\[
f=\frac{A_{ie_i}}{(x-\alpha_i)^{e_i}}
+\cdots
+\frac{A_{i2}}{(x-\alpha_i)^2}
+\frac{A_{i1}}{(x-\alpha_i)}
+\cdots
\]
where the $A_{ij}$'s are the same as those in \ref{Int3}. Thus, this approach
can be seen as expanding the integrand into series around all the
poles (including $\infty$), then integrating the series termwise, and
then interpolating for the answer, by summing all the polar terms,
obtaining the integral of \ref{Int3}. In addition, this alternative shows
that any rational function $f \in K(x)$ has an elementary integral of
the form
\begin{equation}\label{Int4}
\int{f}=v+c_1\log(u_1)+\cdots+c_m\log(u_m)
\end{equation}
where $v,u_1,\ldots,u_m \in \overline{K}(x)$ are the rational
functions, and $c_1,\ldots,c_m \in \overline{K}$ are constants. The
original Risch algorithm is essentially a generalization of this
approach that searches for integrals of arbitrary elementary functions
in a form similar to \ref{Int4}.

\subsection{The Hermite reduction}
The major computational inconvenience of the full partial fraction
approach is the need to factor polynomials over $\mathbb{R}$,
$\mathbb{C}$, or $\overline{K}$, thereby introducing algebraic numbers
even if the integrand and its integral are both in $\mathbb{Q}(x)$. On
the other hand, introducing algebraic numbers may be necessary, for
example it is proven in \cite{Risc69a} that any field containing an
integral of $1/(x^2+2)$ must also contain $\sqrt{2}$. Modern research
has yielded so-called ``rational'' algorithms that
\begin{itemize}
\item compute as much of the integral as possible with all
calculations being done in $K(x)$, and
\item compute the minimal algebraic extension of $K$ necessary to
express the integral
\end{itemize}
The first rational algorithms for integration date back to the
$19^{{\rm th}}$ century, when both Hermite \cite{Herm1872} and
Ostrogradsky \cite{Ostr1845} invented methods for 
computing the $v$ of \ref{Int4}
entirely within $K(x)$. We describe here only Hermite's method, since
it is the one that has been generalized to arbitrary elementary
functions. The basic idea is that if an irreducible $p \in K[x]$
appears with multiplicity $k > 1$ in the factorization of the
denominator of the integrand, then \ref{Int2} implies that it appears with
multiplicity $k-1$ in the denominator of the integral. Furthermore, it
is possible to compute the product of all such irreducibles for each
$k$ without factoring the denominator into irreducibles by computing
its {\sl squarefree factorization}, {\sl i.e} a factorization
$D=D_1D_2^2\cdots D_m^m$, where each $D_i$ is squarefree and 
gcd$(D_i,D_j)=1$ for $i \ne j$. A straightforward way to compute it is
as follows: let $R={\rm gcd}(D,D^{\prime})$, 
then $R=D_2D_2^3\cdots D_m^{m-1}$, so 
$D/R=D_1D_2\cdots D_m$ and gcd$(R,D/R)=D_2\cdots D_m$, which implies
finally that
\[
D_1=\frac{D/R}{{\rm gcd}(R,D/R)}
\]
Computing recursively a squarefree factorization of $R$ completes the
one for $D$. Note that \cite{Yun76} presents a more efficient method for
this decomposition. Let now $f \in K(x)$ be our integrand, and write
$f=P+A/D$ where $P,A,D \in K[x]$, gcd$(A,D)=1$, and\\
${\rm deg}(A)<{\rm deg}(D)$. 
Let $D=D_1D_2^2\cdots D_m^m$ be a squarefree factorization of $D$ and
suppose that $m \ge 2$ (otherwise $D$ is already squarefree). Let then
$V=D_m$ and $U=D/V^m$. Since gcd$(UV^{\prime},V)=1$, we can use the
extended Euclidean algorithm to find $B,C \in K[x]$ such that
\[
\frac{A}{1-m}=BUV^{\prime}+CV
\]
and ${\rm deg}(B) < {\rm deg}(V)$. 
Multiplying both sides by $(1-m)/(UV^m)$ gives
\[
\frac{A}{UV^m}=\frac{(1-m)BV^{\prime}}{V^m}+\frac{(1-m)C}{UV^{m-1}}
\]
so, adding and subtracting $B^{\prime}/V^{m-1}$ to the right hand side, we
get
\[
\frac{A}{UV^m}=\left(\frac{B^{\prime}}{V^{m-1}}-
\frac{(m-1)BV^{\prime}}{V^m}\right)
+\frac{(1-m)C-UB^{\prime}}{UV^{m-1}}
\]
and integrating both sides yields
\[
\int\frac{A}{UV^m}=\frac{B}{V^{m-1}}+\int\frac{(1-m)C-UB^{\prime}}{UV^{m-1}}
\]
so the integrand is reduced to one with a smaller power of $V$ in the
denominator. This process is repeated until the denominator is
squarefree, yielding $g,h \in K(x)$ such that $f=g^{\prime}+h$ and $h$ has
a squarefree denominator.

\subsection{The Rothstein-Trager and Lazard-Rioboo-Trager algorithms}
Following the Hermite reduction, we only have to integrate fractions
of the form $f=A/D$ with ${\rm deg}(A)<{\rm deg}(D)$ and $D$ squarefree. It
follows from \ref{Int2} that
\[
\int{f}=\sum_{i=1}^n a_i\log(x-\alpha_i)
\]
where the $\alpha_i$'s are the zeros of $D$ in $\overline{K}$, and the
$a_i$'s are the residues of $f$ at the $\alpha_i$'s. The problem
is then to compute those residues without splitting $D$. Rothstein
\cite{Roth77} and Trager \cite{Trag76} independently proved that the
$\alpha_i$'s are exactly the zeros of
\begin{equation}\label{Int5}
R={\rm resultant}_x(D,A-tD^{\prime}) \in K[t]
\end{equation}
and that the splitting field of $R$ over $K$ is indeed the minimal
algebraic extension of $K$ necessary to express the integral in the
form \ref{Int4}. The integral is then given by
\begin{equation}\label{Int6}
\int\frac{A}{D}=\sum_{i=1}^m\sum_{a|R_i(a)=0}a\log(\gcd(D,A-aD^{\prime}))
\end{equation}
where $R=\prod_{i=1}^m R_i^{e_i}$ is the irreducible factorization of
$R$ over $K$. Note that this algorithm requires factoring $R$ into
irreducibles over $K$, and computing greatest common divisors in
$(K[t]/(R_i))[x]$, hence computing with algebraic numbers. Trager and
Lazard \& Rioboo \cite{Laza90} independently discovered that those
computations can be avoided, if one uses the subresultant PRS
algorithm to compute the resultant of \ref{Int5}: let 
$(R_0,R_1,\ldots R_k\ne 0,0,\ldots)$ be the subresultant PRS with
respect to $x$ of $D$ and $A-tD^{\prime}$ and $R=Q_1Q_2^2\ldots Q_m^m$ be a 
{\sl squarefree} factorization of their resultant. Then,
\[
\sum_{a|Q_i(a)=0} a\log(\gcd(D,A-aD^{\prime}))=\hbox{\hskip 5.0cm}
\]
\[
\left\{
\begin{array}{ll}
\sum_{a|Q_i(a)=0} a \log(D) & {\rm if\ }i = {\rm deg}(D)\\
\sum_{a|Q_i(a)=0} a \log({\rm pp}_x(R_{k_i})(a,x))&
{\rm where\ }{\rm deg}(R_{k_i})=i,1 \le k_i \le n\\
&{\rm if\ }i < {\rm deg}(D)
\end{array}
\right.
\]
Evaluating ${\rm pp}_x(R_{k_i})$ at $t=a$ where $a$ is a root of $Q_i$
is equivalent to reducing each coefficient with respect to $x$ of
${\rm pp}_x(R_{k_i})$ module $Q_i$, hence computing in the algebraic
extension $K[t]/(Q_i)$. Even this step can be avoided: it is in fact
sufficient to ensure that $Q_i$ and the leading coefficient with
respect to $x$ of $R_{k_i}$ do not have a nontrivial common factor,
which implies then that the remainder by $Q_i$ is nonzero, see
\cite{Muld97} for details and other alternatives for computing
${\rm pp}_x(R_{k_i})(a,x)$

\section{Algebraic Functions}
By an {\sl algebraic function}, we mean an element of a finitely
generated algebraic extension $E$ of the rational function field
$K(x)$. This includes nested radicals and implicit algebraic
functions, not all of which can be expressed by radicals. It turns out
that the algorithms we used for rational functions can be extended to
algebraic functions, but with several difficulties, the first one
being to define the proper analogues of polynomials, numerators and
denominators. Since $E$ is algebraic over $K(x)$, for any
$\alpha \in E$, there exists a polynomial $p \in K[x][y]$ such that 
$p(x,\alpha)=0$. We say that $\alpha \in E$ is {\sl integral over}
$K[x]$ if there is a polynomial $p \in K[x][y]$, {\sl monic in y},
such that $p(x,\alpha)=0$. Integral elements are analogous to
polynomials in that their value is defined for any 
$x \in \overline{K}$ (unlike non-integral elements, which must have at
least one pole in $\overline{K}$). The set
\[
{\bf O}_{K[x]} = \{\alpha \in E {\rm\ such\ that\ }\alpha
{\rm\ is\ integral\ over\ }K[x]\}
\]
is called the {\sl integral closure of} $K[x]$ {\sl in E}. It is a
ring and a finitely generated $K[x]$-module. Let $\alpha \in E^{*}$ be
any element and $p=\sum_{i=0}^m a_iy^i \in K[x][y]$ be such that
$p(x,\alpha)=0$ and $a_m \ne 0$. Then, $q(x,a_my)=0$ where
$q=y^m+\sum_{i=0}^{m-1} a_ia_m^{m-i-1}y^i$ is monic in $y$, 
so $a_my \in {\bf O}_{K[x]}$. We need a canonical representation
for algebraic functions similar to quotients of polynomials for
rational functions. Expressions as quotients of integral functions are
not unique, for example, $\sqrt{x}/x=x/\sqrt{x}$. However, $E$ is a
finite-dimensional vector space over $K(x)$, so let $n=[E:K(x)]$ and
$w=(w_1,\ldots,w_n)$ be any basis for $E$ over $K(x)$. By the above
remark, there are $a_1,\ldots,a_n \in K(x)^{*}$ such that
$a_iw_i \in {\bf O}_{K[x]}$ for each $i$. Since
$(a_1w_1,\ldots,a_nw_n)$ is also a basis for $E$ over $K(x)$, we can
assume without loss of generality that the basis $w$ is composed of
integral elements. Any $\alpha \in E$ can be written uniquely as
$\alpha = \sum_{i=1}^n f_iw_i$ for $f_1,\ldots,f_n \in K(x)$, and
putting the $f_i$'s over a monic common denominator $D \in K[x]$, we
get an expression
\[
\alpha = \frac{A_1w_1+\ldots+A_nw_n}{D}
\]
where $A_1,\ldots,A_n \in K[x]$ and $\gcd(D,A_1,\ldots,A_n)=1$. We
call $\sum_{i=1}^n A_iw_i \in {\bf O}_{K[x]}$ and
$D \in K[x]$ respectively the {\sl numerator} and {\sl denominator} of
$\alpha$ with respect to $w$. They are defined uniquely once the basis
$w$ is fixed.
\subsection{The Hermite reduction}
Now that we have numerators and denominators for algebraic functions,
we can attempt to generalize the Hermite reduction of the previous
section, so let $f \in E$ be our integrand, 
$w=(w_1,\ldots,w_n) \in {{\bf O}_{K[n]}}^{n}$ be a basis for $E$
over $K(x)$ and let $\sum_{i=1}^m A_iw_i \in {\bf O}_{K[x]}$
and $D \in K[x]$ be the numerator and denominator of $f$ with respect
to $w$, Let $D=D_1D_2^2\ldots D_m^m$ be a squarefree factorization of
$D$ and suppose that $m \ge 2$. Let then $V=D_m$ and $U=D/V^m$, and we
ask whether we can compute 
$B=\sum_{i=1}^n B_iw_i \in {\bf O}_{K[x]}$ and $h \in E$ such
that ${\rm deg}(B_i) < {\rm deg}(V)$ for each $i$,
\begin{equation}\label{Int7}
\int\frac{\sum_{i=1}^n A_iw_i}{UV^m}=\frac{B}{V^{m-1}}+\int{h}
\end{equation}
and the denominator of $h$ with respect to $w$ has no factor of order
$m$ or higher. This turns out to reduce to solving the following
linear system
\begin{equation}\label{Int8}
f_1S_1+\ldots+f_nS_n=A_1w_1+\ldots+A_nw_n
\end{equation}
for $f_1,\ldots,f_n \in K(x)$, where
\begin{equation}\label{Int9}
S_i=UV^m\left(\frac{w_i}{V^{m-1}}\right)^{\prime}\quad{\rm for\ }1\le i\le n
\end{equation}
Indeed, suppose that \ref{Int8} has a solution $f_1,\ldots,f_n \in K(x)$, and
write $f_i=T_i/Q$, where $Q,T_1,\ldots,T_n \in K[x]$ and
$\gcd(Q,T_1,\ldots,T_n)=1$. Suppose further that $\gcd(Q,V)=1$. Then,
we can use the extended Euclidean algorithm to find $A,R \in K[x]$
such that $AV+RQ=1$, and Euclidean division to find $Q_i,B_i \in K[x]$
such that ${\rm deg}(B_i)<{\rm deg}(V)$ when $B_i \ne 0$ 
and $RT_i=VQ_i+B_i$ for
each $i$. We then have
\[
\begin{array}{lcl}
h&=&\displaystyle
f-\left(\frac{\sum_{i=1}^n B_iw_i}{V^{m-1}}\right)^{\prime}\\
&&\\
&=&\displaystyle
\frac{\sum_{i=1}^nA_iw_i}{UV^m}
-\frac{\sum_{i=1}^nB_i^{\prime}w_i}{V^{m-1}}
-\sum_{i=1}^n(RT_i-VQ_i)\left(\frac{w_i}{V^{m-1}}\right)^{\prime}\\
&&\\
&=&\displaystyle
\frac{\sum_{i=1}^nA_iw_i}{UV^m}
-\frac{R\sum_{i=1}^nT_iS_i}{UV^m}
+V\sum_{i=1}^nQ_i\left(\frac{w_i}{V^{m-1}}\right)^{\prime}
-\frac{\sum_{i=1}^nB_i^{\prime}w_i}{V^{m-1}}\\
&&\\
&=&\displaystyle
\frac{(1-RQ)\sum_{i=1}^nA_iw_i}{UV^m}
+\frac{\sum_{i=1}^nQ_iw_i^{\prime}}{V^{m-2}}
-(m-1)V^{\prime}\frac{\sum_{i=1}^nQ_iw_i}{V^{m-1}}
-\frac{\sum_{i=1}^nB_i^{\prime}w_i}{V^{m-1}}\\
&&\\
&=&\displaystyle
\frac{\sum_{i=1}^nAA_iw_i}{UV^{m-1}}
-\frac{\sum_{i=1}^n((m-1)V^{\prime}Q_i+B_i^{\prime})w_i}{V^{m-1}}
+\frac{\sum_{i=1}^nQ_iw_i^{\prime}}{V^{m-2}}
\end{array}
\]
Hence, if in addition the denominator of $h$ has no factor of order
$m$ or higher, then $B=\sum_{i=1}^nB_iw_i \in {\bf O}_{K[x]}$
and $h$ solve \ref{Int7} and we have reduced the integrand. Unfortunately, it
can happen that the denominator of $h$ has a factor of order $m$ or
higher, or that \ref{Int8} has no solution in $K(x)$ whose denominator is
coprime with $V$, as the following example shows.

\noindent
{\bf Example 1} Let $E=K(x)[y]/(y^4+(x^2+x)y-x^2)$ {\sl with basis}
$w=(1,y,y^2,y^3)$ {\sl over} $K(x)$ {\sl and consider the integrand}
\[
f=\frac{y^3}{x^2}=\frac{w_4}{x^2}\in E
\]
We have $D=x^2$, so $U=1,V=x$ and $m=2$.
Then, $S_1=x^2(1/x)^{\prime}=-1$,
\[
\begin{array}{lcl}
\displaystyle S_2&=&
\displaystyle x^2\left(\frac{y}{x}\right)^{\prime}\\
&&\\
\displaystyle &=&
\displaystyle \frac{24(1-x^2)y^3+32x(1-x)y^2-(9x^4+45x^3+209x^2+63x+18)y
-18x(x^3+x^2-x-1)}{27x^4+108x^3+418x^2+108x+27}\\
&&\\
\displaystyle S_3&=&x^2\left(\frac{y^2}{x}\right)^{\prime}\\
&&\\
\displaystyle &=&
\displaystyle\frac{64x(1-x)y^3+9(x^4+2x^3-2x-1)y^2+12x(x^3+x^2-x-1)y+
48x^2(1-x^2)}{27x^4+108x^3+418x^2+108x+27}\\
&&\\
and&&\\
&&\\
\displaystyle S_4&=&x^2\left(\frac{y^3}{x}\right)^{\prime}\\
&&\\
\displaystyle &=&
\displaystyle\frac{(27x^4+81x^3+209x^2+27x)y^3+18x(x^3+x^2-x-1)y^2
+24x^2(x^2-1)y+96x^3(1-x)}
{27x^4+108x^3+418x^2+108x+27}
\end{array}
\]
so \ref{Int8} becomes
\begin{equation}\label{Int10}
M
\left(
\begin{array}{c}
f_1\\
f_2\\
f_3\\
f_4
\end{array}
\right)=
\left(
\begin{array}{c}
0\\
0\\
0\\
1
\end{array}
\right)
\end{equation}
where
\[
M=\left(
\begin{array}{cccc}
\displaystyle -1&\frac{-18x(x^3+x^2-x-1)}{F}&\frac{48x^2(1-x^2)}{F}
&\frac{96x^3(1-x)}{F}\\
\displaystyle 0&\frac{-(9x^4+45x^3+209x^2+63x+18)}{F}
&\frac{12x(x^3+x^2-x-1)}{F}&\frac{24x^2(x^2-1)}{F}\\
\displaystyle 0&\frac{32x(1-x)}{F}&\frac{9(x^4+2x^3-2x-1)}{F}
&\frac{18x(x^3+x^2-x-1)}{F}\\
\displaystyle 0&\frac{24(1-x^2)}{F}&\frac{64x(1-x)}{F}
&\frac{(27x^4+81x^3+209x^2+27x)}{F}
\end{array}
\right)
\]
and $F=27x^4+108x^3+418x^2+108x+27$. The system \ref{Int10} admits a unique
solution $f_1=f_2=0, f_3=-2$ and $f_4=(x+1)/x$, whose denominator is
not coprime with $V$, so the Hermite reduction is not applicable.

The above problem was first solved by Trager \cite{Trag84}, who proved 
that if $w$ is an {\sl integral basis, i.e.} its elements generate 
${\bf O}_{K[x]}$ over $K[x]$, then the system \ref{Int8} always has a
unique solution in $K(x)$ when $m > 1$, and that solution always has a
denominator coprime with V. Furthermore, the denominator of each
$w_i^{\prime}$ must be squarefree, implying that the denominator of $h$ is
a factor of $FUV^{m-1}$ where $F \in K[x]$ is squarefree and coprime
with $UV$. He also described an algorithm for computing an integral
basis, a necessary preprocessing for his Hermite reduction. The main
problem with that approach is that computing the integral basis,
whether by the method of \cite{Trag84} or the local alternative \cite{Hoei94},
can be in general more expansive than the rest of the reduction
process. We describe here the lazy Hermite reduction \cite{Bron98}, which
avoids the precomputation of an integral basis. It is based on the
observation that if $m > 1$ and \ref{Int8} does not have a solution allowing
us to perform the reduction, then either
\begin{itemize}
\item the $S_i$'s are linearly dependent over $K(x)$, or
\item \ref{Int8} has a unique solution in $K(x)$ whose denominator has a
nontrivial common factor with $V$, or
\item the denominator of some $w_i$ is not squarefree
\end{itemize}
In all of the above cases, we can replace our basis $w$ by a new one,
also made up of integral elements, so that that $K[x]$-module
generated by the new basis strictly contains the one generated by $w$:

\noindent
{\bf Theorem 1 (\cite{Bron98})} {\sl Suppose that $m \ge 2$ and that 
$\{S_1,\ldots,S_n\}$ as given by \ref{Int9} are linearly dependent over $K(x)$,
and let $T_1,\ldots,T_n \in K[x]$ be not all 0 and such that
$\sum_{i=1}^n T_iS_i=0$. Then,
\[
w_0=\frac{U}{V}\sum_{i=1}^n T_iw_i \in {\bf O}_{K[x]}
\]
Furthermore, if $\gcd(T_1,\ldots,T_n)=1$ then
$w_0 \notin K[x]w_1+\cdots+K[x]w_n$.}

\noindent
{\bf Theorem 2 (\cite{Bron98})} {\sl Suppose that $m \ge 2$ and that
$\{S_1,\ldots,S_n\}$ as given by \ref{Int9} are linearly independent over
$K(x)$, and let $Q,T_1,\ldots,T_n \in K[x]$ be such that
\[
\sum_{i=1}^n A_iw_i = \frac{1}{Q}\sum_{i=1}^n T_iS_i
\]
Then,
\[
w_0=\frac{U(V/\gcd(V,Q))}{\gcd(V,Q)}\sum_{i=1}^n T_iw_i \in 
{\bf O}_{K[x]}
\]
Furthermore, 
if $\gcd(Q,T_1,\ldots,T_n)=1$ and $\deg(\gcd(V,Q)) \ge 1$, then
$w_0 \notin K[x]w_1+\cdots+K[x]w_n$.}

{\bf Theorem 3 (\cite{Bron98})} {\sl Suppose that the denominator $F$ of
some $w_i$ is not squarefree, and let $F=F_1F_2^2\cdots F_k^k$ be its
squarefree factorization. Then,}
\[
w_0=F_1\cdots F_kw_i^{\prime} \in {\bf O}_{K[x]} \backslash
(K[x]w_1+\cdots+K[x]w_n).
\]

The lazy Hermite reduction proceeds by solving the system \ref{Int8} in
$K(x)$. Either the reduction will succeed, or one of the above
theorems produces an element
$w_0 \in {\bf O}_{K[x]} \backslash (K[x]w_1+\cdots+K[x]w_n).$ Let then
$\sum_{i=1}^n C_iw_i$ and $F$ be the numerator and denominator of
$w_0$ with respect to $w$. Using Hermitian row reduction, we can zero
out the last row of
\[
\left(
\begin{array}{cccc}
F  &   &      &\\
   &F  &      &\\
   &   &\ddots&\\
   &   &      &F\\
C_1&C_2&\cdots&C_n
\end{array}
\right)
\]
obtaining a matrix of the form
\[
\left(
\begin{array}{cccc}
C_{1,1} & C_{1,2} & \cdots & C_{1,n}\\
C_{2,1} & C_{2,2} & \cdots & C_{2,n}\\
\vdots  & \vdots  &        & \vdots\\
C_{n,1} & C_{n,2} & \cdots & C_{n,n}\\
0       & 0       & \cdots & 0\\
\end{array}
\right)
\]
with $C_{ij} \in K[x]$. Let $\overline{w}_i=(\sum_{j=1}^n
C_{ij}w_j)/F$
for $1 \le i \le n$. Then, 
$\overline{w}=(\overline{w}_1,\ldots,\overline{w}_n)$ is a basis for
$E$ over $K$ and
\[
K[x]\overline{w}_1+\cdots+K[x]\overline{w}_n=K[x]w_1+\cdots+K[x]w_n+K[x]w_0
\]
is a submodule of ${\bf O}_{K[x]}$, which strictly contains
$K[x]w_1+\cdots+K[x]w_n$, since it contains $w_0$. Any strictly
increasing chain of submodules of ${\bf O}_{K[x]}$ must
stabilize after a finite number of steps, which means that this
process produces a basis for which either the Hermite reduction can be
carried out, or for which $f$ has a squarefree denominator.

\noindent
{\bf Example 2} Continuing example 1 for which the Hermite reduction
failed, Theorem 2 implies that
\[
w_0=\frac{1}{x}(-2xw_3+(x+1)w_4)=(-2xy^2+(x+1)y^3)x \in {\bf O}_{K[x]}
\]
Performing a Hermitian row reduction on
\[
\left(
\begin{array}{cccc}
x& &   & \\
 &x&   & \\
 & &x  & \\
 & &   &x\\
0&0&-2x&x+1\\
\end{array}
\right)
\]
yields
\[
\left(
\begin{array}{cccc}
x& & & \\
 &x& & \\
 & &x& \\
 & & &1\\
0&0&0&0\\
\end{array}
\right)
\]
so the new basis is $\overline{w}=(1,y,y^2,y^3/x)$, and the
denominator of $f$ with respect to $\overline{w}$ is $x$, which is
squarefree. 

\subsection{Simple radical extensions}
The integration algorithm becomes easier when $E$ is a simple radical
extension of $K(x)$, {\sl i.e.} $E=K(x)[y]/(y^n-a)$ for some 
$a \in K(x)$. Write $a=A/D$ where $A,D \in K[x]$, and let
$AD^{n-1}=A_1A_2^2\cdots A_k^k$ be a squarefree factorization of 
$AD^{n-1}$. Writing $i=nq_i+r_i$, for $1 \le i \le k$, where
$0 \le r_i < n$, let $F=A_1^{q_1}\cdots A_k^{q_k}$,
$H=A_1^{r_1}\cdots A_k^{r_k}$ and $z=yD/F$. Then,
\[
z^n=\left(y\frac{D}{F}\right)^n=\frac{y^nD^n}{F^n}=\frac{AD^{n-1}}{F}
=A_1^{r_1}\cdots A_k^{r_k}=H
\]
Since $r_i < n$ for each $i$, the squarefree factorization of $H$ is
of the form $H=H_1H_2^2\cdots H_m^m$ with $m<n$. An integral basis is
then $w=(w_1,\ldots,w_n)$ where
\begin{equation}\label{Int11}
w_i=\frac{z^{i-1}}{\prod_{j=1}^m H_j^{\lfloor(i-1)j/n\rfloor}}\quad
1 \le i \le n
\end{equation}
and the Hermite reduction with respect to the above basis is always 
guaranteed to succeed. Furthermore, when using that basis, the system
\ref{Int8} becomes diagonal and its solution can be written explicitly:
writing $D_i=\prod_{j=1}^m H_j^{\lfloor ij/n\rfloor}$ we have
\[
\begin{array}{ccl}
S_i & = &\displaystyle UV^m\left(\frac{w_i}{V^{m-1}}\right)^{\prime}
=UV^m\left(\frac{z^{i-1}}{D_{i-1}V^{m-1}}\right)^{\prime}\\
&&\\
&=&\displaystyle UV^m\left(\frac{i-1}{n}\frac{H^{\prime}}{H}-
\frac{{D_{i-1}}^{\prime}}{D_{i-1}}
-(m-1)\frac{V^{\prime}}{V}\right)\left(\frac{z^{i-1}}{D_{i-1}V^{m-1}}\right)\\
&&\\
&=&\displaystyle U\left(V\left(\frac{i-1}{n}\frac{H^{\prime}}{H}
-\frac{{D_{i-1}}^{\prime}}{D_{i-1}}\right)-(m-1)V^{\prime}\right)w_i
\end{array}
\]
so the unique solution of \ref{Int8} in $K(x)$ is
\begin{equation}\label{Int12}
f_i=\frac{A_i}{U\left(V\left(\frac{i-1}{n}\frac{H^{\prime}}{H}-
\frac{{D_{i-1}}^{\prime}}{D_{i-1}}\right)-(m-1)V^{\prime}\right)}
\quad{\rm for\ }1 \le i \le n
\end{equation}
and it can be shown that the denominator of each $f_i$ is coprime with 
$V$ when $m \ge 2$.

\noindent
{\bf Example 3} {\sl Consider
\[
\int\frac{(2x^8+1)\sqrt{(x^8+1)}}{x^{17}+2x^9+x}~dx
\]
The integrand is
\[
f=\frac{(2x^8+1)y}{x^{17}+2x^9+x} \in E
=\mathbb{Q}(x)[y]/(y^2-x^8-1)
\]
so $H=x^8+1$ which is squarefree, implying that the integral basis
\ref{Int11} is $(w_1,w_2)=(1,y)$. The squarefree factorization of
$x^{17}+2x^9+x$ is $x(x^8+1)^2$ so $U=x$, $V=x^8+1$, $m=2$, and the
solution \ref{Int12} of \ref{Int8} is
\[
f_1=0,\quad
f_2=\frac{2x^8+1}{x\left((x^8+1)\frac{1}{2}\frac{8x^7}{x^8+1}
-8x^7\right)}=-\frac{(2x^8+1)/4}{x^8}
\]
We have $Q=x^8$, so $V-Q=1$, $A=1$, $R=-1$ and $RQf_2=V/2-1/4$,
implying that
\[
B=-\frac{y}{4}\quad {\rm and}\quad h=f-\left(\frac{B}{V}\right)^{\prime}
=\frac{y}{x(x^8+1)}
\]
solve \ref{Int7}, i.e.
\[
\int\frac{(2x^8+1)\sqrt{(x^8+1)}}{x^{17}+2x^9+x}~dx=
-\frac{\sqrt{x^8+1}}{4(x^8+1)}
+\int\frac{\sqrt{x^8+1}}{x(x^8+1)}~dx
\]
and the remaining integrand has a squarefree denominator.}

\subsection{Liouville's Theorem}
Up to this point, the algorithms we have presented never fail, yet it
can happen that an algebraic function does not have an elementary
integral, for example
\[
\int{\frac{x~dx}{\sqrt{1-x^3}}}
\]
which is not an elementary function of $x$. So we need a way to
recognize such functions before completing the integration
algorithm. Liouville was the first to state and prove a precise
theorem from Laplace's observation that we can restrict the elementary
integration problem by allowing only new logarithms to appear linearly
in the integral, all the other terms appearing in the integral being
already in the integrand.

{\bf Theorem 4 (Liouville \cite{Liou1833a,Liou1833b})} {\sl
Let $E$ be an algebraic extension of the rational function field
$K(x)$, and $f \in E$. If $f$ has an elementary integral, then there
exist $v \in E$, constants $c_1,\ldots,c_n \in \overline{K}$ and
$u_1,\ldots,u_k \in E(c_1,\ldots,c_k)^{*}$ such that}
\begin{equation}\label{Int13}
f=v^{\prime}+c_1\frac{u_1^{\prime}}{u_1}+\cdots+c_k\frac{u_k^{\prime}}{u_k}
\end{equation}
The above is a restriction to algebraic functions of the strong
Liouville Theorem, whose proof can be found in \cite{Bron97,Risc69b}. 
An elegant
and elementary algebraic proof of a slightly weaker version can be
found in \cite{Rose72}. As a consequence, we can look for an integral of
the form \ref{Int4}, Liouville's Theorem guaranteeing that there is no
elementary integral if we cannot find one in that form. Note that the
above theorem does not say that every integral must have the above
form, and in fact that form is not always the most convenient one, for
example, 
\[
\int{\frac{dx}{1+x^2}}=arctan(x)=\frac{\sqrt{-1}}{2}
\log\left(\frac{\sqrt{-1}+x}{\sqrt{-1}-x}\right)
\]

\subsection{The integral part}
Following the Hermite reduction, we can assume that we have a basis 
$w=(w_1,\ldots,w_n)$ of $E$ over $K(x)$ made of integral elements such
that our integrand is of the form $f=\sum_{i=1}^n A_iw_i/D$ where 
$D \in K[x]$ is squarefree. Given Liouville's Theorem, we now have to
solve equation \ref{Int13} for $v$, $u_1,\ldots,u_k$ and the constants 
$c_1,\ldots,c_k$. Since $D$ is squarefree, it can be shown that 
$v \in {\bf O}_{K[x]}$ for any solution, and in fact $v$
corresponds to the polynomial part of the integral of rational
functions. It is however more difficult to compute than the integral
of polynomials, so Trager \cite{Trag84} gave a change of variable that
guarantees that either $v^{\prime}=0$ or $f$ has no elementary integral. In
order to describe it, we need to define the analogue for algebraic
functions of having a nontrivial polynomial part: we say that 
$\alpha \in E$ is {\sl integral at infinity} if there is a polynomial
$p=\sum_{i=1}^m a_iy^i \in K[x][y]$ such that $p(x,\alpha)=0$ and
${\rm deg}(a_m) \ge {\rm deg}(a_i)$ for each $i$. Note that a rational function
$A/D \in K(x)$ is integral at infinity if and only if 
${\rm deg}(A) \le {\rm deg}(D)$ 
since it is a zero of $Dy-A$. When $\alpha-E$ is
not integral at infinity, we say that it has a {\sl pole at
infinity}. Let
\[
{\bf O}_\infty = \{\alpha \in E {\rm\ such\ that\ }\alpha
{\rm\ is\ integral\ at\ infinity}\}
\]
A set $(b_1,\ldots,b_n) \in E^n$ is called {\sl normal at infinity} if
there are $r_1,\ldots,r_n \in K(x)$ such that every 
$\alpha \in {\bf O}_\infty$ can be written as
$\alpha = \sum_{i=1}^n B_ir_ib_i/C$ where $C,B_1,\ldots,B_n \in K[x]$
and ${\rm deg}(C) \ge {\rm deg}(B_i)$ for each $i$. 
We say that the differential
$\alpha{}dx$ is integral at infinity if 
$\alpha x^{1+1/r} \in {\bf O}_\infty$ where $r$ is the smallest
ramification index at infinity. Trager \cite{Trag84} described an
algorithm that converts an arbitrary integral basis $w_1,\ldots,w_n$
into one that is also normal at infinity, so the first part of his
integration algorithm is as follows:
\begin{enumerate}
\item Pick any basis $b=(b_1,\ldots,b_n)$ of $E$ over $K(x)$ that is
composed of integral elements.
\item Pick an integer $N \in \mathbb{Z}$ that is not zero of the
denominator of $f$ with respect to $b$, nor of the discriminant of $E$
over $K(x)$, and perform the change of variable $x=N+1/z$,
$dx=-dz/z^2$ on the integrand.
\item Compute an integral basis $w$ for $E$ over $K(z)$ and make it
normal at infinity
\item Perform the Hermite reduction on $f$ using $w$, this yields 
$g,h \in E$ such that $\int{f~dz}=g+\int{h~dz}$ and $h$ has a
squarefree denominator with respect to $w$.
\item If $hz^2$ has a pole at infinity, then $\int{f~dz}$ and
$\int{h~dz}$ are not elementary functions
\item Otherwise, $\int{h~dz}$ is elementary if and only if there are
constants $c_1,\ldots,c_k \in \overline{K}$ and 
$u_1,\ldots,u_k \in E(c_1,\ldots,c_k)^{*}$ such that
\end{enumerate}
\begin{equation}\label{Int14}
h=\frac{c_1}{u_1}\frac{du_1}{dz}+\cdots+\frac{c_k}{u_k}\frac{du_k}{dz}
\end{equation}
The condition that $N$ is not a zero of the denominator of $f$ with
respect to $b$ implies that the $fdz$ is integral at infinity after
the change of variable, and Trager proved that if $hdz$ is not
integral at infinity after the Hermite reduction, then $\int{h~dz}$
and $\int{f~dz}$ are not elementary functions. The condition that $N$
is not a zero of the discriminant of $E$ over $K(x)$ implies that the
ramification indices at infinity are all equal to 1 after the change
of variable, hence that $h~dz$ is integral at infinity if and only if
$hz^2 \in {\bf O}_\infty$. That second condition on $N$ can be
disregarded, in which case we must replace $hz^2$ in step 5 by 
$hz^{1+1/r}$ where $r$ is the smallest ramification index at
infinity. Note that $hz^2 \in {\bf O}_\infty$ implies that 
$hz^{1+1/r} \in {\bf O}_\infty$, but not conversely. Finally, we
remark that for simple radical extensions, the integral basis \ref{Int11} is
already normal at infinity.

Alternatively, we can use lazy Hermite reduction in the above
algorithm: in step 3, we pick any basis made of integral elements,
then perform the lazy Hermite reduction in step 4. If $h \in K(z)$
after the Hermite reduction, then we can complete the integral without
computing an integral basis. Otherwise, we compute an integral basis
and make it normal at infinity between steps 4 and 5. This lazy
variant can compute $\int{f~dx}$ whenever it is an element of $E$
without computing an integral basis.

\subsection{The logarithmic part}
Following the previous sections, we are left with solving equation
\ref{Int14} for the constants $c_1,\ldots,c_k$ and for $u_1,\ldots,u_k$. We
must make at this point the following additional assumptions:
\begin{itemize}
\item we have an integral primitive element for $E$ over $K(z)$, {\sl
i.e.} $y \in {\bf O}_{K[z]}$ such that $E=K(z)(y)$,
\item $[E : K(z)]=[E : \overline{K}(z)]$, {\sl i.e.} the minimal
polynomial for $y$ over $K[z]$ is absolutely reducible, and
\item we have an integral basis $w=(w_1,\ldots,w_n)$ for $E$ over
$K(z)$, and $w$ is normal at infinity
\end{itemize}
A primitive element can be computed by considering linear combinations
of the generators of $E$ over $K(x)$ with random coefficients in
$K(x)$, and Trager \cite{Trag84} describes an absolute factorization
algorithm, so the above assumptions can be ensured, although those
steps can be computationally very expensive, except in the case of
simple radical extensions. Before describing the second part of
Trager's integration algorithm, we need to define some concepts from
the theory of algebraic curves. Given a finite algebraic extension 
$E=K(z)(y)$ of $K(z)$, a {\sl place} $P$ of $E$ is a proper local
subring of $E$ containing $K$, and a {\sl divisor} is a formal sum
$\sum{n_PP}$ with finite support, where the $n_P$'s are integers and
the $P$'s are places. Let $P$ be a place, then its maximal ideal
$\mu_P$ is principal, so let $p\in E$ be a generator of $\mu_P$. The
{\sl order at} $P$ is the function 
$\nu_P : E^{*} \rightarrow \mathbb{Z}$ which maps $f \in E^{*}$ to the
largest $k \in \mathbb{Z}$ such that $f \in p^kP$. Given 
$f \in E^{*}$, the {\sl divisor of} $f$ is $(f) = \sum{\nu_P(f)P}$
where the sum is taken over all the places. It has finite support
since $\nu_P(f) \ne 0$ if and only if $P$ is a pole or zero of
$f$. Finally, we say that a divisor $\delta = \sum{n_PP}$ is
{\sl principal} if $\delta=(f)$ for some $f \in E^{*}$. Note that if 
$\delta$ is principal, the $\sum{n_P}=0$, but the converse is not
generally true, except if $E=K(z)$. Trager's algorithm proceeds
essentially by constructing candidate divisors for the $u_i$'s of
\ref{Int14}: 
\begin{itemize}
\item Let $\sum_{i=1}^n A_iw_i$ be the numerator of $h$ with respect
to $w$, and $D$ be its (squarefree) denominator
\item Write $\sum_{i=1}^n A_iw_i=G/H$, where $G \in K[z,y]$ and 
$H \in K[z]$
\item Let $f \in K[z,y]$ be the (monic) minimum polynomial for $y$
over $K(z)$, $t$ be a new indeterminante and compute
\[
R(t)={\rm resultant_z\ }\left({\rm\ pp_t }\left({\rm\ resultant_y\ }\left(
G-tH\frac{dD}{dz},F\right)\right),D\right) \in K[t]
\]
\item Let $\alpha_1,\ldots,\alpha_s \in \overline{K}$ be the distinct
nonzero roots of $R$, $(q_1,\ldots,q_k)$ be a basis for the vector
space that they generate over $\mathbb{Q}$, write
$\alpha_i=r_{i1}q_1+\cdots+r_{ik}q_k$ for each $i$, where 
$r_{ij} \in \mathbb{Q}$ and let $m > 0$ be a common denominator for
all the $r_{ij}$'s
\item For $1 \le j \le k$, let 
$\delta_j=\sum_{i=1}^s mr_{ij}\sum_l r_lP_l$ where $r_l$ is the
ramification index of $P_l$ and $P_l$ runs over all the places at
which $h~dz$ has residue $r_i\alpha_i$
\item If there are nonzero integers $n_1,\ldots,n_k$ such that 
$n_j\delta_j$ is principal for each $j$, then let
\[
u=h-\frac{1}{m}\sum_{j=1}^k\frac{q_j}{n_ju_j}\frac{du_j}{dz}
\]
where $u_j \in E(\alpha_1,\ldots,\alpha_s)^{*}$ is such that 
$n_j\delta_j=(u_j)$. If $u=0$, then 
$\int{h~dz}=\sum_{j=1}^k q_j\log(u_j)/(mn_j)$, otherwise if either 
$u \ne 0$ or there is no such integer $n_j$ for at least one $j$,
then $h~dz$ has no elementary integral.
\end{itemize}
Note that this algorithm expresses the integral, when it is
elementary, with the smallest possible number of logarithms. Steps 3
to 6 requires computing in the splitting field $K_0$ of $R$ over $K$,
but it can be proven that, as in the case of rational functions, $K_0$
is the minimal algebraic extension of $K$ necessary to express the
integral in the form \ref{Int4}. Trager \cite{Trag84} 
describes a representation
of divisors as fractional ideals and gives algorithms for the
arithmetic of divisors and for testing whether a given divisor is
principal. In order to determine whether there exists an integer $N$
such that $N\delta$ is principal, we need to reduce the algebraic
extension to one over a finite field $\mathbb{F}_{p^q}$ for some
``good'' prime $p \in \mathbb{Z}$. Over $\mathbb{F}_{p^q}$, it is
known that for every divisor $\delta=\sum{n_PP}$ such that
$\sum{n_P}=0$, $M\delta$ is principal for some integer
$1 \le M \le (1+\sqrt{p^q})^{2g}$, where $g$ is the genus of the curve
\cite{Weil71}, so we compute such an $M$ by testing $M=1,2,3,\ldots$ until
we find it. It can then be shown that for almost all primes $p$, if
$M\delta$ is not principal in characteristic 0, the $N\delta$ is not
principal for any integer $N \ne 0$. Since we can test whether the
prime $p$ is ``good'' by testing whether the image in
$\mathbb{F}_{p^q}$ of the discriminant of the discriminant of the
minimal polynomial for $y$ over $K[z]$ is 0, this yields a complete
algorithm. In the special case of hyperelliptic extensions, {\sl i.e.}
simple radical extensions of degree 2, Bertrand \cite{Bert95} describes a
simpler representation of divisors for which the arithmetic and
principality tests are more efficient than the general methods.

\noindent
{\bf Example 4} {\sl
Continuing example 3, we were left with the integrand
\[
\frac{\sqrt{x^8+1}}{x(x^8+1)}=\frac{w_2}{x(x^8+1)} \in E
=\mathbb{Q}(x)[y]/(y^2-x^8-1)
\]
where $(w_1,w_2)=(1,y)$ is an integral basis normal at infinity, and
the denominator $D=x(x^8+1)$ of the integrand is squarefree. Its
numerator is $w_2=y$, so the resultant of step 3 is
\[
resultant_x(pp_t(resultant_y(y-t(9x^8+1),y^2-x^8-1)),x(x^8+1))=
ct^{16}(t^2-1)
\]
where $c$ is a large nonzero integer. Its nonzero roots are $\pm 1$,
and the integrand has residue 1 at the place $P$ corresponding to the
point $(x,y)=(0,1)$ and $-1$ at the place $Q$ corresponding to the
point $(x,y)=(0,-1)$, so the divisor $\delta_1$ of step 5 is 
$\delta_1=P-Q$. It turns out that $\delta_1$, $2\delta_1$, and 
$3\delta_1$ are not principal, but that
\[
4\delta_1=\left(\frac{x^4}{1+y}\right)\quad{\rm\ and\ }\quad
\frac{w_2}{x(x^8+1)}
-\frac{1}{4}\frac{(x^4/(1+y))^{\prime}}{x^4/(1+y)}=0
\]
which implies that
\[
\int{\frac{\sqrt{x^8+1}}{x(x^8+1)}}~dx
=\frac{1}{4}\log\left(\frac{x^4}{1+\sqrt{x^8+1}}\right)
\]}

\noindent
{\bf Example 5} {\sl
Consider
\[
\int{\frac{x~dx}{\sqrt{1-x^3}}}
\]
The integrand is 
\[
f=\frac{xy}{1-x^3} \in E =
\mathbb{Q}(x)[y]/(y^2+x^3-1)
\]
where $(w_1,w_2)=(1,y)$ is an integral basis normal at infinity, and
the denominaotr $D=1-x^3$ of the integrand is squarefree. Its
numerator is $xw_2=xy$, so the resultant of step 3 is
\[
resultant_x(pp_t(resultant_y(xy+3tx^2,y^2+x^3-1)),1-x^3)=729t^6
\]
whose only root is 0. Since $f \ne 0$, we conclude from step 6 that
$\int{f~dx}$ is not an elementary function.}

\noindent
{\bf Example 6} {\sl
\[
\int{\frac{dx}{x\sqrt{1-x^3}}}
\]
The integrand is
\[
f=\frac{y}{x-x^4} \in E =
\mathbb{Q}(x)[y]/(y^2+x^3-1)
\]
where $(w_1,w_2)=(1,y)$ is an integral basis normal at infinity, and
the denominator $D=x-x^4$ of the integrand is squarefree. Its
numerator is $w_2=y$, so the resultant of step 3 is
\[
resultant_x(pp_t(resultant_y(y+t(4x^3-1),y^2+x^3-1)),x-x^4)=729t^6(t^2-1)
\]
Its nonzero roots are $\pm 1$, and the integrand has residue 1 at the
place $P$ corrseponding to the point $(x,y)=(0,1)$ and $-1$ at the
place $Q$ corresponding to the point $(x,y)=(0,-1)$ so the divisor
$\delta_1$ of step 5 is $\delta_1=P-Q$. It turns out that $\delta_1$
and $2\delta_1$ are not principal, but that
\[
3\delta_1=\left(\frac{y-1}{y+1}\right)\quad{\rm and}\quad
\frac{y}{x-x^4}-\frac{1}{3}\frac{((y-1)/(y+1))^{\prime}}{(y-1)/(y+1)}=0
\]
which implies that
\[
\int{\frac{dx}{x\sqrt{1-x^3}}}
=\frac{1}{3}\log\left(\frac{\sqrt{1-x^3}-1}{\sqrt{1-x^3}+1}\right)
\]}

\section{Elementary Functions}
Let $f$ be an arbitrary elementary function. In order to generalize
the algorithms of the previous sections, we need to build an algebraic
model in which $f$ behaves in some sense like a rational or algebraic
function. For that purpose, we need to formally define differential
fields and elementary functions.

\subsection{Differential algebra}
A {\sl differential field} $(K,')$ is a field $K$ with a given map
$a \rightarrow a^{\prime}$ from $K$ into $K$, satisfying
$(a+b)^{\prime}=a^{\prime}+b^{\prime}$ and 
$(ab)^{\prime}=a^{\prime}b+ab^{\prime}$. Such a map is called a
{\sl derivation} on $K$. An element $a \in K$ which satisfies 
$a^{\prime}=0$ is called a {\sl constant}, and the set 
Const($K$)=$\{a \in K {\rm\ such\ that\ }a^{\prime}=0\}$ of all the
constants of $K$ is called a subfield of $K$.

A differential field $(E,')$ is a {\sl differential equation} of
$(K,')$ if $K \subseteq E$ and the derivation on $E$ extends the one
on $K$. In that case, an element $t \in E$ is a {\sl monomial} over
$K$ if $t$ is transcendental over $K$ and $t' \in K[t]$, which implies
that both $K[t]$ and $K(t)$ are closed under $^{\prime}$. An element $t \in E$
is {\sl elementary over} $K$ if either
\begin{itemize}
\item $t'=b'/b$ for some $b \in K^{*}$, in which case we say that $t$
is a {\sl logarithm} over $K$, and write $t=log(b)$, or
\item $t'=b't$ for some $b \in K^{*}$, in which case we say that $t$
is an {\sl exponential} over $K$, and write $t=e^b$, or
\item $t$ is algebraic over $K$
\end{itemize}

A differential extension $(E,')$ of $(K,')$ is {\sl elementary over}
$K$, if there exist $t_1,\ldots,t_m$ in $E$ such that
$E=K(t_1,\ldots,t_m)$ and each $t_i$ is elementary over
$K(t_1,\ldots,t_{i-1})$. We say that $f \in K$ {\sl has an elementary
integral} over $K$ if there exists an elementary extension $(F,')$ of
$(K,')$ and $g \in F$ such that $g'=f$. An {\sl elementary function}
of the variable $x$ is an element of an elementary extension of the
rational function field $(C(x),d/dx)$, where $C={\rm Const}(C(x))$.

Elementary extensions are useful for modeling any function as a
rational or algebraic function of one main variable over the other
terms present in the function: given an elementary integrand
$f(x)~dx$, the integration algorithm first constructs a field $C$
containing all the constants appearing in $f$, then the rational
function field $(C(x),d/dx)$, then an elementary tower 
$E=C(x)(t_1,\ldots,t_k)$ containing $f$. Note that such a tower is not
unique, and in addition, ajoining a logarithm could in fact adjoin a
new constant, and an exponential could in fact be algebraic, for
example $\mathbb{Q}(x)(log(x),log(2x))=\mathbb{Q}(log(2))(x)(log(x))$
and $\mathbb{Q}(x)(e^{log(x)/2})=\mathbb{Q}(x)(\sqrt{x})$. There are
however algorithms that detect all such occurences and modify the
tower accordingly \cite{Risc79}, so we can assume that all the logarithms
and exponentials appearing in $E$ are monomials, and that 
${\rm Const}(E)=C$. Let now $k_0$ be the largest index such that
$t_{k_0}$ is transcendental over $K=C(x)(t_1,\ldots,t_{k_0-1})$ and
$t=t_{k_0}$. Then $E$ is a finitely generated algebraic extension of
$K(t)$, and in the special case $k_0=k$, $E=K(t)$. Thus, $f \in E$ can
be seen as a univariate rational or algebraic function over $K$, the
major difference with the pure rational or algebraic cases being that
$K$ is not constant with respect to the derivation. It turns out that
the algorithms of the previous section can be generalized to such
towers, new methods being required only for the polynomial (or
integral) part. We note that Liouville's Theorem remains valid when
$E$ is an arbitrary differential field, so the integration algorithms
work by attempting to solve equation \ref{Int13} as previously.

\noindent
{\bf Example 7} {\sl
The function (1) is the element $f=(t-t^{-1})\sqrt{-1}/2$ of $E=K(t)$
where $K=\mathbb{Q}(\sqrt{-1})(x)(t_1,t_2)$ with
\[
t_1=\sqrt{x^3-x+1},\quad t_2=e^{2\sqrt{-1}(x^3-t_1)},\quad{\rm and}\quad
t=e^{((1-t_2)/(1+t_2))-x\sqrt{-1}}
\]
which is transcendental over $K$. Alternatively, it can also be
written as the element $f=2\theta/(1+\theta^2)$ of $F=K(\theta)$ where 
$K==\mathbb{Q}(x)(\theta_1,\theta_2)$ with
\[
\theta_1=\sqrt{x^3-x+1},\quad\theta_2=\tan(x^3-\theta_1),\quad{\rm
and}\quad\theta=\tan\left(\frac{x+\theta_2}{2}\right)
\]
which is a transcendental monomial over $K$. It turns out that both
towers can be used in order to integrate $f$.}

The algorithms of the previous sections relied extensively on
squarefree factorization and on the concept of squarefree
polynomials. The appropriate analogue in monomial extensions is the
notion of {\sl normal} polynomials: let $t$ be a monomial over $K$, we
say that $p\in K[t]$ is {\sl normal} (with respect to ') if
$\gcd(p,p')=1$, and that $p$ is {\sl special} if $\gcd(p,p')=p$, 
{\sl i.e.} $p | p'$ in $K[t]$. For $p \in K[t]$ squarefree, let 
$p_s=\gcd(p,p')$ and $p_n=p/p_s$. Then $p=p_sp_n$, while $p_s$ is
special and $p_n$ is normal. Therefore, squarefree factorization can
be used to write any $q \in K[t]$ as a product $q=q_sq_n$, where
$\gcd(q_s,q_n)=1$, $q_s$ is special and all the squarefree factors of
$q_n$ are normal. We call $q_s$ the {\sl special part} of $q$ and
$q_n$ its {\sl normal part}.

\subsection{The Hermite reduction}
The Hermite reductions we presented for rational and algebraic
functions work in exactly the same way algebraic extensions of
monomial extensions of $K$, as long as we apply them only to the
normal part of the denominator of the integrand. Thus, if $D$ is the
denominator of the integrand, we let $S$ be the special part of $D$,
$D_1D_2^2\ldots D_m^m$ be a squarefree factorization of the {\sl
normal} part of $D$, $V=D_m$, $U=D/V^m$ and the rational and algebraic
Hermite reductions proceed normally, eventually yielding an integrand
whose denominator has a squarefree normal part.

\noindent
{\bf Example 8} {\sl
Consider
\[
\int{\frac{x-\tan(x)}{\tan(x)^2}}~dx
\]
The integrand is
\[
f=\frac{x-t}{t^2} \in K(t)\quad {\rm where\ }
K=\mathbb{Q}(x) {\rm\ and\ }t'=t^2+1
\]
Its denominator is $D=t^2$, and $\gcd(t,t')=1$ implying that $t$ is
normal, so $m=2$, $V=t$, $U=D/t^2=1$, and the extended Euclidean
algorithm yields
\[
\frac{A}{1-m}=t-x=-x(t^2+1)+(xt+1)t=-xUV'+(xt+1)V
\]
implying that
\[
\int{\frac{x-\tan(x)}{\tan(x)^2}}~dx=-\frac{x}{\tan(x)}-\int{x}~dx
\]
and the remaining integrand has a squarefree denominator.}

\noindent
{\bf Example 9} {\sl
Consider
\[
\int{\frac{\log(x)^2+2x\log(x)+x^2+(x+1)\sqrt{x+\log(x)}}
{x\log(x)^2+2x^2\log(x)+x^3}}~dx
\]
The integrand is
\[
f=\frac{t^2+2xt+x^2+(x+1)y}{xt^2+2x^2t+x^3} \in E
=K(t)[y]/(y^2-x-t)
\]
where $K=\mathbb{Q}(x)$ and $t=log(x)$. The denominator of $f$ with
respect to the basis $w=(1,y)$ is $D=xt^2+2x^2t+x^3$ whose squarefree
factorization is $x(t+x)^2$. Both $x$ and $t+x$ are normal, so $m=2$,
$V=t+x$, $U=D/V^2=x$, and the solution \ref{Int12} of \ref{Int8} is
\[
f_1=\frac{t^2+2xt+x^2}{x(-(t'+1))}
=-\frac{t^2+2xt+x^2}{x+1},
\]
\[
f_2=\frac{x+1}{x\left((t+x)\frac{1}{2}\frac{t^{\prime}+1}{t+z}-
(t'+1)\right)}=-2
\]
We have $Q=1$, so $0V+1Q=1$, $A=0$, $R=1$, $RQf_1=f_1=-V^2/(x+1)$ and
$RQf_2=f_2=0V-2$, so $B=-2y$ and
\[
h=f-\left(\frac{B}{V}\right)^{\prime}=\frac{1}{x}
\]
implying that
\[
\int{\frac{\log(x)^2+2x\log(x)+x^2+(x+1)\sqrt{x+\log(x)}}
{x\log(x)^2+2x^2\log(x)+x^2}}~dx
=\frac{2}{\sqrt{x+log(x)}}+\int{\frac{dx}{x}}
\]
and the remaining integrand has a squarefree denominator.}

\subsection{The polynomial reduction}
In the transcendental case $E=K(t)$ and when $t$ is a monomial
satisfying ${\rm deg}_t(t') \ge 2$, then it is possible to reduce the
degree of the polynomial part of the integrand until it is smaller
than ${\rm deg}_t(t')$. In the case when $t=\tan(b)$ for some $b \in K$, then
it is possible either to prove that the integral is not elementary, or
to reduce the polynomial part of the integrand to be in $K$. Let 
$f \in K(t)$ be our integrand and write $f=P+A/D$, where
$P,A,D \in K[t]$ and ${\rm deg}(A) < {\rm deg}(D)$. Write 
$P=\sum_{i=1}^e p_it^i$ and $t'=\sum_{i=0}^d c_it^i$ where
$p_0,\ldots,p_e,c_0,\ldots,c_d \in K$, $d \ge 2$, $p_e\ne 0$ and 
$c_d\ne 0$. It is easy to verify that if $e \ge d$, then
\begin{equation}\label{Int15}
P=\left(\frac{a_e}{(e-d+1)c_d}t^{e-d+1}\right)^{\prime}+\overline{P}
\end{equation}
where $\overline{P} \in K[t]$ is such that $\overline{P}=0$ or 
${\rm deg}_t(\overline{P}) < e$. Repeating the above transformation we
obtain $Q,R \in K[t]$ such that $R=0$ or ${\rm deg}_t(R) < d$ and $P=Q'+R$. 
Write then $R=\sum_{i=0}^{d-1} r_it^i$ where
$r_0,\ldots,r_{d-1} \in K$. Again, it is easy to verify that for any
{\sl special} $S \in K[t]$ with ${\rm deg}_t(S) > 0$, we have
\[
R=\frac{1}{{\rm deg}_t(S)}\frac{r_{d-1}}{c_d}\frac{S'}{S}+\overline{R}
\]
where $\overline{R} \in K[t]$ is such that $\overline{R}=0$ or
${\rm deg}_t(\overline{R}) < e-1$. Furthermore, it can be proven \cite{Bron97}
that if $R+A/D$ has an elementary integral over $K(t)$, then 
$r_{d-1}/{c_d}$ is a constant, which implies that
\[
\int{R}=\frac{1}{{\rm deg}_t(S)}\frac{r_{d-1}}{c_d}\log(S)
+\int\left(\overline{R}+\frac{A}{D}\right)
\]
so we are left with an integrand whose polynomial part has degree at
most ${\rm deg}_t(t')-2$. In this case $t=\tan(b)$ for $b \in K$, then
$t'=b't^2+b'$, so $\overline{R} \in K$.

{\bf Example 10} {\sl
Consider
\[
\int(1+x\tan(x)+\tan(x)^2)~dx
\]
The integrand is
\[
f=1+xt+t^2 \in K(t)\quad{\rm where\ }K=\mathbb{Q}(x)
{\rm\ and\ }t'=t^2+1
\]
Using \ref{Int15}, we get $\overline{P}=f-t'=f-(t^2+1)=xt$ so
\[
\int(1+x\tan(x)+\tan(x)^2)~dx=\tan(x)+\int{x\tan(x)}~dx
\]
and since $x'\ne 0$, the above criterion imples that the remaining
integral is not an elementary function.}

\subsection{The residue criterion}
Similarly to the Hermite reduction, the Rothstein-Trager and
Lazard-Rioboo-Trager algorithms are easy to generalize to the
transcendental case $E=K(t)$ for arbitrary monomials $t$: let 
$f\in K(t)$ be our integrand and write $f=P+A/D+B/S$ where
$P,A,D,B,S \in K[t]$, ${\rm deg}(A) < {\rm deg}(D)$, $S$ is special and, following
the Hermite reduction, $D$ is normal. Let then $z$ be a new
indeterminate, $\kappa : K[z] \rightarrow K[z]$ be give by 
$\kappa(\sum_i a_iz^i)=\sum_i a_i^{\prime}z^i$,
\[
R={\rm resultant_t}(D,A-zD') \in K[z]
\]
be the Rothstein-Trager resultant, $R=R_1R_2^2\ldots R_k^k$ be its
squarefree factorization, $Q_i=\gcd_z(R_i,\kappa(R_i))$ for each $i$,
and 
\[
g=\sum_{i=1}^k\sum_{a|Q_i(a)=0} a\log(\gcd{}_t(D,A-aD'))
\]

Note that the roots of each $Q_i$ must all be constants, and that the
arguments of the logarithms can be obtained directly from the
subresultant PRS of $D$ and $A-zD'$ as in the rational function
case. It can then be proven \cite{Bron97} that
\begin{itemize}
\item $f-g'$ is always ``simpler'' than $f$
\item the splitting field of $Q_1\cdots Q_k$ over $K$ is the minimal
algebraic extension of $K$ needed in order to express $\int f$ in the
form \ref{Int4}
\item if $f$ has an elementary integral over $K(t)$, then 
$R | \kappa(R)$ in $K[z]$ and the denominator of $f-q'$ is special
\end{itemize}
Thus, while in the pure rational function case the remaining integrand
is a polynomial, in this case the remaining integrand has a special
denominator. In that case we have additionally that if its integral is
elementary, then \ref{Int13} has a solution such that $v\in K(t)$ has a
special denominator, and each $u_i \in K(c_1,\ldots,c_k)[t]$ is
special.

\noindent
{\bf Example 11} {\sl
Consider
\[
\int{\frac{2\log(x)^2-\log(x)-x^2}{\log(x)^3-x^2\log(x)}}~dx
\]
The integrand is
\[
f=\frac{2t^2-t-x^2}{t^2-xt^2} \in K(t)\quad
{\rm where\ }K=\mathbb{Q}(x){\rm\ and\ }t=\log(x)
\]
Its denominator is $D=t^3-x^2t$, which is normal, and the resultant is 
\[
\begin{array}{ccl}
R&=&\displaystyle
resultant_t\left(t^3-x^2t,\frac{2x-3z}{x}t^2+(2xz-1)t+x(z-x)\right)\\
&&\\
&=&\displaystyle
4x^3(1-x^2)\left(z^3-xz^2-\frac{1}{4}z+\frac{x}{4}\right)
\end{array}
\]
which is squarefree in $K[z]$. We have
\[
\kappa(R)=-x^2(4(5x^2+3)z^3+8x(3x^2-2)z^2+(5x^2-3)z-2x(3x^2-2))
\]
so
\[
Q_1=\gcd{}_z(R,\kappa R)=x^2\left(z^2-\frac{1}{4}\right)
\]
and
\[
\gcd{}_t\left(t^3+x^2t,\frac{2x-3a}{x}t^2+(2xa-1)t+x(a-x)\right)=t+2ax
\]
where $a^2-1/4=0$, whence
\[
g=\sum_{a|a^2-1/4=0} a\log(t+2ax)=\frac{1}{2}\log(t+x)-\frac{1}{2}\log(t-x)
\]
Computing $f-g'$ we find
\[
\int{\frac{2\log(x)^2-\log(x)-x^2}{\log(x)^3-x^2\log(x)}}~dx
=\frac{1}{2}\log\left(\frac{\log(x)+x}{\log(x)-x}\right)
+\int{\frac{dx}{\log(x)}}
\]
and since ${\rm deg}_z(Q_1) < {\rm deg}_z(R)$, it follows that the remaining
integral is not an elementary function (it is in fact the logarithmic
integral $Li(x)$).}

In the most general case, when $E=K(t)(y)$ is algebraic over $K(t)$ and
$y$ is integral over $K[t]$, the criterion part of the above result
remains valid: let $w=(w_1,\ldots,w_n)$ be an integral basis for $E$
over $K(t)$ and write the integrand $f \in E$ as 
$f=\sum_{i=1}^n A_iw_i/D+\sum_{i=1}^n B_iw_i/S$ where $S$ is special
and, following the Hermite reduction, $D$ is normal. Write 
$\sum_{i=1}^n A_iw_i=G/H$, where $G \in K[t,y]$ and $H \in K[t]$, let
$F \in K[t,y]$ be the (monic) minimum polynomial for $y$ over $K(t)$,
$z$ be a new indeterminante and compute
\begin{equation}\label{Int16}
R(z)={\rm resultant_t}({\rm pp_z}({\rm resultant_y}(G-tHD',F)),D) \in K[t]
\end{equation}
It can then be proven \cite{Bron90c} that if $f$ has an elementary integral
over $E$, then $R|\kappa(R)$ in $K[z]$.

{\bf Example 12} {\sl
Consider
\begin{equation}\label{Int17}
\int{\frac{\log(1+e^x)^{(1/3)}}{1+\log(1+e^x)}}~dx
\end{equation}
The integrand is
\[
f=\frac{y}{t+1} \in E = K(t)[y]/(y^3-t)
\]
where $K=\mathbb{Q}(x)(t_1)$, $t_1=e^x$ and $t=\log(1+t_1)$. Its
denominator with respect to the integral basis $w=(1,y,y^2)$ is
$D=t+1$, which is normal, and the resultant is
\[
R={\rm resultant_t}({\rm pp_z}({\rm resultant_y}(y-zt_1/(1+t_1),y^3-t)),t+1)
=-\frac{t_1^3}{(1+t_1)^3}z^3-1
\]
We have
\[
\kappa(R)=-\frac{3t_1^3}{(1+t_1)^4}z^3
\]
which is coprime with $R$ in $K[z]$, implying that the integral \ref{Int17}
is not an elementary function.
}

\subsection{The transcendental logarithmic case}
Suppose now that $t=\log(b)$ for some $b \in K^{*}$, and that
$E=K(t)$. Then, every special polynomial must be in $K$, so, following
the residue criterion, we must look for a solution $v \in K[t]$,
$u_1,\ldots,u_k \in K(c_1,\ldots,c_n)^{*}$ of \ref{Int13}. Furthermore, the
integrand $f$ is also in $K[t]$, so write
$f=\sum_{i=0}^d f_it^i$ where $f_0,\ldots,f_d \in K$ and $f_d \ne 0$. We
must have ${\rm deg}{}_t(v) \le d+1$, so writing $v=\sum_{i=0}^{d+1} v_it^i$,
we get 
\[
\int f_dt^d+\cdots+f_1t+f_0=v_{d+1}t^{d+1}+\cdots+v_1t+v_0
+\sum_{i=1}^k c_i\log(u_i)
\]
If $d=0$, then the above is simply an integration problem for 
$f_0 \in K$, which can be solved recursively. Otherwise,
differentiating both sides and equating the coefficients of $t^d$, we
get ${v_{d+1}}'=0$ and
\begin{equation}\label{Int18}
f_d=v_d'+(d+1)v_{d+1}\frac{b'}{b}
\end{equation}
Since $f_d \in K$, we can recursively apply the integration algorithm
to $f_d$, either proving that \ref{Int18} has no solution, in which case $f$
has no elementary integral, or obtaining the constant $v_{d+1}$, and
$v_d$ up to an additive constant (in fact, we apply recursively a
specialized version of the integration algorithm to equations of the
form \ref{Int18}, see \cite{Bron97} for details). Write then
$v_d=\overline{v_d}+c_d$ where $\overline{v_d} \in K$ is known and 
$c_d \in {\rm Const}(K)$ is undetermined. Equating the coefficients of
$t^{d-1}$ yields
\[
f_{d-1}-d\overline{v_d}\frac{b'}{b}={v_{d-1}}'+dc_d\frac{b'}{b}
\]
which is an equation of the form \ref{Int18}, so we again recursively compute
$c_d$ and $v_{d-1}$ up to an additive constant. We repeat this process
until either one of the recursive integrations fails, in which case $f$
has no elementary integral, or we reduce our integrand to an element
of $K$, which is then integrated recursively. The algorithm of this
section can also be applied to real arc-tangent extensions, {\sl i.e.}
$K(t)$ where $t$ is a monomial satisfying $t'=b'/(1+b^2)$ for some 
$b \in K$.

\subsection{The transcendental exponential case}
Suppose now that $t=e^b$ for some $b \in K$, and that $E =
K(t)$. Then, every nonzero special polynomial must be of the form
$at^m$ for $a \in K^{*}$ and $m \in \mathbb{N}$. Since
\[
\frac{(at^m)'}{at^m}=\frac{a'}{a}+m\frac{t'}{t}=\frac{a'}{a}+mb'
\]
we must then look for a solution $v\in K[t,t^{-1}]$,
$u_1,\ldots,u_k \in K(c_1,\ldots,c_n)^{*}$ of \ref{Int13}. Furthermore, the
integrand $f$ is also in $K[t,t^{-1}]$, so write
$f=\sum_{i=e}^d f_it^i$ where $f_e,\ldots,f_d \in K$ and 
$e,d\in \mathbb{Z}$. Since $(at^{m})'=(a'+mb')t^m$ for any 
$m\in \mathbb{Z}$, we must have $v=Mb+\sum_{i=e}^d v_it^i$ for some
integer $M$, hence
\[
\int\sum_{i=e}^d f_it^i=Mb+\sum_{i=e}^d v_it^i+\sum_{i=1}^k c_i\log(u_i)
\]
Differentiating both sides and equating the coefficients of each power
to $t^d$, we get
\[
f_0=(v_0+Mb)'+\sum_{i=1}^k c_i\frac{u_i^{\prime}}{u_i}
\]
which is simply an integration problem for $f_0 \in K$, and
\[
f_i=v_i^{\prime}+ib'v_i\quad{\rm for\ }e \le i \le d, i \ne 0
\]

The above problem is called a {\sl Risch differential equation over K}. 
Although solving it seems more complicated than solving $g'=f$, it
is actually simpler than an integration problem because we look for
the solutions $v_i$ in $K$ only rather than in an extension of
$K$. Bronstein \cite{Bron90c,Bron91a,Bron97} and Risch
\cite{Risc68,Risc69a,Risc69b} describe algorithms for solving this type
of equation when $K$ is an elementary extension of the rational
function field.

\subsection{The transcendental tangent case}
Suppose now that $t=\tan(b)$ for some $b \in K$, {\sl i.e.}
$t'=b'(1+t^2)$, that $\sqrt{-1} \notin K$ and that $E=K(t)$. Then,
every nonzero special polynomial must be of the form $a(t^2+1)^m$ for
$a \in K^{*}$ and $m \in \mathbb{N}$. Since
\[
\frac{(a(t^2+1)^m)'}{a(t^2+1)^m}
=\frac{a'}{a}+m\frac{(t^2+1)'}{t^2+1}
=\frac{a'}{a}+2mb't
\]
we must look for $v=V/(t^2+1)^m$ where $V\in K[t]$, 
$m_1,\ldots,m_k \in \mathbb{N}$, constants 
$c_1,\ldots,c_k \in \overline{K}$ and
$u_1,\ldots,u_k \in K(c_1,\ldots,c_k)^{*}$ such that
\[
f=v'+2b't\sum_{i=1}^k c_im_i + \sum_{i=1}^k c_i\frac{u_i^{\prime}}{u_i}
\]
Furthermore, the integrand $f \in K(t)$ following the residue
criterion must be of the form $f=A/(t^2+1)^M$ where $A \in K[t]$ and
$M \ge 0$. If $M > 0$, it can be shown that $m=M$ and that
\begin{equation}\label{Int19}
\left(
\begin{array}{c}
c'\\
d'
\end{array}
\right)+
\left(
\begin{array}{cc}
0&-2mb'\\
2mb'&0\\
\end{array}
\right)
\left(
\begin{array}{c}
c\\
d
\end{array}
\right)=
\left(
\begin{array}{c}
a\\
b
\end{array}
\right)
\end{equation}
where $at+b$ and $ct+d$ are the remainders module $t^2+1$ of $A$ and
$V$ respectively. The above is a coupled differential system, which
can be solved by methods similar to the ones used for Risch
differential equations \cite{Bron97}. If it has no solution, then the
integral is not elementary, otherwise we reduce the integrand to 
$h \in K[t]$, at which point the polynomial reduction either proves
that its integral is not elementary, or reduce the integrand to an
element of $K$, which is integrated recursively.

\noindent
{\bf Example 13} {\sl
Consider
\[
\int{\frac{sin(x)}{x}}~dx
\]
The integrand is
\[
f=\frac{2t/x}{t^2+1} \in K(t)\quad{\rm where\ }K=\mathbb{Q}(x)
{\rm\ and\ }t=\tan\left(\frac{x}{2}\right)
\]
Its denominator is $D=t^2+1$, which is special, and the system \ref{Int19}
becomes 
\[
\left(
\begin{array}{c}
c'\\
d'
\end{array}
\right)+
\left(
\begin{array}{cc}
0&-1\\
1&0\\
\end{array}
\right)
\left(
\begin{array}{c}
c\\
d
\end{array}
\right)=
\left(
\begin{array}{c}
2/x\\
0
\end{array}
\right)
\]
which has no solution in $\mathbb{Q}(x)$, implying that the integral
is not an elementary function.}

\subsection{The algebraic logarithmic case}
The transcendental logarithmic case method also generalizes to the
case when $E=K(t)(y)$ is algebraic over $K(t)$, $t=log(b)$ for 
$b \in K^{*}$ and $y$ is integral over $K[t]$: following the residue
criterion, we can assume that $R | \kappa(R)$ where $R$ is given by
\ref{Int16}, hence that all its roots in $\overline{K}$ are constants. The
polynomial part of the integrand is replace by a family of at most
$[E : K(t)]$ Puiseux expansions at infinity, each of the form
\begin{equation}\label{Int20}
a_{-m}\theta^{-m}+\cdots+a_{-1}\theta^{-1}+\sum_{i \ge 0} a_i\theta^i
\end{equation}
where $\theta^r=t^{-1}$ for some positive integer $r$. Applying the
integration algorithm recursively to $a_r \in \overline{K}$, we can
test whether there exist $\rho \in {\rm Const}(\overline{K})$ and 
$v \in \overline{K}$ such that
\[
a_r=v'+\rho\frac{b'}{b}
\]
If there are no such $v$ and $c$ for at least one of the series, then
the integral is not elementary, otherwise $\rho$ is uniquely
determined by $a_r$, so let $\rho_1,\ldots,\rho_q$ where 
$q \le [E : K(t)]$ be the distinct constants we obtain,
$\alpha_1,\ldots,\alpha_s \in \overline{K}$ be the distinct nonzero
roots of $R$, and $(q_1,\ldots,q_k)$ be a basis for the vector space
generated by the $\rho_i$'s and $\alpha_i$'s over $\mathbb{Q}$. Write
$\alpha_i=r_{i1}q_1+\cdots+r_{ik}q_k$ and 
$\rho_i=s_{i1}q_1+\cdots+s_{ik}q_k$ for each $i$, where 
$r_{ij},s_{ij} \in \mathbb{Q}$ and let $m > 0$ be a common denominator
for all the $r_{ij}$'s and $s_{ij}$'s. For $1 \le j \le k$, let
\[
\delta_j=\sum_{i=1}^s mr_{ij} \sum_l r_lP_l 
-\sum_{i=1}^q ms_{ij} \sum_l s_lQ_l
\]
where $r_l$ is the ramification index of $P_l$, $s_l$ is the
ramification index of $Q_l$, $P_l$ runs over all the finite places at
which $h~dz$ has residue $r_l\alpha_i$ and $Q_l$ runs over all the
infinite places at which $\rho=\rho_i$. As in the pure algebraic case,
if there is a $j$ for which $N\delta_j$ is not principal for any
nonzero integer $N$, then the integral is not elementary, otherwise,
let $n_1,\ldots,n_k$ be nonzero integers such that $n_j\delta_j$ is
principal for each $j$, and
\[
h=f-\frac{1}{m}\sum_{j=1}^k\frac{q_j}{n_j}\frac{u_j^{\prime}}{u_j}
\]
where $f$ is the integrand and 
$u_j \in E(\alpha_1,\ldots,\alpha_s,\rho_1,\ldots,\rho_q)^{*}$ is such
that $n_j\delta_j=(u_j)$. If the integral of $h$ is elementary, then
\ref{Int13} must have a solution with $v \in {\bf O}_{K[x]}$ and
$u_1,\ldots,u_k \in \overline{K}$ so we must solve
\begin{equation}\label{Int21}
h=\frac{\sum_{i=1}^n A_iw_i}{D}
=\sum_{i=1}^n v_i^{\prime}w_i+\sum_{i=1}^n v_iw_i^{\prime}
+\sum_{i=1}^k c_i\frac{u_i^{\prime}}{u_i}
\end{equation}
for $v_1,\ldots,v_n \in K[t]$, constants 
$c_1,\ldots,c_n \in \overline{K}$ and
$u_1,\ldots,u_k \in \overline{K}^{*}$ where 
$w=(w_1,\ldots,w_n)$ is an integral basis for $E$ over $K(t)$.

If $E$ is a simple radical extension of $K(t)$, and we use the basis
\ref{Int11} and the notation of that section, then $w_1=1$ and
\begin{equation}\label{Int22}
w_i^{\prime}=\left(\frac{i-1}{n}\frac{H'}{H}-
\frac{D_{i-1}^{\prime}}{D_{i-1}}\right)w_i
\quad{\rm for\ }1 \le i \le n
\end{equation}
This implies that \ref{Int21} becomes
\begin{equation}\label{Int23}
\frac{A_1}{D}=v_1^{\prime}+\sum_{i=1}^k c_i\frac{u_i^{\prime}}{u_i}
\end{equation}
which is simply an integration problem for $A_1/D \in K(t)$, and
\begin{equation}\label{Int24}
\frac{A_i}{D}=v_i^{\prime}+\left(\frac{i-1}{n}\frac{H'}{H}
-\frac{D_{i-1}^{\prime}}{D_{i-1}}\right)v_i\quad{\rm for\ }1 < i \le n
\end{equation}
which are Risch differential equations over $K(t)$

\noindent
{\bf Example 14} {\sl
Consider
\[
\int{\frac{(x^2+2x+1)\sqrt{x+\log(x)}+(3x+1)\log(x)+3x^2+x}
{(x\log(x)+x^2)\sqrt{x+\log(x)}+x^2\log(x)+x^3}}~dx
\]
The integrand is
\[
f=\frac{((3x+1)t-x^3+x^2)y-(2x^2-x-1)t-2x^3+x^2+x}
{xt^2-(x^3-2x^2)t-x^4+x^3} \in E = K(t)[y]/(F)
\]
where $F=y^2-x-t$, $K=\mathbb{Q}(x)$ and $t=\log(x)$. Its denominator
with respect to the integral basis $w=(1,y)$ is
$D=xt^2-(x^3-2x^2)t-x^4+x^3$, which is normal, and the resultant is
\[
\begin{array}{ccl}
R&=&{\rm resultant_t}({\rm pp_z}({\rm resultant_y}(((3x+1)t-x^3+x^2)y\\
&&\\
&&\hbox{\hskip 2.0cm}
-(2x^2-x-1)t-2x^3+x^2+x-zD^{\prime},F)),D)\\
&&\\
&=&x^{12}(2x+1)^2(x+1)^2(x-1)^2z^3(z-2)\\
\end{array}
\]
We have
\[
\kappa(R)=\frac{36x^3+16x^2-28x-12}{x(2x+1)(x+1)(x-1)}R
\]
so $R | \kappa(R)$ in $K[z]$. Its only nonzero root is 2, and the
integrand has residue 2 at the place $P$ corresponding to the point
$(t,y)=(x^2-x,-x)$. There is only one place $Q$ at infinity of
ramification index 2, and the coefficient of $t^{-1}$ in the Puiseux
expansion of $f$ at $Q$ is
\[
a_2=1-2x+\frac{1}{x}=(x-x^2)'+\frac{x'}{x}
\]
which implies that the corresponding $\rho$ is 1. Therefore, the
divisor for the logand is $\delta=2P-2Q$. It turns out that 
$\delta=(u)$ where $u=(x+y)^2 \in E^{*}$, so the new integrand is
\[
h=f-\frac{u'}{u}=f-2\frac{(x+y)'}{x+y}=\frac{(x+1)y}{xt+x^2}
\]
We have $y^2=t+x$, which is squarefree, so \ref{Int23} becomes
\[
0=v_1^{\prime}+\sum_{i=1}^k c_i\frac{u_i^{\prime}}{u_i}
\]
whose solution is $v_1=k=0$ and \ref{Int24} becomes
\[
\frac{x+1}{xt+x^2}=v_2^{\prime}+\frac{x+1}{2xt+2x^2}v_2
\]
whose solution is $v_2=2$, implying that $h=2y'$, hence that
\[
\begin{array}{l}
\displaystyle
\int{\frac{(x^2+2x+1)\sqrt{x+\log(x)}+(3x+1)\log(x)+3x^2+x}
{(x\log(x)+x^2)\sqrt{x+\log(x)}+x^2\log(x)+x^3}}~dx =\\
\\
\displaystyle
\hbox{\hskip 4.0cm}2\sqrt{x+\log(x)}+2\log\left(x+\sqrt{x+\log(x)}\right)
\end{array}
\]}
In the general case when $E$ is not a radical extension of $K(t)$, 
\ref{Int21} is solved by bounding ${\rm deg}_t(v_i)$ and comparing the Puiseux
expansions at infinity of $\sum_{i=1}^n v_iw_i$ with those of the form
\ref{Int20} of $h$, see \cite{Bron90c,Risc68} for details.

\subsection{The algebraic exponential case}
The transcendental exponential case method also generalizes to the
case when $E=K(t)(y)$ is algebraic over $K(t)$, $t=e^b$ for $b \in K$
and $y$ is integral over $K[t]$: following the residue criterion, we
can assume that $R|\kappa(R)$ where $R$ is given by \ref{Int16}, hence that
all its roots in $\overline{K}$ are constants. The denominator of the
integrand must be of the form $D=t^mU$ where $\gcd(U,t)=1$, $U$ is
squarefree and $m \ge 0$.

If $m > 0$, $E$ is a simple radical extension of $K(t)$, and we use the
basis \ref{Int11}, then it is possible to reduce the power of $t$ appearing
in $D$ by a process similar to the Hermite reduction: writing the
integrand $f=\sum_{i=1}^n A_iw_i/(t^mU)$, we ask whether we can
compute $b_1,\ldots,b_n \in K$ and $C_1,\ldots,C_n \in K[t]$ such that
\[
\int\frac{\sum_{i=1}^n A_iw_i}{t^mU}
=\frac{\sum_{i=1}^n b_iw_i}{t^m}
+\int{\frac{\sum_{i=1}^n C_iw_i}{t^{m-1}U}}
\]
Differentiating both sides and multiplying through by $t^m$ we get
\[
\frac{\sum_{i=1}^n A_iw_i}{U}
=\sum_{i=1}^n b_i^{\prime}w_i+\sum_{i=1}^n b_iw_i^{\prime}
-mb'\sum_{i=1}^n b_iw_i+\frac{t\sum_{i=1}^n C_iw_i}{U}
\]
Using \ref{Int22} and equating the coefficients of $w_i$ on both sides, we
get
\begin{equation}\label{Int25}
\frac{A_i}{U}=b_i^{\prime}+(\omega_i-mb')b_i+\frac{tC_i}{U}
\quad{\rm for\ }1 \le i \le n
\end{equation}
where
\[
\omega_i=\frac{i-1}{n}\frac{H'}{H}-\frac{D_{i-1}^{\prime}}{D_{i-1}} \in K(t)
\]
Since $t'/t=b' \in K$, it follows that the denominator of $\omega_i$
is not divisible by $t$ in $K[t]$, hence, evaluating \ref{Int25} at $t=0$, we
get 
\begin{equation}\label{Int26}
\frac{A_i(0)}{U(0)}=b_i^{\prime}+(\omega_i(0)-mb')b_i
\quad{\rm for\ }1 \le i \le n
\end{equation}
which are Risch differential equations over $K(t)$. If any of them has
no solution in $K(t)$, then the integral is not elementary, otherwise
we repeat this process until the denominator of the integrand is
normal. We then perform the change of variable $\overline{t}=t^{-1}$,
which is also exponential over $K$ since
$\overline{t}'=-b'\overline{t}$, and repeat the above process in order
to eliminate the power of $\overline{t}$ from the denominator of the
integrand. It can be shown that after this process, any solution of
\ref{Int13} must have $v \in K$.

\noindent
{\bf Example 15} {\sl
Consider
\[
\int{\frac{3(x+e^x)^{(1/3)}+(2x^2+3x)e^x+5x^2}{x(x+e^x)^{(1/3)}}}~dx
\]
The integrand is
\[
f=\frac{((2x^2+3x)t+5x^2)y^2+3t+3x}{xt+x^2} \in E
=K(t)[y]/(y^3-t-x)
\]
where $K=\mathbb{Q}(x)$ and $t=e^x$. Its denominator with respect to
the integral basis $w=(1,y,y^2)$ is $D=xt+x^2$, which is normal, and the
resultant is
\[
\begin{array}{l}
R={\rm resultant_t}({\rm pp_z}({\rm resultant_y}
(((2x^2+3x)t+5x^2)y^2+3t+3x-zD',\\
\hbox{\hskip 5.0cm}y^3-t-x)),D)=x^8(1-x)^3z^3
\end{array}
\]
We have
\[
\kappa(R)=\frac{11x-8}{x(x-1)}R
\]
so $R|\kappa(R)$ in $K[z]$, its only root being 0. Since $D$ is not
divisible by $t$, let $\overline{t}=t^{-1}$ and $z=\overline{t}y$. We
have $\overline{t}'=-\overline{t}$ and 
$z^3-\overline{t}^2-x\overline{t}^3=0$, so the integral basis \ref{Int11} is
\[
\overline{w}=(\overline{w}_1,\overline{w}_2,\overline{w}_3)
=\left(1,z,\frac{z^2}{\overline{t}}\right)
\]
Writing $f$ in terms of that basis gives
\[
f=\frac{3x\overline{t}^2+3\overline{t}
+(5x^2\overline{t}+2x^2+3x)\overline{w}_3}
{x^2\overline{t}^2+x\overline{t}}
\]
whose denominator $\overline{D}=\overline{t}(x+x^2\overline{t})$ is
divisible by $\overline{t}$. We have
$H=\overline{t}^2(1+x\overline{t})$ so $D_0=D_1=1$ and
$D_2=\overline{t}$, implying that
\[
\omega_1=0, \omega_2=\frac{(1-3x)\overline{t}-2}{3x\overline{t}+3},
{\rm\ and\ } \omega_3=\frac{(2-3x)\overline{t}-1}{3x\overline{t}+3}
\]
Therefore the equations \ref{Int26} become
\[
0=b_1^{\prime}+b_1,0=b_2^{\prime}+\frac{1}{3}b_2,{\rm\ and\ }
2x+3=b_3^{\prime}+\frac{2}{3}b_3
\]
whose solutions are $b_1=b_2=0$ and $b_3=3x$, implying that the new
integrand is
\[
h=f-\left(\frac{3x\overline{w}_3}{\overline{t}}\right)^{\prime}=\frac{3}{x}
\]
hence that
\[
\int{\frac{3(x+e^x)^{(1/3)}+(2x^2+3x)e^x+5x^2}{x(x+e^x)^{(1/3)}}}~dx
=3x(x+e^x)^{(2/3)}+3\int{\frac{dx}{x}}
\]
}

In the general case when $E$ is not a radical extension of $K(t)$,
following the Hermite reduction, any solution of \ref{Int13} must have
$v=\sum_{i=1}^n v_iw_i/t^m$ where $v_1,\ldots,v_m \in K[t]$. We can
compute $v$ by bounding ${\rm deg}_t(v_i)$ and comparing the Puiseux
expansions at $t=0$ and at infinity of $\sum_{i=1}^n v_iw_i/t^m$ with
those of the form \ref{Int20} of the integrand, 
see \cite{Bron90c,Risc68} for details.

Once we are reduced to solving \ref{Int13} for $v \in K$, constants
$c_1,\ldots,c_k \in \overline{K}$ and 
$u_1,\ldots,u_k \in E(c_1,\ldots,c_k)^{*}$, constants
$\rho_1,\ldots,\rho_s \in \overline{K}$ can be determined at all the
places above $t=0$ and at infinity in a manner similar to the
algebraic logarithmic case, at which point the algorithm proceeds by
constructing the divisors $\delta_j$ and the $u_j$'s as in that
case. Again, the details are quite technical and can be found in 
\cite{Bron90c,Risc68,Risc69a}.

\chapter{Singular Value Decomposition}
\section{Singular Value Decomposition Tutorial}

When you browse standard web sources like Wikipedia to learn about 
Singular Value Decomposition \cite{Puff09} 
or SVD you find many equations, but 
not an intuitive explanation of what it is or how it works. SVD 
is a way of factoring matrices into a series of linear approximations 
that expose the underlying structure of the matrix. Two important 
properties are that the linear factoring is exact and optimal. Exact 
means that the series of linear factors, added together, exactly 
equal the original matrix. Optimal means that, for the standard 
means of measuring matrix similarity (the Frobenius norm), these 
factors give the best possible linear approximation at each step 
in the series.

SVD is extraordinarily useful and has many applications such as 
data analysis, signal processing, pattern recognition, image 
compression, weather prediction, and Latent Sematic Analysis or 
LSA (also referred to as Latent Semantic Indexing). Why is SVD 
so useful and how does it work?

As a simple example, let's look at golf scores. Suppose Phil, 
Tiger, and Vijay play together for 9 holes and they each make 
par on every hole. Their scorecard, which can also be viewed as 
a (hole x player) matrix might look like this.

\begin{tabular}{|c|c|c|c|c|}
\hline
Hole & Par & Phil &  Tiger & Vijay\\
\hline
1  & 4  & 4  & 4  & 4\\
2  & 5  & 5  & 5  & 5\\
3  & 3  & 3  & 3  & 3\\
4  & 4  & 4  & 4  & 4\\
5  & 4  & 4  & 4  & 4\\
6  & 4  & 4  & 4  & 4\\
7  & 4  & 4  & 4  & 4\\
8  & 3  & 3  & 3  & 3\\
9  & 5  & 5  & 5  & 5\\
\hline
\end{tabular}

Let's look at the problem of trying to predict what score each 
player will make on a given hole. One idea is give each hole a 
HoleDifficulty factor, and each player a PlayerAbility factor. 
The actual score is predicted by multiplying these two factors 
together.

PredictedScore = HoleDifficulty * PlayerAbility

For the first attempt, let's make the HoleDifficulty be the par 
score for the hole, and let's make the player ability equal to 1. 
So on the first hole, which is par 4, we would expect a player 
of ability 1 to get a score of 4.

PredictedScore = HoleDifficulty * PlayerAbility = 4 * 1 = 4

For our entire scorecard or matrix, all we have to do is multiply 
the PlayerAbility (assumed to be 1 for all players) by the 
HoleDifficulty (ranges from par 3 to par 5) and we can exactly 
predict all the scores in our example.

In fact, this is the one dimensional (1-D) SVD factorization of 
the scorecard. We can represent our scorecard or matrix as the 
product of two vectors, the HoleDifficulty vector and the 
PlayerAbility vector. To predict any score, simply multiply the 
appropriate HoleDifficulty factor by the appropriate PlayerAbility 
factor. Following normal vector multiplication rules, we can 

generate the matrix of scores by multiplying the HoleDifficulty 
vector by the PlayerAbility vector, according to the following 
equation.
 
\begin{tabular}{|c|c|c|}
\hline
Phil  & Tiger  & Vijay\\
\hline
4  & 4  & 4\\
5  & 5  & 5\\
3  & 3  & 3\\
4  & 4  & 4\\
4  & 4  & 4\\
4  & 4  & 4\\
4  & 4  & 4\\
3  & 3  & 3\\
5  & 5  & 5\\
\hline
\end{tabular}
 =  
\begin{tabular}{|c|}
\hline
4\\
5\\
3\\
4\\
4\\
4\\
4\\
3\\
5\\
\hline
\end{tabular}
 *  
\begin{tabular}{|c|c|c|}
\hline
Phil  & Tiger  & Vijay\\
\hline
1  & 1  & 1\\
\hline
\end{tabular}

which is HoleDifficulty * PlayerAbility

Mathematicians like to keep everything orderly, so the convention 
is that all vectors should be scaled so they have length 1. For 
example, the PlayerAbility vector is modified so that the sum of 
the squares of its elements add to 1, instead of the current 
$12 + 12 + 12 = 3$. To do this, we have to divide each element by 
the square root of 3, so that when we square it, it becomes 
and the three elements add to 1. Similarly, we have to divide 
each HoleDifficulty element by the square root of 148. The square 
root of 3 times the square root of 148 is our scaling factor 21.07. 
The complete 1-D SVD factorization (to 2 decimal places) is:
 
\begin{tabular}{|c|c|c|}
\hline
Phil  & Tiger  & Vijay\\
\hline
4  & 4  & 4\\
5  & 5  & 5\\
3  & 3  & 3\\
4  & 4  & 4\\
4  & 4  & 4\\
4  & 4  & 4\\
4  & 4  & 4\\
3  & 3  & 3\\
5  & 5  & 5\\
\hline
\end{tabular}
 =  
\begin{tabular}{|c|}
\hline
0.33\\
0.41\\
0.25\\
0.33\\
0.33\\
0.33\\
0.33\\
0.25\\
0.41\\
\hline
\end{tabular}
 *  
\begin{tabular}{|c|}
\hline
21.07\\
\hline
\end{tabular}
 *  
\begin{tabular}{|c|c|c|}
\hline
Phil  & Tiger  & Vijay\\
\hline
0.58  & 0.58  & 0.58\\
\hline
\end{tabular}

which is HoleDifficulty * ScaleFactor * PlayerAbility

Our HoleDifficulty vector, that starts with 0.33, is called the 
Left Singular Vector. The ScaleFactor is the Singular Value, and 
our PlayerAbility vector, that starts with 0.58 is the Right 
Singular Vector. If we represent these 3 parts exactly, and multiply 
them together, we get the exact original scores. This means our 
matrix is a rank 1 matrix, another way of saying it has a simple 
and predictable pattern.

More complicated matrices cannot be completely predicted just by 
using one set of factors as we have done. In that case, we have to 
introduce a second set of factors to refine our predictions. To do 
that, we subtract our predicted scores from the actual scores, 
getting the residual scores. Then we find a second set of 
HoleDifficulty2 and PlayerAbility2 numbers that best predict the 
residual scores.

Rather than guessing HoleDifficulty and PlayerAbility factors and 
subtracting predicted scores, there exist powerful algorithms than 
can calculate SVD factorizations for you. Let's look at the actual 
scores from the first 9 holes of the 2007 Players Championship as 
played by Phil, Tiger, and Vijay.

\begin{tabular}{|c|c|c|c|c|}
\hline
Hole  & Par  & Phil  & Tiger  & Vijay\\
\hline
1  & 4  & 4  & 4  & 5\\
2  & 5  & 4  & 5  & 5\\
3  & 3  & 3  & 3  & 2\\
4  & 4  & 4  & 5  & 4\\
5  & 4  & 4  & 4  & 4\\
6  & 4  & 3  & 5  & 4\\
7  & 4  & 4  & 4  & 3\\
8  & 3  & 2  & 4  & 4\\
9  & 5  & 5  & 5  & 5\\
\hline
\end{tabular}

The 1-D SVD factorization of the scores is shown below. To make 
this example easier to understand, I have incorporated the ScaleFactor 
into the PlayerAbility and HoleDifficulty vectors so we can ignore 
the ScaleFactor for this example.
 
\begin{tabular}{|c|c|c|}
\hline
Phil  & Tiger  & Vijay\\
\hline
3.95  & 4.64  & 4.34\\
4.27  & 5.02  & 4.69\\
2.42  & 2.85  & 2.66\\
3.97  & 4.67  & 4.36\\
3.64  & 4.28  & 4.00\\
3.69  & 4.33  & 4.05\\
3.33  & 3.92  & 3.66\\
3.08  & 3.63  & 3.39\\
4.55  & 5.35  & 5.00\\
\hline
\end{tabular}
 =  
\begin{tabular}{|c|}
\hline
4.34\\
4.69\\
2.66\\
4.36\\
4.00\\
4.05\\
3.66\\
3.39\\
5.00\\
\hline
\end{tabular}
 *  
\begin{tabular}{|c|c|c|}
\hline
Phil  & Tiger  & Vijay\\
\hline
0.91  & 1.07  & 1.00\\
\hline
\end{tabular}

which is HoleDifficulty * PlayerAbility

Notice that the HoleDifficulty factor is almost the average of that 
hole for the 3 players. For example hole 5, where everyone scored 4, 
does have a factor of 4.00. However hole 6, where the average score 
is also 4, has a factor of 4.05 instead of 4.00. Similarly, the 
PlayerAbility is almost the percentage of par that the player 
achieved, For example Tiger shot 39 with par being 36, and 
$39/36 = 1.08$ which is almost his PlayerAbility factor (for these 
9 holes) of 1.07.

Why don't the hole averages and par percentages exactly match the 
1-D SVD factors? The answer is that SVD further refines those 
numbers in a cycle. For example, we can start by assuming 
HoleDifficulty is the hole average and then ask what PlayerAbility 
best matches the scores, given those HoleDifficulty numbers? Once 
we have that answer we can go back and ask what HoleDifficulty best 
matches the scores given those PlayerAbility numbers? We keep 
iterating this way until we converge to a set of factors that best 
predict the score. SVD shortcuts this process and immediately give 
us the factors that we would have converged to if we carried out 
the process.

One very useful property of SVD is that it always finds the optimal 
set of factors that best predict the scores, according to the 
standard matrix similarity measure (the Frobenius norm). That is, 
if we use SVD to find the factors of a matrix, those are the best 
factors that can be found. This optimality property means that we 
don't have to wonder if a different set of numbers might predict 
scores better.

Now let's look at the difference between the actual scores and our 
1-D approximation. A plus difference means that the actual score is 
higher than the predicted score, a minus difference means the actual 
score is lower than the prediction. For example, on the first hole 
Tiger got a 4 and the predicted score was 4.64 so we get 
$4 - 4.64 = -0.64$. In other words, we must add -0.64 to our prediction 
to get the actual score.

Once these differences have been found, we can do the same thing 
again and predict these differences using the formula 
HoleDifficulty2 * PlayerAbility2. Since these factors are trying 
to predict the differences, they are the 2-D factors and we have 
put a 2 after their names (ex. HoleDifficulty2) to show they are 
the second set of factors.
 
\begin{tabular}{|c|c|c|}
\hline
Phil  & Tiger  & Vijay\\
\hline
0.05  & -0.64  & 0.66\\
-0.28  & -0.02  & 0.31\\
0.58  & 0.15  & -0.66\\
0.03  & 0.33  & -0.36\\
0.36  & -0.28  & 0.00\\
-0.69  & 0.67  & -0.05\\
0.67  & 0.08  & -0.66\\
-1.08  & 0.37  & 0.61\\
0.45  & -0.35  & 0.00\\
\hline
\end{tabular}
 =  
\begin{tabular}{|c|}
\hline
-0.18\\
-0.38\\
0.80\\
0.15\\
0.35\\
-0.67\\
0.89\\
-1.29\\
0.44\\
\hline
\end{tabular}
 *  
\begin{tabular}{|c|c|c|}
\hline
Phil  & Tiger  & Vijay\\
\hline
0.82  & -0.20  & -0.53\\
\hline
\end{tabular}

which is HoleDifficulty(2) * PlayerAbility(2)

There are some interesting observations we can make about these 
factors. Notice that hole 8 has the most significant HoleDifficulty2 
factor (1.29). That means that it is the hardest hole to predict. 
Indeed, it was the only hole on which none of the 3 players made 
par. It was especially hard to predict because it was the most 
difficult hole relative to par 
$(HoleDifficulty - par) = (3.39 - 3) = 0.39$, and yet Phil birdied 
it making his score more than a stroke below his predicted score 
(he scored 2 versus his predicted score of 3.08). Other holes that 
were hard to predict were holes 3 (0.80) and 7 (0.89) because Vijay 
beat Phil on those holes even though, in general, Phil was playing 
better.

The full SVD for this example matrix (9 holes by 3 players) has 3 
sets of factors. In general, a m x n matrix where m >= n can have 
at most n factors, so our $9 x 3$ matrix cannot have more than 3 sets 
of factors. Here is the full SVD factorization (to two decimal places).
 
\begin{tabular}{|c|c|c|}
\hline
Phil  & Tiger  & Vijay\\
\hline
4  & 4  & 5\\
4  & 5  & 5\\
3  & 3  & 2\\
4  & 5  & 4\\
4  & 4  & 4\\
3  & 5  & 4\\
4  & 4  & 3\\
2  & 4  & 4\\
5  & 5  & 5\\
\hline
\end{tabular}
 =  
\begin{tabular}{|c|c|c|}
\hline
4.34  & -0.18  & -0.90\\
4.69  & -0.38  & -0.15\\
2.66  & 0.80  & 0.40\\
4.36  & 0.15  & 0.47\\
4.00  & 0.35  & -0.29\\
4.05  & -0.67  & 0.68\\
3.66  & 0.89  & 0.33\\
3.39  & -1.29  & 0.14\\
5.00  & 0.44  & -0.36\\
\hline
\end{tabular}
 *  
\begin{tabular}{|c|c|c|}
\hline
Phil  & Tiger  & Vijay\\
\hline
0.91  & 1.07  & 1.00\\
0.82  & -0.20  & -0.53\\
-0.21  & 0.76  & -0.62\\
\hline
\end{tabular}

which is HoleDifficulty(1-3) * PlayerAbility(1-3)

By SVD convention, the HoleDifficulty and PlayerAbility vectors 
should all have length 1, so the conventional SVD factorization 
is:
 
\begin{tabular}{|c|c|c|}
\hline
Phil  & Tiger  & Vijay\\
\hline
4  & 4  & 5\\
4  & 5  & 5\\
3  & 3  & 2\\
4  & 5  & 4\\
4  & 4  & 4\\
3  & 5  & 4\\
4  & 4  & 3\\
2  & 4  & 4\\
5  & 5  & 5\\
\hline
\end{tabular}
 =  
\begin{tabular}{|c|c|c|}
\hline
0.35  & 0.09  & -0.64\\
0.38  & 0.19  & -0.10\\
0.22  & -0.40  & 0.28\\
0.36  & -0.08  & 0.33\\
0.33  & -0.18  & -0.20\\
0.33  & 0.33  & 0.48\\
0.30  & -0.44  & 0.23\\
0.28  & 0.64  & 0.10\\
0.41  & -0.22  & -0.25\\
\hline
\end{tabular}
 *  
\begin{tabular}{|c|c|c|}
\hline
21.07  & 0  & 0\\
0  & 2.01  & 0\\
0  & 0  & 1.42\\
\hline
\end{tabular}
 * 
\begin{tabular}{|c|c|c|}
\hline
Phil  & Tiger  & Vijay\\
\hline
0.53  & 0.62  & 0.58\\
-0.82  & 0.20  & 0.53\\
-0.21  & 0.76  & -0.62\\
\hline
\end{tabular}

which is HoleDifficulty(1-3)* ScaleFactor(1-3) * PlayerAbility(1-3)

We hope that you have some idea of what SVD is and how it can be 
used. The next section covers applying SVD to Latent Sematic 
Analysis or LSA. Although the domain is different, the concepts 
are the same. We are trying to predict patterns of how words occur 
in documents instead of trying to predict patterns of how players 
score on holes.

\chapter{Quaternions}
from \cite{Altm05}:
\begin{quotation}
Quaternions are inextricably linked to rotations.
Rotations, however, are an accident of three-dimensional space.
In spaces of any other dimensions, the fundamental operations are
reflections (mirrors). The quaternion algebra is, in fact, merely a
sub-algebra of the Clifford algebra of order three. If the quaternion
algebra might be labelled the algebra of rotations, then the Clifford
algebra is the algebra of mirrors and it is thus vastly more general
than quaternion algebra. 
\end{quotation}
\begin{center}
\bigskip
\large Peter Guthrie Tait, Robert S. Sutor, Timothy Daly

\end{center}
\section*{Preface}
\addcontentsline{toc}{section}{Preface}
The Theory of Quaternions is due to Sir William Rowan Hamilton,
Royal Astronomer of Ireland, who presented his first paper on the
subject to the Royal Irish Academy in 1843. His Lectures on
Quaternions were published in 1853, and his Elements, in 1866,
shortly after his death. The Elements of Quaternions by Tait \cite{Tait1890} 
is the accepted text-book for advanced students.

Large portions of this file are derived from a public domain version
of Tait's book combined with the algebra available in Axiom.
The purpose is to develop a tutorial introduction to the Axiom
domain and its uses.
\newpage

\section{Quaternions}

\section{Vectors, and their Composition}

{\bf 1}. For at least two centuries the geometrical representation 
of the negative and imaginary algebraic quantities, $-1$ and $\sqrt{-1}$
has been a favourite subject of speculation with mathematicians. 
The essence of almost all of the proposed processes consists in 
employing such expressions to indicate the DIRECTION, not the 
{\sl length}, of lines. 

{\bf 2}. Thus it was long ago seen that if positive quantities were 
measured off in one direction along a fixed line, a useful and lawful 
convention enabled us to express negative quantities of the same 
kind by simply laying them off on the same line in the opposite 
direction. This convention is an essential part of the Cartesian 
method, and is constantly employed in Analytical Geometry and 
Applied Mathematics. 

{\bf 3}. Wallis, towards the end of the seventeenth century, proposed 
to represent the impossible roots of a quadratic equation by going 
{\sl out} of the line on which, if real, they would have been laid off. 
This construction is equivalent to the consideration of $\sqrt{-1}$ as a 
directed unit-line perpendicular to that on which real quantities 
are measured. 

{\bf 4}. In the usual notation of Analytical Geometry of two 
dimensions, when rectangular axes are employed, this amounts 
to reckoning each unit of length along $Oy$ as $+\sqrt{-1}$, and on 
$Oy^{\prime}$ as $-\sqrt{-1}$ ; while on $Ox$ each unit is $+1$, and on 
$Ox$ it is $-1$. 

If we look at these four lines in circular order, i.e. in the order of 
positive rotation (that of the northern hemisphere of the earth 
about its axis, or {\sl opposite} to that of the hands of a watch), they 
give 
$$ 1, \sqrt{-1}, -1, -\sqrt{-1}$$

\boxer{4.6in}{
\vskip 0.1cm
In Axiom the same elements would be written as complex numbers
which are constructed using the function {\bf complex}:
\spadcommand{complex(1,0)}
$$1$$
\returnType{Type: Complex Integer}
}
\boxer{4.6in}{
\vskip 0.1cm
\spadcommand{complex(0,1)}
$$\%i$$
\returnType{Type: Complex Integer}
}
\boxer{4.6in}{
\vskip 0.1cm
\spadcommand{complex(-1,0)}
$$-1$$
\returnType{Type: Complex Integer}
}
\boxer{4.6in}{
\vskip 0.1cm
\spadcommand{complex(0,-1)}
$$-i$$
\returnType{Type: Complex Integer}
}
\boxer{4.6in}{
\vskip 0.1cm
Note that \%i is of type Complex(Integer), that is, the imaginary
part of a complex number. The apparently equivalent expression
\spadcommand{sqrt(-1)}
$$\sqrt{-1}$$
\returnType{Type: AlgebraicNumber}
has the type AlgebraicNumber which means that it is the root of
a polynomial with rational coefficients.\\
}

In this series each expression is derived from that which precedes 
it by multiplication by the factor $\sqrt{-1}$. Hence we may consider 
$\sqrt{-1}$ as an operator, analogous to a handle perpendicular to the 
plane of $xy$, whose effect on any line in that plane is to make it 
rotate (positively) about the origin through an angle of $90^{\circ}$. 

\boxer{4.6in}{
\vskip 0.1cm
In Axiom 
\spadcommand{\%i*\%i}
$$-1$$
\returnType{Type: Complex Integer}
}

{\bf 5}. In such a system, (which seems to have been first developed, 
in 1805, by Bu\'ee) a point in the plane of reference is defined by a 
single imaginary expression. Thus $a + b\sqrt{-1}$ may be considered 
as a single quantity, denoting the point, $P$, whose coordinates are 
$a$ and $b$. Or, it may be used as an expression for the line $OP$ 
joining that point with the origin. In the latter sense, the expression 
$a + b\sqrt{-1}$ implicitly contains the {\sl direction}, as well as the 
{\sl length}, of this line ; since, as we see at once, the direction is 
inclined at an angle $\tan^{-1}(b/a)$ to the axis of $x$, 
and the length is $\sqrt{a^2+b^2}$. Thus, say we have 
$$OP = a + b\sqrt{-1}$$
the line $OP$ considered as that by which we pass from one 
extremity, $O$, to the other, $P$. In this sense it is called a VECTOR. 
Considering, in the plane, any other vector, 
$$OQ = a^{\prime}+b^{\prime}\sqrt{-1}$$

\boxer{4.6in}{
\vskip 0.1cm
In order to created superscripted variables we use the superscript
function from the SYMBOL domain. So we can create $a^{\prime}$ as ``ap''
(that is, ``a-prime'') and $b^{\prime}$ as ``bp'' (``b-prime'') thus
(also note that the underscore character is Axiom's escape character
which removes any special meaning of the next character, in this case,
the quote character):
\spadcommand{ap:=superscript(a,[\_'])}
$$a^{\prime}$$
\returnType{Type: Symbol}
}
\boxer{4.6in}{
\vskip 0.1cm
\spadcommand{bp:=superscript(b,[\_'])}
$$b^{\prime}$$
\returnType{Type: Symbol}
}
\boxer{4.6in}{
\vskip 0.1cm
at this point we can type
\spadcommand{ap+bp*\%i}
$$a^{\prime}+b^{\prime}\ \%i$$
\returnType{Type: Complex Polynomial Integer}
}

the addition of these two lines obviously gives 
$$OR = a + a^{\prime} + (b + b^{\prime})\sqrt{-1}$$

\boxer{4.6in}{
\vskip 0.1cm
In Axiom the computation looks like:
\spadcommand{op:=complex(a,b)}
$$a + b\ \%i$$
\returnType{Type: Complex Polynomial Integer}
}
\boxer{4.6in}{
\vskip 0.1cm
\spadcommand{oq:=complex(ap,bp)}
$$a^{\prime} + b^{\prime}\ \%i$$
\returnType{Type: Complex Polynomial Integer}
}
\boxer{4.6in}{
\vskip 0.1cm
\spadcommand{op + oq}
$$a + a^{\prime} + (b + b^{\prime})\%i$$
\returnType{Type: Complex Polynomial Integer}
}

and we see that the sum is the diagonal of the parallelogram on 
$OP$, $OQ$. This is the law of the composition of simultaneous 
velocities; and it contains, of course, the law of subtraction of one 
directed line from another. 

{\bf 6}. Operating on the first of these symbols by the factor $\sqrt{-1}$,
it becomes $- b + a\sqrt{-1}$; and now, of course, denotes the point 
whose $x$ and $y$ coordinates are $- b$ and $a$; or the line joining this 
point with the origin. The length is still $\sqrt{a^2+b^2}$, but the angle 
the line makes with the axis of $x$ is $\tan^{-1}(- a/b)$; which is 
evidently greater by $\pi/2$ than before the operation. 

\boxer{4.6in}{
\vskip 0.1cm
\spadcommand{op*complex(0,1)}
$$-b+a\ i$$
\returnType{Type: Complex Polynomial Integer}
}

{\bf 7}. De Moivre's Theorem tends to lead us still further in the 
same direction. In fact, it is easy to see that if we use, instead 
of $\sqrt{-1}$, the more general factor $\cos \alpha + \sqrt{-1} \sin \alpha$, 
its effect on 
any line is to turn it through the (positive) angle $\alpha$. in the plane 
of $x$, $y$. [Of course the former factor, $\sqrt{-1}$, is merely the 
particular case of this, when $\alpha=\frac{\pi}{2}$].

Thus 
$$
\begin{array}{ll}
  &(\cos \alpha + \sqrt{-1} \sin \alpha) (a + b \sqrt{-1})\\
= & a \cos \alpha - b \sin \alpha + \sqrt{-1} (a \sin \alpha + b \cos \alpha)
\end{array}
$$

by direct multiplication. The reader will at once see that the new 
form indicates that a rotation through an angle $\alpha$ has taken place, 
if he compares it with the common formulae for turning the coordinate 
axes through a given angle. Or, in a less simple manner, thus 

$$
\begin{array}{rcl}
Length & = & \sqrt{(a \cos \alpha - b \sin \alpha)^2 +
                   (a \sin \alpha + b \cos \alpha)^2} \\
       & = & \sqrt{a^2 + b^2}
\end{array}
$$
as before. 

Inclination to axis of $x$
$$
\begin{array}{cl}
= & \tan^{-1}{\frac{a \sin \alpha + b \cos \alpha}
                   {a \cos \alpha - b \sin \alpha}}\\
= & \tan^{-1}{\frac{\tan \alpha + \frac{b}{a}}
                   {1 - \frac{b}{a} \tan \alpha}}\\
= & \alpha + \tan^{-1}{\frac{b}{a}}
\end{array}
$$

{\bf 8}. We see now, as it were, why it happens that 

$$(\cos \alpha + \sqrt{-1} \sin \alpha)^m = 
\cos m\alpha + \sqrt{-1} \sin m\alpha
$$ 

In fact, the first operator produces $m$ successive rotations in the 
same direction, each through the angle $\alpha$ ; the second, a single 
rotation through the angle $m\alpha$. 

{\bf 9}. It may be interesting, at this stage, to anticipate so far as to 
remark that in the theory of Quaternions the analogue of 

$$
\begin{array}{lclr}
               & \textrm{     } & \cos \theta + \sqrt{-1} \sin \theta &\\
\textrm{is}    & \textrm{     } & \cos \theta + \omega \sin \theta   &\\
\textrm{where} & \textrm{     } & \omega^2 = -1                      & \\
\end{array}
$$

Here, however, $\omega$ is not the algebraic $\sqrt{-1}$, but is 
{\sl any directed unit-line} whatever in space. 

{\bf 10}. In the present century Argand, Warren, Mourey, and 
others, extended the results of Wallis and Bu\'ee. They attempted 
to express as a line the product of two lines each represented by a 
symbol such $a+b\sqrt{-1}$. To a certain extent they succeeded, 
but all their results remained confined to two dimensions. 

The product, $\prod$, of two such lines was defined as the fourth 
proportional to unity and the two lines, thus 

$$
\begin{array}{lclr}
               & \textrm{     } & 
1 : a+b\sqrt{-1} :: a^{\prime}+b^{\prime}\sqrt{-1} : \prod\\
\textrm{or}    & \textrm{     } & 
\prod = (aa^{\prime} - bb^{\prime})+(a^{\prime}b+b^{\prime}a)\sqrt{-1}
\end{array}
$$

The length of $\prod$ is obviously the product of the lengths of the 
factor lines; and its direction makes an angle with the axis of $x$ 
which is the sum of those made by the factor lines. From this 
result the quotient of two such lines follows immediately. 

{\bf 11}. A very curious speculation, due to Servois and published 
in 1813 in Gergonne's {\sl Annales}, is one of the very few, so far as has 
been discovered, in which a well-founded guess at a possible mode 
of extension to three dimensions is contained. Endeavouring to 
extend to {\sl space} the form $a+b\sqrt{-1}$ for the plane, he is guided by 
analogy to write for a directed unit-line in space the form 

$$p \cos \alpha + q \cos \beta + r \cos \gamma$$

where $\alpha$, $\beta$, $\gamma$ 
are its inclinations to the three axes. He perceives 
easily that $p$, $q$, $r$ must be {\sl non-reals}: 
but, he asks, ``seraient-elles 
{\sl imaginaires} r\'eductibles \`a la forme g\'en\'erale $A+B\sqrt{-1}$?'' 
The $i$,$j$, $k$ of the Quaternion Calculus furnish an answer to this 
question. (See Chap. II.) But it may be remarked that, in applying the 
idea to lines in a plane, a vector $OP$ will no longer be represented 
(as in \S 5) by 
$$
\begin{array}{lclr}
                            & \textrm{   OP} & = & a + b\sqrt{-1}\\
\textrm{but by}             & \textrm{   OP} & = & pa + qb\\
\textrm{And if, similarly,} & \textrm{   OQ} & = & pa^{\prime} + qb^{\prime}\\
\end{array}
$$

the addition of these two lines gives for $OR$ (which retains its 
previous signification) 
$$OR = p(a+a^{\prime} + q(b+b^{\prime})$$

{\bf 12}. Beyond this, few attempts were made, or at least recorded, 
in earlier times, to extend the principle to space of three dimensions; 
and, though many such had been made before 1843, none, 
with the single exception of Hamilton's, have resulted in simple, 
practical methods; all, however ingenious, seeming to lead almost 
at once to processes and results of fearful complexity. 

For a lucid, complete, and most impartial statement of the 
claims of his predecessors in this field we refer to the Preface to 
Hamilton's {\sl Lectures on Quaternions}. He there shows how his long 
protracted investigations of Sets culminated in this unique system 
of tridimensional-space geometry. 

{\bf 13}. It was reserved for Hamilton to discover the use and 
properties of a class of symbols which, though all in a certain sense 
square roots of -1, may be considered as {\sl real} unit lines, tied down 
to no particular direction in space ; the expression for a vector is, 
or may be taken to be, 
$$ \rho = ix + jy + kz$$

but such vector is considered in connection with an {\sl extraspatial}
magnitude $w$, and we have thus the notion of a QUATERNION 

$$w + \rho$$

This is the fundamental notion in the singularly elegant, and 
enormously powerful, Calculus of Quaternions. 

While the schemes for using the algebraic $\sqrt{-1}$ to indicate 
direction make one direction in space expressible by real numbers, 
the remainder being imaginaries of some kind, and thus lead to 
expressions which are heterogeneous ; Hamilton s system makes all 
directions in space equally imaginary, or rather equally real, thereby 
ensuring to his Calculus the power of dealing with space 
indifferently in all directions. 

In fact, as we will see, the Quaternion method is independent 
of axes or any supposed directions in space, and takes its reference 
lines solely from the problem it is applied to. 

{\bf 14}. But, for the purpose of elementary exposition, it is best 
to begin by assimilating it as closely as we can to the ordinary 
Cartesian methods of Geometry of Three Dimensions, with which 
the student is supposed to be, to some extent at least, acquainted. 
Such assistance, it will be found, can (as a rule) soon be dispensed 
with; and Hamilton regarded any apparent necessity for an oc 
casional recurrence to it, in higher applications, as an indication 
of imperfect development in the proper methods of the new 
Calculus. 

We commence, therefore, with some very elementary geometrical 
ideas, relating to the theory of vectors in space. It will subsequently 
appear how we are thus led to the notion of a Quaternion. 

{\bf 15}. Suppose we have two points $A$ and $B$ in {\sl space}, and suppose 
$A$ given, on how many numbers does $B$'s relative position depend ? 

If we refer to Cartesian coordinates (rectangular or not) we find 
that the data required are the excesses of $B$'s three coordinates 
over those of $A$. Hence three numbers are required. 

Or we may take polar coordinates. To define the moon's 
position with respect to the earth we must have its Geocentric 
Latitude and Longitude, or its Right Ascension and Declination, 
and, in addition, its distance or radius-vector. {\sl Three} again. 

{\bf 16}. Here it is to be carefully noticed that nothing has been 
said of the {\sl actual} coordinates of either A or B, or of the earth 
and moon, in space; it is only the {\sl relative} coordinates that are 
contemplated. 

Hence any expression, as $\overline{AB}$, denoting a line considered with 
reference to direction and currency as well as length, (whatever 
may be its actual position in space) contains implicitly {\sl three}
numbers, and all lines parallel and equal to $AB$, and concurrent 
with it, depend in the same way upon the same three. Hence, {\sl all 
lines which are equal, parallel, and concurrent, may be represented 
by a common symbol, and that symbol contains three distinct numbers}. 
In this sense a line is called a VECTOR, since by it we pass from 
the one extremity, $A$, to the other, $B$, and it may thus be 
considered as an instrument which {\sl carries} $A$ to $B$: so that a 
vector may be employed to indicate a definite {\sl translation} in space. 

[The term " currency " has been suggested by Cayley for use 
instead of the somewhat vague suggestion sometimes taken to 
be involved in the word "direction." Thus parallel lines have 
the same direction, though they may have similar or opposite 
currencies. The definition of a vector essentially includes its 
currency.] 

{\bf 17}. We may here remark, once for all, that in establishing a 
new Calculus, we are at liberty to give any definitions whatever 
of our symbols, provided that no two of these interfere with, or 
contradict, each other, and in doing so in Quaternions {sl simplicity}
and (so to speak) {\sl naturalness} were the inventor's aim. 

{\bf 18}. Let $\overline{AB}$ be represented by $\alpha$, we know that 
$\alpha$ involves 
{\sl three} separate numbers, and that these depend solely upon the 
position of $B$ {\sl relatively} to $A$. 
Now if $CD$ be equal in length to $AB$ 
and if these lines be parallel, and have the same currency, we may 
evidently write 
$$\overline{CD} = \overline{AB} = \alpha$$
where it will be seen that the sign of equality between vectors 
contains implicitly {\sl equality in length}, {\sl parallelism in direction}, 
and {\sl concurrency}. So far we have {\sl extended} the meaning of an 
algebraical symbol. And it is to be noticed that an equation 
between vectors, as 
$$\alpha = \beta$$
contains {\sl three} distinct equations between mere numbers. 

{\bf 19}. We must now define $+$ (and the meaning of $-$ will follow) 
in the new Calculus. Let $A$, $B$, $C$ be any three points, and (with 
the above meaning of $=$ ) let 
$$\overline{AB} = \alpha, \overline{BC} = \beta, \overline{AC} = \gamma$$
If we define $+$ (in accordance with the idea (\S 16) that a vector 
represents a {\sl translation}) by the equation 
$$
\begin{array}{lcl}
            & \textrm{     } & \alpha + \beta = \gamma\\
            &                & \\
\textrm{or} & \textrm{     } & 
\overline{AB} + \overline{BC} = \overline{AC}
\end{array}
$$
we contradict nothing that precedes, but we at once introduce the 
idea that {\sl vectors are to be compounded}, 
{\sl in direction and magnitude},
{\sl like simultaneous velocities}. A reason for this may be seen in 
another way if we remember that by {\sl adding} the (algebraic) differences 
of the Cartesian coordinates of $B$ and $A$, to those of the 
coordinates of $C$ and $B$, we get those of the coordinates of $C$ and 
$A$. Hence these coordinates enter {\sl linearly} into the expression for 
a vector. (See, again, \S 5.) 

{\bf 20}. But we also see that if $C$ and $A$ coincide (and $C$ may be 
{\sl any} point) 
$$\overline{AC} = 0$$
for no vector is then required to carry $A$ to $C$. Hence the above 
relation may be written, in this case, 
$$\overline{AB}+\overline{BA} = 0$$
or, introducing, and by the same act defining, the symbol $-$, 
$$\overline{AB} = -\overline{BA}$$

Hence, {\sl the symbol $-$, applied to a vector, simply shows that its 
currency is to be reversed}. 
And this is consistent with all that precedes; for instance, 
$$
\begin{array}{lcrcl}
             & \textrm{     } & \overline{AB} + \overline{BC}  &=& \overline{AC}\\
\textrm{and} & \textrm{     } & \overline{AB} = \overline{AC} &-& \overline{BC} \\
\textrm{or}  & \textrm{     } & = \overline{AC} &+& \overline{CB} \\
\end{array}
$$
are evidently but different expressions of the same truth. 

{\bf 21}. In any triangle, $ABC$, we have, of course, 
$$\overline{AB} + \overline{BC} + \overline{CA} = 0$$
and, in any closed polygon, whether plane or gauche, 
$$\overline{AB}+\overline{BC}+\ldots+\overline{YZ}+\overline{ZA} = 0$$ 

In the case of the polygon we have also 
$$\overline{AB}+\overline{BC}+\ldots+\overline{YZ} = \overline{AZ}$$

These are the well-known propositions regarding composition 
of velocities, which, by Newton's second law of motion, give us 
the geometrical laws of composition of forces acting at one point. 

{\bf 22}. If we compound any number of {\bf parallel} vectors, the result
is obviously a numerical multiple of any one of them. 
Thus, if $A$, $B$, $C$ are in one straight line, 
$$\overline{BC} = x\overline{AB}$$
where $x$ is a number, positive when $B$ lies between $A$ and $C$, 
otherwise negative; but such that its numerical value, independent 
of sign, is the ratio of the length of $BC$ to that of $AB$. This is 
at once evident if $AB$ and $BC$ be commensurable; and is easily 
extended to incommensurables by the usual {\sl reductio ad absurdum}. 

{\bf 23}. An important, but almost obvious, proposition is that {\sl any 
vector may be resolved, and in one way only, into three components 
parallel respectively to any three given vectors, no two of which are 
parallel, and which are not parallel to one plane}.

\begin{center}
\includegraphics{ps/quat1.ps}
\end{center}
\vskip 0.5cm

Let $OA$, $OB$, $OC$ be the three fixed 
vectors, $OP$ any other vector. From $P$ draw 
$PQ$ parallel to $CO$, meeting the plane $BOA$ 
in $Q$. [There must be a definite point $Q$, 
else $PQ$, and therefore $CO$, would be parallel 
to $BOA$, a case specially excepted.] From $Q$ 
draw $QR$ parallel to $BO$, meeting $OA$ in $R$. 

Then we have $\overline{OP}=\overline{OR} + \overline{RQ} + \overline{QP}$
(\S 21), 
and these components are respectively parallel to the three given 
vectors. By \S 22 we may express $\overline{OR}$ as a numerical multiple 
of $\overline{OA}$, $\overline{RQ}$ of $\overline{OB}$, and 
$\overline{QP}$ of $\overline{OC}$. Hence we have, generally, for 
any vector in terms of three fixed non-coplanar vectors, $\alpha$,
$\beta$, $\gamma$
$$\overline{OP} = \rho = x\alpha + y\beta + z\gamma$$
which exhibits, in one form, the {\sl three} numbers on which a vector 
depends (\S 16). Here $x$, $y$, $z$ are perfectly definite, and can have 
but single values. 

{\bf 24}. Similarly any vector, as $\overline{OQ}$, in the same plane with 
$\overline{OA}$ and $\overline{OB}$, 
can be resolved (in one way only) into components $\overline{OR}$, 
$\overline{RQ}$, 
parallel respectively to $\overline{OA}$ and 
$\overline{OB}$; so long, at least, as these 
two vectors are not parallel to each other. 

{\bf 25}. There is particular advantage, in certain cases, in employing 
a series of {\sl three mutually perpendicular unit-vectors} as 
lines of reference. This system Hamilton denotes by $i$,$j$, $k$. 

Any other vector is then expressible as 
$$\rho= xi + yj + zk$$
Since $i$, $j$, $k$ are unit-vectors, $x$, $y$, $z$ are here the lengths of 
conterminous edges of a rectangular parallelepiped of which $\rho$
is the vector-diagonal; so that the length of $\rho$ is, in this case, 
$$\sqrt{x^2+y^2+z^2}$$
Let \hbox{\hskip 4cm}$\omega = \xi i + \eta j + \zeta k$\\
be any other vector, then (by the proposition of \S 23) the vector 
$$
\begin{array}{lcr}
\textrm{equation} & \textrm{     } & \rho = \omega
\end{array}
$$
obviously involves the following three equations among numbers, 
$$x=\xi, y=\eta, z=\zeta$$
Suppose $i$ to be drawn eastwards, $j$ northwards, and $k$ upwards, 
this is equivalent merely to saying that {\sl if two points coincide, they 
are equally to the east (or west) of any third point, equally to the 
north (or south) of it, and equally elevated above (or depressed below) 
its level.} 

{\bf 26}. It is to be carefully noticed that it is only when 
$\alpha$, $\beta$, $\gamma$
are not coplanar that a vector equation such as 
$$\rho = \omega$$
or\hbox{\hskip 3cm}
$x\alpha + y\beta + z\gamma = \xi \alpha + \eta \beta + \zeta \gamma$\\
necessitates the three numerical equations 
$$x = \xi, y = \eta, z = \zeta$$
For, if $\alpha$, $\beta$, $\gamma$ be coplanar (\S 24), 
a condition of the following form must hold 
$$\gamma = a\alpha + b\beta$$
Hence,\hbox{\hskip 3cm}$\rho=(x+za)\alpha+(y+zb)\beta$\\
\hbox{\hskip 4cm}$\omega=(\xi+\zeta a)\alpha + (\eta+\zeta b)\beta$\\
and the equation\hbox{\hskip 3cm}$\rho=\omega$\\
now requires only the two numerical conditions 
$$x+za=\xi+\zeta a\hbox{\hskip 1cm}y+zb = \eta+\zeta b$$

{\bf 27}. {\sl The Commutative and Associative Laws hold in the combination 
of vectors by the signs $+$ and $-$}. It is obvious that, if we 
prove this for the sign $+$, it will be equally proved for $-$, because 
$-$ before a vector (\S 20) merely indicates that it is to be reversed 
before being considered positive. 

Let $A$, $B$, $C$, $D$ be, in order, the corners of a parallelogram ; we 
have, obviously, 
$$\overline{AB} = \overline{DC}\hbox{\hskip 1cm}\overline{AD}=\overline{BC}$$
And \hbox{\hskip 2cm}$\overline{AB}+\overline{BC}=
\overline{AC}=
\overline{AD}+\overline{DC}=
\overline{BC}+\overline{AB}$

Hence the commutative law is true for the addition of any two 
vectors, and is therefore generally true. 

Again, whatever four points are represented by $A$, $B$, $C$, $D$, we 
$$\overline{AD}=\overline{AB}+\overline{BD}=\overline{AC}+\overline{CD}$$
or substituting their values for $\overline{AD}$, $\overline{BD}$, 
$\overline{AC}$ respectively, in these three expressions, 
$$\overline{AB}+\overline{BC}+\overline{CD}=
\overline{AB}+(\overline{BC}+\overline{CD})=
(\overline{AB}+\overline{BC})+\overline{CD}$$
And thus the truth of the associative law is evident. 

{\bf 28}. The equation 
$$\rho = x\beta$$
where $\rho$ is the vector connecting a variable point with the origin, 
$\beta$ a definite vector, and $x$ an indefinite number, represents the 
straight line drawn from the origin parallel to $\beta$ (\S 22). 

The straight line drawn from $A$, where $\overline{OA} = \alpha$, 
and parallel to $\beta$, has the equation 
\begin{equation}\label{Vec1}
\rho = \alpha + x\beta
\end{equation}
In words, we may pass directly from $O$ to $P$ by the vector $\overline{OP}$ 
or $\rho$; or we may pass first to $A$, by means of $\overline{OA}$ or 
$\alpha$, and then to $P$ along a vector parallel to $\beta$ (\S 16). 

Equation \ref{Vec1} is one of the many useful forms into which 
Quaternions enable us to throw the general equation of a straight 
line in space. As we have seen (\S 25) it is equivalent to three 
numerical equations; but, as these involve the indefinite quantity 
$x$, they are virtually equivalent to but {\sl two}, as in ordinary Geometry 
of Three Dimensions. 

{\bf 29}. A good illustration of this remark is furnished by the fact 
that the equation 
$$\rho = y\alpha + x\beta$$
which contains two indefinite quantities, is virtually equivalent to
only one numerical equation. And it is easy to see that it represents 
the plane in which the lines $\alpha$ and $\beta$ lie; or the surface 
which is formed by drawing, through every point of $OA$, a line 
parallel to $OB$. In fact, the equation, as written, is simply \S 24 
in symbols. 

And it is evident that the equation 
$$\rho = \gamma + y\alpha + x\beta$$
is the equation of the plane passing through the extremity of $\gamma$, 
and parallel to $\alpha$ and $\beta$.

It will now be obvious to the reader that the equation 
$$\rho = p_1\alpha_1+p_2\alpha_2+\ldots=\sum{p\alpha}$$
where $\alpha_1$, $\alpha_2$ , \&c. are given vectors, 
and $p_1$, $p_2$, \&c. numerical quantities, 
{\sl represents a straight line} 
if $p_1$, $p_2$, \&c. be linear functions of 
{\sl one} indeterminate number; and a {\sl plane}, if they be linear 
expressions containing two indeterminate numbers. Later (\S 31 (l)), 
this theorem will be much extended. 

Again, the equation 
$$\rho = x\alpha + y\beta +z\gamma$$
refers to {\sl any} point whatever in space, provided 
$\alpha$, $\beta$, $\gamma$ are not coplanar. (Ante, \S 23) 

{\bf 30}. The equation of the line joining any two points $A$ and $B$, 
where $\overline{OA} = \alpha$ and $\overline{OB} = \beta$, is obviously 
$$\rho = \alpha + x(\beta-\alpha)$$
or \hbox{\hskip 4.2cm}$\rho=\beta+y(\alpha-\beta)$\\
These equations are of course identical, as may be seen by putting 
$1-y$ for $x$.

The first may be written 
$$\rho+(x-1)\alpha-x\beta = 0$$
or\hbox{\hskip 4cm}$p\rho+q\alpha+r\beta=0$\\
subject to the condition $p + q + r = 0$ identically. That is -- 
A homogeneous linear function of three vectors, equated to zero, 
expresses that the extremities of these vectors are in one straight 
line, {\sl if the sum of the coefficients be identically zero}.

Similarly, the equation of the plane containing the extremities 
$A$, $B$, $C$ of the three non-coplanar vectors 
$\alpha$, $\beta$, $\gamma$ is
$$\rho=\alpha+x(\beta-\alpha)+y(\gamma-\beta)$$
where $x$ and $y$ are each indeterminate. 

This may be written 
$$p\rho + q\alpha + r\beta +s\gamma = 0$$
with the identical relation 
$$p+q+r+x=0$$
which is one form of the condition that four points may lie in one plane. 

{\bf 31}. We have already the means of proving, in a very simple 
manner, numerous classes of propositions in plane and solid 
geometry. A very few examples, however, must suffice at this 
stage; since we have hardly, as yet, crossed the threshold of the 
subject, and are dealing with mere linear equations connecting two 
or more vectors, and even with them {\sl we are restricted as yet to 
operations of mere addition}. We will give these examples with a 
painful minuteness of detail, which the reader will soon find to be 
necessary only for a short time, if at all. 

(a) {\sl The diagonals of a parallelogram bisect each other}.

Let $ABCD$ be the parallelogram, $O$ the point of intersection of 
its diagonals. Then 
$$\overline{AO}+\overline{OB}=\overline{AB}=\overline{DC}=
\overline{DO}+\overline{OC}$$
which gives\hbox{\hskip 2cm}$\overline{AO}-\overline{OC}=
\overline{DO}-\overline{OB}$\\
The two vectors here equated are parallel to the diagonals respectively. 
Such an equation is, of course, absurd unless 
\begin{enumerate}
\item The diagonals are parallel, in which case the figure 
is not a parallelogram; 
\item $\overline{AO} = \overline{OC}$, and 
$\overline{DO} = \overline{OB}$, the proposition. 
\end{enumerate}

(b) {\sl To shew that a triangle can be constructed, whose sides 
are parallel, and equal, to the bisectors of the sides of any 
triangle}. 

Let $ABC$ be any triangle, $Aa$, $Bb$, $Cc$ the bisectors of the 
sides. 

Then 
$$
\begin{array}{ccc}
\overline{Aa} & =\overline{AB}+\overline{Ba} 
                       & =\overline{AB}+\frac{1}{2}\overline{BC}\\
\overline{Bb} & \ldots & = \overline{BC} + \frac{1}{2}\overline{CA}\\
\overline{Cc} & \ldots & = \overline{CA} + \frac{1}{2}\overline{AB}
\end{array}
$$
Hence \hbox{\hskip 2cm}$\overline{Aa}+\overline{Bb}+\overline{Cc}=
\frac{3}{2}(\overline{AB}+\overline{BC}+\overline{CA})=0$\\
which (\S 21) proves the proposition. 

Also 
$$
\begin{array}{rcl}
\overline{Aa} & = & \overline{AB}+\frac{1}{2}\overline{BC}\\
              & = & \overline{AB}-\frac{1}{2}(\overline{CA}+\overline{AB})\\
              & = & \frac{1}{2}(\overline{AB}-\overline{CA})\\
              & = & \frac{1}{2}(\overline{AB}+\overline{AC})
\end{array}
$$
results which are sometimes useful. They may be easily verified 
by producing $\overline{Aa}$ to twice its length and joining the extremity 
with $B$. 

($b^{\prime}$) {\sl The bisectors of the sides of a triangle meet in a point, 
which trisects each of them}.

Taking $A$ as origin, and putting $\alpha$, $\beta$, $\gamma$
for vectors parallel, and 
equal, to the sides taken in order $BC$, $CA$, $AB$; the equation of 
$Bb$ is (\S 28 (1)) 
$$\rho=\gamma+x(\gamma+\frac{\beta}{2})=(1+x)\gamma+\frac{x}{2}\beta$$
That of $Cc$ is, in the same way, 
$$\rho=-(1+y)\beta-\frac{y}{2}\gamma$$
At the point $O$, where $Bb$ and $Cc$ intersect, 
$$\rho=(1+x)\gamma+\frac{x}{2}\beta=-(1+y)\beta-\frac{y}{2}\gamma$$
Since $\gamma$ and $\beta$ are not parallel, this equation gives 
$$1+x=-\frac{y}{2}\textrm{\ \ and\ \ }\frac{x}{2}=-(1+y)$$
From these\hbox{\hskip 3cm}$x=y=-\frac{2}{3}$

Hence\hbox{\hskip 1cm}$\overline{AO}=\frac{1}{3}(\gamma-\beta)=
\frac{2}{3}\overline{Aa}$ (See Ex. (b))\\

This equation shows, being a vector one, that $\overline{Aa}$ passes 
through $O$, and that $AO$ : $Oa$ :: 2:1. 

(c) If 
$$\overline{OA}=\alpha$$
$$\overline{OB}=\beta$$
$$\overline{OC}=l\alpha+m\beta$$
{\sl be three given co-planar vectors, $c$ the intersection of $AB$, $OC$, and 
if the lines indicated in the figure be drawn, the points 
$a_1$,$b_1$,$c_1$ lie in a straight line. }

\begin{center}
\includegraphics{ps/quat2.ps}
\end{center}
\vskip 0.5cm

We see at once, by the process indicated in \S 30, that 
$$\overline{Oc}=\frac{l\alpha+m\beta}{l+m},\hbox{\hskip 1cm}
\overline{Ob}=\frac{l\alpha}{1-m},\hbox{\hskip 1cm}
\overline{Oa}=\frac{m\beta}{1-l}$$
Hence we easily find 
$$\overline{Oa_1}=-\frac{m\beta}{1-l-2m},\hbox{\hskip 0.5cm}
\overline{Ob_1}=-\frac{l\alpha}{1-2l-m},\hbox{\hskip 0.5cm}
\overline{Oc_1}=\frac{-l\alpha+m\beta}{m-l}$$
These give 
$$-(1-l-2m)\overline{Oa_1}+(1-2l-m)\overline{Ob_1}-(m-l)\overline{Oc_1}=0$$
But\hbox{\hskip 1cm}$-(1-l-2m)+(1-2l-m)-(m - l)=0$ identically. 

This, by \S 30, proves the proposition. 

(d) Let $\overline{OA} = \alpha$, 
$\overline{OB} = \beta$, be any two vectors. If $MP$ be a 
given line parallel to $OB$; and $OQ$, $BQ$, be drawn parallel to $AP$, 
$OP$ respectively ; the locus of $Q$ is a straight line parallel to $OA$. 

\begin{center}
\includegraphics{ps/quat3.ps}
\end{center}
\vskip 0.5cm

\noindent
Let \hbox{\hskip 4cm}$\overline{OM}=e\alpha$\\
Then \hbox{\hskip 3cm}$\overline{AP}=\overline{e-1}\alpha+x\beta$

Hence the equation of $OQ$ is 
$$\rho=y(\overline{e-1}\alpha+x\beta)$$
and that of $BQ$ is\hbox{\hskip 1cm}$\rho=\beta+z(e\alpha+x\beta)$\\
At Q we have, therefore, 
$$
\left.
\begin{array}{c}
xy=1+zx\\
y(e-1)=ze
\end{array}
\right\}
$$
These give $xy = e$, and the equation of the locus of $Q$ is 
$$\rho = e\beta+y^{\prime}\alpha$$
i.e. a straight line parallel to $OA$, drawn through $N$ in $OB$ 
produced, so that 
$$ON : OB :: OM : OA$$

COR. If $BQ$ meet $MP$ in $q$, $\overline{Pq} = \beta$; 
and if $AP$ meet $NQ$ in $p$, $\overline{Qp} = \alpha$. 

Also, for the point $R$ we have $\overline{pR} = \overline{AP}$, 
$\overline{QR} = \overline{Bq}$. 

Further, the locus of $R$ is a hyperbola, of which $MP$ and $NQ$ 
are the asymptotes. See, in this connection, \S 31 (k) below. 

Hence, {\sl if from any two points, $A$ and $B$, lines be drawn intercepting 
a given length $Pq$ on a given line $Mq$ ; and if, from $R$ their 
point of intersection, $Rp$ be laid off $= PA$, and $RQ = qB$ ; $Q$ and $p$ 
lie on a fixed straight line, and the length of $Qp$ is constant}. 

(e) {\sl To find the centre of inertia of any system of masses.}
 
If $\overline{OA} = \alpha$, $\overline{OB} = \alpha_1$, 
be the vector sides of any triangle, the 
vector from the vertex dividing the base $AB$ in $C$ so that 
$$BC : CA :: m : m_1$$
is \hbox{\hskip 4cm}$\frac{m\alpha+m_1\alpha_1}{m+m_1}$\\

For $AB$ is $\alpha_1-\alpha$, and therefore $\overline{AC}$ is 
$$\frac{m_1}{m+m_1}(\alpha_1-\alpha)$$

Hence\hbox{\hskip 3cm}$\overline{OC}=\overline{OA}+\overline{AC}$
$$=\alpha+\frac{m_1}{m+m_1}(\alpha_1-\alpha)$$
$$=\frac{m\alpha +m_1\alpha_1}{m+m_1}$$
This expression shows how to find the centre of inertia of two 
masses ; $m$ at the extremity of $\alpha$, $m_1$ at that of $\alpha_1$. 
Introduce $m_2$ at the extremity of $a_2$, 
then the vector of the centre of inertia of the 
three is, by a second application of the formula, 
$$\frac{(m+m_1)(\frac{m\alpha+m_1\alpha_1}{m+m_1})+m_2\alpha_2}
{(m+m_1)+m_2}=\frac{m\alpha+m_1\alpha_1+m_2\alpha_2}{m+m_1+m_2}$$
From this it is clear that, for any number of masses, expressed 
generally by $m$ at the extremity of the vector $\alpha$, the vector of the 
centre of inertia is 
$$\beta=\frac{\sum(m\alpha)}{\sum(m)}$$
This may be written\hbox{\hskip 1cm}$\sum m(\alpha-\beta)=0$\\
Now a $\alpha_1-\beta$ 
is the vector of $m_1$ with respect to the centre of inertia. 
Hence the theorem, {\sl If the vector of each element of a mass, drawn 
from the centre of inertia, be increased in length in proportion to the 
mass of the element, the sum of all these vectors is zero.}

(f) We see at once that the equation 

\begin{center}
\includegraphics{ps/quat4.ps}
\end{center}
\vskip 0.5cm

$$\rho=\alpha t +\frac{\beta t^2}{2}$$
where $t$ is an indeterminate 
number, and $\alpha$, $\beta$ given vectors, 
represents a parabola. 
The origin, $O$, is a point on 
the curve, $\beta$ is parallel to 
the axis, i.e. is the diameter 
$OB$ drawn from the origin, 
and $\alpha$ is $OA$ the tangent at the origin. In the figure 
$$\overline{QP}=\alpha t,\hbox{\hskip 1cm}\overline{OQ}=\frac{\beta t^2}{2}$$

The secant joining the points where $t$ has the values $t$ and $t^{\prime}$ is 
represented by the equation 
$$
\begin{array}{rcl}
\rho&=&\alpha t +\frac{\beta t^2}{2}+
x\left(\alpha t^{\prime}+\frac{\beta t^{'2}}{2}
-\alpha t-\frac{\beta t^2}{2}\right)\hbox{\hskip 1cm}(\S 30)\\
&=&\alpha t+\frac{\beta t^2}{2}+
x(t^{\prime}-t)\left\{\alpha+\beta\frac{t^{\prime}-t}{2}
\right\}
\end{array}
$$
Write $x$ for $x(t^{\prime}-t)$ [which may have any value], then put 
$t^{\prime}=t$, and the equation of the tangent at the point ($t$) is 
$$\rho=\alpha t + \frac{\beta t^2}{2}+x(\alpha+\beta t)$$
In this put $x = -t$, and we have 
$$\rho=-\frac{\beta t^2}{2}$$
or the intercept of the tangent on the diameter is equal in length 
to the abscissa of the point of contact, but has the opposite 
currency. 

Otherwise: the tangent is parallel to the vector $\alpha+\beta t$ or 
$\alpha t + \beta t^2$ or $\frac{\beta t^2}{2}+\alpha t+\frac{\beta t^2}{2}$
or $\overline{OQ}+\overline{OP}$. 
But $\overline{TP}=\overline{TO}+\overline{OP}$,
hence $\overline{TO} = \overline{OQ}$. 

(g) Since the equation of any tangent to the parabola is 
$$\rho=\alpha t + \frac{\beta t^2}{2} + x(\alpha+\beta t)$$
let us find the tangents which can be drawn from a given point. 
Let the vector of the point be 
$$\rho=p\alpha + q\beta\hbox{\hskip 0.5cm}(\S 24)$$
Since the tangent is to pass through this point, we have, as con 
ditions to determine $t$ and $x$, 
$$t+x=p$$
$$\frac{t^2}{2}+xt=q$$
by equating respectively the coefficients of $\alpha$ and $\beta$.

Hence\hbox{\hskip 3.5cm}$t=p \pm \sqrt{p^2-2q}$

Thus, in general, {\sl two} tangents can be drawn from a given point. 
These coincide if $$p^2=2q$$
that is, if the vector of the point from which they are to be drawn 
is $$\rho=p\alpha+q\beta=p\alpha+\frac{p^2}{2}\beta$$
i.e. if the point lies on the parabola. They are imaginary if 
$2q > p^2$, that is, if the point be 
$$\rho=p\alpha+\left(\frac{p^2}{2}+r\right)\beta$$
$r$ being {\sl positive}. Such a point is evidently {\sl within} the curve, 
as at $R$, where $\overline{OQ}=\frac{p^2}{2}\beta$, 
$\overline{QP}=p\alpha$, $\overline{PR} = r\beta$. 

(h) Calling the values of $t$ for the two tangents found in (g) 
$t_1$ and $t_2$ respectively, it is obvious that the vector joining the 
points of contact is 
$$\alpha t_1+\frac{\beta t_1^2}{2}-\alpha t_2 - \frac{\beta t_2^2}{2}$$
which is parallel to\hbox{\hskip 2cm}
$\alpha+\beta\frac{t_1+t_2}{2}$
or, by the values of $t_1$ and $t_2$ in (g), 
$$\alpha+p\beta$$
Its direction, therefore, does not depend on $q$. In words, {\sl If pairs of 
tangents be drawn to a parabola from points of a diameter produced, 
the chords of contact are parallel to the tangent at the vertex of the 
diameter.} This is also proved by a former result, for we must have 
$\overline{OT}$ for each tangent equal to $\overline{QO}$. 

(i) The equation of the chord of contact, for the point whose vector is 
$$\rho=p\alpha+q\beta$$
is thus\hbox{\hskip 3cm}
$\rho=\alpha t_1+\frac{\beta t_1^2}{2}+y(\alpha+p\beta)$

Suppose this to pass always through the point whose vector is 
$$\rho=a\alpha+b\beta$$
Then we must have
$$
\left.
\begin{array}{rcl}
t_1+y & = & a\\
\frac{t_1^2}{2}+py & = & b
\end{array}
\right\}
$$
or\hbox{\hskip 4cm}$t_1=p\pm\sqrt{p^2-2p\alpha+2\beta}$

Comparing this with the expression in (g), we have 
$$
q = pa - b 
$$
that is, the point from which the tangents are drawn has the vector 
a straight line (\S 28 (1)).

The mere form of this expression contains the proof of the usual 
properties of the pole and polar in the parabola ; but, for the sake 
of the beginner, we adopt a simpler, though equally general, process. 

Suppose $\alpha = 0$. This merely restricts the pole to the particular 
diameter to which we have referred the parabola. Then the pole 
is $Q$, where $$\rho = b\beta$$
and the polar is the line $TU$, for which 
$$\rho=-b\beta+p\alpha$$
{\sl Hence the polar of any point is parallel to the tangent at the 
extremity of the diameter on which the point lies, and its intersection 
with that diameter is as far beyond the vertex as the pole 
is within, and vice versa. }

(j) As another example let us prove the following theorem. 
{\sl If a triangle be inscribed in a parabola, the three points in which 
the sides are met by tangents at the angles lie in a straight line. }

Since $O$ is any point of the curve, we may take it as one corner 
of the triangle. Let $t$ and $t_1$ determine the others. Then, if 
$\omega_1$,$\omega_2$,$\omega_3$
represent the vectors of the points of intersection of the 
tangents with the sides, we easily find 
$$
\begin{array}{rcl}
\omega_1 & = & \frac{t_1^2}{2t_1-t}
\left(\alpha+\frac{t}{2}\beta\right)\\
&&\\
\omega_2 & = & \frac{t^2}{2t-t_1}
\left(\alpha+\frac{t_1}{2}\beta\right)\\
&&\\
\omega_3 & = & \frac{tt_1}{t_1+t}\alpha
\end{array}
$$
These values give 
$$\frac{2t_1-t}{t_1}\omega_1 -
\frac{2t-t_1}{t}\omega_2 -
\frac{t_1^2-t^2}{tt_1}\omega_3 = 0$$
Also
$$\frac{2t_1-t}{t_1} -
\frac{2t-t_1}{t} -
\frac{t_1^2-t^2}{tt_1} = 0$$
identically. 

Hence, by \S 30, the proposition is proved. 

(k) Other interesting examples of this method of treating 
curves will, of course, suggest themselves to the student. Thus 
$$\rho = \alpha\cos t + \beta\sin t$$
or
$$\rho=\alpha x + \beta\sqrt{1-x^2}$$
represents an ellipse, of which the given vectors $\alpha$ and $\beta$ 
are semiconjugate diameters. If $t$ represent time, the radius-vector of this 
ellipse traces out equal areas in equal times. [We may anticipate 
so far as to write the following : 
$$2 \textrm{Area} = T\int V \rho d\rho = TV\alpha\beta.\int dt$$
which will be easily understood later.] 

Again, 
$$\rho=\alpha t+\frac{\beta}{t}\textrm{  or  }
\rho=\alpha\tan x + \beta\cot x$$
evidently represents a hyperbola referred to its asymptotes. [If 
$t$ represent time, the sectorial area traced out is proportional to 
$\log t$, taken between proper limits.] 
Thus, also, the equation 
$$\rho = \alpha(t + \sin t)+\beta\cos t$$
in which $\alpha$ and $\beta$ are of equal lengths, and at right angles to one 
another, represents a cycloid. The origin is at the middle point of 
the axis ($2\beta$) of the curve. [It may be added that, if t represent 
{\sl time}, this equation shows the motion of the tracing point, provided 
the generating circle rolls uniformly, revolving at the rate of a 
radian per second.] 

When the lengths of $\alpha$, $\beta$ are not equal, this equation gives the 
cycloid distorted by elongation of its ordinates or abscissae : {\sl not} a 
trochoid. The equation of a trochoid may be written 
$$\rho = \alpha(et + \sin t)+\beta\cos t$$
$e$ being greater or less than 1 as the curve is prolate or curtate. 
The lengths of $\alpha$ and $\beta$ are still taken as equal. 

But, so far as we have yet gone with the explanation of the 
calculus, as we are not prepared to determine the lengths or 
inclinations of vectors, we can investigate only a very limited class of 
the properties of curves, represented by such equations as those 
above written. 

(l) We may now, in extension of the statement in \S 29, make 
the obvious remark that 
$$\rho = \sum p\alpha$$
(where, as in \S 23, the number of vectors, $\alpha$, can always be reduced 
to {\sl three}, at most) is the equation of a curve in space, if the 
numbers $p_1$, $p_2$, \&c.  are functions of one indeterminate. In such 
a case the equation is sometimes written 
$$\rho=\phi(t)$$
But, if $p_1$, $p_2$, \&c. be functions of {\sl two} indeterminates, 
the locus of the extremity of $\rho$ is a {\sl surface}; 
whose equation is sometimes written 
$$\rho = \phi(t,u)$$

[It may not be superfluous to call the reader's attention to the 
fact that, in these equations, $\phi(t)$ or $\phi(t, u)$ 
is necessarily a vector expression, since it is equated to a vector, $\rho$.] 

(m) Thus the equation 
\begin{equation}\label{Quat1}
\rho = \alpha\cos t+\beta\sin t + \gamma t
\end{equation}
belongs to a helix, 
while 
\begin{equation}\label{Quat2}
\rho = \alpha\cos t+\beta\sin t + \gamma u
\end{equation}
represents a cylinder whose generating lines are parallel to $\gamma$,
and 
whose base is the ellipse 
$$\rho=\alpha\cos t + \beta\sin t$$
The helix above lies wholly on this cylinder. 

Contrast with (2) the equation 
$$\rho = u(\alpha\cos t + \beta\sin t + \gamma)\eqno(3)$$
which represents a cone of the second degree
made up, in fact, 
of all lines drawn from the origin to the ellipse 
$$\rho=\alpha\cos t + \beta\sin t + \gamma$$
If, however, we write 
$$\rho = u(\alpha\cos t + \beta\sin t + \gamma t)$$
we form the equation of the transcendental cone whose vertex is 
at the origin, and on which lies the helix (1). 

In general 
$$\rho=u\phi(t)$$
is the cone whose vertex is the origin, and on which lies the curve 
$$\rho=\phi(t)$$
while\hbox{\hskip 4cm}$\rho=\phi(t)+u\alpha$\\
is a cylinder, with generating lines parallel to $\alpha$, standing on the 
same curve as base. 

Again,\hbox{\hskip 3cm}$\rho=p\alpha+q\beta+r\gamma$\\
with a condition of the form 
$$ap^2+bq^2+cr^2=1$$
belongs to a central surface of the second order, of which 
$\alpha$, $\beta$, $\gamma$
are the directions of conjugate diameters. If $a$, $b$, $c$ be all positive, 
the surface is an ellipsoid. 

{\bf 32}. In Example ($f$) above we performed an operation equivalent 
to the differentiation of a vector with reference to a single 
{\sl numerical} variable of which it was given as an explicit function. 
As this process is of very great use, especially in quaternion 
investigations connected with the motion of a particle or point; and as it 
will afford us an opportunity of making a preliminary step towards 
overcoming the novel difficulties which arise in quaternion differentiation; 
we will devote a few sections to a more careful, though 
very elementary, exposition of it. 

{\bf 33}. It is a striking circumstance, when we consider the way 
in which Newton's original methods in the Differential Calculus 
have been decried, to find that Hamilton was {\sl obliged} to employ 
them, and not the more modern forms, in order to overcome the 
characteristic difficulties of quaternion differentiation. Such a thing 
as {\sl a differential coefficient has absolutely no meaning in quaternions}, 
except in those special cases in which we are dealing with degraded 
quaternions, such as numbers, Cartesian coordinates, \&c. But a 
quaternion expression has always a {\sl differential}, which is, simply, 
what Newton called a {\sl fluxion}. 

As with the Laws of Motion, the basis of Dynamics, so with the 
foundations of the Differential Calculus ; we are gradually coming 
to the conclusion that Newton s system is the best after all. 

{\bf 34}. Suppose $\rho$ to be the vector of a curve in space. Then, 
generally, $\rho$ may be expressed as the sum of a number of terms, 
each of which is a multiple of a constant vector by a function of some 
{\sl one} indeterminate; or, as in \S 31 ($l$), 
if $P$ be a point on the curve, 
$$\overline{OP}=\rho=\phi(t)$$

And, similarly, if $Q$ be {\sl any other} point on the curve, 
$$\overline{OQ}=\rho_1=\rho+\delta\rho=\phi(t_1)=\phi(t+\delta t)$$
where $\delta t$ is {\sl any number whatever}. 

The vector-chord $\overline{PQ}$ is therefore, rigorously, 
$$\delta p = \rho_1-\rho = \phi(t+\delta t)-\phi t$$

{\bf 35}. It is obvious that, in the present case, {\sl because the vectors 
involved in $\phi$ are constant, and their numerical multipliers alone vary}, 
the expression $\phi(t+\delta t)$ is, by Taylor's Theorem, equivalent to 
$$\phi(t)+\frac{d\phi(t)}{dt}\delta t+
\frac{d^2\phi(t)}{dt^2}\frac{(\delta t)^2}{1\textrm{ . }2}+\ldots$$

Hence, 
$$\delta \rho=\frac{d\phi(t)}{dt}\delta t+
\frac{d^2\phi(t)}{dt^2}\frac{(\delta t)^2}{1\textrm{ . }2}+\textrm{\&c.}$$
And we are thus entitled to write, when $\delta t$ has been made 
indefinitely small, 
$$\textrm{Limit}\left(
\begin{array}{c}
\delta p\\
\delta t
\end{array}
\right)_{\delta t=0}
=\frac{d\rho}{dt}
=\frac{d\phi(t)}{dt}
=\phi^{\prime}(t)$$

In such a case as this, then, we are permitted to differentiate, 
or to form the differential coefficient of, a vector, according to the 
ordinary rules of the Differential Calculus. But great additional 
insight into the process is gained by applying Newton's method. 

{\bf 36}. Let $\overline{OP}$ be 
$$\rho=\phi(t)$$
and $overline{OQ}_1$
$$\rho_1=\phi(t+dt)$$
where $dt$ is any number whatever. 

\begin{center}
\includegraphics{ps/quat5.ps}
\end{center}
\vskip 0.5cm

The number $t$ may here be taken 
as representing {\sl time}, i.e. we may 
suppose a point to move along the 
curve in such a way that the value 
of $t$ for the vector of the point $P$ of 
the curve denotes the interval which 
has elapsed (since a fixed epoch) when the moving point has 
reached the extremity of that vector. If, then, $dt$ represent any 
interval, finite or not, we see that 
$$\overline{OQ}_1=\phi(t+dt)$$
will be the vector of the point after the additional interval $dt$. 

But this, in general, gives us little or no information as to the 
velocity of the point at $P$. We will get a better approximation 
by halving the interval $dt$, and finding $Q_2$ , 
where $\overline{OQ}_2 = \phi(t + \frac{1}{2}dt)$,
as the position of the moving point at that time. Here the vector 
virtually described in $\frac{1}{2}dt$ is 
$\overline{PQ}_2$ . To find, on this supposition, 
the vector described in $dt$, we must double 
$\overline{PQ}_2$ , and we find, as a 
second approximation to the vector which the moving point would 
have described in time $dt$, if it had moved for that period in the 
direction and with the velocity it had at $P$, 
$$
\begin{array}{rcl}
\overline{Pq}_2=2\overline{PQ}_2 & = & 2(\overline{OQ}_2-\overline{OP})\\
& = & 2\{\phi(t+\frac{1}{2}dt)-\phi(t)\}
\end{array}
$$
The next approximation gives 
$$
\begin{array}{rcl}
\overline{Pq}_3=3\overline{PQ}_3 & = & 3(\overline{OQ}_3-\overline{OP})\\
& = & 3\{\phi(t+\frac{1}{3}dt)-\phi(t)\}
\end{array}
$$
And so on, each step evidently leading us nearer the sought truth. 
Hence, to find the vector which would have been described in time 
$dt$ had the circumstances of the motion at $P$ remained undisturbed, 
we must find the value of 
$$d\rho=\overline{Pq}=L_{x=\infty}x\left\{\phi\left(t+\frac{1}{x}dt\right)
-\phi(t)\right\}$$

We have seen that in this particular case we may use Taylor's 
Theorem. We have, therefore, 
$$
\begin{array}{rcl}
d\rho & = & L_{x=\infty}x
\left\{
\phi^{\prime}(t)\frac{1}{x}dt+
\phi^{\prime\prime}(t)\frac{1}{x^2}\frac{(dt)^2}{1\textrm{ . }2}+
\textrm{\&c}
\right\}\\
&&\\
& = & \phi^{\prime}(t)dt
\end{array}
$$
And, if we choose, we may now write 
$$\frac{d\rho}{dt}=\phi^{\prime}(t)$$

{\bf 37}. But it is to be most particularly remarked that in the 
whole of this investigation no regard whatever has been paid to 
the magnitude of $dt$. The question which we have now answered 
may be put in the form -- {\sl A point describes a given curve in a given 
manner. At any point of its path its motion suddenly ceases to be 
accelerated. What space will it describe in a definite interval?} As 
Hamilton well observes, this is, for a planet or comet, the case 
of a 'celestial Atwood's machine'. 

{\bf 38}. If we suppose the variable, in terms of which $\rho$ is expressed, 
to be the arc, $s$, of the curve measured from some fixed point, we 
find as before 
$$d\rho = \phi^{\prime}(x)ds$$
From the very nature of the question it is obvious that the length 
of $dp$ must in this case be $ds$, so that $\phi^{\prime}(s)$ 
is necessarily a unit-vector. 
This remark is of importance, as we will see later; and 
it may therefore be useful to obtain afresh the above result without 
any reference to time or velocity. 

{\bf 39}. Following strictly the process of Newton s VIIth Lemma, 
let us describe on $Pq_2$ an arc similar to $PQ_2$, and so on. Then 
obviously, as the subdivision of $ds$ is carried farther, the new arc 
(whose length is always $ds$) more and more nearly (and without 
limit) coincides with the line which expresses the corresponding 
approximation to $dp$. 

{\bf 40}. As additional examples let us take some well-known 
{\sl plane} curves; and first the hyperbola (\S 31 ($k$))
$$\rho=\alpha t + \frac{\beta}{t}$$
Here
$$d\rho=\left(\alpha-\frac{\beta}{t^2}\right)dt$$
This shows that the tangent is parallel to the vector 
$$\alpha t - \frac{\beta}{t}$$
In words, {\sl if the vector (from the centre) of a point in a hyperbola 
be one diagonal of a parallelogram, two of whose sides coincide with 
the asymptotes, the other diagonal is parallel to the tangent at the 
point, and cuts off a constant area from the space between the 
asymptotes}. (For the sides of this triangular area are $t$ times the 
length of $\alpha$, and $1/t$
times the length of $\beta$, respectively; the angle 
between them being constant.) 

Next, take the cycloid, as in \S 31 ($k$), 
$$\rho=\alpha(t+\sin t)+\beta\cos t$$
We have 
$$d\rho=\{\alpha(1+\cos t)-\beta\sin t\}dt$$
At the vertex 
$$t=0,\hbox{\hskip 0.5cm}
\cos t=1,\hbox{\hskip 0.5cm}
\sin t=0,\hbox{\hskip 0.5cm}
\textrm{ and }d\rho=2\alpha dt$$
At a cusp 
$$t=\pi,\hbox{\hskip 0.5cm}
\cos t=-1,\hbox{\hskip 0.5cm}
\sin t=0,\hbox{\hskip 0.5cm}
\textrm{ and }d\rho = 0$$

This indicates that, at the cusp, the tracing point is (
instantaneously) at rest. To find the direction of the tangent, and the 
form of the curve in the vicinity of the cusp, put $t=\pi+\tau$,
where powers of $\tau$ above the second are omitted. We have 
$$d\rho=\beta\tau dt + \frac{\alpha\tau^2}{2}dt$$
so that, at the cusp, the tangent is parallel to $\beta$. By making the 
same substitution in the expression for $\rho$, we find that the part of 
the curve near the cusp is a semicubical parabola, 
$$\rho=\alpha(\pi+\tau^3/6)-\beta(1-\tau^2/2)$$
or, if the origin be shifted to the cusp ($\rho=\pi\alpha-\beta$),
$$\rho=\alpha\tau^3/6+\beta\tau^2/2$$

{\bf 41}. Let us reverse the first of these questions, and {\sl seek the 
envelope of a line which cuts off from two fixed axes a triangle of 
constant area}. 

If the axes be in the directions of $\alpha$ and $\beta$, the intercepts may 
evidently be written $\alpha t$ and $\frac{\beta}{t}$.
Hence the equation of the line is (\S 30) 
$$\rho=\alpha t + x\left(\frac{\beta}{t}-\alpha t\right)$$

The condition of envelopment is, obviously, (see Chap. IX.) 
$$d\rho = 0$$
This gives 
$0 = \left\{\alpha-x\left(\frac{\beta}{t^2}+\alpha\right)\right\}dt+
\left(\frac{\beta}{t}-\alpha t\right)dx$
%tpdhere -- this should use an asterisk, a number looks like a superscript
\setcounter{footnote}{1}%% use a dagger because number looks like superscript
\footnote{
Here we have opportunity for a remark (very simple indeed, but) 
of the utmost importance. {\sl We are not to equate separately to zero the 
coefficients of dt and dx}; for we must remember that this equation is 
of the form 
$$0=p\alpha + q\beta$$
where $p$ and $q$ are numbers; and that, so long as $\alpha$ and $\beta$ 
are actual and non-parallel vectors, the existence of such an equation 
requires (\S 24)}

%tpdhere this is a cheesy use of \leqno. figure out how to do it right.
\noindent
$$\leqno{\textrm{Hence}}\hbox{\hskip 4cm}(1-x)dt-tdx=0$$
%tpdhere -- this text comes out little. 
$$\leqno{\textrm{and}}\hbox{\hskip 4cm}-\frac{x}{t^2}dt+\frac{dx}{t}=0$$
From these, at once, $x = \frac{1}{2}$, since $dx$ and $dt$ are indeterminate. 
Thus the equation of the envelope is 
$$\begin{array}{rcl}
\rho & = & \alpha t + \frac{1}{2}\left(\frac{\beta}{t}-\alpha t\right)\\
     & = & \frac{1}{2}\left(\alpha t + \frac{\beta}{t}\right)
\end{array}$$
the hyperbola as before; $\alpha$, $\beta$ being portions of its asymptotes. 

{\bf 42}. It may assist the student to a thorough comprehension 
of the above process, if we put it in a slightly different form. 
Thus the equation of the enveloping line may be written 
$$\rho=\alpha t(1-x)+\beta\frac{x}{t}$$
which gives 
$$d\rho = 0 = \alpha d\{t(1-x)\}+\beta d\left(\frac{x}{t}\right)$$
Hence, as $\alpha$ is not parallel to $\beta$, we must have 
$$d\{t(1-x)\}=0,\hbox{\hskip 1cm}d\left(\frac{x}{t}\right)=0$$
and these are, when expanded, the equations we obtained in the 
preceding section. 

{\bf 43}. For farther illustration we give a solution not directly 
employing the differential calculus. The equations of any two of 
the enveloping lines are 
$$\rho=\alpha t + x\left(\frac{\beta}{t}-\alpha t\right)$$
$$\rho=\alpha t_1 + x_1\left(\frac{\beta}{t_1}-\alpha t_1\right)$$
$t$ and $t_1$ being given, while $x$ and $x_1$ are indeterminate. 

At the point of intersection of these lines we have (\S 26), 
$$
\left.
\begin{array}{rcl}
t(1-x) & = & t_1(1-x_1)\\
\frac{x}{t} & = & \frac{x_1}{t_1}
\end{array}
\right\}$$
These give, by eliminating $x_1$
$$t(1-x)=t_1\left(1-\frac{t_1}{t}x\right)$$
$$\leqno{\textrm{or}}\hbox{\hskip 4cm}x=\frac{t}{t_1+t}$$
Hence the vector of the point of intersection is 
$$\rho=\frac{\alpha tt_1+\beta}{t_1+t}$$
and thus, for the ultimate intersections, where $L\frac{t_1}{t}=1$,
$$\rho=\frac{1}{2}\left(\alpha t+\frac{\beta}{t}\right)
\textrm{ as before }$$
COR. If. instead of the {\sl ultimate} intersections, we consider 
the intersections of pairs of these lines related by some law, we 
obtain useful results. Thus let 
$$tt_1 = 1$$
$$\rho=\frac{\alpha+\beta}{t+\frac{1}{t}}$$
or the intersection lies in the diagonal of the parallelogram on 
$\alpha$, $\beta$.

If $t_1=mt$, where $m$ is constant, 
$$\rho=\frac{mt\alpha+\frac{\beta}{t}}{m+1}$$
But we have also $x=\frac{1}{m+1}$

Hence {\sl the locus of a point which divides in a given ratio a line 
cutting off a given area from two fixed axes, is a hyperbola of which 
these axes are the asymptotes}. 

If we take either 
$$tt_1(t+t_1)=\textrm{constant, or }
\frac{t^2t_1^2}{t+t_1}=\textrm{constant}$$
the locus is a parabola; and so on. 

It will be excellent practice for the student, at this stage, to 
work out in detail a number of similar questions relating to the 
envelope of, or the locus of the intersection of selected pairs from, a 
series of lines drawn according to a given law. And the process 
may easily be extended to planes. Thus, for instance, we may 
form the general equation of planes which cut off constant tetrahedra 
from the axes of coordinates. Their envelope is a surface of 
the third degree whose equation may be written 
$$\rho=x\alpha+y\beta+z\gamma$$
$$\leqno{\textrm{where}}\hbox{\hskip 4cm}xyz=\alpha^3$$

Again, find the locus of the point of intersection of three of 
this group of planes, such that 
the first intercepts on $\beta$ and $\gamma$, 
the second on $\gamma$ and $\alpha$, 
the third on $\alpha$ and $\beta$, lengths all equal to one 
another, \&c. But we must not loiter with such simple matters as 
these. 

{\bf 44}. The reader who is fond of Anharmonic Ratios and Trans 
versals will find in the early chapters of Hamilton's {\sl Elements of 
Quaternions} an admirable application of the composition of vectors 
to these subjects. The Theory of Geometrical Nets, in a plane, 
and in space, is there very fully developed; and the method is 
shown to include, as particular cases, the corresponding processes of 
Grassmann's {\sl Ausdehnungslehre} and M\"obius' {\sl Barycentrische Calcul}. 
Some very curious investigations connected with curves and surfaces 
of the second and third degrees are also there founded upon the 
composition of vectors. 

\section{Examples To Chapter 1.}

1. The lines which join, towards the same parts, the extremities 
of two equal and parallel lines are themselves equal and parallel. 
({\sl Euclid}, I. xxxiii.) 

2. Find the vector of the middle point of the line which joins 
the middle points of the diagonals of any quadrilateral, plane or 
gauche, the vectors of the corners being given; and so prove that 
this point is the mean point of the quadrilateral. 

If two opposite sides be divided proportionally, and two new 
quadrilaterals be formed by joining the points of division, the mean 
points of the three quadrilaterals lie in a straight line. 

Show that the mean point may also be found by bisecting the 
line joining the middle points of a pair of opposite sides. 

3. Verify that the property of the coefficients of three vectors 
whose extremities are in a line (\S 30) is not interfered with by 
altering the origin. 

4. If two triangles $ABC$, $abc$, be so situated in space that $Aa$, 
$Bb$, $Cc$ meet in a point, the intersections of $AB$, $ab$, of $BC$, $bc$, 
and of $CA$, $ca$, lie in a straight line. 

5. Prove the converse of 4, i.e. if lines be drawn, one in each 
of two planes, from any three points in the straight line in which 
these planes meet, the two triangles thus formed are sections of a 
common pyramid. 

6. If five quadrilaterals be formed by omitting in succession 
each of the sides of any pentagon, the lines bisecting the diagonals 
of these quadrilaterals meet in a point. (H. Fox Talbot.) 

7. Assuming, as in \S 7, that the operator 
$$\cos\theta + \sqrt{-1}\sin\theta$$
turns any radius of a given circle through an angle $\theta$ in the 
positive direction of rotation, without altering its length, deduce 
the ordinary formulae for $\cos(A+B)$, $\cos(A-B)$, $\sin(A+B)$, and 
$\sin(A-B)$, in terms of sines and cosines of $A$ and $B$. 

8. If two tangents be drawn to a hyperbola, the line joining 
the centre with their point of intersection bisects the lines join 
ing the points where the tangents meet the asymptotes : and the 
secant through the points of contact bisects the intercepts on 
the asymptotes. 

9. Any two tangents, limited by the asymptotes, divide each 
other proportionally. 

10. If a chord of a hyperbola be one diagonal of a parallelogram 
whose sides are parallel to the asymptotes, the other diagonal passes 
through the centre. 

11. Given two points $A$ and $B$, and a plane, $C$. Find the 
locus of $P$, such that if $AP$ cut $C$ in $Q$, and $BP$ cut $C$ in $R$, 
$\overline{QR}$ may be a given vector. 

12. Show that\hbox{\hskip 1cm} $\rho = x^2\alpha+y^2\beta+(x+y)^2\gamma$\\
is the equation of a cone of the second degree, and that its section 
by the plane 
$$\rho=\frac{p\alpha+q\beta+r\gamma}{p+q+r}$$
is an ellipse which touches, at their middle points, the sides of the 
triangle of whose corners $\alpha$, $\beta$, $\gamma$ 
are the vectors. (Hamilton, {\sl Elements}, p. 96.) 

13. The lines which divide, proportionally, the pairs of opposite 
sides of a gauche quadrilateral, are the generating lines of a 
hyperbolic paraboloid. ({\sl Ibid}. p. 97.) 

14. Show that\hbox{\hskip 2cm} $\rho=x^3\alpha+y^3\beta+z^3\gamma$\\
where\hbox{\hskip 4cm} $x+y+z=0$\\
represents a cone of the third order, and that its section by the plane 
$$\rho=\frac{p\alpha+q\beta+r\gamma}{p+q+r}$$
is a cubic curve, of which the lines 
$$\rho=\frac{p\alpha+q\beta}{p+q},\textrm{ \&c}$$
are the asymptotes and the three (real) tangents of inflection. Also 
that the mean point of the triangle formed by these lines is a 
conjugate point of the curve. Hence that the vector $\alpha+\beta+\gamma$
is a conjugate ray of the cone. ({\sl Ibid}. p. 96.) 

\section{Products And Quotients of Vectors}

{\bf 45}. We now come to the consideration of questions in which 
the Calculus of Quaternions differs entirely from any previous 
mathematical method; and here we will get an idea of what a 
Quaternion is, and whence it derives its name. These questions 
are fundamentally involved in the novel use of the symbols of 
multiplication and division. And the simplest introduction to 
the subject seems to be the consideration of the quotient, or ratio, 
of two vectors. 

{\bf 46}. If the given vectors be parallel to each other, we have 
already seen (\S 22) that either may be expressed as a numerical 
multiple of the other; the multiplier being simply the ratio of 
their lengths, taken positively if they have similar currency, 
negatively if they run opposite ways. 

{\bf 47}. If they be not parallel, let $\overline{OA}$ and 
$\overline{OB}$ be drawn parallel 
and equal to them from any point $O$; and the question is reduced 
to finding the value of the ratio of two vectors drawn from the 
same point. Let us first find {\sl upon how many distinct numbers this 
ratio depends}.

We may suppose $\overline{OA}$ to be changed into 
$\overline{OB}$ by the following successive processes. 

1st. Increase or diminish the length of $\overline{OA}$ till it becomes 
equal to that of $\overline{OB}$. For this only one number is required, viz. 
the ratio of the lengths of the two vectors. As Hamilton remarks, 
this is a positive, or rather a {\sl signless}, number. 

2nd. Turn $\overline{OA}$ about $O$, in the common plane of the two 
vectors, until its direction coincides with that of $\overline{OB}$, and 
(remembering the effect of the first operation) we see that the two vectors 
now coincide or become identical. To specify this operation three 
numbers are required, viz. two angles (such as node and inclination 
in the case of a planet's orbit) to fix the plane in which the rotation 
takes place, and {\sl one} angle for the amount of this rotation. 

Thus it appears that the ratio of two vectors, or the multiplier 
required to change one vector into another, in general depends upon 
{\sl four} distinct numbers, whence the name QUATERNION. 

A quaternion q is thus {\sl defined} as expressing a relation 
$$\beta=q\alpha$$
between two vectors $\alpha$, $\beta$. 
By what precedes, the vectors $\alpha$, $\beta$, 
which serve for the definition of a given quaternion, must be in a 
given plane, at a given inclination to each other, and with their 
lengths in a given ratio ; but it is to be noticed that they may be 
{\sl any} two such vectors. [{\sl Inclination} is understood to include sense, 
or currency, of rotation from $\alpha$ to $\beta$.] 

The particular case of perpendicularity of the two vectors, where 
their quotient is a vector perpendicular to their plane, is fully 
considered below; \S\S 64, 65, 72, \&c. 

{\bf 48}. It is obvious that the operations just described may be 
performed, with the same result, in the opposite order, being perfectly 
independent of each other. Thus it appears that a quaternion, 
considered as the factor or agent which changes one definite vector 
into another, may itself be decomposed into two factors of which 
the order is immaterial. 

The {\sl stretching} factor, or that which performs the first operation 
in \S 47, is called the TENSOR, and is denoted by prefixing $T$ to the 
quaternion considered. 

The {\sl turning factor}, or that corresponding to the second operation 
in \S 47, is called the VERSOR, and is denoted by the letter $U$ prefixed 
to the quaternion. 

{\bf 49}. Thus, if $\overline{OA} = \alpha$, 
$\overline{OB} = \beta$, and if $q$ be the quaternion 
which changes $\alpha$ to $\beta$, we have 
$$\beta = q\alpha$$
which we may write in the form 
$$\frac{\beta}{\alpha} = q\textrm{,  or  }\beta\alpha^{-1}=q$$
if we agree to {\sl define} that 
$$\frac{\beta}{\alpha}\alpha = \beta\alpha^{-1}\alpha = \beta$$
Here it is to be particularly noticed that we write $q$ 
{\sl before} $\alpha$ to 
signify that $\alpha$ is multiplied by (or operated on by) $q$, not $q$ 
multiplied by $\alpha$.

This remark is of extreme importance in quaternions, for, as we 
will soon see, the Commutative Law does not generally apply to 
the factors of a product. 

We have also, by \S\S 47, 48, 
$$q=TqUq=UqTq$$
where, as before, $Tq$ depends merely on the relative lengths of 
$\alpha$ and $\beta$, and $Uq$ depends solely on their directions. 

Thus, if $\alpha_1$ and $\beta_1$ 
be vectors of unit length parallel to $\alpha$ and $\beta$
respectively, 
$$T\frac{\beta_1}{\alpha_1} = T\beta_1/T\alpha_1 = 1\textrm{,     }
U\frac{\beta_1}{\alpha_1} = U\beta_1/U\alpha_1 = U\frac{\beta}{\alpha}$$
As will soon be shown, when $\alpha$ is perpendicular to $\beta$, 
i.e. when the versor of the quotient is quadrantal, it is a unit-vector. 

{\bf 50}. We must now carefully notice that the quaternion which 
is the quotient when $\beta$ is divided by $\alpha$ in no way depends upon 
the {\sl absolute} lengths, or directions, of these vectors. Its value 
will remain unchanged if we substitute for them any other pair 
of vectors which 

\noindent
\hbox{\hskip 2cm}(1) have their lengths in the same ratio,

\noindent
\hbox{\hskip 2cm}(2) have their common plane the same or parallel,

\noindent
and\hbox{\hskip 1.32cm} (3) make the same angle with each other. 

Thus in the annexed figure 

\begin{center}
\includegraphics{ps/quat6.ps}
\end{center}
\vskip 0.5cm

$$\frac{O_1B_1}{O_1A_1} = \frac{\overline{OB}}{\overline{OA}}$$
if, and only if, 

\noindent
\hbox{\hskip 2cm}$(1)\textrm{     }\frac{O_1B_1}{O_1A_1} = \frac{OB}{OA}$

\noindent
\hbox{\hskip 2cm}$(2)\textrm{     plane }AOB\textrm{ parallel to plane }
A_1O_1B_1$

\noindent
\hbox{\hskip 2cm}$(3)\textrm{     }\angle{}AOB = \angle A_1O_1B_1$

[Equality of angles is understood to include 
concurrency of rotation. Thus in the annexed 
figure the rotation about an axis drawn upwards 
from the plane is negative (or clock- wise) from 
$OA$ to $OB$, and also from $O_1A_1$ to $O_1B_1$.] 

It thus appears that if 
$$\beta = q\alpha\textrm{,  }\delta = q\gamma$$
the vectors $\alpha$, $\beta$, $\gamma$, $\delta$
are parallel to one plane, and may be repre 
sented (in a highly extended sense) as {\sl proportional} to one another, 
thus: --
$$\beta : \alpha = \delta : \gamma$$

And it is clear from the previous part of this section that this 
may be written not only in the form 
$$\alpha : \beta = \gamma : \delta$$
but also in either of the following forms: --
$$\gamma : \alpha = \delta : \beta$$
$$\alpha : \gamma = \beta : \delta$$

While these proportions are true as equalities of ratios, they 
do not usually imply equalities of products. 

Thus, as the first of these was equivalent to the equation 
$$\frac{\beta}{\alpha}=\frac{\delta}{\gamma}=q\textrm{,  or  }
\beta\alpha^{-1}=\delta\gamma^{-1}=q$$
the following three imply separately, (see next section) 
$$\frac{\alpha}{\beta}=\frac{\gamma}{\delta}=q^{-1}\textrm{,   }
\frac{\gamma}{\alpha}=\frac{\delta}{\beta}=r\textrm{,   }
\frac{\alpha}{\gamma}=\frac{\beta}{\delta}=r^{-1}$$
or, if we please, 
$$\alpha\beta^{-1}=\gamma\delta^{-1}=q^{-1}\textrm{,   }
\gamma\alpha^{-1}=\delta\beta^{-1}=r\textrm{,   }
\alpha\gamma^{-1}=\beta\delta^{-1}=r^{-1}$$
where $r$ is a {\sl new} quaternion, which has not necessarily anything 
(except its plane), in common with $q$. 

But here great caution is requisite, for we are {\sl not} entitled to 
conclude from these that 
$$\alpha\delta=\beta\gamma\textrm{, \&c.}$$

This point will be fully discussed at a later stage. Meanwhile 
we may merely {\sl state} that from 
$$\frac{\alpha}{\beta}=\frac{\gamma}{\delta}\textrm{,  or  }
\frac{\beta}{\alpha} = \frac{\delta}{\gamma}$$
we are entitled to deduce a number of equivalents such as 
$$\alpha\beta^{-1}\delta=\gamma\textrm{, or  }
\alpha=\gamma\delta^{-1}\beta\textrm{, or  }
\beta^{-1}\delta=\alpha^{-1}\gamma\textrm{, \&c}$$

{\bf 51}. The {\sl Reciprocal} of a quaternion $q$ is defined by the 
equation 
$$\frac{1}{q}q=q^{-1}=1=q\frac{1}{q}=qqe^{-1}$$
Hence if
$$\frac{\beta}{\alpha}=q\textrm{,  or}$$
$$\beta=q\alpha$$
we must have
$$\frac{\alpha}{\beta}=\frac{1}{q}=q^{-1}$$
For this gives
$$\frac{\alpha}{\beta}\beta=q^{-1}q\alpha$$
and each member of the equation is evidently equal to $\alpha$.
Or thus: --
$$\beta=q\alpha$$
Operate {\sl by} $q^{-1}$
$$q^{-1}\beta = \alpha$$
Operate {\sl on} $\beta^{-1}$
$$q^{-1} = \alpha\beta^{-1} = \frac{\alpha}{\beta}$$

Or, we may reason thus: -- since $q$ changes $\overline{OA}$ to 
$\overline{OA}$, $q^{-1}$ must
change $\overline{OB}$ to $\overline{OA}$, 
and is therefore expressed by $\frac{\alpha}{\beta}$ (\S 49). 

The tensor of the reciprocal of a quaternion is therefore the 
reciprocal of the tensor; and the versor differs merely by the 
{\sl reversal} of its representative angle. The versor, it must be 
remembered, gives the plane and angle of the turning -- it has 
nothing to do with the extension. 

[{\sl Remark}. In \S\S 49--51, above, we had such expressions as 
$\frac{\beta}{\alpha}=\beta\alpha^{-1}$. 
We have also met with $\alpha^{-1}\beta$. Cayley suggests that this 
also may be written in the ordinary fractional form by employing 
the following distinctive notation: --
$$\frac{\beta}{\alpha}=\beta\alpha^{-1}=\frac{\beta|}{|\alpha}\textrm{,   }
\alpha^{-1}\beta=\frac{|\beta}{\alpha|}$$

(It might, perhaps, be even simpler to use the {\sl solidus} as 
recommended by Stokes, along with an obviously correlative 
type:-- thus, 
$$\frac{\beta}{\alpha}=\beta\alpha^{-1}=\beta/\alpha\textrm{,   }
\alpha^{-1}\beta=\alpha\\ \beta$$

I have found such notations occasionally convenient for private 
work, but I hesitate to introduce changes unless they are abso 
lutely required. See remarks on this point towards the end of the 
{\sl Preface to the Second Edition} reprinted above.] 

{\bf 52}. The {\sl Conjugate} of a quaternion $q$, written $Kq$, has the 
same tensor, plane, and angle, only the angle is taken the reverse 
way; or the versor of the conjugate is the reciprocal of the versor 
of the quaternion, or (what comes to the same thing) the versor of 
the reciprocal. 

\begin{center}
\includegraphics{ps/quat7.ps}
\end{center}
\vskip 0.5cm

Thus, if $OA$, $OB$, $OA^{\prime}$ , lie in one plane, and if 
$OA^{\prime} = OA$, and $\angle A^{\prime}OB = \angle BOA$, we have 
$$\frac{\overline{OB}}{\overline{OA}}=q$$, 
and 
$$\frac{\overline{OB}}{\overline{OA^{\prime}}}=\textrm{  congugate of }q=Kq$$

By last section we see that 
$$Kq=(Tq)^2q^{-1}$$
Hence\hbox{\hskip 4cm}$qKq=Kqq=(Tq)^2$

This proposition is obvious, if we recollect that 
the tensors of $q$ and $Kq$ are equal, and that the 
versors are such that either {\sl annuls} the effect of the other; while 
the order of their application is indifferent. The joint effect of 
these factors is therefore merely to multiply twice over by the 
common tensor. 

{\bf 53}. It is evident from the results of \S 50 that, if $\alpha$ and $\beta$
be of equal length, they may be treated as of unit-length so far as 
their quaternion quotient is concerned. This quotient is therefore 
a versor (the tensor being unity) and may be represented indifferently 
by any one of an infinite number of concurrent arcs of 
given length lying on the circumference of a circle, of which the 
two vectors are radii. This is of considerable importance in the 
proofs which follow. 

\begin{center}
\includegraphics{ps/quat8.ps}
\end{center}
\vskip 0.5cm

Thus the versor 
${\displaystyle \frac{\overline{OB}}{\overline{OA}}}$ may be represented 
in magnitude, plane, and currency of rotation (\S 50) 
by the arc $AB$, which may in this extended sense be written 
${\stackrel{\frown}{AB}}$. 

And, similarly, the versor 
${\displaystyle \frac{\overline{OB_1}}{\overline{OA_1}}}$ 
may be represented by 
${\stackrel{\frown}{A_1B_1}}$
which is equal to (and concurrent with) 
${\stackrel{\frown}{AB}}$ if
$$\angle A_1OB_1 = \angle AOB$$
i.e. if the versors are {\sl equal}, in the quaternion meaning of the 
word. 

{\bf 54}. By the aid of this process, when a versor is represented as 
an arc of a great circle on the unit-sphere, we can easily prove 
that {\sl quaternion multiplication is not generally commutative}.

\begin{center}
\includegraphics{ps/quat9.ps}
\end{center}
\vskip 0.5cm

Thus let $q$ be the versor ${\stackrel{\frown}{AB}}$ or
${\displaystyle \frac{\overline{OB}}{\overline{OA}}}$,
where $O$ is the centre of the sphere. 

Take ${\stackrel{\frown}{BC}} = {\stackrel{\frown}{AB}}$, 
(which, it must be remembered, makes the points $A$, $B$, $C$, lie 
in one great circle), then $q$ may also be 
represented by ${\displaystyle \frac{\overline{OC}}{\overline{OB}}}$.

In the same way any other versor $r$ may be represented by 
${\stackrel{\frown}{DB}}$ or ${\stackrel{\frown}{BE}}$ and by 
${\displaystyle \frac{\overline{OB}}{\overline{OD}}}$ or
${\displaystyle \frac{\overline{OE}}{\overline{OB}}}$.

[The line $OB$ in the figure is definite, and is given by the 
intersection of the planes of the two versors.] 

Now $r\overline{OD} = \overline{OB}$, and $q\overline{OB}=\overline{OC}$. 

Hence $qr\overline{OD} = \overline{OC}$, 

or $qr = {\displaystyle \frac{\overline{OC}}{\overline{OD}}}$, 
and may therefore be represented by the arc ${\stackrel{\frown}{DC}}$ of 
a great circle. 

But $rq$ is easily seen to be represented by the arc 
${\stackrel{\frown}{AE}}$. 

For $q\overline{OA}=\overline{OB}$, and $r\overline{OB}=\overline{OE}$, 

whence $rq\overline{OA} = \overline{OE}$. and 
$rq = {\displaystyle \frac{\overline{OE}}{\overline{OA}}}$.

Thus the versors $rq$ and $qr$, though represented by arcs of equal 
length, are not generally in the same plane and are therefore 
unequal: unless the planes of $q$ and $r$ coincide. 

Remark. We see that we have assumed, or defined, in the 
above proof, that $q . r\alpha = qr . \alpha$. 
and $r.q\alpha = rq.\alpha$ in the special case 
when $q\alpha$, $r\alpha$, $q.r\alpha$ and $r.q\alpha$ are all {\sl vectors}. 

{\bf 55}. Obviously ${\stackrel{\frown}{CB}}$ is $Kq$, 
${\stackrel{\frown}{BD}}$ is $Kr$, and 
${\stackrel{\frown}{CD}}$ is $K (qr)$. But 
${\stackrel{\frown}{CD}} = {\stackrel{\frown}{BD}}.{\stackrel{\frown}{CB}}$
as we see by applying both to OC. This gives us 
the very important theorem 
$$K (qr) = Kr . Kq$$
i.e. {\sl the conjugate of the product of two versors is the product of their 
conjugates in inverted order}. This will, of course, be extended to 
any number of factors as soon as we have proved the associative 
property of multiplication. (\S 58 below.) 

{\bf 56}. The propositions just proved are, of course, true of quater 
nions as well as of versors; for the former involve only an additional 
numerical factor which has reference to the length merely, and not 
the direction, of a vector (\S 48), and is therefore commutative with 
all other factors. 

{\bf 57}. Seeing thus that the commutative law does not in general 
hold in the multiplication of quaternions, let us enquire whether 
the Associative Law holds generally. That is if $p$, $q$, $r$ be three 
quaternions, have we 
$$p.qr = pq.r?$$

This is, of course, obviously true if $p$, $q$, $r$ be numerical quantities, 
or even any of the imaginaries of algebra. But it cannot be con 
sidered as a truism for symbols which do not in general give 

$$pq = qp$$

We have assumed it, in definition, for the special case when $r$, 
$qr$, and $pqr$ are all vectors. (\S 54.) But we are not entitled to 
assume any more than is absolutely required to make our 
definitions complete. 

{\bf 58}. In the first place we remark that $p$, $q$, and $r$ may be 
considered as versors only, and therefore represented by arcs of  
great circles on the unit sphere, for their tensors may obviously 
(\S 48) be divided out from both sides, being commutative with the 
versors. 

Let ${\stackrel{\frown}{AB}}=p$,
${\stackrel{\frown}{ED}} = {\stackrel{\frown}{CA}} = q$, and 
${\stackrel{\frown}{FE}} = r$. 

Join $BC$ and produce the great circle till it meets $EF$ in $H$, and 
make ${\stackrel{\frown}{KH}}={\stackrel{\frown}{FE}} = r$, 
and ${\stackrel{\frown}{HG}} = {\stackrel{\frown}{CB}} =pq$ (\S 54).
 
\begin{center}
\includegraphics{ps/quat10.ps}
\end{center}
\vskip 0.5cm

Join $GK$. Then 
${\stackrel{\frown}{KG}} = 
{\stackrel{\frown}{HG}} . {\stackrel{\frown}{KH}} = pq . r$. 

Join $FD$ and produce it to meet $AB$ in $M$. Make 
$${\stackrel{\frown}{LM}} = {\stackrel{\frown}{FD}}
\textrm{,  and  }{\stackrel{\frown}{MN}} = {\stackrel{\frown}{AB}}$$ 

and join $NL$. Then 
$${\stackrel{\frown}{LN}}= {\stackrel{\frown}{MN}} .
{\stackrel{\frown}{LM}}=p.qr$$. 

Hence to show that $p . qr = pq . r$ 

all that is requisite is to prove that $LN$, and $KG$, described as 
above, are {\sl equal arcs of the same great circle}, since, by the figure, 
they have evidently similar currency. This is perhaps most easily 
effected by the help of the fundamental properties of the curves 
known as {\sl Spherical Conics}. As they are not usually familiar to 
students, we make a slight digression for the purpose of proving 
these fundamental properties ; after Chasles, by whom and Magnus 
they were discovered. An independent proof of the associative 
principle will presently be indicated, and in Chapter VIII. we will 
employ quaternions to give an independent proof of the theorems 
now to be established. 

{\bf 59}.* DEF. {\sl A spherical conic is the curve of intersection of a 
cone of the second degree with a sphere, the vertex of the cone being 
the centre of the sphere}. 

LEMMA. If a cone have one series of circular sections, it has 
another series, and any two circles belonging to different series lie 
on a sphere. This is easily proved as follows. 

Describe a sphere, $A$, cutting the cone in one circular section, 
$C$, and in any other point whatever, and let the side $OpP$ of the 
cone meet $A$ in $p$, $P$ ; $P$ being a point in $C$. Then $PO.Op$ is 
constant, and, therefore, since $P$ lies in a plane, $p$ lies on a sphere, 
$a$, passing through $0$. Hence the locus, $c$, of $p$ is a circle, being 
the intersection of the two spheres $A$ and $a$. 

Let $OqQ$ be any other side of the cone, $q$ and $Q$ being points in 
$c$, $C$ respectively. Then the quadrilateral $qQPp$ is inscribed in a 
circle (that in which its plane cuts the sphere $A$) and the exterior
 
\begin{center}
\includegraphics{ps/quat11.ps}
\end{center}
\vskip 0.5cm

angle at $p$ is equal to the interior angle at $Q$. If $OL$, $OM$ be the 
lines in which the plane $POQ$ cuts the {\sl cyclic planes} (planes through 
$O$ parallel to the two series of circular sections) they are obviously 
parallel to $pq$, $QP$, respectively; and therefore 

$$\angle LOp = \angle Opq = \angle OQP = \angle MOQ$$ 

Let any third side, $OrR$, of the cone be drawn, and let the 
plane $OPR$ cut the cyclic planes in $0l$, $Om$ respectively. Then, 
evidently, 
$$\angle lOL = \angle qpr$$
$$\angle MOm = \angle QPR$$

and these angles are independent of the position of the points $p$ 
and $P$, if $Q$ and $R$ be fixed points. 

\begin{center}
\includegraphics{ps/quat12.ps}
\end{center}
\vskip 0.5cm

In the annexed section of the above space-diagram by a sphere 
whose centre is $O$, $lL$, $Mm$ are the great circles which represent 
the cyclic planes, $PQR$ is the spherical conic which represents the 
cone. The point $P$ represents the line $OpP$, and so with the 
others. The propositions above may now be stated thus, 

$$\textrm{Arc  } PL = \textrm{arc } MQ$$ 

and, if $Q$ and $R$ be fixed, $Mm$ and $lL$ are constant arcs whatever be 
the position of $P$. 

{\bf 60}. The application to \S 58 is now obvious. In the figure of 
that article we have 
$$
{\stackrel{\frown}{FE}}={\stackrel{\frown}{KH}}\textrm{,  }
{\stackrel{\frown}{ED}}={\stackrel{\frown}{CA}}\textrm{,  }
{\stackrel{\frown}{HG}}={\stackrel{\frown}{CB}}\textrm{,  }
{\stackrel{\frown}{LM}}={\stackrel{\frown}{FD}}
$$

Hence $L$, $C$, $G$, $D$ are points of a spherical conic whose cyclic 
planes are those of $AB$, $FE$. Hence also $KG$ passes through $L$, 
and with $LM$ intercepts on $AB$ an arc equal to 
${\stackrel{\frown}{AB}}$. That is, it 
passes through $N$, or $KG$ and $LN$ are arcs of the same great circle : 
and they are equal, for $G$ and $L$ are points in the spherical 
conic. 

Also, the associative principle holds for any number of 
quaternion factors. For, obviously, 

$$qr . st = qrs . t = \textrm{\&c., \&c.,}$$ 

since we may consider $qr$ as a single quaternion, and the above 
proof applies directly. 

{\bf 61}. That quaternion addition, and therefore also subtraction, 
is commutative, it is easy to show. 

\begin{center}
\includegraphics{ps/quat13.ps}
\end{center}
\vskip 0.5cm

For if the planes of two quaternions, 
$q$ and $r$, intersect in the line $OA$, we 
may take any vector $\overline{OA}$ in that line, 
and at once find two others, $\overline{OB}$ and 
$\overline{OC}$, such that 

$$\overline{OB} = q\overline{OA}$$ 
and\hbox{\hskip 4cm}$\overline{CO} = r\overline{OA}$ 

And\hbox{\hskip 2cm}$(q + r)\overline{OA}
\overline{OB}+\overline{OC}=\overline{OC}+\overline{OB}=
(r + q) \overline{OA}$ 

since vector addition is commutative (\S 27). 

Here it is obvious that $(q + r) \overline{OA}$, being the diagonal of the 
parallelogram on $\overline{OB}$, $\overline{OC}$, 
divides the angle between $OB$ and $OC$ 
in a ratio depending solely on the ratio of the lengths of these 
lines, i.e. on the ratio of the tensors of $q$ and $r$. This will be useful 
to us in the proof of the distributive law, to which we proceed. 

{\bf 62}. Quaternion multiplication, and therefore division, is 
distributive. One simple proof of this depends on the possibility, 
shortly to be proved, of representing {\sl any} quaternion as a linear 
function of three given rectangular unit- vectors. And when the 
proposition is thus established, the associative principle may readily 
be deduced from it. 

[But Hamilton seems not to have noticed that we may employ 
for its proof the properties of Spherical Conies already employed 

\begin{center}
\includegraphics{ps/quat14.ps}
\end{center}
\vskip 0.5cm

in demonstrating the truth of the associative principle. "For 
continuity we give an outline of the proof by this process. 

Let ${\stackrel{\frown}{BA}}$, 
${\stackrel{\frown}{CA}}$ 
represent the versors of $q$ and $r$, and be the great 
circle whose plane is that of $p$. 

Then, if we take as operand the vector $\overline{OA}$, it is obvious that 
$U (q + r)$ will be represented by some such arc as 
${\stackrel{\frown}{DA}}$ where 
$B$, $D$, $C$ are in one great circle; 
for $(q + r) \overline{OA}$ is in the same plane 
as $q\overline{OA}$ and $r\overline{OA}$, 
and the relative magnitude of the arcs $BD$ and 
$DC$ depends solely on the tensors of $q$ and $r$. Produce $BA$, $DA$, 
$CA$ to meet be in $b$, $d$, $c$ respectively, and make 

$${\stackrel{\frown}{Eb}} = {\stackrel{\frown}{BA}}, 
{\stackrel{\frown}{Fd}} = {\stackrel{\frown}{DA}}, 
{\stackrel{\frown}{Gc}} = {\stackrel{\frown}{CA}}$$ 

Also make 
${\stackrel{\frown}{b\beta}} = 
{\stackrel{\frown}{d\delta}} = 
{\stackrel{\frown}{c\gamma}}=p$. Then $E$, $F$, $G$, $A$ lie on a spherical 
conic of which $BC$ and $bc$ are the cyclic arcs. And, because 
${\stackrel{\frown}{b\beta}} = 
{\stackrel{\frown}{d\delta}} = 
{\stackrel{\frown}{c\gamma}}$, 
${\stackrel{\frown}{\beta E}}$, 
${\stackrel{\frown}{\delta F}}$, 
${\stackrel{\frown}{\gamma G}}$, when produced, meet in a point $H$ 
which is also on the spherical conic (\S 59*). Let these arcs meet $BC$
in $J$, $L$, $K$ respectively. Then we have 
$${\stackrel{\frown}{JH}}={\stackrel{\frown}{E\beta}}=pUq$$
$${\stackrel{\frown}{LH}}={\stackrel{\frown}{F\delta}}=pU(q+r)$$
$${\stackrel{\frown}{KH}}={\stackrel{\frown}{G\gamma}}=pUr$$
Also\hbox{\hskip 4cm}${\stackrel{\frown}{LJ}}={\stackrel{\frown}{DB}}$\\
and\hbox{\hskip 4cm}${\stackrel{\frown}{KL}}={\stackrel{\frown}{CD}}$

And, on comparing the portions of the figure bounded respectively 
by $HKJ$ and by $ACB$ we see that (when considered with reference 
to their effects as factors multiplying 
$\overline{OH}$ and $\overline{OA}$ respectively) 

\hbox{\hskip 2cm}$pU(q4+r)$ bears the same relation to $pUq$ and $pUr$\\ 
that\hbox{\hskip 1cm}$U(q+r)$ bears to $Uq$ and $Ur$.\\
But\hbox{\hskip 1cm}$T(q+r)U(q+r)=q+r=TqUq+TrUr$. \\
Hence\hbox{\hskip 1cm}$T(q+r).pU(q+r)=Tq .pUq+Tr.pUr$;\\
or, since the tensors are mere numbers and commutative with all 
other factors, 
$$p(q+r)=pq+pr$$
In a similar manner it may be proved that 
$$(q+ )p=qp+rp$$
And then it follows at once that 
$$(p + q) (r + s) = pr + ps + qr + qs$$ 
where, by \S 61, the order of the partial products is immaterial.] 

{\bf 63}. By similar processes to those of \S 53 we see that versors, 
and therefore also quaternions, are subject to the index-law 
$$q^m.q^n=q^{m+n}$$
at least so long as $m$ and $n$ are positive integers. 

The extension of this property to negative and fractional 
exponents must be deferred until we have defined a negative or 
fractional power of a quaternion. 

{\bf 64}. We now proceed to the special case of {\sl quadrantal} versors, 
from whose properties it is easy to deduce all the foregoing 
results of this chapter. It was, in fact, these properties whose 
invention by Hamilton in 1843 led almost intuitively to the 
establishment of the Quaternion Calculus. We will content 
ourselves at present with an assumption, which will be shown 
to lead to consistent results ; but at the end of the chapter we 
will show that no other assumption is possible, following for this 
purpose a very curious quasi-metaphysical speculation of Hamilton. 

{\bf 65}. Suppose we have a system of three mutually perpendicular 
unit-vectors, drawn from one point, which we may call for shortness 
{\bf i}, {\bf j}, {\bf k}. 
Suppose also that these are so situated that a positive 
(i.e. {\sl left-handed}) rotation through a right angle about {\bf i} 
as an axis 
brings {\bf j} to coincide with {\bf k}. Then it is obvious that positive 
quadrantal rotation about {\bf j} will make {\bf k} coincide with {\bf i}; 
and, about {\bf k}, will make {\bf i} coincide with {\bf j}. 

For defniteness we may suppose {\bf i} to be drawn {\sl eastwards}, {\bf j} 
{\sl northwards}, and {\bf k} {\sl upwards}. 
Then it is obvious that a positive 
(left-handed) rotation about the eastward line ({\bf i}) brings the northward
line ({\bf j}) into a vertically upward position ({\bf k}) ; and so of the 
others. 

{\bf 66}. Now the operator which turns {\bf j} into {\bf k} is a quadrantal 
versor (\S 53) ; and, as its axis is the vector {\bf i}, 
we may call it {\sl i}. 

Thus $$\frac{{\rm {\bf k}}}{{\rm {\bf j}}}=
i\textrm{, or }{\rm {\bf k}}=i{\rm {\bf j}}\eqno{(1)}$$

Similary we may put$$\frac{{\rm {\bf i}}}{{\rm {\bf k}}}=
j\textrm{, or }{\rm {\bf i}}=j{\rm {\bf k}}\eqno{(2)}$$

and $$\frac{{\rm {\bf j}}}{{\rm {\bf i}}}=
k\textrm{, or }{\rm {\bf j}}=k{\rm {\bf i}}\eqno{(3)}$$

[It may be here noticed, merely to show the symmetry of the 
system we arc explaining, that if the three mutually perpendicular 
vectors {\bf i}, {\bf j}, {\bf k} 
be made to revolve about a line equally inclined to 
all, so that {\bf i} is brought to coincide with {\bf j}, 
{\bf j} will then coincide 
with {\bf k}, and {\bf k} with {\bf i}: 
and the above equations will still hold good, 
only (1) will become (2), (2) will become (3), and (3) will become (1).] 

{\bf 67}. By the results of \S 50 we see that 
$$\frac{-{\rm {\bf j}}}{\rm {\bf k}}=\frac{{\rm {\bf k}}}{\rm {\bf j}}$$
i.e. a southward unit- vector bears the same ratio to an upward 
unit-vector that the latter does to a northward one; and therefore 
we have 

Thus $$\frac{-{\rm {\bf j}}}{{\rm {\bf k}}}=
i\textrm{, or }-{\rm {\bf j}}=i{\rm {\bf k}}\eqno{(4)}$$

Similary t$$\frac{-{\rm {\bf k}}}{{\rm {\bf i}}}=
j\textrm{, or }-{\rm {\bf k}}=j{\rm {\bf i}}\eqno{(5)}$$

and $$\frac{-{\rm {\bf i}}}{{\rm {\bf j}}}=
k\textrm{, or }-{\rm {\bf i}}=k{\rm {\bf j}}\eqno{(6)}$$

{\bf 68}. By (4) and (1) we have 

$$-j = ik = i(ij)\textrm{  (by the assumption in \S 54) }= i^2j$$

Hence 
$$i^2 = - 1\eqno{(7)}$$

Arid in the same way, (5) and (2) give 
$$j^2=-1\eqno{(8)}$$
and (6) and (3)
$$k^2=-1\eqno{(9)}$$

Thus, as the directions of {\bf i}, {\bf j}, {\bf k} 
are perfectly arbitrary, we see that 
{\sl the square of every quadrantal versor is negative unity}. 

[Though the following proof is in principle exactly the same as 
the foregoing, it may perhaps be of use to the student, in showing 
him precisely the nature as well as the simplicity of the step we 
have taken. 

\begin{center}
\includegraphics{ps/quat15.ps}
\end{center}
\vskip 0.5cm

Let $ABA^{\prime}$ be a semicircle, whose centre 
is $0$, and let $OB$ be perpendicular to $AOA^{\prime}$. 

Then ${\displaystyle\frac{\overline{OB}}{\overline{OA^{\prime}}}}=q$ 
suppose, is a quadrantal versor, and is evidently equal to 
${\displaystyle\frac{\overline{OA^{\prime}}}{\overline{OB}}}$ ;

\S\S 50, 53. Hence 

$$q^2=\frac{\overline{OA^{\prime}}}{\overline{OB}}.
\frac{\overline{OB}}{\overline{OA}}=
\frac{\overline{OA^{\prime}}}{\overline{OA}}=-1]$$

{\bf 69}. 
Having thus found that the squares of {\sl i}, {\sl j}, {\sl k} are each 
equal to negative unity ; it only remains that we find the values of 
their products two and two. For, as we will see, the result is such 
as to show that the value of any other combination whatever of 
{\sl i},{\sl j}, {\sl k} 
(as factors of a product) may be deduced from the values of 
these squares and products. 

Now it is obvious that 
$$\frac{{\rm {\bf k}}}{\rm {\bf -i}}=
\frac{{\rm {\bf i}}}{\rm {\bf k}}=j$$
(i.e. the versor which turns a westward unit-vector into an upward 
one will turn the upward into an eastward unit) ; or 
$${\rm {\bf k}}=j({\rm {\bf -i}}) = -j{\rm {\bf i}}\eqno{(10)}$$

Now let us operate on the two equal vectors in (10) by the 
same versor, {\sl i}, and we have 
$$i{\rm {\bf k}} = i(-j{\rm {\bf i}}) = -j{\rm {\bf i}}$$ 
But by (4) and (3) 
$$i{\rm {\bf k}}={\rm {\bf -j}}=-k{\rm {\bf i}}$$

Comparing these equations, we have 
$$-ij{\rm {\bf i}}=-k{\rm {\bf i}}$$
$$
\left.
\begin{array}{lr}
\textrm{or, \S 54 (end), } &ij=k\\
\textrm{and symmetry gives}&jk=i\\
                           &ki=j\\
\end{array}
\right\}\eqno{(11)}
$$

The meaning of these important equations is very simple ; and 
is, in fact, obvious from our construction in \S 54 for the multiplication 
of versors ; as we see by the annexed figure, where we must 
remember that {\sl i}, {\sl j}, {\sl k} 
are quadrantal versors whose planes are at 
right angles, so that the figure represents 
a hemisphere divided into quadrantal 
triangles. [The arrow-heads indicate the 
direction of each vector arc.] 

\begin{center}
\includegraphics{ps/quat16.ps}
\end{center}
\vskip 0.5cm

Thus, to show that $ij = k$, we have, 
$O$ being the centre of the sphere, $N$, $E$, 
$S$, $W$ the north, east, south, and west, 
and $Z$ the zenith (as in \S 65) ; 

$$j\overline{OW}=\overline{OZ}$$
whence\hbox{\hskip 2cm}$ij\overline{OW}=i\overline{OZ}=
\overline{OS} = k\overline{OW}$ 

* The negative sign, being a mere numerical factor, is evidently commutative 
with $j$  indeed we may, if necessary, easily assure ourselves of the fact 
that to turn the negative (or reverse) of a vector through a right 
(or indeed any) angle, is the same thing as to turn the vector through 
that angle and then reverse it. 

{\bf 70}. But, by the same figure, 
$$i\overline{ON}=\overline{OZ}$$
whence\hbox{\hskip 1cm}$ji\overline{ON} =j\overline{OZ}
= \overline{OE} = -\overline{OW} = - k\overline{ON}$. 

{\bf 71}. From this it appears that 
$$
\left.
\begin{array}{c}
ji = -k\\
kj = -i\\
ik = -j\\
\end{array}   
\right\}\eqno{(12)}
$$
and thus, by comparing (11), 
$$
\left.
\begin{array}{c}
ij=-ji=k\\
jk=-kj=i\\
ki=-ik=j\\
\end{array}
\right\}\eqno{(11),(12)}
$$

These equations, along with 
$$i^2=j^2=k^2=-1\eqno{((7),(8),(9))}$$
contain essentially the whole of Quaternions. But it is easy to see 
that, for the first group, we may substitute the single equation 
$$ijk=-1\eqno{(13)}$$
since from it, by the help of the values of the squares of 
{\sl i}, {\sl j}, {\sl k}, all 
the other expressions may be deduced. We may consider it proved 
in this way, or deduce it afresh from the figure above, thus 
$$k\overline{ON}=\overline{OW}$$
$$jk\overline{ON}=j\overline{OW}=\overline{OZ}$$
$$ijk\overline{ON}=ij\overline{OW}=i\overline{OZ}=
\overline{OS}=-\overline{ON}$$

{\bf 72}. One most important step remains to be made, to wit the 
assumption referred to in \S 64. We have treated 
{\sl i}, {\sl j}, {\sl k} simply as 
quadrantal versors ; and 
{\bf i}, {\bf j}, {\bf k} as unit-vectors at right angles to 
each other, and coinciding with the axes of rotation of these versors. 
But if we collate and compare the equations just proved we have 

\hbox{\hskip 4cm {\Huge \{}
\vbox{
\hbox{$i^2=-1$\hbox{\hskip 5cm}(7)}
\hbox{${\rm {\bf i}}^2=-1$\hbox{\hskip 5cm}(\S 9)}}}

\hbox{\hskip 4cm {\Huge \{}
\vbox{
\hbox{$ij=k$\hbox{\hskip 5.3cm}(11)}
\hbox{$i{\rm {\bf j}}={\rm {\bf k}}$\hbox{\hskip 5.3cm}(1)}}}

\hbox{\hskip 4cm {\Huge \{}
\vbox{
\hbox{$ji=-k$\hbox{\hskip 5cm}(11)}
\hbox{$j{\rm {\bf i}}=-{\rm {\bf k}}$\hbox{\hskip 5cm}(1)}}}

with the other similar groups symmetrically derived from them. 

Now the meanings we have assigned to {\sl i}, {\sl j}, {\sl k} are quite 
independent of, and not inconsistent with, those assigned to 
{\rm {\bf i}}, {\rm {\bf j}}, {\rm {\bf k}}. 
And it is superfluous to use two sets of characters when one will 
suffice. Hence it appears that {\sl i}, {\sl j}, {\sl k} 
may be substituted for {\rm {\bf i}}, {\rm {\bf j}}, {\rm {\bf k}}; 
in other words, {\sl a unit-vector when employed as a factor may be 
considered as a quadrantal versor whose plane is perpendicular to the 
vector}. (Of course it follows that every vector can be treated as the 
product of a number and a quadrantal versor.) This is one of the 
main elements of the singular simplicity of the quaternion calculus. 

{\bf 73}. Thus {\sl the product, and therefore the quotient, of two 
perpendicular vectors is a third vector perpendicular to both}.

Hence the reciprocal (\S 51) of a vector is a vector which has 
the {\sl opposite} direction to that of the vector, arid its length is the 
reciprocal of the length of the vector. 

The conjugate (\S 52) of a vector is simply the vector reversed. 

Hence, by \S 52, if $\alpha$ be a vector 
$$(Ta)^2 = \alpha K\alpha = \alpha ( - \alpha) = -\alpha{}^2$$

{\bf 74}. We may now see that {\sl every versor may be represented by 
a power of a unit-vector}.

For, if $\alpha$ be any vector perpendicular to $i$ (which is 
{\sl any} definite unit-vector), 
$i\alpha = \beta$ is a vector equal in length to $\alpha$,
but perpendicular to both $i$ and $\alpha$
$$
\begin{array}{ccl}
i^2\alpha  & = & -\alpha\\
\i^3\alpha & = & -i\alpha = -\beta\\
\i^4\alpha & = & -i\beta = -i^2\alpha = \alpha
\end{array}
$$
Thus, by successive applications of $i$, $\alpha$. 
is turned round $i$ as an axis 
through successive right angles. Hence it is natural to {\sl define} 
$i^m$ {\sl as 
a versor which turns any vector perpendicular to i through m right 
angles in the positive direction of rotation about i as an axis}. Here 
$m$ may have any real value whatever, whole or fractional, for it is 
easily seen that analogy leads us to interpret a negative value of $m$ 
as corresponding to rotation in the negative direction. 

{\bf 75}. From this again it follows that {\sl any quaternion may be 
expressed as a power of a vector}. For the tensor and versor 
elements of the vector may be so chosen that, when raised to the 
same power, the one may be the tensor and the other the versor 
of the given quaternion. The vector must be, of course, perpen 
dicular to the plane of the quaternion. 

{\bf 76}. And we now see, as an immediate result of the last two 
sections, that the index-law holds with regard to powers of a 
quaternion (\S 63). 

{\bf 77}. So far as we have yet considered it, a quaternion has been 
regarded as the {\sl product} of a tensor and a versor: we are now to 
consider it as a {\sl sum}. The easiest method of so analysing it seems 
to be the following. 

\begin{center}
\includegraphics{ps/quat17.ps}
\end{center}
\vskip 0.5cm

Let ${\displaystyle\frac{\overline{OB}}{\overline{OA}}}$
represent any quaternion. Draw $BC$ perpendicular to $OA$, 
produced if necessary. 

Then, \S 19, $\overline{OB} = \overline{OC} + \overline{CB}$\\ 

But, \S 22, $\overline{OC}=x\overline{OA}$\\ 
where $x$ is a number, whose sign is the same 
as that of the cosine of $\angle AOB$. 

Also, \S 73, since $CB$ is perpendicular to $OA$, 
$$\overline{CB}=\gamma\overline{OA}$$
where $\gamma$ is a vector perpendicular to $OA$ and $CB$, i.e. to the plane 
of the quaternion; and, as the figure is drawn, directed {\sl towards} the 
reader. 

Hence 
$$\frac{\overline{OB}}{\overline{OA}}=
\frac{x\overline{OA}+\gamma\overline{OA}}{\overline{OA}}=
x+\gamma$$

Thus a quaternion, in general, may be decomposed into the sum 
of two parts, one numerical, the other a vector. Hamilton calls 
them the SCALAR, and the VECTOR, and denotes them respectively 
by the letters $S$ and $V$ prefixed to the expression for the 
quaternion. 

{\bf 78}. Hence $q = Sq+ Vq$, and if in the above example 
$$\frac{\overline{OB}}{\overline{OA}}=q$$
then
$$\overline{OB}=\overline{OC}+\overline{CB}=
Sq.\overline{OA}+Vq.\overline{OA}
\setcounter{footnote}{1}%% use a dagger because number looks like superscript
\footnote{
The points are inserted to show that $S$ and $V$ 
apply only to $q$, and not to $q\overline{OA}$.} 
$$

The equation above gives 
$$\overline{OC}=Sq.\overline{OA}$$
$$\overline{CB}=Vq.\overline{OA}$$

{\bf 79}. If, in the last figure, we produce $BC$ to $D$, so as to double 
its length, and join $OD$, we have, by \S 52, 
$$\frac{\overline{OD}}{\overline{OA}}=Kq=SKq+VKq$$
so that\hbox{\hskip 1cm}
$\overline{OD}=\overline{OC}+\overline{CD}=
SKq.\overline{OA}+VKq.\overline{OA}$\\
Hence\hbox{\hskip 3.6cm}$\overline{OC}=SKq.\overline{OA}$\\
and\hbox{\hskip 4cm}$\overline{CD}=VKq.\overline{OA}$\\ 
Comparing this value of $\overline{OC}$ with that in last section, we find 
$$SKq=Sq\eqno{(1)}$$
or {\sl the scalar of the conjugate of a quaternion is equal to the scalar of 
the quaternion}. 

Again, $\overline{CD} = -\overline{CB}$ 
by the figure, and the substitution of their values gives 
$$VKq=-Vq\eqno{(2)}$$
or {\sl the vector of the conjugate of a quaternion is the vector of the 
quaternion reversed}. 

We may remark that the results of this section are simple con 
sequences of the fact that the symbols $S$, $V$, $K$ are commutative
\setcounter{footnote}{1}%% use a dagger because number looks like superscript
\footnote{
It is curious to compare the properties of these quaternion symbols with those 
of the Elective Symbols of Logic, as given in BOOLE'S 
wonderful treatise on the {\sl Laws of Thought}; and to think that the 
same grand science of mathematical analysis, by processes remarkably 
similar to each other, reveals to us truths in the science of position 
far beyond the powers of the geometer, and truths of deductive 
reasoning to which unaided thought could never have led the logician. }. 

Thus\hbox{\hskip 3cm}$SKq = KSq = Sq$, \\
since the conjugate of a number is the number itself; and 
$$VKq=KVq=-Vq (\S 73)$$

Again, it is obvious that, 
$$\sum{Sq}=S\sum{q},\;\;\;\;\sum{Vq}=V\sum{q}$$
and thence\hbox{\hskip 3cm}$\sum{Kq}=K\sum{q}$

{\bf 80}. Since any vector whatever may be represented by 
$$xi+yj+zk$$
where $x$, $y$, $z$ are numbers (or Scalars), 
and $i$, $j$, $k$ may be any three 
non-coplanar vectors, \S\S 23, 25 though they are usually understood 
as representing a rectangular system of unit-vectors and 
since any scalar may be denoted by $w$; we may write, for any 
quaternion $q$, the expression 
$$q=w+xi+yj+zk (\S 78)$$

Here we have the essential dependence on four distinct numbers, 
from which the quaternion derives its name, exhibited in the most 
simple form. 

And now we see at once that an equation such as 
$$q^{\prime}=q$$
where\hbox{\hskip 3cm}$q^{\prime}=
w^{\prime}+x^{\prime}i+y^{\prime}j+z^{\prime}k$\\
involves, of course, the {\sl four} equations 
$$
w^{\prime}=w\textrm{,  }
x^{\prime}=x\textrm{,  }
y^{\prime}=y\textrm{,  }
z^{\prime}=z
$$

{\bf 81}. We proceed to indicate another mode of proof of the distributive 
law of multiplication. 

We have already defined, or assumed (\S 61), that 
$$\frac{\beta}{\alpha}+\frac{\gamma}{\alpha}=\frac{\beta+\gamma}{\alpha}$$
or\hbox{\hskip 3cm}$\beta\alpha^{-1}+\gamma\alpha^{-1}=
(\beta+\gamma)\alpha^{-1}$\\
and have thus been able to understand what is meant by adding 
two quaternions. 

But, writing $\alpha$ for $\alpha^{-1}$, 
we see that this involves the equality 
$$(\beta+\gamma)\alpha = \beta\alpha+\gamma\alpha$$
from which, by taking the conjugates of both sides, we derive 
$$\alpha^{\prime}(\beta^{\prime}+\gamma^{\prime})=
\alpha^{\prime}\beta^{\prime}+\alpha^{\prime}\gamma^{\prime}
(\S 55)$$
And a combination of these results (putting 
$\beta+\gamma$ for $\alpha^{\prime}$ in the latter, for instance) gives 
$$
\begin{array}{lcr}
(\beta+\gamma)(\beta^{\prime}+\gamma^{\prime}) & = &
(\beta+\gamma)\beta^{\prime}+(\beta+\gamma)\gamma^{\prime}\\
& = & \beta\beta^{\prime}+\gamma\beta^{\prime}+
\beta\gamma^{\prime}+\gamma\gamma^{\prime}
\end{array}
$$
by the former.

Hence the {\sl distributive principle is true in the multiplication of 
vectors}.

It only remains to show that it is true as to the scalar and 
vector parts of a quaternion, and then we will easily attain the 
general proof. 

Now, if $a$ be any scalar, $\alpha$ any vector, and $q$ any quaternion, 
$$(a+\alpha)q=aq+\alpha q$$

For, if $\beta$ be the vector in which the plane of $q$ is intersected by 
a plane perpendicular to $\alpha$, we can find other two vectors, 
$\gamma$ and $\delta$ one in each of these planes such that 
$$\alpha=\frac{\gamma}{\beta},\;\;\;\;\;q=\frac{\beta}{\delta}$$
And, of course, $a$ may be written 
${\displaystyle\frac{a\beta}{\beta}}$; so that 
$$
\begin{array}{ccl}
(a+\alpha)q & = & \frac{a\beta+\gamma}{\beta}.\frac{\beta}{\delta}
=\frac{a\beta+\gamma}{\delta}\\
& & \\
& = & a\frac{\beta}{\delta}+\frac{\gamma}{\delta}=
a\frac{\beta}{\delta}+\frac{\gamma}{\beta}.\frac{\beta}{\delta}\\
& & \\
& = & aq + \alpha q
\end{array}
$$
And the conjugate may be written 
$$q^{\prime}(a^{\prime}+\alpha^{\prime})=
q^{\prime}a^{\prime}+q^{\prime}\alpha^{\prime} (\S 55)$$
Hence, generally, 
$$(a+\alpha)(b+\beta)=ab+a\beta+b\alpha+\alpha\beta$$
or, breaking up $a$ and $b$ each into the sum of two scalars, and 
$\alpha$, $\beta$ each into the sum of two vectors, \\
$(a_1+a_2+\alpha_1+\alpha_2)(b_1+b_2+\beta_1+\beta_2)$
$$=(a_1+a_2)(b_1+b_2)
+(a_1+a_2)(\beta_1+\beta_2)
+(b_1+b_2)(\alpha_1+\alpha_2)
+(\alpha_1+\alpha_2)(\beta_1+\beta_2)
$$
(by what precedes, all the factors on the right are distributive, so 
that we may easily put it in the form) 
$$=(a_1+\alpha_1)(b_1+\beta_1)
+(a_1+\alpha_1)(b_2+\beta_2)
+(a_2+\alpha_2)(b_1+\beta_1)
+(a_2+\alpha_2)(b_2+\beta_2)
$$

Putting $a_1+\alpha_1=p,\;\;\;$ 
$a_2+\alpha_2=q,\;\;\;$ 
$b_1+\beta_1=r,\;\;\;$ $b_2+\beta_2=s$,\\
we have $(p+q)(r+s)=pr+ps+qr+qs$

{\bf 82}. Cayley suggests that the laws of quaternion multiplication 
may be derived more directly from those of vector multiplication, 
supposed to be already established. Thus, let $\alpha$ be the unit vector 
perpendicular to the vector parts of $q$ and of $q^{\prime}$. Then let 
$$\rho=q.\alpha,\;\;\;\sigma=-\alpha .q^{\prime}$$
as is evidently permissible, and we have 
$$p\alpha=q.\alpha\alpha=-q;\;\;\;\alpha\sigma=
-\alpha\alpha.q^{\prime}=q^{\prime}$$
so that\hbox{\hskip 4cm}$-q.q^{\prime}=\rho\alpha.\alpha\sigma=-\rho.\sigma$

The student may easily extend this process. 

For variety, we will now for a time forsake the geometrical 
mode of proof we have hitherto adopted, and deduce some of our 
next steps from the analytical expression for a quaternion given in 
\S 80, and the properties of a rectangular system of unit-vectors as 
in \S 71. 

We will commence by proving the result of \S 77 anew. 

{\bf 83}. Let 
$$\alpha=xi+yj+zk$$
$$\beta=x^{\prime}i+y^{\prime}j+z^{\prime}k$$
Then, because by \S 71 every product or quotient of $i$, $j$, $k$ is reducible 
to one of them or to a number, we are entitled to assume 
$$q=\frac{\beta}{\alpha}=\omega+\xi i+\eta j +\zeta k$$
where $\omega$, $\xi$, $\eta$, $\zeta$ are numbers. 
This is the proposition of \S 80. 

[Of course, with this expression for a quaternion, there is no 
necessity for a formal proof of such equations as 
$$p + (q+r) = (p + q) + r$$
where the various sums are to be interpreted as in \S 61. 

All such things become obvious in view of the properties of $i$, $j$ ,$k$.] 

{\bf 84}. But it may be interesting to find $\omega$, $\xi$, $\eta$, $\zeta$ 
in terms of $x$, $y$, $z$, $x^{\prime}$, $y^{\prime}$ , $z^{\prime}$ . 

We have 
$$\beta=q\alpha$$
or 
$$x^{\prime}i+y^{\prime}j+z^{\prime}k=(\omega+\xi i+\eta j+\zeta k)(xi+yj+zk)$$
$$=-(\xi x+\eta y+\zeta z)
+(\omega x+\eta z-\zeta y)i
+(\omega y+\zeta x-\xi z)j
+(\omega z+\xi y-\eta x)k
$$
as we easily see by the expressions for the powers and products of 
$i$, $j$, $k$ given in \S 71. But the student must pay particular attention 
to the {\sl order} of the factors, else he is certain to make mistakes. 

This (\S 80) resolves itself into the four equations 
$$
\begin{array}{lllllllll}
0      & = &          &   & \xi x & + & \eta y & + & \zeta z\\
x^{\prime}  & = & \omega x &   &       & + & \eta z & - & \zeta y\\
y^{\prime}  & = & \omega y & - & \xi z &   &        & + & \zeta x\\
z^{\prime}  & = & \omega z & + & \xi y & - & \eta x\\
\end{array}
$$
The three last equations give 
$$xx^{\prime}+yy^{\prime}+zz^{\prime}=\omega(x^2+y^2+z^2)$$
which determines $\omega$. 

Also we have, from the same three, by the help of the first, 
$$\xi x^{\prime}+\eta y^{\prime}+\zeta z^{\prime} = 0$$
which, combined with the first, gives
$$\frac{\xi}{yz^{\prime}-zy^{\prime}}
=\frac{\eta}{zx^{\prime}-xz^{\prime}}
=\frac{\zeta}{xy^{\prime}-yx^{\prime}}
$$
and the common value of these three fractions is then easily seen 
to be 
$$\frac{1}{x^2+y^2+z^2}$$

It is easy enough to interpret these expressions by means of 
ordinary coordinate geometry : but a much simpler process will 
be furnished by quaternions themselves in the next chapter, and, in 
giving it, we will refer back to this section. 

{\bf 85}. The associative law of multiplication is now to be proved 
by means of the distributive (\S 81). We leave the proof to the 
student. He has merely to multiply together the factors 
$$w+xi+yj+zk,\;\;\;\; 
w+x^{\prime}i+y^{\prime}j+z^{\prime}k,\;\;\;\;\textrm{ and }
w^{\prime\prime} + x^{\prime\prime}i + y^{\prime\prime}j + 
z^{\prime\prime}k$$

as follows : 

First, multiply the third factor by the second, and then multiply 
the product by the first; next, multiply the second factor by the 
first and employ the product to multiply the third: always remembering 
that the multiplier in any product is placed {\sl before} the 
multiplicand. He will find the scalar parts and the coefficients of 
$i$, $j$, $k$, in these products, respectively equal, each to each. 

{\bf 86}. 
With the same expressions for $\alpha$, $\beta$, as in section 83, we 
have 
$$\alpha\beta=(xi+yj+zk)(x^{\prime}i+y^{\prime}j+z^{\prime}k)$$
$$\;\;=-(xx^{\prime}+yy^{\prime}+zz^{\prime})
+(yz^{\prime}-zy^{\prime})i
+(zx^{\prime}-xz^{\prime})j
+(xy^{\prime}-yx^{\prime})k
$$

But we have also 
$$\beta\alpha=
-(xx^{\prime}+yy^{\prime}+zz^{\prime})
-(yz^{\prime}-zy^{\prime})i
-(zx^{\prime}-xz^{\prime})j
-(xy^{\prime}-yx^{\prime})k
$$

The only difference is in the sign of the vector parts. Hence 
$$S\alpha\beta=S\beta\alpha\eqno{(1)}$$
$$V\alpha\beta=-V\beta\alpha\eqno{(2)}$$
$$\alpha\beta+\beta\alpha=2S\alpha\beta\eqno{(3)}$$
$$\alpha\beta-\beta\alpha=2V\alpha\beta\eqno{(4)}$$
$$\alpha\beta=K.\beta\alpha\eqno{(5)}$$

{\bf 87}. If $\alpha=\beta$ we have of course (\S 25) 
$$x=x^{\prime},\;\;\;\;y=y^{\prime},\;\;\;\;z=z^{\prime}$$
and the formulae of last section become 
$$\alpha\beta=\beta\alpha=\alpha^2=-(x^2+y^2+z^2)$$
which was anticipated in \S 73, where we proved the formula 
$$(T\alpha)^2=-\alpha^2$$
and also, to a certain extent, in \S 25. 

{\bf 88}. Now let $q$ and $r$ be any quaternions, then 
$$
\begin{array}{rcl}
S.qr & = & S.(Sq+Vq)(Sr+Vr)\\
 & = & S.(SqSr+Sr.Vq+Sq.Vr+VqVr)\\
 & = & SqSr+S.VqVr
\end{array}
$$
since the two middle terms are vectors. 
Similarly,
$$S.rq=SrSq+S.VrVq$$
Hence, since by (1) of \S 86 we have 
$$S.VqVr=S.VrVq$$
we see that
$$S.qr=S.rq\eqno{(1)}$$
a formula of considerable importance. 

It may easily be extended to any number of quaternions, 
because, $r$ being arbitrary, we may put for it $rs$. Thus we have 
$$
\begin{array}{rcl}
S.qrs & = & S.rsq\\
& = & S.sqr
\end{array}
$$
by a second application of the process. In words, we have the 
theorem {\sl the scalar of the product of any number of given 
quaternions depends only upon the cyclical order in which they are 
arranged}.

{\bf 89}. An important case is that of three factors, each a vector. 
The formula then becomes 
$$S.\alpha\beta\gamma=S.\beta\gamma\alpha=S.\gamma\alpha\beta$$
But 
$$\begin{array}{rcll}
S.\alpha\beta\gamma & = & S\alpha(S\beta\gamma+V\beta\gamma) &\\
 & = & S\alpha V\beta\gamma & \textrm{since }\alpha S\beta\gamma
\textrm{ is a vector}\\
 & = & -S\alpha V\gamma\beta & \textrm{by (2) of \S 86}\\
 & = & -S\alpha(S\gamma\beta+V\gamma\beta) &\\
 & = & -S.\alpha\gamma\beta
\end{array}
$$
Hence {\sl the scalar of the product of three vectors changes sign when 
the cyclical order is altered.}

By the results of \S\S 55, 73, 79 we see that, for any number 
of vectors, we have 
$$K.\alpha\beta\gamma\ldots\phi\chi=
\pm\chi\phi\ldots\gamma\beta\alpha$$
(the positive sign belonging to the product of an even number of 
vectors) so that 
$$S.\alpha\beta\ldots\phi\chi=\pm S.\chi\phi\ldots\beta\alpha$$

Similarly 
$$V.\alpha\beta\ldots\phi\chi=\mp V.\chi\phi\ldots\beta\alpha$$
Thus we may generalize (3) and (4) of \S 86 into 
$$2S.\alpha\beta\ldots\phi\chi=
\alpha\beta\ldots\chi\phi\pm\phi\chi\ldots\beta\alpha$$
$$2V.\alpha\beta\ldots\phi\chi=
\alpha\beta\ldots\chi\phi\mp\phi\chi\ldots\beta\alpha$$
the upper sign still being used when the -number of factors is 
even. 

Other curious propositions connected with this will be given 
later (some, indeed, will be found in the Examples appended to 
this chapter), as we wish to develop the really fundamental 
formulae in as compact a form as possible. 

{\bf 90}. By (4) of \S 86, 
$$2V\beta\gamma=\beta\gamma-\gamma\beta$$
Hence
$$2V.\alpha V\beta\gamma=V.\alpha(\beta\gamma-\gamma\beta)$$
(by multiplying both by $\alpha$, and taking the vector parts of each side) 
$$=V(\alpha\beta\gamma+\beta\alpha\gamma-\beta\alpha\gamma-\alpha\gamma\beta)$$
(by introducing the null term $\beta\alpha\gamma-\beta\alpha\gamma$).

\noindent
That is 
$$2V.\alpha V\beta\gamma=V.(\alpha\beta+\beta\alpha)\gamma
-V(\beta S\alpha\gamma+\beta V\alpha\gamma+S\alpha\gamma .\beta+
V\alpha\gamma .\beta$$
$$=V.(2S\alpha\beta)\gamma-2V\beta S\alpha\gamma$$
(if we notice that $V(V\alpha\gamma .\beta)=-V.\beta V\alpha\gamma$
by (2) of \S 86). 
Hence 
$$V.\alpha V\beta\gamma=\gamma S\alpha\beta-\beta S\gamma\alpha\eqno{(1)}$$
a formula of constant occurrence. 

Adding $\alpha S\beta\gamma$ to both sides, we get another most valuable 
formula 
$$V.\alpha\beta\gamma
=\alpha S\beta\gamma
-\beta S\gamma\alpha
+\gamma S\alpha\beta\eqno{(2)}
$$
and the form of this shows that we may interchange $\gamma$ and $\alpha$
without altering the right-hand member. This gives 
$$V.\alpha\beta\gamma = V.\gamma\beta\alpha$$
a formula which may be greatly extended. (See \S89, above.) 

Another simple mode of establishing (2) is as follows : 
$$
\begin{array}{rcl}
K.\alpha\beta\gamma & = & -\gamma\beta\alpha\\
\therefore 2V.\alpha\beta\gamma & = & 
\alpha\beta\gamma-K.\alpha\beta\gamma\textrm{ (by \S 79(2))}\\
& = & \alpha\beta\gamma + \gamma\beta\alpha\\
& = & \alpha(\beta\gamma+\gamma\beta)
-(\alpha\gamma+\gamma\alpha)\beta
+\gamma(\alpha\beta+\beta\alpha)\\
& = & 2\alpha S\beta\gamma-2\beta S\alpha\gamma+2\gamma S\alpha\beta
\end{array}
$$

{\bf 91}. We have also 
$$VV\alpha\beta V\gamma\delta = -VV\gamma\delta V\alpha\beta\;\;\;\;
\textrm{ by (2) of \S 86}$$
$$=\delta S\gamma V\alpha\beta-\gamma S\delta V\alpha\beta
=\delta S.\alpha\beta\gamma-\gamma S.\alpha\beta\delta$$
$$=-\beta S\alpha V\gamma\delta+\alpha S\beta V\gamma\delta
=-\beta S.\alpha\gamma\delta+\alpha S.\beta\gamma\delta$$
all of these being arrived at by the help of \S 90 (1) and of \S 89; 
and by treating alternately $V\alpha\beta$ and 
$V\gamma\delta$ as {\sl simple} vectors. 

Equating two of these values, we have 
$$\delta S.\alpha\beta\gamma
=\alpha S.\beta\gamma\delta
+\beta S.\gamma\alpha\delta 
+\gamma S.\alpha\beta\delta\eqno{(3)}
$$
a very useful formula, expressing any vector whatever in terms 
of three given vectors. [This, of course, presupposes that
$\alpha$, $\beta$, $\gamma$
are not coplanar, \S 23. In fact, if they be coplanar, the factor 
$S.\alpha\beta\gamma$ vanishes, 
and thus (3) does not give an expression for $\delta$.
This will be shown in \S 101 below.] 

{\bf 92}. That such an expression as (3) is possible we knew already 
by \S 23. For variety we may seek another expression of a similar 
character, by a process which differs entirely from that employed 
in last section. 

$\alpha$, $\beta$, $\gamma$
being any three non-coplanar vectors, we may derive 
from them three others $V\alpha\beta$, $V\beta\gamma$, $V\gamma\alpha$
and, as these will not be 
coplanar, any other vector $\delta$ may be expressed as the sum of the 
three, each multiplied by some scalar. It is required to find this 
expression for $\delta$.

Let 
$$\delta=xV\alpha\beta+yV\beta\gamma+zV\gamma\alpha$$
Then
$$S\gamma\delta=xS.\gamma\alpha\beta =xS.\alpha\beta\gamma$$
the terms in y and z going out, because 
$$S\gamma V\beta\gamma = S.\gamma\beta\gamma=S\beta\gamma^2
=\gamma^2 S\beta=0$$
for $\gamma^2$ is (\S 73) a number. 

Similarly 
$$S\beta\delta=zS.\beta\gamma\alpha=zS.\alpha\beta\gamma$$
and 
$$S\alpha\delta=qS.\alpha\beta\gamma$$
Thus 
$$\delta S.\alpha\beta\gamma=V\alpha\beta S\gamma\delta
+V\beta\gamma S\alpha\delta
+V\gamma\alpha S\beta\delta\eqno{(4)}
$$

{\bf 93}. We conclude the chapter by showing (as promised in \S 64) 
that the assumption that the product of two parallel vectors is 
a number, and the product of two perpendicular vectors a third 
vector perpendicular to both, is not only useful and convenient, 
but absolutely inevitable, if our system is to deal indifferently with 
all directions in space. We abridge Hamilton s reasoning. 

Suppose that there is no direction in space pre-eminent, and 
that the product of two vectors is something which has quantity, 
so as to vary in amount if the factors are changed, and to have its 
sign changed if that of one of them is reversed ; if the vectors be 
parallel, their product cannot be, in whole or in part, a vector 
{\sl inclined} to them, for there is nothing to determine the direction in 
which it must lie. It cannot be a vector {\sl parallel} to them; for by 
changing the signs of both factors the product is unchanged, 
whereas, as the whole system has been reversed, the product 
vector ought to have been reversed. Hence it must be a number. 
Again, the product of two perpendicular vectors cannot be wholly 
or partly a number, because on inverting one of them the sign of 
that number ought to change; but inverting one of them is simply 
equivalent to a rotation through two right angles about the other, 
and (from the symmetry of space) ought to leave the number 
unchanged. Hence the product of two perpendicular vectors must 
be a vector, and a simple extension of the same reasoning shows 
that it must be perpendicular to each of the factors. It is easy to 
carry this farther, but enough has been said to show the character 
of the reasoning. 

\section{Examples To Chapter 2.}

{\bf 1}. It is obvious from the properties of polar triangles that any 
mode of representing versors by the {\sl sides} of a spherical triangle 
must have an equivalent statement in which they are represented 
by {\sl angles} in the polar triangle. 

Show directly that the product of two versors represented 
by two angles of a spherical triangle is a third versor represented 
by the {\sl supplement} of the remaining angle of the triangle ; and 
determine the rule which connects the {\sl directions} in which these 
angles are to be measured. 

{\bf 2}. Hence derive another proof that we have not generally 
$$pq=qp$$

{\bf 3}. Hence show that the proof of the associative principle, 
\S 57, may be made to depend upon the fact that if from any point 
of the sphere tangent arcs be drawn to a spherical conic, and also 
arcs to the foci, the inclination of either tangent arc to one of the 
focal arcs is equal to that of the other tangent arc to the other 
focal arc. 

{\bf 4}. Prove the formulae 
$$2S.\alpha\beta\gamma = \alpha\beta\gamma-\gamma\beta\alpha$$
$$2V.\alpha\beta\gamma = \alpha\beta\gamma+\gamma\beta\alpha$$

{\bf 5}. Show that, whatever odd number of vectors be represented 
by $\alpha$, $\beta$, $\gamma$ \&c., we have always 
$$
V.\alpha\beta\gamma\delta\epsilon=V.\epsilon\delta\gamma\beta\alpha
$$
$$
V.\alpha\beta\gamma\delta\epsilon\zeta\eta
=V.\eta\zeta\epsilon\delta\gamma\beta\alpha,\textrm{ \&c.}
$$

{\bf 6}. Show that 
$$
S.V\alpha\beta V\beta\gamma V\gamma\alpha=-(S.\alpha\beta\gamma)^2
$$
$$
V.V\alpha\beta V\beta\gamma V\gamma\alpha=
V\alpha\beta(\gamma^2S\alpha\beta-S\beta\gamma S\gamma\alpha)+\ldots
$$
and
$$
V(V\alpha\beta V.V\beta\gamma V\gamma\alpha)
=(\beta S\alpha\gamma-\alpha S\beta\gamma)S.\alpha\beta\gamma
$$

{\bf 7}. If $\alpha$, $\beta$, $\gamma$
be any vectors at right angles to each other, show that 
$$
(\alpha^3+\beta^3+\gamma^3)S.\alpha\beta\gamma
=\alpha^4V\beta\gamma
+\beta^4V\gamma\alpha
+\gamma^4V\alpha\beta
$$
$$
(\alpha^{2n-1}+\beta^{2n-1}+\gamma^{2n-1})S.\alpha\beta\gamma
=\alpha^{2n}V\beta\gamma
+\beta^{2n}V\gamma\alpha
+\gamma^{2n}V\alpha\beta
$$

{\bf 8}. If $\alpha$, $\beta$, $\gamma$
be non-coplanar vectors, find the relations among 
the six scalars, $x$, $y$, $z$ and $\xi$, $\eta$, $\zeta$
which are implied in the 
equation 
$$
x\alpha+y\beta+z\gamma
=\xi V\beta\gamma+\eta V\gamma\alpha+\zeta V\alpha\beta
$$

{\bf 9}. If $\alpha$, $\beta$, $\gamma$
be any three non-coplanar vectors, express any 
fourth vector, $\delta$, as a linear function of each of the following sets of 
three derived vectors. 
$$
V.\gamma\alpha\beta,\;\;\;\;V.\alpha\beta\gamma,\;\;\;\;
V.\beta\gamma\alpha
$$
and
$$
V.V\alpha\beta V\beta\gamma V\gamma\alpha,\;\;\;\;
V.V\beta\gamma V\gamma\alpha V\alpha\beta,\;\;\;\;
V.V\gamma\alpha V\alpha\beta V\beta\gamma
$$

{\bf 10}. Eliminate $\rho$ from the equations 
$$
S\alpha\rho=a,\;\;\;\;
S\beta\rho=b,\;\;\;\;
S\gamma\rho=c,\;\;\;\;
S\delta\rho=d
$$
where $\alpha$, $\beta$, $\gamma$, $\delta$
are vectors, and $a$, $b$, $c$, $d$ scalars. 

{\bf 11}. In any quadrilateral, plane or gauche, the sum of the 
squares of the diagonals is double the sum of the squares of the 
lines joining the middle points of opposite sides. 

\section{Interpretations And Transformations} 

{\bf 94}. Among the most useful characteristics of the Calculus of 
Quaternions, the ease of interpreting its formulae geometrically, 
and the extraordinary variety of transformations of which the 
simplest expressions are susceptible, deserve a prominent place. 
We devote this Chapter to some of the more simple of these, 
together with a few of somewhat more complex character but of 
constant occurrence in geometrical and physical investigations. 
Others will appear in every succeeding Chapter. It is here, 
perhaps, that the student is likely to feel most strongly the 
peculiar difficulties of the new Calculus. But on that very account 
he should endeavour to master them, for the variety of forms 
which any one formula may assume, though puzzling to the 
beginner, is of the utmost advantage to the advanced student, not 
alone as aiding him in the solution of complex questions, but 
as affording an invaluable mental discipline. 

{\bf 95}. If we refer again to the figure of \S 77 we see that 
$$OC=OB\cos AOB$$
$$CB=OB \sin AOB$$
Hence if 
$$\overline{AB}=\alpha,\;\;\;\;
\overline{OB}=\beta,\;\;\;\;\textrm{ and }
\angle AOB=\theta
$$
we have
$$OB=T\beta,\;\;\;\;OA=T\alpha$$
$$OC=T\beta\cos\theta,\;\;\;\;CB=T\beta\sin\theta
$$
Hence
$$S\frac{\beta}{\alpha}=
\frac{OC}{OA}=
\frac{T\beta}{T\alpha}\cos\theta
$$
Similarly,
$$
TV\frac{\beta}{\alpha}=\frac{CB}{OA}=\frac{T\beta}{T\alpha}\sin\theta
$$

Hence, if $\eta$ be a unit-vector perpendicular to 
$\alpha$ and $\beta$, and such 
that positive rotation about it, through the angle $\theta$, turns $\alpha$
towards $\beta$ or 
$$
\eta=
\frac{U\overline{CB}}{U\overline{OA}}=
U\frac{\overline{CB}}{\overline{OA}}=
UV\frac{\beta}{\alpha}
$$
we have
$$
V\frac{\beta}{\alpha}=
\frac{T\beta}{T\alpha}\sin\theta .\eta\;\;\;\;\;\textrm{ (See, again, \S 84)}
$$

{\bf 96}. In the same way, or by putting 
$$
\begin{array}{rcl}
\alpha\beta & = & S\alpha\beta+V\alpha\beta\\
 & = & S\beta\alpha - V\beta\alpha\\
 & = & \alpha^2\left(S\frac{\beta}{\alpha}-V\frac{\beta}{\alpha}\right)\\
 & = & T\alpha^2\left(-S\frac{\beta}{\alpha}+V\frac{\beta}{\alpha}\right)
\end{array}
$$
we may show that 
$$S\alpha\beta=-T\alpha T\beta\cos\theta$$
$$TV\alpha\beta = T\alpha T\beta\sin\theta$$
and
$$V\alpha\beta=T\alpha T\beta\sin\theta . \eta$$
where
$$\eta=UV\alpha\beta = U(-V\beta\alpha)=UV\frac{\beta}{\alpha}$$

Thus {\sl the scalar of the product of two vectors is the continued 
product of their tensors and of the cosine of the supplement of the 
contained angle}. 

{\sl The tensor of the vector of the product of two vectors is the con 
tinued product of their tensors and the sine of the contained angle ; 
and the versor of the same is a unit-vector perpendicular to both, 
and such that the rotation about it from the first vector (i. e. the 
multiplier) to the second is left-handed or positive}. 

{\sl Hence also $TV\alpha\beta$ 
is double the area of the triangle two of whose 
sides are $\alpha$, $\beta$.}

{\bf 97}. (a) In any plane triangle $ABC$ we have 
$$\overline{AC}=\overline{AB}+\overline{BC}$$
Hence,
$$
\overline{AC}^2=S.\overline{AC}\overline{AC}=
S.\overline{AC}(\overline{AB}+\overline{BC})
$$

With the usual notation for a plane triangle the interpretation 
of this formula is 
$$b^2 = -bc\cos A-ab\cos C$$
or
$$b=c\cos C+c\cos A$$

(b) Again we have, obviously, 
$$
\begin{array}{rcl}
V.\overline{AB}\;\overline{AC}&=&V.\overline{AB}(\overline{AB}+\overline{BC})\\
&=&V.\overline{AB}\;\overline{BC}
\end{array}
$$
or
$$cb\sin A = ca\sin B$$
whence
$$\frac{\sin A}{a}=\frac{\sin B}{b}=\frac{\sin C}{c}$$

These are truths, but not truisms, as we might have been led 
to fancy from the excessive simplicity of the process employed. 

{\bf 98}. 
From \S 96 it follows that, if $\alpha$ and $\beta$ be both actual (i. e. 
real and non-evanescent) vectors, the equation 
$$S\alpha\beta = 0$$


shows that $\cos\theta=0$, or that 
$\alpha$ is {\sl perpendicular} to $\beta$. And, in fact, 
we know already that the product of two perpendicular vectors is 
a vector. 

Again if 
$$V\alpha\beta=0$$
we must have $\sin\theta=0$, or 
$\alpha$ is {\sl parallel} to $\beta$. We know already 
that the product of two parallel vectors is a scalar. 

Hence we see that 
$$S\alpha\beta=0$$
is equivalent to
$$\alpha=V\gamma\beta$$
where $\gamma$ is an undetermined vector; and that 
$$V\alpha\beta=0$$
is equivalent to
$$\alpha=x\beta$$
where $x$ is an undetermined scalar. 

{\bf 99}. If we write, as in \S\S 83, 84, 
$$\alpha=ix+jy+kz$$
$$\beta=ix^{\prime}+jy^{\prime}+kz^{\prime}$$
we have, at once, by \S 86, 
$$\begin{array}{rcl}
S\alpha\beta&=&-xx^{\prime}-yy^{\prime}-zz^{\prime}\\
&=&-rr^{\prime}\left(
\frac{x}{r}\frac{x^{\prime}}{r^{\prime}}+
\frac{y}{r}\frac{y^{\prime}}{r^{\prime}}+
\frac{z}{r}\frac{z^{\prime}}{r^{\prime}}
\right)
\end{array}
$$
where
$$
r=\sqrt{x^2+y^2+z^2},\;\;\;\;
r^{\prime}=\sqrt{x^{'2}+y^{'2}+z^{'2}}
$$
Also
$$
V\alpha\beta=rr^{\prime}\left\{
\frac{yz^{\prime}-zy^{\prime}}{rr^{\prime}}i+
\frac{zx^{\prime}-xz^{\prime}}{rr^{\prime}}j+
\frac{xy^{\prime}=yx^{\prime}}{rr^{\prime}}k
\right\}
$$

These express in Cartesian coordinates the propositions we have 
just proved. In commencing the subject it may perhaps assist 
the student to see these more familiar forms for the quaternion 
expressions ; and he will doubtless be induced by their appearance 
to prosecute the subject, since he cannot fail even at this stage to 
see how much more simple the quaternion expressions are than 
those to which he has been accustomed. 

{\bf 100}. The expression
$$S.\alpha\beta\gamma$$
may be written 
$$SV(\alpha\beta)\gamma$$
because the quaternion $\alpha\beta\gamma$ may be broken up into 
$$S(\alpha\beta)\gamma+V(\alpha\beta)\gamma$$
of which the first term is a vector. 

But, by \S 96, 
$$SV(\alpha\beta)\gamma=T\alpha T\beta\sin\theta S\eta\gamma$$
Here $T\eta=1$, let $\phi$ be the angle between $\eta$ and $\gamma$, 
then finally 
$$S.\alpha\beta\gamma = -T\alpha T\beta T\gamma\sin\theta\cos\phi$$

But as $\eta$ is perpendicular to $\alpha$ and $\beta$, 
$T\gamma\cos\phi$ is the length of the 
perpendicular from the extremity of $\gamma$ 
upon the plane of $\alpha$, $\beta$. And 
as the product of the other three factors is (\S 96) the area of the 
parallelogram two of whose sides are $\alpha$, $\beta$, we see that the 
magnitude of $S.\alpha\beta\gamma$, 
independent of its sign, is {\sl the volume of the 
parallelepiped of which three coordinate edges 
are $\alpha$, $\beta$, $\gamma$};
or six times the volume of the pyramid which has 
$\alpha$, $\beta$, $\gamma$ for edges. 

{\bf 101}. Hence the equation 
$$S.\alpha\beta\gamma=0$$
if we suppose $\alpha\beta\gamma$ to be actual vectors, shows either that 
$$\sin\theta=0$$
or
$$\cos\phi=0$$
i. e. {\sl two of the three vectors are parallel}, 
or {\sl all three are parallel to one plane}. 

This is consistent with previous results, for if $\gamma=p\beta$ we have 
$$S.\alpha\beta\gamma=pS.\alpha\beta^2=0$$
and, if $\gamma$ be coplanar with $\alpha$,$\beta$, we have 
$\gamma=p\alpha+q\beta$ and
$$S.\alpha\beta\gamma=S.\alpha\beta(p\alpha+q\beta)=0$$

{\bf 102}. 
This property of the expression $S.\alpha\beta\gamma$ prepares us to 
find that it is a determinant. And, in fact, if we take $\alpha$,$\beta$ as in 
\S 83, and in addition 
$$\gamma=ix^{\prime\prime}+jy^{\prime\prime}+kz^{\prime\prime}$$
we have at once 
$$S.\alpha\beta\gamma=-x^{\prime\prime}(yz^{\prime}-zy^{\prime})-
y^{\prime\prime}(zx^{\prime}-xz^{\prime})-
z^{\prime\prime}(xy^{\prime}-yx^{\prime})$$
$$
=-\left\vert
\begin{array}{ccc}
x & y & z\\
x^{\prime} & y^{\prime} & z^{\prime}\\
x^{\prime\prime}&y^{\prime\prime}&z^{\prime\prime}
\end{array}
\right\vert
$$
The determinant changes sign if we make any two rows change 
places. This is the proposition we met with before (\S 89) in the 
form 
$$S.\alpha\beta\gamma=-S.\beta\alpha\gamma=S.\beta\gamma\alpha
\textrm{, \&c}$$

If we take three new vectors 
$$\alpha_1=ix+jx^{\prime}+kx^{\prime\prime}$$
$$\beta_1 =iy+jy^{\prime}+ky^{\prime\prime}$$
$$\gamma_1=iz+jz^{\prime}+kz^{\prime\prime}$$
we thus see that they are coplanar if $\alpha$, $\beta$, $\gamma$ are so. 
That is, if 
$$S.\alpha\beta\gamma=0$$
then
$$S.\alpha_1\beta_1\gamma_1=0$$

{\bf 103}. We have, by \S 52, 
$$
\begin{array}{rcl}
(Tq)^2 &=&qKq = (Sq+Vq)(Sq-Vq)\;\;\;\;\textrm{(\S 79)}\\
&=&(Sq)^2-(Vq)^2\;\;\;\;\;\textrm{by algebra}\\
&=&(Sq)^2+(TVq)^2\;\;\;\;\textrm{(\S 73)}\\
\end{array}
$$
If $q=\alpha\beta$, we have $Kq = \beta\alpha$, and the formula becomes 
$$\alpha\beta . \beta\alpha = \alpha^2\beta^2=
(S\alpha\beta)^2-(V\alpha\beta)^2
$$

In Cartesian coordinates this is\\
\vskip 0.1cm
$(x^2+y^2+z^z)(x^{'2}+y^{'2}+z^{'2})$
$$
=(xx^{\prime}+yy^{\prime}+zz^{\prime})^2+(yz^{\prime}-zy^{\prime})^2+
(zx^{\prime}-xz^{\prime})^2+(xy^{\prime}-yx^{\prime})^2
$$
More generally we have 
$$
\begin{array}{rcl}
(T(qr))^2&=&(Tq)^2(Tr)^2\\
&=&(S.qr)^2-(V.qr)^2
\end{array}
$$
If we write 
$$q=w+\alpha=w+ix+jy+kz$$
$$r=w^{\prime}+\beta=w^{\prime}+ix^{\prime}+jy^{\prime}+kz^{\prime}$$
this becomes 
$$(w^2+x^2+y^2+z^2)(w^{'2}+x^{'2}+y^{'2}+z^{'2})$$
$$=(ww^{\prime}-xx^{\prime}-yy^{\prime}-zz^{\prime})^2+
(wx^{\prime}+w^{\prime}x+yz^{\prime}-zy^{\prime})^2$$
$$=(xy^{\prime}+w^{\prime}y+zx^{\prime}-xz^{\prime})^2+
(wz^{\prime}+w^{\prime}z+xy^{\prime}-yx^{\prime})^2$$
a formula of algebra due to Euler. 

{\bf 104}. We have, of course, by multiplication, 
$$
(\alpha+\beta)^2=
\alpha^2+\alpha\beta+\beta\alpha+\beta^2=
\alpha^2+2S\alpha\beta+\beta^2\;\;\;\;\;\textrm{(\S 86 (3))}
$$
Translating into the usual notation of plane trigonometry, this 
becomes 
$$c^2=a^2-2ab\cos C+b^2$$
the common formula. 

Again,
$$
V.(\alpha+\beta)(\alpha-\beta)=
-V\alpha\beta+V\beta\alpha=
-2V\alpha\beta\;\;\;\;\;\textrm{(\S 86 (2)}
$$
Taking tensors of both sides we have the theorem, {\sl the parallelogram 
whose sides are parallel and equal to the diagonals of a 
given parallelogram, has double its area} (\S 96). 

Also 
$$S(\alpha+\beta)(\alpha-\beta)=\alpha^2-\beta^2$$
and vanishes only when $\alpha^2=\beta^2$, 
or $T\alpha=T\beta$; that is, {\sl the diagonals 
of a parallelogram are at right angles to one another, when, and 
only when, it is a rhombus}. 

Later it will be shown that this contains a proof that the angle 
in a semicircle is a right angle. 

{\bf 105}. The expression\hbox{\hskip 1cm}$\rho=\alpha\beta\alpha^{-1}$\\
obviously denotes a vector whose tensor is equal to that of $\beta$. 

But we have\hbox{\hskip 2cm}$S.\beta\alpha\rho=0$\\
so that $\rho$ is in the plane of $\alpha$, $\beta$

Also we have\hbox{\hskip 2cm}$S\alpha\rho=S\alpha\beta$\\
so that $\beta$ and $\rho$ make equal angles with $\alpha$, 
evidently on opposite 
sides of it. Thus if $\alpha$ be the perpendicular to a reflecting surface 
and $\beta$ the path of an incident ray, $-\rho$ will be the path of the 
reflected ray. 

Another mode of obtaining these results is to expand the above 
expression, thus, \S 90 (2), 
$$
\begin{array}{rcl}
\rho&=&2\alpha^{-1}S\alpha\beta-\beta\\
&=&2\alpha^{-1}S\alpha\beta-\alpha^{-1}(S\alpha\beta+V\alpha\beta)\\
&=&\alpha^{-1}(S\alpha\beta-V\alpha\beta)
\end{array}
$$
so that in the figure of \S 77 we see that if $\overline{OA}=\alpha$,
and $\overline{OB}=\beta$, we
have $\overline{OD} = \rho = \alpha\beta\alpha^{-1}$

Or, again, we may get the result at once by transforming the 
equation to $\frac{\rho}{\alpha}=K(\alpha^{-1}\rho)=K\frac{\beta}{\alpha}$

{\bf 106}. For any three coplanar vectors the expression 
$$\rho=\alpha\beta\gamma$$
is (\S 101) a vector. It is interesting to determine what this vector 
is. The reader will easily see that if a circle be described about 
the triangle, two of whose sides are (in order) $\alpha$ and $\beta$, 
and if from 
the extremity of $\beta$ a line parallel to $\gamma$ be drawn, 
again cutting the 
circle, the vector joining the point of intersection with the origin 
of $\alpha$ is the direction of the vector $\alpha\beta\gamma$. 
For we may write it in the form 
$$
\rho=\alpha\beta^2\beta^{-1}\gamma=
-(T\beta)^2\alpha\beta^{-1}\gamma=
-(T\beta)^2\frac{\alpha}{\beta}\gamma
$$
which shows that the versor $\displaystyle\left(\frac{\alpha}{\beta}\right)$
which turns $\beta$ into a direction 
parallel to $\alpha$, turns $\gamma$ into a direction parallel to $\rho$. 
And this expresses the long-known property of opposite angles of a 
quadrilateral inscribed in a circle. 

Hence if $\alpha$, $\beta$, $\gamma$ 
be the sides of a triangle taken in order, the 
tangents to the circumscribing circle at the angles of the triangle 
are parallel respectively to 
$$
\alpha\beta\gamma,\;\;\;\;
\beta\gamma\alpha,\;\;\;\;\textrm{ and }
\gamma\alpha\beta
$$

Suppose two of these to be parallel, i. e. let 
$$\alpha\beta\gamma=x\beta\gamma\alpha=x\alpha\gamma\beta\;\;\;\;(\S 90)$$
since the expression is a vector. Hence 
$$\beta\gamma=x\gamma\beta$$
which requires either 
$$x=1,\;\;\;\;V\gamma\beta=0\;\;\;\;\textrm{ or }\gamma \vert\vert \beta$$
a case not contemplated in the problem; or 
$$x=-1,\;\;\;\;S\beta\gamma=0$$
i. e. the triangle is right-angled. And geometry shows us at once 
that this is correct. 

Again, if the triangle be isosceles, the tangent at the vertex is 
parallel to the base. Here we have 
$$x\beta=\alpha\beta\gamma$$
or
$$x(\alpha+\gamma)=\alpha(\alpha+\gamma)\gamma$$
whence $x=\gamma^2=\alpha^2$, or $T\gamma=T\alpha$, as required. 

As an elegant extension of this proposition the reader may 
prove that the vector of the continued product $\alpha\beta\gamma\delta$ 
of the vectorsides of any quadrilateral inscribed in a sphere 
is parallel to the radius drawn to the corner ($\alpha$, $\delta$). 
[For, if $\epsilon$ be the vector from $\delta$,
$\alpha$ to $\beta$, $\gamma$, $\alpha\beta\epsilon$ and 
$\epsilon\gamma\delta$ are (by what precedes) vectors {\sl touching} the 
sphere at $\alpha$, $\delta$. And their product (whose vector part must be 
parallel to the radius at $\alpha$, $\delta$) is 
$$\alpha\beta\epsilon . \epsilon\gamma\delta=\epsilon^2 . 
\alpha\beta\gamma\delta]$$

{\bf 107}. To exemplify the variety of possible transformations 
even of simple expressions, we will take cases which are of 
frequent occurrence in applications to geometry. 

Thus $$T(\rho+\alpha)=T(\rho-\alpha)$$
[which expresses that if 
$$
\overline{OA}=\alpha\;\;\;\;
\overline{OA^{\prime}}=-\alpha\;\;\;\;\textrm{ and }\;\;\;\;
\overline{OP}=\rho
$$
we have\hbox{\hskip 4cm}$AP=A^{\prime}P$\\
and thus that $P$ is any point equidistant from two fixed points,] 
may be written $$(\rho+\alpha)^2=(\rho-\alpha)^2$$
or\hbox{\hskip 3cm}$\rho^2+2S\alpha\rho+\alpha^2=
\rho^2-2S\alpha\rho+\alpha^2\;\;\;\;\textrm{(\S 104)}$\\
whence\hbox{\hskip 4cm}$S\alpha\rho=0$\\
This may be changed to 
$$\alpha\rho+\rho\alpha=0$$
or
$$\alpha\rho+K\alpha\rho=0$$
$$SU\frac{\rho}{\alpha}=0$$
or finally,
$$TVU\frac{\rho}{\alpha}=1$$
all of which express properties of a plane. 

Again,\hbox{\hskip 4cm}$T\rho=T\alpha$\\
may be written\hbox{\hskip 3.2cm}$\displaystyle T\frac{\rho}{\alpha}=1$
$$\left(S\frac{\rho}{\alpha}\right)^2-\left(V\frac{\rho}{\alpha}\right)^2=1$$
$$(\rho+\alpha)^2-2S\alpha(\rho+\alpha)=0$$
$$\rho=(\rho+\alpha)^{-1}\alpha(\rho+\alpha)$$
$$S(\rho+\alpha)(\rho-\alpha)=0$$
or finally,
$$T.(\rho+\alpha)(\rho-\alpha)=2TV\alpha\rho$$

All of these express properties of a sphere. They will be 
interpreted when we come to geometrical applications. 

{\bf 108}. {\sl To find the space relation among five points.}

A system of five points, so far as its internal relations are 
concerned, is fully given by the vectors from one to the other four. 
If three of these be called $\alpha$, $\beta$, $\gamma$, the fourth, 
$\delta$, is necessarily expressible as 
$x\alpha+y\beta+z\gamma$. Hence the relation required must be 
independent of x, y, z. 

But 
$$
\left.
\begin{array}{rlll}
S\alpha\delta &=\;x\alpha^2     &+\;yS\alpha\beta &+\;zS\alpha\gamma\\
S\beta\delta  &=\;xS\beta\alpha &+\;y\beta^2      &+\;zS\beta\gamma\\
S\gamma\delta &=\;xS\gamma\alpha &+\;yS\gamma\beta &+\;z\gamma^2\\
S\delta\delta=\delta^2 &=\;xS\delta\alpha &+\;yS\delta\beta &+\;zS\delta\gamma
\end{array}
\right\}\eqno{(1)}
$$
The elimination of $x$, $y$, $z$ gives a determinant of the fourth order, 
which may be written 
$$
\left\vert
\begin{array}{cccc}
S\alpha\alpha & S\alpha\beta & S\alpha\gamma & S\alpha\delta\\
S\beta\alpha  & S\beta\beta  & S\beta\gamma  & S\beta\delta\\
S\gamma\alpha & S\gamma\beta & S\gamma\gamma & S\gamma\delta\\
S\delta\alpha & S\delta\beta & S\delta\gamma & S\delta\delta
\end{array}
\right\vert=0
$$
Now each term may be put in either of two forms, thus 
$$S\beta\gamma=\frac{1}{2}\left\{\beta^2+\gamma^2-(\beta-\gamma)^2\right\}=
-T\beta T\gamma\cos\widehat{\beta\gamma}$$


If the former be taken we have the expression connecting the 
distances, two and two, of five points in the form given by Muir 
(Proc. R. S. E. 1889) ; if we use the latter, the tensors divide out 
(some in rows, some in columns), and we have the relation among 
the cosines of the sides and diagonals of a spherical quadrilateral. 

We may easily show (as an exercise in quaternion manipulation 
merely) that this is the {\sl only} condition, by showing that from it 
we can get the condition when any other of the points is taken as 
origin. Thus, let the origin be at $\alpha$, the vectors are 
$\alpha$, $\beta-\alpha$, $\gamma-\alpha$, $\delta-\alpha$.
But, by changing the signs of the first row, and first 
column, of the determinant above, and then adding their values 
term by term to the other rows and columns, it becomes 
$$
\left\vert
\begin{array}{cccc}
S(\;\;\;-\alpha)(-\alpha) & S(\;\;\;-\alpha)(\beta-\alpha) 
& S(\;\;\;-\alpha)(\gamma-\alpha) & S(\;\;\;-\alpha)(\delta-\alpha)\\
S(\beta-\alpha)(-\alpha)  & S(\beta-\alpha)(\beta-\alpha)  
& S(\beta-\alpha)(\gamma-\alpha)  & S(\beta-\alpha)(\delta-\alpha)\\
S(\gamma-\alpha)(-\alpha) & S(\gamma-\alpha)(\beta-\alpha) 
& S(\gamma-\alpha)(\gamma-\alpha) & S(\gamma-\alpha)(\delta-\alpha)\\
S(\delta-\alpha)(-\alpha) & S(\delta-\alpha)(\beta-\alpha) 
& S(\delta-\alpha)(\gamma-\alpha) & S(\delta-\alpha)(\delta-\alpha)
\end{array}
\right\vert
$$
which, when equated to zero, gives the same relation as before. 
[See Ex. 10 at the end of this Chapter.] 

An additional point, with $\epsilon=x^{\prime}\alpha+
y^{\prime}\beta+z^{\prime}\gamma$
gives six additional equations like (1) ; i. e. 
$$
\begin{array}{rlll}
S\alpha\epsilon&=x^{\prime}\alpha^2&+
y^{\prime}S\alpha\beta&+z^{\prime}S\alpha\gamma\\
S\beta\epsilon&=x^{\prime}S\beta\alpha&+y^{\prime}\beta^2&+
z^{\prime}S\beta\gamma\\
S\gamma\epsilon&=x^{\prime}S\gamma\alpha&+y^{\prime}S\gamma\beta&+
z^{\prime}\gamma^2\\
S\delta\epsilon&=x^{\prime}S\delta\alpha&+y^{\prime}S\delta\beta&+
z^{\prime}S\delta\gamma\\
&=xS\epsilon\alpha&+yS\epsilon\beta&+zS\epsilon\gamma\\
\epsilon^2&=x^{\prime}S\alpha\epsilon&+y^{\prime}S\beta\epsilon&+
z^{\prime}S\gamma\epsilon
\end{array}
$$
from which corresponding conclusions may be drawn. 

Another mode of solving the problem at the head of this 
section is to write the {\sl identity}
$$
\sum m(\alpha-\theta)^2=\sum m\alpha^2-sS.\theta\sum m\alpha+\theta^2\sum m
$$
where the $m$s are undetermined scalars, and the $\alpha$s are given 
vectors, while $\theta$ is any vector whatever. 

Now, {\sl provided that the number of given vectors exceeds four}, we 
do not completely determine the ms by imposing the conditions 
$$\sum m=0,\;\;\;\;\sum m\alpha=0$$
Thus we may write the above identity, for each of five vectors 
successively, as 
$$
\begin{array}{rcl}
\sum m(\alpha-\alpha_1)^2 &=& \sum m\alpha^2\\
\sum m(\alpha-\alpha_2)^2 &=& \sum m\alpha^2\\
\ldots\ldots &=& \ldots\\
\sum m(\alpha-\alpha_n)^2 &=& \sum m\alpha^2\\
\end{array}
$$
Take, with these,\hbox{\hskip 3cm}$\sum m = 0$\\
and we have six linear equations from which to eliminate the $m$s. 
The resulting determinant is 
$$
\left\vert
\begin{array}{cccccc}
\overline{\alpha_1-\alpha_1^2} & \overline{\alpha_1-\alpha_s^2} &
\overline{\alpha_1-\alpha_3^2} & . & 
\overline{\alpha_1-\alpha_5^2} & 1\\
\overline{\alpha_2-\alpha_1^2} & \overline{\alpha_2-\alpha_s^2} &
\overline{\alpha_2-\alpha_3^2} & . & 
\overline{\alpha_2-\alpha_5^2} & 1\\
. & . & . & & . & \\
. & . & . & & . & \\
\overline{\alpha_5-\alpha_1^2} & \overline{\alpha_5-\alpha_s^2} &
\overline{\alpha_5-\alpha_3^2} & . & 
\overline{\alpha_5-\alpha_5^2} & 1\\
1 & 1 & . & . & 1 & 0\\
\end{array}
\right\vert
\sum m\alpha^2=0
$$

This is equivalent to the form in which Cayley gave the 
relation among the mutual distances of five points. (Camb. Math. 
Journ. 1841.) 

{\bf 109}. We have seen in \S 95 that a quaternion may be divided 
into its scalar and vector parts as follows: 
$$
\frac{\beta}{\alpha}=S\frac{\beta}{\alpha}+V\frac{\beta}{\alpha}=
\frac{T\beta}{T\alpha}(\cos\theta+\epsilon\sin\theta)
$$
where $\theta$ is the angle between the directions of 
$\alpha$ and $\beta$ and $\displaystyle \epsilon=UV\frac{\beta}{\alpha}$
is the unit-vector perpendicular to the plane of $\alpha$ 
and $\beta$ so situated 
that positive (i.e. left-handed) rotation about it turns 
$\alpha$ towards $\beta$

Similarly we have (\S 96) 
$$
\begin{array}{rl}
\alpha\beta&=S\alpha\beta + V\alpha\beta\\
&=T\alpha T\beta(-\cos\theta +\epsilon\sin\theta)
\end{array}
$$
$\theta$ and $\epsilon$ having the same signification as before. 

{\bf 110}. Hence, considering the versor parts alone, we have 
$$U\frac{\beta}{\alpha}=\cos\theta+\epsilon\sin\theta$$
Similarly
$$U\frac{\gamma}{\beta}=\cos\phi+\epsilon\sin\phi$$
$\phi$ being the positive angle between the directions of 
$\gamma$ and $\beta$, and $\epsilon$
the same vector as before, if $\alpha$, $\beta$, $\gamma$ be coplanar. 

Also we have 
$$U\frac{\gamma}{\alpha}=\cos(\theta+\phi)+\epsilon\sin(\theta+\phi)$$
But we have always 
$$\frac{\gamma}{\beta}.\frac{\beta}{\alpha}=\frac{\gamma}{\alpha}$$
and therefore
$$U\frac{\gamma}{\beta}.U\frac{\beta}{\alpha}=U\frac{\gamma}{\alpha}$$
or
$$
\cos(\phi+\theta)+\epsilon\sin(\phi+\theta)=
(\cos\phi+\epsilon\sin\phi)(\cos\theta+\epsilon\sin\theta)
$$
$$
=\cos\phi\cos\theta-\sin\phi\sin\theta+
\epsilon(\sin\phi\cos\theta+\cos\phi\sin\theta)
$$
from which we have at once the fundamental formulae for the 
cosine and sine of the sum of two arcs, by equating separately the 
scalar and vector parts of these quaternions. 

And we see, as an immediate consequence of the expressions 
above, that 
$$\cos m\theta+\epsilon\sin m\theta=(\cos\theta+\epsilon\sin\theta)^m$$
if $m$ be a positive whole number. For the left-hand side is a versor 
which turns through the angle $m\theta$ at once, while the right-hand 
side is a versor which effects the same object by $m$ successive turn 
ings each through an angle $\theta$. See \S\S 8, 9. 

{\bf 111}. To extend this proposition to fractional indices we have 
only to write $\displaystyle \frac{\theta}{n}$ for $\theta$,
when we obtain the results as in ordinary trigonometry. 

From De Moivre's Theorem, thus proved, we may of course 
deduce the rest of Analytical Trigonometry. And as we have 
already deduced, as interpretations of self-evident quaternion 
transformations (\S\S 97, 104), the fundamental formulae for the solution 
of plane triangles, we will now pass to the consideration of spherical 
trigonometry, a subject specially adapted for treatment by qua 
ternions; but to which we cannot afford more than a very few 
sections. (More on this subject will be found in Chap. XI in 
connexion with the Kinematics of rotation.) The reader is referred to 
Hamilton s works for the treatment of this subject by quaternion 
exponentials. 

{\bf 112}. Let $\alpha$, $\beta$, $\gamma$
be unit-vectors drawn from the centr to the 
corners $A$, $B$, $C$ of a triangle on the unit-sphere. Then it is evident 
that, with the usual notation, we have (\S 96), 
$$
S\alpha\beta=-\cos c,\;\;\;\;
S\beta\gamma=-\cos a,\;\;\;\;
S\gamma\alpha=-\cos b
$$
$$
TV\alpha\beta=\sin c,\;\;\;\;
TV\beta\gamma=\sin a,\;\;\;\;
TV\gamma\alpha=\sin b
$$
Also $UV\alpha\beta$, $UV\beta\gamma$, $UV\gamma\alpha$
are evidently the vectors of the corners of the polar triangle. 

Hence 
$$S.UV\alpha\beta UV\beta\gamma=\cos B\textrm{, \&c.}$$
$$TV.UV\alpha\beta UV\beta\gamma=\sin B\textrm{, \&c.}$$

Now (\S 90 (1)) we have 
$$
\begin{array}{rcl}
SV\alpha\beta V\beta\gamma&=&S.\alpha V(\beta V\beta\gamma)\\
&=&-S\alpha\beta S\beta\gamma + \beta^2S\alpha\gamma
\end{array}
$$
Remembering that we have 
$$
SV\alpha\beta V\beta\gamma=
TV\alpha\beta TV\beta\gamma S.UV\alpha\beta UV\beta\gamma
$$
we see that the formula just written is equivalent to 
$$\sin a \sin c \cos B = -\cos a \cos c + \cos b$$
or
$$\cos b = \cos a \cos c + \sin a \sin c \cos B$$

{\bf 113}. Again,
$$V.V\alpha\beta V\beta\gamma=-\beta S\alpha\beta\gamma$$
which gives 
$$
TV.V\alpha\beta V\beta\gamma=
TS.\alpha\beta\gamma=
TS.\alpha V\beta\gamma=
TS.\beta V\gamma\alpha=
TS.\gamma V\alpha\beta
$$
or
$$\sin a \sin c \sin B = \sin a \sin p_a = \sin b \sin p_b = \sin c \sin p_c$$
where $p_a$ is the arc drawn from $A$ perpendicular to $BC$, \&c. 
Hence
$$\sin p_a = \sin c \sin B$$
$$\sin p_b = \frac{\sin a \sin c}{\sin b} \sin B$$
$$\sin p_c = \sin a \sin B$$

{\bf 114}. Combining the results of the last two sections, we have 
$$V\alpha\beta .V\beta\gamma=\sin a \sin c \cos B-\beta \sin a \sin c \sin B$$
$$= \sin a \sin c (\cos B - \beta \sin B)$$
$$
\left.
\begin{array}{lcl}
\textrm{Hence} & \hbox{\hskip 1cm} 
& U.V\alpha\beta V\beta\gamma=(\cos B-\beta\sin B)\\
\textrm{and} & \hbox{\hskip 1.1cm} 
& U.V\gamma\beta V\beta\alpha=(\cos B+\beta\sin B)
\end{array}
\right\}
$$
These are therefore versors which turn all vectors perpendicular to 
$OB$ negatively or positively about $OB$ through the angle $B$. 

[It will be shown later (\S 119) that, in the combination 
$$(\cos B+\beta\sin B)(\;\;\;\;)(\cos B -\beta\sin B)$$
the system operated on is made to rotate, as if rigid, round the 
vector axis $\beta$ through an angle $2B$.] 

As another instance, we have 
$$
\begin{array}{rl}
\tan B &=\displaystyle \frac{\sin B}{\cos B}\\
&\\
&=\displaystyle 
\frac{TV.V\alpha\beta V\beta\gamma}{S.V\alpha\beta V\beta\gamma}\\
&\\
&=\displaystyle 
-\beta^{-1}\frac{V.V\alpha\beta V\beta\gamma}{S.V\alpha\beta V\beta\gamma}\\
&\\
&=\displaystyle 
-\frac{S.\alpha\beta\gamma}{S\alpha\gamma+S\alpha\beta S\beta\gamma}
=\textrm{ \&c}
\end{array}
\eqno{(1)}
$$

The interpretation of each of these forms gives a different theorem 
in spherical trigonometry. 

{\bf 115}. Again, let us square the equal quantities 
$$
V.\alpha\beta\gamma\;\;\;\;\textrm{   and   }\;\;\;\;
\alpha S\beta\gamma-\beta S\alpha\gamma+\gamma S\alpha\beta$$
supposing $\alpha$, $\beta$, $\gamma$ to be any unit-vectors whatever. We have 
$$
-(V.\alpha\beta\gamma)^2=
S^2\beta\gamma + S^2\gamma\alpha + 
S^2\alpha\beta + 2S\beta\gamma S\gamma\alpha S\alpha\beta
$$
But the left-hand member may be written as 
$$T^2.\alpha\beta\gamma-S^2.\alpha\beta\gamma$$
whence
$$
1-S^2.\alpha\beta\gamma=S^2\beta\gamma +S^2\gamma\alpha +
S^2\alpha\beta+2S\beta\gamma S\gamma\alpha S\alpha\beta
$$
or
$$
1-\cos^2 a - \cos^2 b - \cos^2 c + 2 \cos a \cos b \cos c$$
$$= \sin^2 a \sin^2 p_a = \textrm{ \&c.}$$
$$= \sin^2 a \sin^2 b \sin^2 C  = \textrm{ \&c.}$$
all of which are well-known formulae. 

{\bf 116}. Again, for any quaternion, 
$$q=Sq+Vq$$
so that, if $n$ be a positive integer, 
$$q^n=(Sq)^n+n(Sq)^{n-1}Vq+
\frac{n.\overline{n-1}}{1.2}(Sq)^{n-2}(Vq)^2+\ldots$$
From this at once 
$$S.q^n=(Sq)^n-\frac{n.\overline{n-1}}{1.2}(Sq)^{n-2}T^2Vq$$
$$+\frac{n.\overline{n-1}.\overline{n-2}.\overline{n-3}}{1.2.3.4}
(Sq)^{n-4}T^4(Vq)-\textrm{\&c.,}$$
$$V.q^n=Vq\left[n(Sq)^{n-1}-
\frac{n.\overline{n-1}.\overline{n-2}}{1.2.3}
(Sq)^{n-3}T^2Vq+\textrm{\&c.,}\right]$$
If $q$ be a versor we have 
$$q=\cos u +\theta\sin u$$
so that 
$$
\begin{array}{rl}
S.q^n & =\displaystyle
(\cos u)^n-\frac{n.\overline{n-1}}{1.2}(\cos u)^{n-2}(\sin u)^2+\ldots\\
&\\
&=\displaystyle\cos nu;\\
&\\
V.q^n & =\displaystyle
\theta\sin u\left[n(\cos u)^{n-1}-
\frac{n.\overline{n-1}.\overline{n-2}}{1.2.3}
(\cos u)^{n-3}(\sin u)^2+\ldots\right]\\
&\\
&=\displaystyle\theta\sin nu;
\end{array}
$$
as we might at once have concluded from \S 110. 

Such results may be multiplied indefinitely by any one who has 
mastered the elements of quaternions. 

{\bf 117}. A curious proposition, due to Hamilton, gives us a 
quaternion expression for the {\sl spherical excess} in any triangle. 
The following proof, which is very nearly the same as one of his, 
though by no means the simplest that can be given, is chosen here 
because it incidentally gives a good deal of other information. 
We leave the quaternion proof as an exercise. 

Let the unit-vectors drawn from the centre of the sphere to 
$A$, $B$, $C$, respectively, be $\alpha$, $\beta$, $\gamma$.
It is required to express, as an 
arc and as an angle on the sphere, the quaternion 
$$\beta\alpha^{-1}\gamma$$

\begin{center}
\includegraphics{ps/quat18.ps}
\end{center}
\vskip 0.5cm

The figure represents an orthographic projection made on a 
plane perpendicular to $\gamma$. Hence $C$ is the centre of the circle $DEe$. 
Let the great circle through $A$, $B$ meet $DEe$ in $E$, $e$, and let $DE$ be 
a quadrant. Thus 
${\stackrel{\frown}{DE}}$ represents $\gamma$ (\S 72). Also make 
${\stackrel{\frown}{EF}} = {\stackrel{\frown}{AB}}$
$=\beta\alpha^{-1}$ Then, evidently, 
$${\stackrel{\frown}{DF}}=\beta\alpha^{-1}\gamma$$
which gives the arcual representation required. 

Let $DF$ cut $Ee$ in $G$. Make $Ca = EG$, and join $D$, $a$, and $a$, $F$. 
Obviously, as $D$ is the pole of $Ee$, $Da$ is a quadrant ; and since 
$EG = Ca$, $Ga = EG$, a quadrant also. Hence $a$ is the pole of $DG$, 
and therefore the quaternion may be represented by the angle 
$DaF$. 

Make $Cb = Ca$, and draw the arcs $Pa\beta$, $Pb\alpha$ from $P$, the pole of 
$AB$. Comparing the triangles $Eb\alpha$ and $ea\beta$, 
we see that $E\alpha = e\beta$.
But, since $P$ is the pole of $AB$, $F\beta a$ is a right angle: and therefore 
as $Fa$ is a quadrant, so is $F\beta$. Thus $AB$ is the complement of $E\alpha$
or $\beta e$, and therefore 
$$\alpha\beta=2AB$$

Join $bA$. and produce it to $c$ so that $Ac = bA$; join $c$, $P$, cutting 
$AB$ in $o$. Also join $c$, $B$, and $B$, $a$. 

Since $P$ is the pole of $AB$, the angles at $o$ are right angles; 
and therefore, by the equal triangles $b\alpha A$, $coA$, we have 
$$\alpha A = Ao$$
But
$$\alpha\beta = 2AB$$
whence
$$oB=B\beta$$
and therefore the triangles $coB$ and $Ba\beta$ are equal, and $c$, $B$, $a$ 
lie on the same great circle. 

Produce $cA$ and $cB$ to meet in $H$ (on the opposite side of the 
sphere). $H$ and $c$ are diametrically opposite, and therefore $cP$, 
produced, passes through $H$. 

Now $Pa = Pb = PH$, for they differ from quadrants by the 
equal arcs $a\beta$, $b\alpha$, $oc$. Hence these arcs divide the 
triangle $Hab$ into three isosceles triangles. 

But
$$\angle PHb + \angle PHA = \angle aHb = \angle bca$$
Also
$$\angle Pab = \pi - \angle cab - \angle PaH$$
$$\angle Pba = \angle Pab = \pi - \angle cba - \angle PbH$$
Adding,
$$2\angle Pab = 2\pi - \angle cab - \angle cba - \angle bca$$
$$= \pi - (\textrm{spherical excess of }abc)$$
But, as $\angle Fa\beta$ and $\angle Dae$ are right angles, we have
$$
\textrm{angle of }\beta\alpha^{-1}\gamma =
\angle FaD = \beta ae = \angle Pab
$$
$$=\frac{\pi}{2} - \frac{1}{2}(\textrm{spherical excess of }abc)$$

[Numerous singular geometrical theorems, easily proved {\sl ab 
initio} by quaternions, follow from this: e.g. The arc $AB$, which 
bisects two sides of a spherical triangle $abc$, intersects the base at 
the distance of a quadrant from its middle point. All spherical 
triangles, with a common side, and having their other sides 
bisected by the same great circle (i.e. having their vertices in a 
small circle parallel to this great circle) have equal areas, \&c. ]

{\bf 118}. Let $\overline{Oa}=\alpha^{\prime}$, $\overline{Ob}=\beta^{\prime}$,
$\overline{Oc}=\gamma^{\prime}$, and we have 
$$
\begin{array}{rcl}
\left(\frac{\alpha^{\prime}}{\beta^{\prime}}\right)^{\frac{1}{2}}
\left(\frac{\beta^{\prime}}{\gamma^{\prime}}\right)^{\frac{1}{2}}
\left(\frac{\gamma^{\prime}}{\alpha^{\prime}}\right)^{\frac{1}{2}}&=&
{\stackrel{\frown}{Ca}}.{\stackrel{\frown}{cA}}.{\stackrel{\frown}{Bc}}\\
&=&{\stackrel{\frown}{Ca}}.{\stackrel{\frown}{BA}}\\
&=&{\stackrel{\frown}{EG}}.{\stackrel{\frown}{FE}}=
{\stackrel{\frown}{FG}}
\end{array}
$$

But $FG$ is the complement of $DF$. Hence the {\sl angle of the 
quaternion}
$$
\left(\frac{\alpha^{\prime}}{\beta^{\prime}}\right)^{\frac{1}{2}}
\left(\frac{\beta^{\prime}}{\gamma^{\prime}}\right)^{\frac{1}{2}}
\left(\frac{\gamma^{\prime}}{\alpha^{\prime}}\right)^{\frac{1}{2}}
$$
{\sl is half the spherical excess of the triangle whose angular points are 
at the extremities of the unit-vectors} $\alpha^{\prime}$, $\beta^{\prime}$, 
and $\gamma^{\prime}$.

[In seeking a purely quaternion proof of the preceding proposi 
tions, the student may commence by showing that for any three 
unit-vectors we have 
$$
\frac{\beta}{\alpha}\frac{\gamma}{\beta}\frac{\alpha}{\gamma}=
-(\beta\alpha^{-1}\gamma)^2
$$

The angle of the first of these quaternions can be easily assigned; 
and the equation shows how to find that of $\beta\alpha^{-1}\gamma$. 

Another easy method is to commence afresh by forming from 
the vectors of the corners of a spherical triangle three new vectors 
thus: 
$$
\alpha^{\prime}=\left(\frac{\beta+\gamma}{\alpha}^{2}\right)^2 .\;
\alpha,\;\;\;\;\;
\textrm{\&c.}
$$

Then the angle between the planes of $\alpha$, $\beta^{\prime}$ and
$\gamma^{\prime}$, $\alpha$; or of $\beta$, $\gamma^{\prime}$ 
and $\alpha^{\prime}$,
$\beta$; or of $\gamma$, $\alpha^{\prime}$ and $\beta^{\prime}$, $\gamma$
is obviously the spherical excess. 

But a still simpler method of proof is easily derived from the 
composition of rotations.] 

{\bf 119}. It may be well to introduce here, though it belongs 
rather to Kinematics than to Geometry, the interpretation of the 
operator 
$$q(\;\;\;)q^{-1}$$

By a rotation, about the axis of $q$, through double the angle of $q$, 
the quaternion $r$ becomes the quaternion $qrq^{-1}$ . Its tensor and 
angle remain unchanged, its plane or axis alone varies. 

\begin{center}
\includegraphics{ps/quat19.ps}
\end{center}
\vskip 0.5cm

A glance at the figure is sufficient for 
the proof, if we note that of course 
$T . qrq^{-1} = Tr$, and therefore that we need 
consider the {\sl versor} parts only. Let $Q$ 
be the pole of $q$. 
$$
{\stackrel{\frown}{AB}}=q,\;\;\;\;
{\stackrel{\frown}{AB^{-1}}}=q^{-1},\;\;\;\;
{\stackrel{\frown}{B^{\prime}C^{\prime}}}=r
$$
Join $C^{\prime}A$, and make 
${\stackrel{\frown}{AC}}={\stackrel{\frown}{C^{\prime}A}}$. Join $CB$.

Then ${\stackrel{\frown}{CB}}$ is $qrq^{-1}$, 
its arc $CB$ is evidently equal in length to that 
of $r$, $B^{\prime}C^{\prime}$; and its plane (making the same angle with 
$B^{\prime}B$ that that of 
$B^{\prime}C^{\prime}$ does) has evidently been made to revolve about $Q$, the 
pole of $q$, through double the angle of $q$. 

It is obvious, from the nature of the above proof, that this 
operation is distributive; i.e. that 
$$q(r+s)q^{-1}=qrq^{-1}+qsq^{-1}$$

If $r$ be a vector, $=\rho$, then $q\rho q^{-1}$ 
(which is also a vector) is the 
result of a rotation through double the angle of $q$ about the axis 
of $q$. Hence, as Hamilton has expressed it, if $B$ represent a rigid 
system, or assemblage of vectors, 
$$qBq^{-1}$$
is its new position after rotating through double the angle of $q$ 
about the axis of $q$. 

{\bf 120}. To compound such rotations, we have 
$$r . qBq^{-1} . r^{-1} = rq . B . (rq)^{-1}$$

To cause rotation through an angle $t$-fold the double of the angle 
of $q$ we write 
$$q^{t}Bq^{-t}$$

To reverse the direction of this rotation write
$$q^{-t}Bq^{t}$$

To {\sl translate} the body $B$ without rotation, each point of it moving 
through the vector $\alpha$, we write $\alpha + B$. 

To produce rotation of the translated body about the same axis, 
and through the same angle, as before, 
$$q(\alpha+B)q^{-1}$$

Had we rotated first, and then translated, we should have had 
$$\alpha+qBq^{-1}$$

From the point of view of those who do not believe in the 
Moon s rotation, the former of these expressions ought to be 
$$q\alpha q^{-1}+B$$
instead of 
$$q\alpha q^{-1}+qBq^{-1}$$
But to such men quaternions are unintelligible. 

{\bf 121}. The operator above explained finds, of course, some 
of its most direct applications in the ordinary questions of 
Astronomy, connected with the apparent diurnal rotation of the 
stars. If $\lambda$ be a unit-vector parallel to the polar axis, and $h$ the 
hour angle from the meridian, the operator is 
$$
\left(\cos\frac{h}{2}-\lambda\sin\frac{h}{2}\right)
\left(\;\;\;\;\right)
\left(\cos\frac{h}{2}+\lambda\sin\frac{h}{2}\right)
$$
or
$$L^{-1}\left(\;\;\;\;\right)L$$

the inverse going first, because the {\sl apparent} rotation is negative 
(clockwise). 

If the upward line be $i$, and the southward $j$, we have 
$$\lambda = i\sin l-j\cos l$$
where $l$ is the latitude of the observer. The meridian equatorial 
unit vector is 
$$\mu = i\cos l+j\sin l$$
and $\lambda$, $\mu$, $k$ of course form a rectangular unit system. 

The meridian unit-vector of a heavenly body is 
$$\delta=i\cos(l-d)+j\sin(l-d)$$
$$=\lambda\sin d+\mu\cos d$$
where $d$ is its declination. 

Hence when its hour-angle is $h$, its vector is 
$$\delta^{\prime}=L^{-1}\delta L$$

The vertical plane containing it intersects the horizon in 
$$iVi\delta^{\prime}=jSj\delta^{\prime}+kSk\delta^{\prime}$$
so that 
$$\tan(azimuth)=\frac{Sk\delta^{\prime}}{Sj\delta^{\prime}}\eqno{(1)}$$

[This may also be obtained directly from the last formula (1) 
of \S 114.] 

To find its Amplitude, i.e. its azimuth at rising or setting, 
the hour-angle must be obtained from the condition 
$$Si\delta^{\prime}=0\eqno{(2)}$$

These relations, with others immediately deducible from them, 
enable us (at once and for ever) to dispense with the hideous 
formulae of Spherical Trigonometry. 

{\bf 122}. To show how readily they can be applied, let us 
translate the expressions above into the ordinary notation. This 
is effected at once by means of the expressions for $\lambda$, $\mu$, $L$,
and $\delta$ above, which give by inspection 
$$\delta^{\prime}=\lambda\sin d+(\mu\cos h-k\sin h)\cos d$$
= x sin d + (fjb cos h k sin h) cos d, 
and we have from (1) and (2) of last section respectively 
$$
\tan(azimuth)=
\frac{\sin h\cos d}{\cos l\sin d-\sin l\cos d\cos h}\eqno{(1)}
$$
$$
\cos h+\tan l \tan d=0\eqno{(2)}
$$

In Capt. Weir s ingenious {\sl Azimuth Diagram}, these equations 
are represented graphically by the rectangular coordinates of a 
system of confocal conics: viz. 
$$
\left.
\begin{array}{c}
x = \sin h \sec l \\
y = \cos h \tan l
\end{array}
\right\}\eqno{(3)}
$$

The ellipses of this system depend upon $l$ alone, the hyperbolas 
upon $h$. Since (1) can, by means of (3), be written as 
$$\tan(azimuth)=\frac{x}{\tan d-y}$$
we see that the azimuth can be constructed at once by joining 
with the point $0$, $-\tan d$, the intersection of the proper ellipse and 
hyperbola. 

Equation (2) puts these expressions for the coordinates in the 
form 
$$
\left.
\begin{array}{c}
x=\sec l\sqrt{1-\tan^{2} l\tan^{2} d}\\
y=-\tan^{2} l \tan d
\end{array}
\right\}
$$

The elimination of $d$ gives the ellipse as before, but that of $l$
gives, instead of the hyperbolas, the circles 
$$x^{2}+y^{2}-y(\tan d-\cot d)=1$$

The radius is 
$$\frac{1}{2}(\tan d+ \cot d)$$
and the coordinates of the centre are 
$$0,\;\;\;\frac{1}{2}(\tan d- \cot d)$$

123. A scalar equation in $\rho$, the vector of an undetermined 
point, is generally the equation of a {\sl surface}; since we may use 
in it the expression 
$$\rho=x\alpha$$
where $x$ is an unknown scalar, and $\alpha$ any assumed unit-vector. 
The result is an equation to determine $x$. Thus one or more 
points are found on the vector $x\alpha$, whose coordinates satisfy the 
equation; and the locus is a surface whose degree is determined 
by that of the equation which gives the values of $x$. 

But a {\sl vector} equation in $\rho$, as we have seen, generally leads to 
three scalar equations, from which the three rectangular or other 
components of the sought vector are to be derived. Such a vector 
equation, then, usually belongs to a definite number of {\sl points} in 
space. But in certain cases these may form a {\sl line}, and even a 
{\sl surface}, the vector equation losing as it were one or two of the 
three scalar equations to which it is usually equivalent. 

Thus while the equation 
$$\alpha\rho=\beta$$
gives at once 
$$\rho=\alpha^{-1}\beta$$
which is the vector of a definite point, since by making $\rho$ a {\sl vector}
we have evidently assumed 
$$S\alpha\beta=0$$
the closely allied equation
$$V\alpha\rho=\beta$$
is easily seen to involve
$$S\alpha\beta=0$$
and to be satisfied by
$$\rho=\alpha^{-1}\beta+x\alpha$$
whatever be $x$. Hence the vector of any point whatever in the 
line drawn parallel to $\alpha$ from the extremity of $\alpha^{-1}\beta$
satisfies the given equation. [The difference between the results depends 
upon the fact that $S\alpha\rho$ is indeterminate in the second form, but 
definite (= 0) in the first.] 

{\bf 124}. Again,
$$V\alpha\rho . V\rho\beta=(V\alpha\beta)^{2}$$
is equivalent to but two scalar equations. For it shows that $V\alpha\rho$
and $V\beta\rho$ are parallel, i.e. $\rho$ 
lies in the same plane as $\alpha$ and $\beta$, and 
can therefore be written (\S 24) 
$$\rho=x\alpha+y\beta$$
where $x$ and $y$ are scalars as yet undetermined. 

We have now 
$$V\alpha\rho=yV\alpha\beta$$
$$V\rho\beta=xV\alpha\beta$$
which, by the given equation, lead to 
$$xy=1,\;\;\;\textrm{or}\;\;\;y=\frac{1}{x}$$
or finally
$$\rho=x\alpha+\frac{1}{x}\beta$$
which (\S 40) is the equation of a hyperbola whose asymptotes are 
in the directions of $\alpha$ and $\beta$.

{\bf 125}. Again, the equation 
$$V . V\alpha\beta V\alpha\rho=0$$
though apparently equivalent to three scalar equations, is really 
equivalent to one only. In fact we see by \S 91 that it may be 
written 
$$-\alpha S.\alpha\beta\rho=0$$
whence, if $\alpha$ be not zero, we have 
$$S . \alpha\beta\rho=0$$
and thus (\S 101) the only condition is that $\rho$ is coplanar with 
$\alpha$, $\beta$.
Hence the equation represents the plane in which 
$\alpha$ and $\beta$ lie. 

{\bf 126}. Some very curious results are obtained when we extend 
these processes of interpretation to functions of a {\sl quaternion}
$$q=w+\rho$$
instead of functions of a mere {\sl vector} $\rho$. 

A scalar equation containing such a quaternion, along with 
quaternion constants, gives, as in last section, the equation of a 
surface, if we assign a definite value to $w$. Hence for successive 
values of $w$, we have successive surfaces belonging to a system ; 
and thus when $w$ is indeterminate the equation represents not a 
{\sl surface}, as before, but a {\sl volume}, 
in the sense that the vector of any 
point within that volume satisfies the equation. 

Thus the equation 
$$(Tq)^2=a^2$$
or
$$w^2-\rho^2=a^2$$
or
$$(TP)^2=a^2-w^2$$
represents, for any assigned value of $w$, not greater than $a$, a sphere 
whose radius is $\sqrt{a^2-w^2}$. Hence the equation is satisfied by the 
vector of any point whatever in the {\sl volume} of a sphere of radius $a$, 
whose centre is origin. 

Again, by the same kind of investigation, 
$$(T (q-\beta))^2=a^2$$
where $q=w+\rho$, is easily seen to represent the volume of a sphere 
of radius $a$ described about the extremity of $\beta$ as centre. 

Also $S(q^2) = -a^2$ is the equation of infinite space less the space 
contained in a sphere of radius $a$ about the origin. 

Similar consequences as to the interpretation of vector 
equations in quaternions may be readily deduced by the reader. 

{\bf 127}. The following transformation is enuntiated without proof 
by Hamilton ({\sl Lectures}, p. 587, and {\sl Elements}, p. 299). 
$$r^{-1}(r^2q^2)^{\frac{1}{2}}q^{-1}=U(rq+KrKq)$$
To prove it, let
$$r^{-1}(r^2q^2)^{\frac{1}{2}}q^{-1}=t$$
then 
$$Tt=1$$
and therefore
$$Kt=t^{-1}$$
But
$$(r^2q^2)^{\frac{1}{2}}=rtq$$
or
$$r^2q^2=rtqrtq$$
or
$$rq=tqrt$$
Hence
$$KqKr=t^{-1}KrKqt^{-1}$$
or
$$KrKq=tKqKrt$$
Thus we have
$$U(rq\pm KrKq)=tU(qr\pm KqKr)t$$
or, if we put
$$s=U(qr\pm KqKr)$$
$$Ks=\pm tst$$
Hence
$$sKs=(Ts)^2=1=\pm stst$$
which, if we take the positive sign, requires 
$$st=\pm 1$$
or
$$t=\pm s^{-1}=\pm UKs$$
which is the required transformation. 

[It is to be noticed that there are other results which might 
have been arrived at by using the negative sign above ; some 
involving an arbitrary unit-vector, others involving the imaginary 
of ordinary algebra.] 

{\bf 128}. As a final example, we take a transformation of Hamilton's, 
of great importance in the theory of surfaces of the second order. 

Transform the expression 
$$(S\alpha\rho)^2+(S\beta\rho)^2+(S\gamma\rho)^2$$
in which $\alpha$, $\beta$, $\gamma$ 
are any three mutually rectangular vectors, into the form
$$\left(\frac{T(\iota\rho+\rho\kappa)}{\kappa^2-\iota^2}\right)^2$$
which involves only two vector-constants, $\iota$, $\kappa$.

[The student should remark here that $\iota$, $\kappa$, two undetermined 
vectors, involve six disposable constants : and that $\alpha$, $\beta$,
$\gamma$, being a {\sl rectangular} system, involve also only six constants.] 
$$
\begin{array}{rcl}
\{T(\iota\rho+\rho\kappa)\}^2 
&=& (\iota\rho+\rho\kappa)(\rho\iota+\kappa\rho)\;\;\;\;(\S\S 52,55)\\
&=& (\iota^2+\kappa^2)\rho^2+(\iota\rho\kappa\rho+\rho\kappa\rho\iota)\\
&=& (\iota^2+\kappa^2)\rho^2+2S.\iota\rho\kappa\rho\\
&=& (\iota-\kappa)^2\rho^2+4S\iota\rho S\kappa\rho
\end{array}
$$
Hence
$$
(S\alpha\rho)^2+(S\beta\rho)^2+(S\gamma\rho)^2=
\frac{(\iota-\kappa)^2}{(\kappa^2-\iota^2)^2}\rho^2+
4\frac{S\iota\rho S\kappa\rho}{(\kappa^2-\iota^2)^2}
$$
But
$$
\alpha^{-2}(S\alpha\rho)^2+
\beta^{-2}(S\beta\rho)^2+
\gamma^{-2}(S\gamma\rho)^2=
\rho^2\;\;\;\;(\S\S 25,73).
$$
Multiply by $\beta^2$ and subtract, we get 
$$
\left(1-\frac{\beta^2}{\alpha^2}\right)(S\alpha\rho)^2-
\left(\frac{\beta^2}{\gamma^2}-1\right)(S\gamma\rho)^2=
\left\{\frac{(\iota-\kappa)^2}{(\kappa^2-\iota^2)^2}-\beta^2\right\}\rho^2+
4\frac{S\iota\rho S\kappa\rho}{(\kappa^2-\iota^2)^2}
$$

The left side breaks up into two real factors if $\beta^2$ be intermediate 
in value to $\alpha^2$ and $\gamma^2$: 
and that the right side may do so the term 
in $\rho^2$ must vanish. This condition gives 
$$\beta^2=\frac{(\iota-\kappa)^2}{(\kappa^2-\iota^2)^2}$$
and the identity becomes 
$$
S\left\{
\alpha\sqrt{\left(1-\frac{\beta^2}{\alpha^2}\right)}+
\gamma\sqrt{\left(\frac{\beta^2}{\gamma^2}-1\right)}
\right\}
\rho S\left\{
\alpha\sqrt{\left(1-\frac{\beta^2}{\alpha^2}\right)}-
\gamma\sqrt{\left(\frac{\beta^2}{\gamma^2}-1\right)}
\right\}\rho=
4\frac{S\iota\rho S\kappa\rho}{(\kappa^2-\iota^2)^2}
$$
Hence we must have 
$$
\frac{2\iota}{\kappa^2-\iota^2}=
p\left\{
\alpha\sqrt{\left(1-\frac{\beta^2}{\alpha^2}\right)}+
\gamma\sqrt{\left(\frac{\beta^2}{\gamma^2}-1\right)}
\right\}
$$
$$
\frac{2\kappa}{\kappa^2-\iota^2}=
\frac{1}{p}\left\{
\alpha\sqrt{\left(1-\frac{\beta^2}{\alpha^2}\right)}-
\gamma\sqrt{\left(\frac{\beta^2}{\gamma^2}-1\right)}
\right\}
$$
where $\rho$ is an undetermined scalar. 

To determine $\rho$, substitute in the expression for $\beta^2$, and we find 
$$
\begin{array}{rcl}
4\beta^2=\frac{4(\iota-\kappa)^2}{(\kappa^2-\iota^2)^2}
&=&\left(p-\frac{1}{p}\right)^2(\alpha^2-\beta^2)+
\left(p+\frac{1}{p}\right)^2(\beta^2-\gamma^2)\\
&=&\left(p^2+\frac{1}{p^2}\right)(\alpha^2-\gamma^2)-
2(\alpha^2+\gamma^2)+4\beta^2
\end{array}
$$

Thus the transformation succeeds if 
$$p^2+\frac{1}{p^2}=\frac{2(\alpha^2+\gamma^2)}{\alpha^2-\gamma^2}$$
which gives
$$p+\frac{1}{p}=\pm 2\sqrt{\frac{\alpha^2}{\alpha^2-\gamma^2}}$$
$$p-\frac{1}{p}=\pm 2\sqrt{\frac{\gamma^2}{\alpha^2-\gamma^2}}$$
Hence 
$$
\frac{4(\kappa^2-\iota^2)}{(\kappa^2-\iota^2)^2}=
\left(\frac{1}{p^2}-p^2\right)(\alpha^2-\gamma^2)=
\pm 4\sqrt{\alpha^2\gamma^2}
$$
$$
(\kappa^2-\iota^2)^{-1}=\pm T\alpha T\gamma$$

Again
$$
p=\frac{T\alpha +T\gamma}{\sqrt{\gamma^2-\alpha^2}},\;\;\;\;
\frac{1}{p}=\frac{T\alpha -T\gamma}{\sqrt{\gamma^2-\alpha^2}}
$$
and therefore 
$$
2\iota=\frac{T\alpha +T\gamma}{T\alpha T\gamma}
\left(
\sqrt{\frac{\beta^2-\alpha^2}{\gamma^2-\alpha^2}}U\alpha+
\sqrt{\frac{\gamma^2-\beta^2}{\gamma^2-\alpha^2}}U\gamma
\right)
$$
$$
2\kappa=\frac{T\alpha -T\gamma}{T\alpha T\gamma}
\left(
\sqrt{\frac{\beta^2-\alpha^2}{\gamma^2-\alpha^2}}U\alpha-
\sqrt{\frac{\gamma^2-\beta^2}{\gamma^2-\alpha^2}}U\gamma
\right)
$$
Thus we have proved the possibility of the transformation, and 
determined the transforming vectors $\iota$, $\kappa$.

{\bf 129}. By differentiating the equation 
$$
(S\alpha\rho)^2+(S\beta\rho)^2+(S\gamma\rho)^2=
\left(\frac{T(\iota\rho+\rho\kappa)}{(\kappa^2-\iota^2)}\right)^2
$$
we obtain, as will be seen in Chapter IV, the following, 
$$
S\alpha\rho S\alpha\rho^{\prime}+
S\beta\rho S\beta\rho^{\prime}+
S\gamma\rho S\gamma\rho^{\prime}=
\frac{S.(\iota\rho+\rho\kappa)(\kappa\rho^{\prime}+\rho^{\prime}\iota)}
{(\kappa^2-\iota^2)^2}
$$
where $\rho$ also may be any vector whatever. 

This is another very important formula of transformation ; and 
it will be a good exercise for the student to prove its truth by 
processes analogous to those in last section. We may merely 
observe, what indeed is obvious, that by putting $\rho^{\prime}=\rho$ it becomes 
the formula of last section. And we see that we may write, with 
the recent values of $\iota$ and $\kappa$ in terms of 
$\alpha$, $\beta$, $\gamma$, the identity 
$$
\begin{array}{rcl}
\alpha S\alpha\rho+\beta S\beta\rho+\gamma S\gamma\rho
&=&\displaystyle 
\frac{(\iota^2+\kappa^2)\rho+2V.\iota\rho\kappa}{(\kappa^2-\iota^2)^2}\\
&&\\
&=&\displaystyle 
\frac{(\iota-\kappa)^2\rho+2(\iota S\kappa\rho+\kappa S\iota\rho)}
{(\kappa^2-\iota^2)^2}
\end{array}
$$

{\bf 130}. In various quaternion investigations, especially in such 
as involve {\sl imaginary} intersections of curves and surfaces, the old 
imaginary of algebra of course appears. But it is to be particularly 
noticed that this expression is analogous to a scalar and not to a 
vector, and that like real scalars it is commutative in 
multiplication with all other factors. Thus it appears, by the same proof 
as in algebra, that any quaternion expression which contains this 
imaginary can always be broken up into the sum of two parts, one 
real, the other multiplied by the first power of $\sqrt{-1}$. Such an 
expression, viz. 
$$q=q^{\prime}+\sqrt{-1}q^{\prime\prime}$$
where $q^{\prime}$ and $q^{\prime\prime}$ are real quaternions, 
is called by Hamilton a 
BIQUATERNION. [The student should be warned that the term 
Biquaternion has since been employed by other writers in the 
sense sometimes of a ``set'' of 8 elements, analogous to the 
Quaternion 4 ; sometimes for an expression $q^{\prime} + \theta q^{\prime\prime}$ 
where $\theta$ is not 
the algebraic imaginary. By them Hamilton s Biquaternion is 
called simply a quaternion with non-real constituents.] Some 
little care is requisite in the management of these expressions, but 
there is no new difficulty. The points to be observed are: first, 
that any biquaternion can be divided into a real and an imaginary 
part, the latter being the product of $\sqrt{-1}$ by a real quaternion; 
second, that this $\sqrt{-1}$ is commutative with all other quantities in 
multiplication; third, that if two biquaternions be equal, as 
$$q^{\prime}+\sqrt{-1}\;q^{\prime\prime}=
r^{\prime}+\sqrt{-1}\;r^{\prime\prime}$$
we have, as in algebra, 
$$q^{\prime}=r^{\prime},\;\;\;\;q^{\prime\prime}=r^{\prime\prime}$$
so that an equation between biquaternions involves in general 
{\sl eight} equations between scalars. Compare \S 80. 

{\bf 131}. We have obviously, since $\sqrt{-1}$ is a scalar, 
$$S(q^{\prime}+\sqrt{-1}\;q^{\prime\prime})=
Sq^{\prime}+\sqrt{-1}\;Sq^{\prime\prime}$$
$$V(q^{\prime}+\sqrt{-1}\;q^{\prime\prime})=
Vq^{\prime}+\sqrt{-1}\;Vq^{\prime\prime}$$
Hence (\S 103) 
$$\{T(q^{\prime}+\sqrt{-1}\;q^{\prime\prime})\}^2$$
$$
=(Sq^{\prime}+\sqrt{-1}\;Sq^{\prime\prime}+
Vq^{\prime}+\sqrt{-1}\;Vq^{\prime\prime})
(Sq^{\prime}+\sqrt{-1}\;Sq^{\prime\prime}-Vq^{\prime}-
\sqrt{-1}\;Vq^{\prime\prime})
$$
$$=(Sq^{\prime}+\sqrt{-1}\;Sq^{\prime\prime})^2-
(Vq^{\prime}+\sqrt{-1}\;Vq^{\prime\prime})^2$$
$$=(Tq^{\prime})^2-(Tq^{\prime\prime})^2+
2\sqrt{-1}\;S.q^{\prime}Kq^{\prime\prime}$$

The only remark which need be made on such formulae is this, that 
{\sl the tensor of a biquaternion may vanish while both of the component 
quaternions are finite}. 

Thus, if 
$$Tq^{\prime}=Tq^{\prime\prime}$$
and
$$S.q^{\prime}Kq^{\prime\prime}=0$$
the above formula gives 
$$T(q^{\prime}+\sqrt{-1}\;q^{\prime\prime})=0$$
The condition 
$$S.q^{\prime}Kq^{\prime\prime}=0$$
may be written 
$$
Kq^{\prime\prime}=q^{'-1}\alpha,\;\;\;\textrm{ or }\;\;\;
q^{\prime\prime}=-\alpha Kq^{'-1}=-\frac{\alpha q^{\prime}}{(Tq^{\prime})^2}
$$
where $\alpha$ is any vector whatever. 

Hence 
$$Tq^{\prime}=Tq^{\prime\prime}=TKq^{\prime\prime}=
\frac{T\alpha}{Tq^{\prime\prime}}$$
and therefore 
$$
Tq^{\prime}(Uq^{\prime}-\sqrt{-1}\;U\alpha . Uq^{\prime})=
(1-\sqrt{-1}\;U\alpha)q^{\prime}
$$
is the general form of a biquaternion whose tensor is zero. 

{\bf 132}. More generally we have, $q$, $r$, $q^{\prime}$, $r^{\prime}$ 
being any four real and non-evanescent quaternions, 
$$
(q+\sqrt{-1}\;q^{\prime})(r+\sqrt{-1}\;r^{\prime})=
qr-q^{\prime}r^{\prime}+\sqrt{-1}\;(qr^{\prime}+q^{\prime}r)
$$
That this product may vanish we must have 
$$qr=q^{\prime}r^{\prime}$$
and
$$qr^{\prime}=-q^{\prime}r$$
Eliminating $r^{\prime}$ we have
$$qq^{'-1}qr=-q^{\prime}r$$
which gives 
$$(q^{'-1}q)^2=-1$$
i.e.
$$q=q^{\prime}\alpha$$
where $\alpha$ is some unit-vector. 

And the two equations now agree in giving 
$$-r=\alpha r^{\prime}$$
so that we have the biquaternion factors in the form 
$$q^{\prime}(\alpha +\sqrt{-1})\;\;\;\textrm{ and }
\;\;\;-(\alpha-\sqrt{-1})r^{\prime}$$
and their product is 
$$-q^{\prime}(\alpha +\sqrt{-1})(\alpha -\sqrt{-1})r^{\prime}$$
which, of course, vanishes. 

[A somewhat simpler investigation of the same proposition 
may be obtained by writing the biquaternions as 
$$
q^{\prime}(q^{'-1}q+\sqrt{-1})\;\;\;\textrm{ and }\;\;\;
(rr^{'-1}+\sqrt{-1})r^{\prime}
$$
or
$$ 
q^{\prime}(q^{\prime\prime}+\sqrt{-1})\;\;\;\textrm{ and }\;\;\;
(r^{\prime\prime}+\sqrt{-1})r^{\prime}
$$
and showing that 
$$q^{\prime\prime}=
-r^{\prime\prime}=\alpha \;\;\;\textrm{ where }\;T\alpha=1]$$

From this it appears that if the product of two {\sl bivectors}
$$
\rho+\sigma\sqrt{-1}\;\;\;\textrm{ and }\;\;\;
\rho^{\prime}+\sigma^{\prime}\sqrt{-1}
$$
is zero, we must have 
$$\sigma^{-1}\rho=-\rho^{\prime}\sigma^{'-1}=U\alpha$$
where $\alpha$ may be any vector whatever. But this result is still more 
easily obtained by means of a direct process. 

{\bf 133}. It may be well to observe here (as we intend to avail our 
selves of them in the succeeding Chapters) that certain abbreviated 
forms of expression may be used when they are not liable to confuse, 
or lead to error. Thus we may write 
$$T^2q\;\;\;\textrm{for}\;\;\;(Tq)^2$$
just as we write
$$\cos^2\theta\;\;\;\textrm{for}\;\;\;(\cos\theta)^2$$
although the true meanings of these expressions are 
$$T(Tq)\;\;\;\textrm{and}\;\;\;\cos(\cos\theta)$$

The former is justifiable, as $T(Tq) = Tq$, and therefore $T^2q$ is not 
required to signify the second tensor (or tensor of the tensor) of $q$. 
But the trigonometrical usage is defensible only on the score of 
convenience, and is habitually violated by the employment of 
$cos^{-1}x$ in its natural and proper sense. 
Similarly we may write 
$$S^2q\;\;\;\textrm{for}\;\;\;(Sq)^2,\;\;\;\textrm{\&c.}$$
but it may be advisable not to use 
$$Sq^2$$
as the equivalent of either of those just written; inasmuch as it 
might be confounded with the (generally) different quantity 
$$S.q^2\;\;\;\textrm{or}\;\;\;S(q^2)$$
although this is rarely written without the point or the brackets. 

The question of the use of points or brackets is one on which 
no very definite rules can be laid down. A beginner ought to use 
them freely, and he will soon learn by trial which of them are 
absolutely necessary to prevent ambiguity. 

In the present work this course has been adopted:-- the 
earlier examples in each part of the subject being treated with 
a free use of points and brackets, while in the later examples 
superfluous marks of the kind are gradually got rid of. 

It may be well to indicate some general principles which 
regulate the omission of these marks. Thus in $S.\alpha\beta$ or
$V.\alpha\beta$
the point is obviously unnecessary:-- because $S\alpha=0$, and 
$V\alpha=\alpha$
so that the $S$ would annihilate the term if it applied to $\alpha$ alone, 
while in the same case the $V$ would be superfluous. But in $S.qr$
and $V.qr$, the point (or an equivalent) is indispensable, for $Sq.r$, 
and $Vq.r$ are usually quite different from the first written 
quantities. In the case of $K$, and of $d$ (used for scalar differentiation),
the {\sl omission} of the point indicates that the operator acts 
{\sl only} on the nearest factor:-- thus 
$$Kqr=(Kq)r=Kq.r,\;\;\;dqr=(dq)r=dq.r$$
Kqr = (Kq) r = Kq . r, dqr = (dq) r=dq.r; 
while, if its action extend farther, we write 
$$K.qr=K(qr),\;\;\;d.qr=d(qr)\;\;\;\textrm{\&c.}$$

In more complex cases we must be ruled by the general 
principle of dropping nothing which is essential. Thus, for 
instance 
$$V(pK(dq)V(Vq.r))$$
may be written without ambiguity as 
$$V(pK(dq)V(Vq.r))$$
but nothing more can be dropped without altering its value. 

Another peculiarity of notation, which will occasionally be 
required, shows {\sl which portions} of a complex product are affected 
by an operator. Thus we write 
$$\nabla S\sigma\tau$$
if $\nabla$ operates on $\sigma$ and also on $\tau$, but 
$$\nabla_1S\sigma\tau_1$$
if it operates on $\tau$ alone. See, in this connection, the last Example 
at the end of Chap. IV. below. 

{\bf 134}. The beginner may expect to be at first a little puzzled 
with this aspect of the notation; but, as he learns more of the 
subject, he will soon see clearly the distinction between such an 
expression as 
$$S.V\alpha\beta V\beta\gamma$$
where we may omit at pleasure either the point or the first V 
without altering the value, and the very different one 
$$S\alpha\beta .V\beta\gamma$$
which admits of no such changes, without alteration of its value. 

All these simplifications of notation are, in fact, merely examples 
of the transformations of quaternion expressions to which part of 
this Chapter has been devoted. Thus, to take a very simple ex 
ample, we easily see that 
$$
\begin{array}{rcl}
S.V\alpha\beta V\beta\gamma
&=&SV\alpha\beta V\beta\gamma
=S.\alpha\beta V\beta\gamma
=S\alpha V.\beta V\beta\gamma
=-S\alpha V.(V\beta\gamma)\beta\\
&=&S\alpha V.(V\gamma\beta)\beta
=S.\alpha V(\gamma\beta)\beta
=S.V(\gamma\beta)\beta\alpha
=SV\gamma\beta V\beta\alpha\\
&=&S.\gamma\beta V\beta\alpha
=S.K(\beta\gamma)V\beta\alpha
=S.\beta\gamma KV\beta\alpha
=-S.\beta\gamma V\beta\alpha\\
&=&S.V\gamma\beta V\beta\alpha,\textrm{\&c., \&c.}
\end{array}
$$

The above group does not nearly exhaust the list of even the simpler 
ways of expressing the given quantity. We recommend it to the 
careful study of the reader. He will find it advisable, at first, to 
use stops and brackets pretty freely; but will gradually learn to 
dispense with those which are not absolutely necessary to prevent 
ambiguity. 

There is, however, one additional point of notation to which 
the reader s attention should be most carefully directed. A very 
simple instance will suffice. Take the expressions 
$$
\frac{\beta}{\gamma}.\frac{\gamma}{\alpha}\;\;\;\;\textrm{and}\;\;\;\;
\frac{\beta\gamma}{\gamma\alpha}
$$

The first of these is 
$$\beta\gamma^{-1}.\gamma\alpha^{-1}=\beta\alpha^{-1}$$
and presents no difficulty. But the second, though at first sight 
it closely resembles the first, is in general totally different in 
value, being in fact equal to 
$$\beta\gamma\alpha^{-1}\gamma^{-1}$$

For the denominator must be treated as {\sl one quaternion}. If, 
then, we write 
$$\frac{\beta\gamma}{\gamma\alpha}=q$$
we have 
$$\beta\gamma=q\gamma\alpha$$
so that, as stated above, 
$$q=\beta\gamma\alpha^{-1}\gamma^{-1}$$
We see therefore that 
$$
\frac{\beta}{\gamma}.\frac{\gamma}{\alpha}=
\frac{\beta}{\alpha}=
\frac{\beta\gamma}{\alpha\gamma};\;\;\;\textrm{but {\sl not}}\;\;=
\frac{\beta\gamma}{\gamma\alpha}
$$

\section{Examples to Chapter 3}

{\bf 1}. Investigate, by quaternions, the requisite formulae for 
changing from any one set of coordinate axes to another ; and 
derive from your general result, and also from special investiga 
tions, the usual expressions for the following cases: 

\begin{itemize}
\item[(a)] Rectangular axes turned about z through any angle. 

\item[(b)] Rectangular axes turned into any new position by rota 
tion about a line equally inclined to the three. 

\item[(c)] Rectangular turned to oblique, one of the new axes 
lying in each of the former coordinate planes. 
\end{itemize}

{\bf 2}. Point out the distinction between 
$$
\left(\frac{\alpha+\beta}{\alpha}\right)^2\;\;\;\textrm{and}\;\;\;
\frac{(\alpha+\beta)^2}{\alpha^2}
$$
and find the value of their difference. 

If
$$
T\beta/\alpha=1\;\;\;\textrm{and}\;\;\;
U\frac{\alpha+\beta}{\alpha}=\left(\frac{\beta}{\alpha}\right)^{\frac{1}{2}}
$$

Show also that 
$$
\frac{\alpha+\beta}{\alpha-\beta}=
\frac{V\alpha\beta}{1+S\alpha\beta^{\prime}}
$$
and 
$$
\frac{\alpha-\beta}{\alpha+\beta}=
-\frac{V\alpha\beta}{1-S\alpha\beta^{\prime}}
$$
provided $\alpha$ and $\beta$ be unit-vectors. If these conditions are not 
fulfilled, what are the true values ? 

{\bf 3}. Show that, whatever quaternion $r$ may be, the expression 
$$\alpha r+r\beta$$
in which $\alpha$ and $\beta$ are any two unit- vectors, is reducible to the 
form 
$$l(\alpha+\beta)+m(\alpha\beta-1)$$
where $l$ and $m$ are scalars. 

{\bf 4}. If $Tp=T\alpha=T\beta=1$, and $S.\alpha\beta\rho=0$
show by direct transformations that 
$$S.U(\rho-\alpha)U(\rho-\beta)=\pm\sqrt{\frac{1}{2}(1-S\alpha\beta)}$$
Interpret this theorem geometrically. 

{\bf 5}. If $S\alpha\beta=0$, $T\alpha=T\beta=1$, show that
$$
(1+\alpha^{m})\beta=
2\cos\frac{m\pi}{4}\alpha^{\frac{m}{2}}\beta=
2S\alpha^{\frac{m}{2}}.\alpha^{\frac{m}{2}}\beta
$$

{\bf 6}. Put in its simplest form the equation 
$$
\rho S.V\alpha\beta V\beta\gamma V\gamma\alpha=
aV.V\gamma\alpha V\alpha\beta+
bV.V\alpha\beta V\beta\gamma+
cV.V\beta\gamma V\gamma\alpha
$$
and show that 
$$a=S.\beta\gamma\rho,\;\;\;\textrm{\&c.}$$

{\bf 7}. Show that any quaternion may in general, in one way only, 
be expressed as a homogeneous linear function of four given 
quaternions. Point out the nature of the exceptional cases. Also 
find the simplest form in which any quaternion may generally be 
expressed in terms of two given quaternions. 

{\bf 8}. Prove the following theorems, and exhibit them as properties 
of determinants : 

\begin{itemize}
\item[(a)] $S.(\alpha+\beta)(\beta+\gamma)(\gamma+\alpha)=
2S.\alpha\beta\gamma$
\item[(b)]$S.V\alpha\beta V\beta\gamma V\gamma\alpha=
-(S.\alpha\beta\gamma)^2$
\item[(c)]$S.V(\alpha+\beta)(\beta+\gamma)V(\beta+\gamma)(\gamma+\alpha)
V(\gamma+\alpha)(\alpha+\beta)=-4(S.\alpha\beta\gamma)^2$
\item[(d)]$S.V(V\alpha\beta V\beta\gamma)V(V\beta\gamma V\gamma\alpha)
V(V\gamma\alpha V\alpha\beta)=-(S.\alpha\beta\gamma)^4$
\item[(e)]$S.\delta\epsilon\zeta=-16(S.\alpha\beta\gamma)^4$\\
where 
$$\delta=V(V(\alpha+\beta)(\beta+\gamma)V(\beta+\gamma)(\gamma+\alpha))$$
$$\epsilon=V(V(\beta+\gamma)(\gamma+\alpha)V(\gamma+\alpha)(\alpha+\beta))$$
$$\zeta=V(V(\gamma+\alpha)(\alpha+\beta)V(\alpha+\beta)(\beta+\gamma))$$
\end{itemize}

{\bf 9}. Prove the common formula for the product of two determinants 
of the third order in the form 
$$
S.\alpha\beta\gamma S.\alpha_1\beta_1\gamma_1=
\left|
\begin{array}{ccc}
S\alpha\alpha_1 & S\beta\alpha_1 & S\gamma\alpha_1\\
S\alpha\beta_1  & S\beta\beta_1  & S\gamma\beta_1\\
S\alpha\gamma_1 & S\beta\gamma_1 & S\gamma\gamma_1
\end{array}
\right|
$$

{\bf 10}. Show that, whatever be the eight vectors involved, 
$$
\left|
\begin{array}{cccc}
S\alpha\alpha_1 & S\alpha\beta_1 & S\alpha\gamma_1 & S\alpha\delta_1\\
S\beta\alpha_1  & S\beta\beta_1  & S\beta\gamma_1  & S\beta\delta_1\\
S\gamma\alpha_1 & S\gamma\beta_1 & S\gamma\gamma_1 & S\gamma\delta_1\\
S\delta\alpha_1 & S\delta\beta_1 & S\delta\gamma_1 & S\delta\delta_1
\end{array}
\right|
=S.\alpha\beta\gamma S. \beta_1\gamma_1\delta_1S\alpha_1(\delta-\delta)=0
$$

If the single term $S\alpha\alpha_1$, be changed to $S\alpha_0\alpha_1$,
the value of the determinant is 
$$S.\beta\gamma\delta S.\beta_1\gamma_1\delta_1 S\alpha_1(\alpha_0-\alpha)$$

State these as propositions in spherical trigonometry. 

Form the corresponding null determinant for any two groups 
of five quaternions : and give its geometrical interpretation. 

{\bf 11}. If, in \S 102, $\alpha$, $\beta$, $\gamma$ be three mutually
perpendicular vectors, can anything be predicated as to $\alpha_1$,
$\beta_1$, $\gamma_1$?  If $\alpha$, $\beta$, $\gamma$ be rectangular
unit-vectors, what of $\alpha_1$, $\beta_1$, $\gamma_1$?

{\bf 12}. If $\alpha$, $\beta$, $\gamma$, $\alpha^{\prime}$, $\beta^{\prime}$,
$\gamma^{\prime}$ be two sets of rectangular unit-vectors, show that 
$$
S\alpha\alpha^{\prime}=
S\gamma\beta^{\prime}S\beta\gamma^{\prime}=
S\beta\beta^{\prime}S\gamma\gamma^{\prime}\;\;\;\textrm{\&c. \&c.}
$$

{\bf 13}. The lines bisecting pairs of opposite sides of a quadrilateral 
(plane or gauche) are perpendicular to each other when the 
diagonals of the quadrilateral are equal. 

{\bf 14}. Show that 
\begin{itemize}
\item [(a)]$S.q^2=2S^2q-T^2q$
\item [(b)]$S.q^3=S^3q-3SqT^2Vq$
\item [(c)]$\alpha^2\beta^2\gamma^2+S^2.\alpha\beta\gamma=
V^2.\alpha\beta\gamma$
\item [(d)]$S(V.\alpha\beta\gamma V.\beta\gamma\alpha V.\gamma\alpha\beta)=
4S\alpha\beta S\beta\gamma S\gamma\alpha S.\alpha\beta\gamma$
\item [(e)]$V.q^3=(2S^2q-T^2Vq)Vq$
\item [(f)]$qUVq^{-1}=-Sq.UVq+TVq$
\end{itemize}

and interpret each as a formula in plane or spherical trigonometry. 

{\bf 15}. If $q$ be an undetermined quaternion, what loci are represented by 
\begin{itemize}
\item[(a)]$(q\alpha^{-1})^2=-a^2$
\item[(b)]$(q\alpha^{-1})^4=a^4$
\item[(c)]$S.(q-\alpha)^2=a^2$
\end{itemize}
where $a$ is any given scalar and $\alpha$ any given vector ? 

{\bf 16}. If $q$ be any quaternion, show that the equation 
$$Q^2=q^2$$
is satisfied, not alone by $Q = \pm q$, but also by 
$$Q=\pm \sqrt{-1}(Sq.UVq-TVq)$$

\begin{flushright}
(Hamilton, {\sl Lectures}, p. 673.)
\end{flushright}

{\bf 17}. Wherein consists the difference between the two equations 
$$
T^2\frac{\rho}{\alpha}=1\;\;\;\textrm{and}\;\;\;
\left(\frac{\rho}{\alpha}\right)^2=-1
$$

What is the full interpretation of each, $\alpha$ being a given, and p an 
undetermined, vector? 

{\bf 18}. Find the {\sl full} consequences of each of the following 
groups of equations, as regards both the unknown vector $\rho$ and 
the given vectors $\alpha$, $\beta$, $\gamma$:
$$
\begin{array}{crcrcr}
 & S.\alpha\beta\rho=0 & & S\alpha\rho=0 & & S\alpha\rho=0\\
(a) &  & (b) & S.\alpha\beta\rho=0 & (c) & S.\alpha\beta\rho=0\\
  & S.\beta\gamma\rho=0 & & S\beta\rho=0 & & S.\alpha\beta\gamma\rho=0
\end{array}
$$

{\bf 19}. From \S\S 74, 110, show that, if $\epsilon$ 
be any unit-vector, and $m$ any scalar, 
$$\epsilon^{m}=\cos\frac{m\pi}{2}+\epsilon\sin\frac{m\pi}{2}$$
Hence show that if $\alpha$, $\beta$, $\gamma$ 
be radii drawn to the corners of a triangle on the unit-sphere, 
whose spherical excess is $m$ right angles, 
$$
\frac{\alpha+\beta}{\beta+\gamma}.
\frac{\gamma+\alpha}{\alpha+\beta}.
\frac{\beta+\gamma}{\gamma+\alpha}=
\alpha^m
$$
Also that, if $A$, $B$, $C$ be the angles of the triangle, we have 
$$
\gamma^{\frac{2C}{\pi}}
\beta^{\frac{2B}{\pi}}
\alpha^{\frac{2A}{\pi}}
=-1
$$

{\bf 20}. Show that for any three vectors $\alpha$, $\beta$, $\gamma$ we have
$$
(U\alpha\beta)^2+(U\beta\gamma)^2+(U\alpha\gamma)^2+(U.\alpha\beta\gamma)^2+
4U\alpha\gamma .SU\alpha\beta SU\beta\gamma=-2
$$

\begin{flushright}
(Hamilton, {\sl Elements}, p. 388.)
\end{flushright}

{\bf 21}. If $a_1$, $a_2$, $a_3$, $x$ be any four scalars, 
and $\rho_1$, $\rho_2$, $\rho_3$ any three vectors, show that 
$$
(S.\rho_1\rho_2\rho_3)^2+
(\sum.a_1V\rho_2\rho_3)^2+
x^2(\sum V\rho_1\rho_2)^2-
$$
$$
x^2(\sum.a_1(\rho_2-\rho_3))^2
+2\prod(x^2+S\rho_1\rho_2+a_1a_2)
$$
$$
=2\prod(x^2+\rho^2)+
2\prod a^2+
$$
$$
\sum\{(x^2+a_1^2+\rho_1^2)((V\rho_2\rho_3)^2+
2a_2a_3(x^2+S\rho_2\rho_3)-x^2(\rho_2-\rho_3)^2)\}
$$
where $\displaystyle \prod a^2=a_1^2a_2^2a_3^2$

Verify this formula by a simple process in the particular case 
$$a_1=a_2=a_3=x=0$$

\begin{flushright}
({\sl Ibid})
\end{flushright}

{\bf 22}. Eliminate $p$ from the equations 
$$V.\beta\rho\alpha\rho=0,\;\;\;S\gamma\rho=0$$
and state the problem and its solution in a geometrical form. 

{\bf 23}. If $p$, $q$, $r$, $s$ be four versors, such that 
$$qp=-sr=\alpha$$
$$rq=-ps=\beta$$
where $\alpha$ and $\beta$ are unit-vectors; show that 
$$S(V.VsVqV.VrVp)=0$$
Interpret this as a property of a spherical quadrilateral. 

{\bf 24}. Show that, if $pq$, $rs$, $pr$, and $qs$ be vectors, we have 
$$S(V.VpVsV.VqVr)=0$$

{\bf 25}. If $\alpha$, $\beta$, $\gamma$ be unit-vectors, 
$$
V\beta\gamma S.\alpha\beta\gamma=
-\alpha(1-S^2\beta\gamma)-
\beta(S\alpha\gamma S\beta r + S\alpha\beta)-
\gamma(S\alpha\beta S\beta\gamma+S\alpha\gamma)
$$

{\bf 26}. If $i$, $j$, $k$, $i^{\prime}$, $j^{\prime}$, $k^{\prime}$,
be two sets of rectangular unit-vectors, show that 
$$
\begin{array}{rcl}
S.Vii^{\prime}Vjj^{\prime}Vkk^{\prime}&=&(Sij^{\prime})^2-(Sji^{\prime})^2\\
                       &=&(Sjk^{\prime})^2-(Skj^{\prime})^2=\textrm{\&c.}
\end{array}
$$
and find the values of the vector of the same product. 

{\bf 27}. If $\alpha$, $\beta$, $\gamma$
be a rectangular unit-vector system, show that, 
whatever be $\lambda$, $\mu$, $\nu$
$$\lambda S^2i\alpha +\mu S^2j\gamma +\nu S^2k\beta$$
$$\lambda S^2k\gamma +\mu S^2i\beta  +\nu S^2j\alpha$$
and
$$\lambda S^2j\beta  +\mu S^2k\alpha +\nu S^2i\gamma$$
are coplanar vectors. What is the connection between this and 
the result of the preceding example ? 

\vfill
\newpage
\section{Axiom Examples}
The basic operation for creating quaternions is {\bf quatern}.
This is a quaternion over the rational numbers.
\boxer{4.6in}{
\spadcommand{q:=quatern(2/11,-8,3/4,1)}
$$
{2 \over {11}} -{8 \  i}+{{3 \over 4} \  j}+k 
$$
\returnType{Type: Quaternion Fraction Integer}
}

This is a quaternion over the integers.
\boxer{4.6in}{
\spadcommand{r:=quatern(1,2,3,4)}
$$
1+{2 \  i}+{3 \  j}+{4 \  k} 
$$
\returnType{Type: Quaternion Integer}
}

We can also construct quaternions with complex components.
First we construct a complex number.
\boxer{4.6in}{
\spadcommand{b:=complex(3,4)}
$$
3+{4 \  i} 
$$
\returnType{Type: Complex Integer}
}
and then we use it as a component in a quaternion.
\boxer{4.6in}{
\spadcommand{s:=quatern(3,1/7,b,2)}
$$
3+{{1 \over 7} \  i}+{{\left( 3+{4 \  i} \right)}\  j}+{2 \  k} 
$$
\returnType{Type: Quaternion Complex Fraction Integer}
}
Notice that the $i$ component of the complex number has no
relation to the $i$ component of the quaternion even though
they use the same symbol by convention.

The four parts of a quaternion are the real part, the $i$ imaginary 
part, the $j$ imaginary part, and the $k$ imaginary part. The
{\bf real} function returns the real part.
\boxer{4.6in}{
\spadcommand{real q}
$$
2 \over {11} 
$$
\returnType{Type: Fraction Integer}
}

The {\bf imagI} function returns the $i$ imaginary part.
\boxer{4.6in}{
\spadcommand{imagI q}
$$
-8 
$$
\returnType{Type: Fraction Integer}
}

The {\bf imagJ} function returns the $j$ imaginary part.
\boxer{4.6in}{
\spadcommand{imagJ q}
$$
3 \over 4 
$$
\returnType{Type: Fraction Integer}
}

The {\bf imagK} function returns the $k$ imaginary part.
\boxer{4.6in}{
\spadcommand{imagK q}
$$
1
$$
\returnType{Type: Fraction Integer}
}

Quaternions satisfy a very fundamental relationship between the parts, 
namely that
$$i^2 = j^2 = k^2 = ijk = -1$$. This is similar to the requirement
in complex numbers of the form $a+bi$ that $i^2 = -1$.

The set of quaternions is denoted by $\mathbb{H}$, whereas the integers
are denoted by $\mathbb{Z}$ and the complex numbers by $\mathbb{C}$.

Quaternions are not commutative which means that in general
$$AB \ne BA$$
for any two quaternions, A and B. So, for instance,
\boxer{4.6in}{
\spadcommand{q*r}
$$
{{437} \over {44}} -{{{84} \over {11}} \  i}+{{{1553} \over {44}} \  j} 
-{{{523} \over {22}} \  k} 
$$
\returnType{Type: Quaternion Fraction Integer}
}

\boxer{4.6in}{
\spadcommand{r*q}
$$
{{437} \over {44}} -{{{84} \over {11}} \  i} -{{{1439} \over {44}} \  
j}+{{{599} \over {22}} \  k} 
$$
\returnType{Type: Quaternion Fraction Integer}
}
and these are clearly not equal.

Complex $2\times2$ matrices form an alternate, equivalent 
representation of quaternions. These matrices have the form:
$$
\left[
\begin{array}{cc}
u & v \\ 
-\overline{v} & \overline{u} 
\end{array}
\right]
$$
=
$$
\left[
\begin{array}{cc}
a+bi & c+di \\ 
-c+di & a-bi
\end{array}
\right]
$$
where $u$ and $v$ are complex, $\overline{u}$ is complex conjugate
of $u$, $\overline{z}$ is the complex conjugate of $z$, and a,b,c,
and d are real.

Within the quaternion each component operator represents a basis
element in $\mathbb{R}^4$ thus:
$$
1 =
\left[
\begin{array}{cccc}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 1\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1
\end{array}
\right]
$$

$$
i =
\left[
\begin{array}{cccc}
0 & 1 & 0 & 0\\
-1 & 0 & 0 & 1\\
0 & 0 & 0 & 1\\
0 & 0 & -1 & 0
\end{array}
\right]
$$

\chapter{Clifford Algebra}

This is quoted from John Fletcher's web page \cite{Flet09} (with permission).

The theory of Clifford Algebra includes a statement that each Clifford
Algebra is isomorphic to a matrix representation. Several authors
discuss this and in particular Ablamowicz \cite{Abla98} gives examples of
derivation of the matrix representation. A matrix will itself satisfy
the characteristic polynomial equation obeyed by its own
eigenvalues. This relationship can be used to calculate the inverse of
a matrix from powers of the matrix itself. It is demonstrated that the
matrix basis of a Clifford number can be used to calculate the inverse
of a Clifford number using the characteristic equation of the matrix
and powers of the Clifford number. Examples are given for the algebras
Clifford(2), Clifford(3) and Clifford(2,2).

\section{Introduction}

Introductory texts on Clifford algebra state that for any chosen
Clifford Algebra there is a matrix representation which is equivalent.
Several authors discuss this in more detail and in particular,
Ablamowicz \cite{Abla98} 
shows that the matrices can be derived for each algebra
from a choice of idempotent, a member of the algebra which when
squared gives itself.  The idea of this paper is that any matrix obeys
the characteristic equation of its own eigenvalues, and that therefore
the equivalent Clifford number will also obey the same characteristic
equation.  This relationship can be exploited to calculate the inverse
of a Clifford number. This result can be used symbolically to find the
general form of the inverse in a particular algebra, and also in
numerical work to calculate the inverse of a particular member.  This
latter approach needs the knowledge of the matrices.  Ablamowicz has
provided a method for generating them in the form of a Maple
implementation. This knowledge is not believed to be new, but the
theory is distributed in the literature and the purpose of this paper
is to make it clear.  The examples have been first developed using a
system of symbolic algebra described in another paper by this
author \cite{Flet01}.  

\section{Clifford Basis Matrix Theory}

The theory of the matrix basis is discussed extensively by
Ablamowicz.  This theory will be illustrated here following the
notation of Ablamowicz by reference to Clifford(2) algebra and can
be applied to other Clifford Algebras. For most Clifford algebras
there is at least one primitive idempotent, such that it squares to
itself. For Clifford (2), which has two basis members $e_1$ and $e_2$, one
such idempotent involves only one of the basis members, $e_1$, i.e.

\[f_1 = f = \frac{1}{2} (1 + e_1)\]

If the idempotent is mutiplied by the other basis function $e_2$, other
functions can be generated:

\[f_2 = e_2 f = \left(\frac{1}{2}-\frac{1}{2}e_1\right)e_2\]

\[f_3 = f e_2 = \left(\frac{1}{2}+\frac{1}{2}e_1\right)e_2\]

\[f_4 = e_2 f e_2 = \frac{1}{2}-\frac{1}{2}e_1\]

Note that $fe_22f = 0$.  These four functions provide a means of
representing any member of the space, so that if a general member c is
given in terms of the basis members of the algebra

\[ c = a_0 + a_1e_1 + a_2e_2 + a_3e_1e_2\]

it can also be represented by a series of terms in the idempotent and
the other functions.

\[
\begin{array}{rcl}
c&=&a_{11}f_1 + a_{21}f_2 + a_{12}f_3 + a_{22}f_4\\
&&\\
 &=&\frac{1}{2}a_{11} + \frac{1}{2}a_{11}e_1 + \frac{1}{2}a_{21}e_2
-\frac{1}{2}a_{21}e_1e_2 +\\
&&\\
&&\frac{1}{2}a_{12}e_2 + \frac{1}{2}a_{12}e_1e_2 + \frac{1}{2}a_{22} 
-\frac{1}{2}a_{22}e_1
\end{array}
\]


Equating coefficients it is clear that the following equations apply.
\[
\begin{array}{rcl}
a_0 &=& \frac{1}{2}a_{11} + \frac{1}{2}a_{22}\\
&&\\
a_1 &=& \frac{1}{2}a_{11} - \frac{1}{2}a_{22}\\
&&\\
a_2 &=& \frac{1}{2}a_{12} + \frac{1}{2}a_{21}\\
&&\\
a_3 &=& \frac{1}{2}a_{12} - \frac{1}{2}a_{21}
\end{array}
\]

The reverse equations can be recovered by multiplying the two forms of
c by different combinations of the functions $f_1$, $f_2$ and $f_3$. 
The equation

\[
\begin{array}{rcl}
f_1cf_1 &=& f_1(a_{11}f_1 + a_{21}f_2 + a_{12}f_3 + a_{22}f_4)f_1\\
&&\\
        &=& f_1(a_0 + a_1e_1 + a_2e_2 + a_3e_1e_2)f_1
\end{array}
\]

reduces to the equation

\[a_{11}f = (a_0 + a_1)f\]

and similar equations can be deduced from other combinations of the
functions as follows.

\[
\begin{array}{rcl}
f_1cf_2 : a_{12}f &=& (a_2 + a_3)f\\
&&\\
f_2cf_1 : a_{21}f &=& (a_2 - a_3)f\\
&&\\
f_3cf_2 : a_{22}f &=& (a_0 - a_1)f
\end{array}
\]

If a matrix is defined as

\[
A = \left(
\begin{array}{cc}
a_{11} & a_{12} \\
a_{21} & a_{22}
\end{array}
\right)
\]

so that

\[
Af = \left(
\begin{array}{cc}
a_{11}f & a_{12}f \\
a_{21}f & a_{22}f
\end{array}
\right) 
=
\left(
\begin{array}{cc}
a_0+a_1 & a_2+a_3 \\
a_2-a_3 & a_0-a_1
\end{array}
\right) f
\]

then the expression

\[
\left(
\begin{array}{cc}
1 & e_2
\end{array}
\right)
\left(
\begin{array}{cc}
a_{11}f & a_{12}f \\
a_{21}f & a_{22}f
\end{array}
\right)
\left(
\begin{array}{c}
1\\
e_2
\end{array}
\right)
=
a_{11}f_1 + a_{21}f_2 + a_{12}f_3 + a_{22}f_4 = c
\]

generates the general Clifford object c.  All that remains to form the
basis matrices is to make c each basis member in turn, and named as
shown.

\[
\begin{array}{lrclcr}
c=1: & Af & = & 
\left(
\begin{array}{cc}
f & 0\\
0 & f
\end{array}
\right)
& = & E_0f\\
c=e_1 & Af & = &
\left(
\begin{array}{cc}
f & 0\\
0 & -f
\end{array}
\right)
& = & E_1f\\
c=e_2 & Af & = &
\left(
\begin{array}{cc}
0 & f\\
f & 0
\end{array}
\right)
& = & E_2f\\
c=e_1e_2 & Af & = &
\left(
\begin{array}{cc}
0 & f\\
-f & 0
\end{array}
\right)
& = & E_{12}f
\end{array}
\]

These are the usual basis matrices for Clifford (2) except that they
are multiplied by the idempotent.

This approach provides an explanation for the basis matrices in terms
only of the Clifford Algebra itself.  They are the matrix
representation of the basis objects of the algebra in terms of an
idempotent and an associated vector of basis functions.  This has been
shown for Clifford (2) and it can be extended to other algebras once
the idempotent and the vector of basis functions have been identified.
This has been done in many cases by Ablamowicz.  This will now be
developed to show how the inverse of a Clifford number can be obtained
from the matrix representation.  

\section{Calculation of the inverse of a Clifford number}

The matrix basis demonstrated above can be used to calculate the
inverse of a Clifford number.  In simple cases this can be used to
obtain an algebraic formulation.  For other cases the algebra is too
complex to be clear, but the method can still be used to obtain the
numerical value of the inverse.  To apply the method it is necessary
to know a basis matrix representation of the algebra being used.

The idea of the method is that the matrix representation will have a
characteristic polynomial obeyed by the eigenvalues of the matrix and
also by the matrix itself.  There may also be a minimal polynomial
which is a factor of the characteristic polynomial, which will have
also be satisfied by the matrix.  It is clear from the proceding
section that if $A$ is a matrix representation of $c$ in a Clifford
Algebra then if some function $f(A) = 0$ then the corresponding Clifford
function $f(c) = 0$ must also be zero.  In particular if $f(A) = 0$ is the
characteristic or minimal polynomial of $A$, then $f(c) = 0$ implies that
$c$ also satisfies the same polynomial.  Then if the inverse of the
Clifford number, $c^{-1}$ is to be found, then 

\[c^{-1}f(c)=0\]

provides a relationship for $c^{-1}$ in terms of multiples a small number
of low powers of $c$, with the maximum power one less than the order of
the polynomial. The method suceeds unless the constant term in the
polynomial is zero, which means that the inverse does not exist.  For
cases where the basis matrices are of order two, the inverse will be
shown to be a linear function of $c$.

The method can be summed up as follows.
\begin{enumerate}
\item Find the matrix basis of the Clifford algebra.
\item Find the matrix representation of the Clifford number whose
inverse is required.
\item Compute the characteristic or minimal polynomial.
\item Check for the existence of the inverse.
\item Compute the inverse using the coefficients from the polynomial.
\end{enumerate}

Step 1 need only be done once for any Clifford algebra, and this can
be done using the method in the previous section, where needed.

Step 2 is trivially a matter of accumulation of the correct multiples
of the matrices.

Step 3 may involve the use of a computer algebra system to find the
coefficients of the polynomial, if the matrix size is at all large.

Steps 4 and 5 are then easy once the coefficients are known.

The method will now be demonstrated using some examples.

\subsection{Example 1: Clifford (2)}

In this case the matrix basis for a member of the Clifford algebra

\[c = a_0 + a_1e_1 + a_2e_2 + a_3e_1e_2\]

was developed in the previous section as

\[A=\left(
\begin{array}{cc}
a_0+a_1 & a_2+a_3\\
a_2-a_3 & a_0-a_1
\end{array}
\right)\]

This matrix has the characteristic polynomial

\[X^2 - 2Xa_0 + a^2_0 - a^2_1 - a^2_2 + a^2_3 = 0\]

and therefore

\[X^{-1}(X^2 - 2Xa_0 + a^2_0 - a^2_1 - a^2_2 + a^2_3) = 0\]

and

\[X^{-1} = (2a_0 - X)/(a^2_0 - a^2_1 - a^2_2 + a^2_3) = 0\]

which provides a general solution to the inverse in this algebra.

\[c^{-1} = (2a_0 - c)/(a^2_0 - a^2_1 - a^2_2 + a^2_3) = 0\]

\subsection{Example 2: Clifford (3)}

A set of basis matrices for Clifford (3) as given by Abalmowicz and
deduced are

\[
\begin{array}{cc}
E_0 =
\left(
\begin{array}{cc}
1&0\\
0&1
\end{array}\right) &
E_1 =
\left(
\begin{array}{cc}
1&0\\
0&-1
\end{array}\right) \\
E_2 =
\left(
\begin{array}{cc}
0&1\\
1&0
\end{array}\right) &
E_3 =
\left(
\begin{array}{cc}
0&-j\\
j&0
\end{array}\right) \\
E_1E_2 =
\left(
\begin{array}{cc}
0&1\\
-1&0
\end{array}\right) &
E_1E_3 =
\left(
\begin{array}{cc}
0&-j\\
-j&0
\end{array}\right) \\
E_2E_3 =
\left(
\begin{array}{cc}
j&0\\
0&-j
\end{array}\right) &
E_1E_2E_3 =
\left(
\begin{array}{cc}
j&0\\
0&j
\end{array}\right) \\
\end{array}
\]

for the idempotent 

\[f = \frac{(1 + e_1)}{2}, {\rm\ where\ } j^2 = -1.\]

The general member of the algebra

\[c_3 = a_0 +a_1e_1 + a_2e_2 + a_3e_3 + 
a_{12}e_1e_2 + a_{13}e_1e_3 + a_{23}e_2e_3 + a_{123}e_1e_2e_3\]

has the matrix representation

\[
\begin{array}{rcl}
A_3&=&a_0E_0 + a_1E_1 + a_2E_2 +a_3E_3 + a_{12}E_1E_2\\
&& +a_{13}E_1E_3 + a_{23}E_2E_3 + a_{123}E_1E_2E_3\\
&&\\
&=&\left(
\begin{array}{cc}
a_0 + a_1 + ja_{23} + ja_{123}& a_2 -ja_3 +a_{12} -ja_{13}\\
a_2 + ja_3- a_{12}- ja_{13}& a_0- a_1- ja_{23} + ja_{123}
\end{array}
\right)
\end{array}
\]

This has the characteristic polynomial

\[
\begin{array}{rl}
&a^2_0-a^2_1-a^2_2-a^2_3+a^2_{12}+a^2_{13}+a^2_{23}-a^2_{123}\\
&\\
+&2j(a_0a_{123}-a_1a_{23}-a_{12}a_3+a_{13}a_2)\\
&\\
-&2(a_0+ja_{123})X + X^2=0
\end{array}
\]

and the expression for the inverse is

\[
\begin{array}{rcl}
X^{-1}&=&(2a_0 + 2ja_{123} -X) /\\
&&(a^2_0-a^2_1-a^2_2-a^2_3+a^2_{12}+a^2_{13}+a^2_{23}-a^2_{123}\\
&&+2j(a_0a_{123}-a_1a_{23}-a_{12}a_3+a_{13}a_2))
\end{array}
\]

Complex terms arise in two cases,

\[a_{123} \ne 0\]

and

\[(a_0a_{123}-a_1a_{23}-a_{12}a_3+a_{13}a_2) \ne 0\]

Two simple cases have real minumum polynomials:

Zero and first grade terms only:

\[
\begin{array}{rcl}
A_1&=&a_0E_0 + a_1E_1 + a_2E_2 + a_3E_3\\
&=&\left(
\begin{array}{cc}
a_0+a_1 & a_2-ja_3\\
a_2+ja_3 & a_0-a_1
\end{array}
\right)
\end{array}
\]

which has the minimum polynomial

\[a^2_0-a^2_1-a^2_2-a^2_3-2a_0X+X^2=0\]

which gives

\[X^{-1} = (2a_0- X) / (a^2_0- a^2_1- a^2_2 - a^2_3)\]

Zero and second grade terms only (ie. the even subspace).

\[
\begin{array}{rcl}
A_2&=&a_0E_0 + a_{12}E_1E_2 + a_{13}E_1E_3 + a_{23}E_2E_3\\
&&\left(
\begin{array}{cc}
a_0+ja_{23}     & a_{12}-ja_{13}\\
-a_{12}-ja_{13} & a_0-ja_{23}
\end{array}
\right)
\end{array}
\]

which has minimum polynomial

\[a^2_0+a^2_{23}+a^2_{12}+a^2_{13}-2a_0X+X^2 = 0\]

giving

\[X^{-1} = (2a_0- X) /(a^2_0 + a^2_{23} + a^2_{12} + a^2_{13})\]

This provides a general solution for the inverse together with two
simple cases of wide usefulness.

\subsection{Example 3: Clifford (2,2)}

The following basis matrices are given by Ablamowicz \cite{Abla98}

\[
\begin{array}{cc}
E_1=\left(
\begin{array}{cccc}
0 & 1 & 0 & 0\\
1 & 0 & 0 & 0\\
0 & 0 & 0 & 1\\
0 & 0 & 1 & 0
\end{array}
\right)&
E_2=\left(
\begin{array}{cccc}
0 & 0 & 1 & 0\\
0 & 0 & 0 & -1\\
1 & 0 & 0 & 0\\
0 & -1 & 0 & 0
\end{array}
\right)\\
E_3=\left(
\begin{array}{cccc}
0 & -1 & 0 & 0\\
1 & 0 & 0 & 0\\
0 & 0 & 0 & -1\\
0 & 0 & 1 & 0
\end{array}
\right)&
E_4=\left(
\begin{array}{cccc}
0 & 0 & -1 & 0\\
0 & 0 & 0 & 1\\
1 & 0 & 0 & 0\\
0 & -1 & 0 & 0
\end{array}
\right)
\end{array}
\]

for the idempotent 
\[f = \frac{(1 +e_1e_3) (1+ e_1e_3)}{4}.\]

 Note that this implies that the order of the basis members is such
that $e_1$ and $e_2$ have square $+1$ and $e_3$ and $e_4$ have square
$-1$. Other orderings are used by other authors. The remaining basis
matrices can be deduced to be as follows.

Second Grade members

\[
\begin{array}{cc}
E_1E_2 = \left(
\begin{array}{cccc}
0 & 0 & 0 & -1\\
0 & 0 & 1 & 0\\
0 & -1 & 0 & 0\\
1 & 0 & 0 & 0
\end{array}\right)&
E_1E_3 = \left(
\begin{array}{cccc}
1 & 0 & 0 & 0\\
0 & -1 & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & -1
\end{array}\right)\\
E_1E_4 = \left(
\begin{array}{cccc}
0 & 0 & 0 & 1\\
0 & 0 & -1 & 0\\
0 & -1 & 0 & 0\\
1 & 0 & 0 & 0
\end{array}\right)&
E_2E_3 = \left(
\begin{array}{cccc}
0 & 0 & 0 & -1\\
0 & 0 & -1 & 0\\
0 & -1 & 0 & 0\\
-1 & 0 & 0 & 0
\end{array}\right)\\
E_2E_4 = \left(
\begin{array}{cccc}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & -1 & 0\\
0 & 0 & 0 & -1
\end{array}\right)&
E_3E_4 = \left(
\begin{array}{cccc}
0 & 0 & 0 & -1\\
0 & 0 & -1 & 0\\
0 & 1 & 0 & 0\\
1 & 0 & 0 & 0
\end{array}\right)\\
\end{array}
\]

Third grade members

\[
\begin{array}{cc}
E_1E_2E_3 = \left(
\begin{array}{cccc}
0 & 0 & -1 & 0\\
0 & 0 & 0 & -1\\
-1 & 0 & 0 & 0\\
0 & -1 & 0 & 0
\end{array}\right)&
E_1E_2E_4 = \left(
\begin{array}{cccc}
0 & 1 & 0 & 0\\
1 & 0 & 0 & 0\\
0 & 0 & 0 & -1\\
0 & 0 & -1 & 0
\end{array}\right)\\
E_1E_3E_4 = \left(
\begin{array}{cccc}
0 & 0 & -1 & 0\\
0 & 0 & 0 & -1\\
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0
\end{array}\right)&
E_2E_3E_4 = \left(
\begin{array}{cccc}
0 & 1 & 0 & 0\\
-1 & 0 & 0 & 0\\
0 & 0 & 0 & -1\\
0 & 0 & 1 & 0
\end{array}\right)
\end{array}
\]

Fourth grade member

\[
E_1E_2E_3E_4 = \left(
\begin{array}{cccc}
-1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & -1
\end{array}
\right)
\]

Zero grade member (identity)

\[
E_0 = \left(
\begin{array}{cccc}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1
\end{array}
\right)
\]

The general member of the Clifford (2,2) algebra can be written as follows.

\[\begin{array}{rcl}
c_{22}&=& a_0 + a_1e_1 + a_2e_2 + a_3e_3 + a_4e_4 +\\
&&a_{12}e_1e_2+a_{13}e_1e_3+a_{14}e_1e_4+a_{23}e_2e_3+a_{24}e_2e_4+
a_{34}e_3e_4\\
&&+ a_{123}e_1e_2e_3 +a_{124}e_1e_2e_4 +a_{134}e_1e_3e_4 + 
a_{234}e_2e_3e_4 + a_{1234}e_1e_2e_3e_4
\end{array}
\]

This has the following matrix representation.

\[
\left(
\begin{array}{cccc}
a_0+a_{13}+    & a_1-a_3+        & a_2-a_4-        & -a_{12}+a_{14}-\\
a_{24}-a_{1234}& a_{124}+a_{234} & a_{123}-a_{134} & a_{23}-a_{34}\\
&&&\\
a_1+a_3+        & a_0-a_{13}+     & a_{12}-a_{14}- & -a_2+a_4-\\
a_{124}-a_{234} & a_{24}+a_{1234} & a_{23}-a_{34}  & a_{123}-a_{134}\\
&&&\\
a_2+a_4-        & -a_{12}-a_{14}- & a_0+a_{13}-     & a_1-a_3-\\
a_{123}+a_{134} & a_{23}+a_{34}   & a_{24}+a_{1234} & a_{124}-a_{234}\\
&&&\\
a_{12}+a_{14}- & -a_2-a_4-       & a_1+a_3-        & a_0-a_{13}-\\
a_{23}+a_{34}  & a_{123}+a_{134} & a_{124}+a_{234} & a_{24}-a_{1234}
\end{array}
\right)
\]

In this case it is possible to generate the characteristic equation
using computer algebra. However, it is too complex to be of practical
use. Instead here are numerical examples of the use of the method to
calculate the inverse. For the case where

\[n1 = 1+ e_1 + e_2 + e_3 + e_4\]

then the matrix representation is

\[N_1 = E_0 +E_1 + E_2 + E_3 + E_4 = 
\left(
\begin{array}{cccc}
1 & 0 & 0 & 0\\
2 & 1 & 0 & 0\\
2 & 0 & 1 & 0\\
0 & -2 & 2 & 1
\end{array}
\right)
\]

This has the minimum polynomial

\[X^2 - 2X + 1 = 0\]

so that

\[X^{-1} = 2- X\]

and

\[n^{-1}_1= 2 - n_1 = 1 - e_1 - e_2- e_3- e_4\]

For

\[n_2 = 1+ e_1 + e_2 + e_3 + e_4 +e_1e_2\]

the matrix representation is

\[N_2 = I + E_1 + E_2 + E_3 +E_4 + E_1E_2 = 
\left(
\begin{array}{cccc}
1 & 0 & 0 & -1\\
2 & 1 & 1 & 0\\
2 & -1 & 1 & 0\\
1 & -2 & 2 & 1
\end{array}
\right)
\]

This has the minimum polynomial

\[X^4 - 4X^3 + 8X^2 - 8X - 4 = 0\]

so that

\[X^{-1} = \frac{X^3 - 4X^2 + 8X - 8}{4}\]

and

\[n^{-1}_2 = \frac{n^3_2- 4n^2_2 + 8n_2 - 8}{4}\]

This expression can be evaluated easily using a computer algebra
system for Clifford algebra such as described in Fletcher \cite{Flet01}. 
The result is

\[
\begin{array}{rcl}
n^{-1}_2 &=& -0.5 + 0.5e_1 + 0.5e_2 - 0.5e_1e_2 - 0.5e_1e_3\\
&& - 0.5e_1e_4 + 0.5e_2e_3 + 0.5e_2e_4 - 0.5e_1e_2e_3 - 0.5e_1e_2e_4
\end{array}
\]


Note that in some cases the inverse is linear in the original Clifford
number, and in others it is nonlinear.

\subsection{Conclusion}

The paper has demonstrated a method for the calculation of inverses of
Clifford numbers by means of the matrix representation of the
corresponding Clifford algebra.  The method depends upon the
calculation of the basis matrices for the algebra.  This can be done
from an idempotent for the algebra if the matrices are not already
available.  The method provides an easy check on the existence of the
inverse.  For simple systems a general algebraic solution can be found
and for more complex systems the algebra of the inverse can be
generated and evaluated numerically for a particular example, given a
system of computer algebra for Clifford algebra.

\chapter{Package for Algebraic Function Fields}

PAFF is a Package for Algebraic Function Fields in one variable
by Ga\'etan Hach\'e

PAFF is a package written in Axiom and one of its many purpose is to
construct geometric Goppa codes (also called algebraic geometric codes
or AG-codes). This package was written as part of Ga\'etan's doctorate
thesis on ``Effective construction of geometric codes'': this thesis was
done at Inria in Rocquencourt at project CODES and under the direction
of Dominique LeBrigand at Université Pierre et Marie Curie (Paris
6). Here is a r\'esum\'e of the thesis.

It is well known that the most difficult part in constructing AG-code
is the computation of a basis of the vector space ``L(D)'' where D is a
divisor of the function field of an irreducible curve. To compute such
a basis, PAFF used the Brill-Noether algorithm which was generalized
to any plane curve by D. LeBrigand and J.J. Risler \cite{LeBr88}. In 
\cite{Hach96}
you will find more details about the algorithmic aspect of the
Brill-Noether algorithm. Also, if you prefer, as I do, a strictly
algebraic approach, see \cite{Hach95}. This is the approach I used in my thesis
(\cite{Hach96}) 
and of course this is where you will find complete details about
the implementation of the algorithm. The algebraic approach use the
theory of algebraic function field in one variable : you will find in
\cite{Stic93} a very good introduction to this theory and AG-codes.

It is important to notice that PAFF can be used for most computation
related to the function field of an irreducible plane curve. For
example, you can compute the genus, find all places above all the
singular points, compute the adjunction divisor and of course compute
a basis of the vector space L(D) for any divisor D of the function
field of the curve.

There is also the package PAFFFF which is especially designed to be
used over finite fields. This package is essentially the same as PAFF,
except that the computation are done over ``dynamic extensions'' of the
ground field. For this, I used a simplify version of the notion of
dynamic algebraic closure as proposed by D. Duval \cite{Duva95}.

Example 1

This example compute the genus of the projective plane curve defined by:
\begin{verbatim}
       5    2 3      4
      X  + Y Z  + Y Z  = 0
\end{verbatim}
over the field GF(2).

First we define the field GF(2).
\begin{verbatim}
K:=PF 2
R:=DMP([X,Y,Z],K)
P:=PAFF(K,[X,Y,Z],BLQT)
\end{verbatim}

We defined the polynomial of the curve.
\begin{verbatim}
C:R:=X**5 + Y**2*Z**3+Y*Z**4
\end{verbatim}

We give it to the package PAFF(K,[X,Y,Z]) which was assigned to the
variable $P$.

\begin{verbatim}
setCurve(C)$P
\end{verbatim}

\chapter{Interpolation Formulas}
{\center{\includegraphics[scale=0.80]{ps/lozenge2.eps}}}

The lozenge diagram is a device for showing that a large number of
formulas which appear to be different are really all the same. The
notation for the binomial coefficients
\[C(u+k,n) = \frac{(u+k)(u+k-1)(u+k-2)\cdots{}(u+k-n+1)}{n!}\]
There are $n$ factors in the numerator and $n$ in the denominator.
Viewed as a function of $u$, $C(u+k,n)$ is a polynomial of degree $n$.

The figure above, Hamming \cite{Hamm62}
calls a lozenge diagram. A line starting at
a point on the left edge and following some path across the page
defines an interpolation formula if the following rules are used.
\begin{itemize}
\item[{\bf 1a}] For a left-to-right step, {\sl add}
\item[{\bf 1b}] For a right-to-left, {\sl subtract}
\item[{\bf 2a}] If the {\sl slope} of the step is {\sl positive}, 
use the product of the difference crossed times the factor 
immediately {\sl below}.
\item[{\bf 2b}] If the {\sl slope} of the step is {\sl negative},
use the product of the difference crossed times the factor
immediately {\sl above}
\item[{\bf 3a}] If the step is {\sl horizontal} and passes through a
{\sl difference}, use the product of the difference times the 
{\sl average} of the factors {\sl above} and {\sl below}.
\item[{\bf 3b}] If the step is {\sl horizontal} and passes through a
{\sl factor}, use the product of the factor times the {\sl average}
of the differences {\sl above} and {\sl below}.
\end{itemize}

As an example of rules {\bf 1a} and {\bf 2a}, consider starting at
$y(0)$ and going down to the right. We get, term by term,
\[y(u)=y(0)+C(u,1)\Delta{}y(0)+C(u,2)\Delta^2y(0)+C(u,3)\Delta^3y(0)+\cdots\]
\[=y(0)+u\Delta{}y(0)+\frac{u(u-1)}{2}\Delta^2y(0)+
\frac{u(u-1)(y-2)}{3!}\Delta^3y(0)+\cdots\]
which is Newton's formula.

Had we gone up and to the right, we would have used {\bf 1a} and {\bf 2a}
to get Newton's backward formula:
\[y(u)=y(0)+C(u,1)\Delta{}y(-1)+C(u+1,2)\Delta^2y(-2)+
C(u+2,3)\Delta^3y(-3)+\cdots\]
\[=y(0)+u\Delta{}y(-1)+\frac{(u+1)u}{2}\Delta^2y(-2)+
\frac{(u+2)(u+1)u}{3!}\Delta^3y(-3)+\cdots\]

To get Stirling's formula, we start at $y(0)$ and go horizontally to
the right, using rules {\bf 3a} and {\bf 3b}:
\[y(u)=y(0)
+u\frac{\Delta{}y_0+\Delta{}y_{-1}}{2}
+\frac{C(u+1,2)+C(u,2)}{2}\Delta^2y_{-1}\\
+C(u+1,3)\frac{\Delta^3y_{-2}+\Delta^3y_{-1}}{2}+\cdots\]
\[=y_0+u\frac{\Delta{}y_0+\Delta{}y_{-1}}{2}
+\frac{u^2}{2}\Delta^2{}y_{-1}
+\frac{u(u^2-1)}{3!}\frac{\Delta^3y_{-2}+\Delta^3y_{-1}}{2}+\cdots\]

If we start midway between $y(0)$ and $y(1)$, we get Bessel's formula:
\[y(u)=1\frac{y_0+y_1}{2}+\frac{C(u,1)+C(u-1,1)}{2}\Delta{}y_0
+C(u,2)\frac{\Delta^2y_{-1}+\Delta^2y_0}{2}+\cdots\]
\[=\frac{y_0+y_1}{2}+(u-\frac{1}{2})\Delta{}y_0+
\frac{u(u-1)}{2}\frac{\Delta^2y_{-1}+\Delta^2y_0}{2}+\cdots\]

If we zigzag properly, we can get Gauss' formula for interpolation:
\[y(u)=y_0+u\Delta{}y_0+\frac{u(u-1)}{2}\Delta^2y(-1)+
\frac{u(u^2-1)}{3!}\Delta^3y(-1)+\cdots\]

\chapter[Type Systems]{Type Systems for Computer Algebra by Andreas Weber}

This chapter is based on a PhD thesis by Andreas Weber\cite{Webe93b}.
Changes have been made to integrate it.

We study type systems for computer algebra systems, which frequently
correspond to the ``pragmatically developed'' typing constructs used
in {\sf Axiom}.

A central concept is that of {\em type classes} which correspond to
{\sf Axiom} categories.  We will show that types can be syntactically
described as terms of a regular order-sorted signature if no type
parameters are allowed.  Using results obtained for the functional
programming language {\sf Haskell} we will show that the problem of
{\em type inference} is decidable.  This result still holds if
higher-order functions are present and {\em parametric polymorphism}
is used.  These additional typing constructs are useful for further
extensions of existing computer algebra systems: These typing concepts
can be used to implement category theoretic constructs and there are
many well known constructive interactions between category theory and
algebra.
 
On the one hand we will show that there are well known techniques to
specify many important type classes algebraically, and we will also
show that a formal and algorithmically Feasible treatment of the
interactions of algebraically specified data types and type classes is
possible.  On the other hand we will prove that there are quite
elementary examples arising in computer algebra which need very
``strong'' formalisms to be specified and are thus hard to handle
algorithmically.

We will show that it is necessary to distinguish between types and
elements as parameters of parameterized type classes.  The type
inference problem for the former remains decidable whereas for the
latter it becomes undecidable. We will also show that such a
distinction can be made quite naturally.

Type classes are second-order types.  Although we will show that there
are constructions used in mathematics which imply that type classes
have to become first-order types in order to model the examples
naturally, we will also argue that this does not seem to be the case
in areas currently accessible for an algebra system.  We will only
sketch some systems that have been developed during the last years in
which the concept of type classes as first-order types can be
expressed.  For some of these systems the type inference problem was
proven to be undecidable.

Another fundamental concept for a type system of a computer algebra
system --- at least for the purpose of a user interface --- are {\em
coercions}.  We will show that there are cases which can be modeled by
coercions but not by an ``inheritance mechanism'', i.\,e.\ the concept
of coercions is not only orthogonal to the one of type classes but
also to more general formalisms as are used in object-oriented
languages.  We will define certain classes of coercions and impose
conditions on important classes of coercions which will imply that the
meaning of an expression is independent of the particular coercions
that are used in order to type it.


We will also impose some conditions on the interaction between
polymorphic operations defined in type classes and coercions that will
yield a unique meaning of an expression independent of the type which
is assigned to it --- if coercions are present there will very
frequently be several possibilities to assign types to expressions.

Often it is not only possible to coerce one type into another but it
will be the case that two types are actually {\em isomorphic}.  We
will show that isomorphic types have properties that cannot be deduced
from the properties of coercions and will shortly discuss other
possibilities to model type isomorphisms.  There are natural examples
of type isomorphisms occurring in the area of computer algebra that
have a ``problematic'' behavior.  So we will prove for a certain
example that the type isomorphisms cannot be captured by a finite set
of coercions by proving that the naturally associated equational
theory is not finitely axiomatizable.

Up to now few results are known that would give a clear dividing line
between classes of coercions which have a decidable type inference
problem and classes for which type inference becomes undecidable.  We
will give a type inference algorithm for some important classes of
coercions.

Other typing constructs which are again quite orthogonal to the
previous ones are those of {\em partial functions} and of {\em types
depending on elements}.  We will link the treatment of {\em partial
functions} in {\sf Axiom} to the one used in order-sorted algebras and
will show some problems which arise if a seemingly more expressive
solution were used.  There are important cases in which {\em types
depending on elements} arise naturally.  We will show that not only
type inference but even type checking is undecidable for relevant
cases occurring in computer algebra.

Types have played an extremely important role in the development and
study of programming languages.  They have become so prevalent that
type theory is now recognized as an area of its own within computer
science.  The benefits which can be derived from the presence of types
in a language are manifold.  Through type checking many errors can be
caught before a program is ever run, thus leading to more reliable
programs.  Types form also an expressive basis for module systems,
since they prescribe a machine-verifiable interface for the code
encapsulated within a module.  Furthermore, they may be used to
improve performance of code generated by a compiler.


However, most computer algebra systems are based on untyped languages.
Nevertheless, at least in the description and specification of many
algorithms a terminology is used which can be seen as attributing
``types'' to the computational objects.  In {\sf Maple~V}
\cite{Char91} and in {\sf Mathematica} \cite{Wolf91}, which
are both based on untyped languages, it is even possible to attach
``tags'' to data structures which describe types corresponding to the
mathematical structures the data are supposed to represent.

In the area of computer algebra, the problem of finding appropriate
type systems which are supported by the language is that on the one
hand, the type system has to consider the requirements of a computer
system and on the other, it should allow for the mathematical
structures a system is dealing with to have corresponding types.

The development of {\sf Axiom} \cite{Jenk84b}, \cite{Suto87},
\cite{Jenk92} is certainly a break-through since the language
itself is typed with types corresponding to the mathematical
structures the system deals with.

However, the typing constructs used in {\sf Axiom} have been
``pragmatically developed.''  Some are not even formally defined and
only very few studies on formal properties of such a system have been
undertaken.  Even if other approaches to a type system in this area
are considered --- such as the ``object-oriented'' one used for {\sf
VIEWS} \cite{Abda86} --- we have found relatively few formal
studies of type systems suited for the purpose of computer algebra
systems in the literature, although a formal treatment of some typing
constructs occurring in computer algebra was already given almost
twenty years ago in \cite{Loos74}.

So the situation is different from the one in other areas of computer
science in which untyped languages are prevalent.  For instance, most
logic programming languages are untyped.  This is a consequence of the
fact that logic programming has its roots in first-order logic, which
is essentially untyped.  Nevertheless, the progress of type theory in
the last decade has allowed the development of several type systems
for logic programming languages.  Moreover, the formal properties of
these type systems have been studied extensively (see e.\,g.\
\cite{Smol89a}, \cite{Frue91}, \cite{Kife91}, and the
articles in the collection \cite{Pfen92}, in which also a
comprehensive bibliography on the topic is given).

We will not design a typed computer algebra language in this thesis in
which the mathematical structures a program deals with have a
correspondence in the type system.  It does not seem possible to
design and implement a language of similar power as {\sf Axiom} within
a PhD-project.  There are several proposals of languages for computer
algebra systems\footnote{The author knows of Foderaro's {\sf NEWSPEAK}
\cite{Fode83}, Coolsaet's {\sf MIKE} \cite{Cool92}, and
Dalmas' {\sf XFun} \cite{Dalm92}.}  which are designed and partly
implemented as part of a PhD-project that incorporate some typing
concepts, but which can be seen --- more or less --- as subsets of the
typing constructs of {\sf Axiom}.

Instead we will treat typing constructs which are similar in power to
the ones of {\sf Axiom}.  We will define type systems of various
strength and will investigate their properties.  Discussing a variety
of examples we will show their relevance for a computer algebra
system.  We will also discuss some examples which are not implemented
in a system as yet in order to give some estimates about the
extendability of a system based on such typing principles.  This is
one of the shortcomings of many other investigations in which very
often only examples that can be modeled are discussed.  We hope that
our discussion of a variety of examples will help to obtain
characterization theorems of mathematical structures which can be
modeled by certain typing constructs.  This would be the best
solution.  However, it seems to be a large-scale task to obtain such
characterization theorems in many cases.  A problem in this connection
is certainly that one has to define precisely a class of mathematical
structures a program is dealing with at all.  Current computer algebra
programs sometimes deal with objects of universal algebra, sometimes
with those of higher-order universal algebra, sometimes with those of
first-order model theory, or sometimes with those of category theory,
to mention only some possibilities.

We will prove several properties of such type systems.  A very
important feature is the possibility of {\em type inference}.  Given
an expression the system should be able to infer a correct type for it
whenever possible and reject it otherwise.  Since the interpretation
of an expression written in the standard mathematical notation
requires a kind of type inference very frequently the possibility of
type inference improves considerably the usefulness of a system for a
user.  Thus we will investigate the problems connected with type
inference extensively and will also give some results on the
computational complexity of various type inference problems.  Another
important problem we will investigate in various, precisely defined
ways is a possible ambiguity of a type system.

Some of the results we give are contained in some form in the
literature, especially in papers on type systems for functional
languages.  Nevertheless, it seems to have escaped prior notice that
these results are applicable to the typing problems arising in
computer algebra.

On the one hand it is useful to have a system which can handle as many
mathematical structures as possible.  For many mathematicians a
computer algebra system would be a very valuable tool if it allowed
some computations in rather complicated mathematical structures.
Since many of those computations would be fairly basic it would
suffice for these users to have a system in which they could model
those structures easily, even if that modeling was not very efficient.
Among the existing systems {\sf Axiom} is one of the few which gives
the possibility for such work.\footnote{The new version of 
{\sf Cayley} \cite{Butl90} allows similar possibilities but fewer
structures have been implemented as yet.}  So it seems to be necessary
to have a safe foundation for the constructs found in such a universal
system as {\sf Axiom}.

On the other hand many computations that have to be performed reach
the limits of existing computing power.  So the algorithms should be
as efficient as possible in order to be useful.  Since it seems to be
impossible to have a general system that is always as efficient as a
more special one --- and this thesis will contain some results which
can be viewed as a proof of this claim --- we will not only develop a
framework for a general computer algebra system and discuss its
properties but will also discuss the properties of some subsystems.
The author hopes that some of these results will be useful for the
design of symbolic manipulation systems or the design of user
interfaces for such systems.

The organization of the thesis will be as follows.

In Sec.~\ref{chprelude} we will collect some definitions and facts
which will be needed later.  Most of the material in this chapter can
be found scattered in the literature.  Moreover, we will fix the
notation and will give some discussion on the terminology used in this
thesis as compared to the one found in the literature.

A central concept is that of {\em type classes} which correspond to
{\sf Axiom} categories and will be the subject of
Sec.~\ref{chtycla}.\footnote{They are similar to the {\em varieties}
of {\sf Cayley}, if a {\sf Cayley} {\em class} is interpreted as a
type, which can be done using the concept of {\em types depending on
elements} (see below).  They are also similar to {\em container
classes} used in object-oriented programming. However, we will not
give a systematic treatment of constructs of object-oriented
programming in this thesis.}  We will show that types can be
syntactically described as terms of a regular order-sorted signature
if no type parameters are allowed.  Using results obtained for the
functional programming language {\sf Haskell} we will show that the
problem of {\em type inference} is decidable.  This result still holds
if higher-order functions are present and {\em parametric
polymorphism} is used.  These additional typing constructs are useful
for further extensions of existing computer algebra systems: These
typing concepts can be used to implement category theoretic constructs
and there are many well known constructive interactions between
category theory and algebra.

On the one hand we will show that there are well known techniques to
specify many important type classes algebraically, and we will also
show that a formal treatment of the interactions of algebraically
specified data types and type classes is possible.  On the other hand
we will prove that there are quite elementary examples arising in
computer algebra which need very ``strong'' formalisms to be
specified.

We will show that it is necessary to distinguish between types and
elements as parameters of parameterized type classes.  The type
inference problem for the former remains decidable whereas for the
latter it becomes undecidable. We will also show that such a
distinction can be made quite naturally.

Type classes are second-order types.  Although we will show that there
are constructions used in mathematics which imply that type classes
have to become first-order types in order to model the examples
naturally, we will also argue that this does not seem to be the case
in areas currently accessible for an algebra system.  We will only
sketch some systems that have been developed during the last years in
which the concept of type classes as first-order types can be
expressed.  For some of these systems the type inference problem was
proven to be undecidable, thus showing one of the drawbacks of
stronger formalisms.

In Sec.~\ref{chapcoer} we will treat the concept of {\em coercions}
which is another fundamental concept for a type system of a computer
algebra system, at least for the purpose of a user interface.  We will
show that there are cases which can be modeled by coercions but not by
an ``inheritance mechanism'', i.\,e.\ the concept of coercions is not
only orthogonal to the one of type classes but also to formalisms
extending type classes.  We will define certain classes of coercions
and impose conditions on important classes of coercions which will
imply that the meaning of an expression is independent of the
particular coercions that are used in order to type it.  These results
will also appear in \cite{Webe95}.

We will also impose some conditions on the interaction between
polymorphic operations defined in type classes and coercions that will
yield a unique meaning of an expression independent of the type which
is assigned to it --- if coercions are present there will very
frequently be several possibilities to assign types to expressions.

Often it is not only possible to coerce one type into another but it
will be the case that two types are actually {\em isomorphic}.  We
will show that isomorphic types have properties that cannot be deduced
from the properties of coercions and will shortly discuss other
possibilities to model type isomorphisms.

Unfortunately, there are natural examples of type isomorphisms
occurring in the area of computer algebra that have a ``problematic''
behavior.  For a major example of types having type isomorphisms that
cannot be captured by a finite set of coercions, we will provide a
proof that no such finite set can be given by proving that the
naturally associated equational theory is not finitely axiomatizable.
This example and the given proof are published by the author in
\cite{Webe05}.

We will give a semi-decision procedure for type inference for a system
having type classes and coercions and a decision procedure for a
subsystem which covers many important cases occurring in computer
algebra.  Up to now few results are known that would give a clear
dividing line between classes of coercions which have a decidable type
inference problem and classes for which type inference becomes
undecidable.  However, even in decidable cases the type inference
problem in the presence of coercions is a hard problem.  Even in cases
in which the possible coercions are rather restricted the type
inference problem was proven to be NP-hard for functional languages.

Two typing constructs which are again quite orthogonal to the previous
ones are treated in Sec.~\ref{chapothtyc}.  We will link the treatment
of {\em partial functions} in {\sf Axiom} to the one used in
order-sorted algebras and will show some problems which arise if a
seemingly more expressive solution were used.  Nevertheless, some
information is lost by the used solution and we sketch a proposal how
the lost information could be regained in certain cases.

There are important cases in which {\em types depending on elements}
arise naturally.  Unfortunately, not only type inference but even type
checking are undecidable for relevant cases occurring in computer
algebra, i.\,e.\ static type checking is not possible.  On the one
hand we will show that already types which have to be given to the
objects in standard algorithms of almost any general purpose computer
algebra program will prohibit static type checking.  On the other hand
it might be possible to restrict the types depending on elements
available to a user of a high-level user interface to classes which
have decidable type checking or even type inference problems.  We will
show that several formalisms have been developed during the last years
which might be relevant in this respect.
 
\section{Prelude}
\label{chprelude}

We will recall some definitions and facts which will be needed later.
 All of this material can be found scattered in the literature.
Moreover, we will fix the notation and will  give some
discussions of the terminology used in this thesis in
comparison to the one found in the literature.

\subsection{Terminology}

\subsubsection{Abstract Data Types}


The term {\em data type} has many informal usages in programming and
programming methodology. For instance, Gries lists seven
interpretations in \cite{Grie78}.

In this thesis we will deal with different meanings of the term {\em
abstract data type} (ADT).  On the one hand there is the meaning used
in the context of algebraic specifications as it is used e.\,g.\ in
the survey of Wirsing \cite{Wirs91}.  In this context an abstract
datatype given by a specification is a class of certain many-sorted
(or order-sorted) algebras which ``satisfy'' the specification.

On the other hand there is the usage of this term for data types whose
representation is hidden.  For instance, in the report on the language
{\sf Haskell} \cite{Huda92} the authors state ``the
characteristic feature of an ADT is that the {\em representation type
is hidden}; all operations on the ADT are done at an abstract level
which does not depend on the representation''.  The explanation given
in the glossary of the book on {\sf Axiom} \cite{Jenk92} is
quite similar:

\begin{quote}
{\bf abstract datatype} \\ a programming language principle used in
{\sf Axiom} where a datatype definition has defined in two parts: (1)
a {\em public} part describing a set of {\em exports}, principally
operations that apply to objects of that type, and (2) a {\em private}
part describing the implementation of the datatype usually in terms of
a {\em representation} for objects of the type.  Programs that create
and otherwise manipulate objects of the type may only do so through
its exports.  The representation and other implementation information
is specifically hidden.
\end{quote}

Usually the purpose of abstract data types in the sense of algebraic
specifications is for the specification of abstract data types in the
sense of the quotations given above.  However, as we will show in this
thesis, the abstract data types in the former sense can also be used
for the specification of other classes of computational objects than
abstract data types in the latter sense.

\subsubsection{Polymorphism}

Although the term {\em polymorphic function} is used in the
literature, there are usually no definitions given.

In the glossary of \cite{Jenk92} only examples of polymorphic
functions are given but no definition.  Also in the book by Aho,
et~al.\ \cite[p.~364]{Ahox86}, the term is explained by
giving examples of polymorphic functions.

In the recent survey of Mitchell \cite{Mitc91a} the author states
explicitly that he does not want to give a definition of {\em
polymorphism}, but that he will only give definitions of some
``polymorphic lambda-calculi''.

There is a distinction between {\em parametric polymorphism} and {\em
ad hoc polymorphism} which seems to go back to Strachey \cite{Stra00}
(cited after \cite{Gogu89}):

\begin{quotation}

In {\em ad hoc} polymorphism there is no simple systematic way of
determining the type of the result from the type of the
arguments. There may be several rules of limited extent which reduce
the number of cases, but these are themselves {\em ad hoc} both in
scope and in content.  All the ordinary arithmetic operations and
functions come into this category. It seems, moreover, that the
automatic insertion of transfer functions by the compiling system is
limited to this class.

Parametric polymorphism is more regular and may be illustrated by an
example. Suppose f is a function whose arguments is of type $\alpha$
and whose result is of type $\beta$ (so that the type of f might be
written $\alpha \longrightarrow \beta$, and that L is a list whose
elements are all of type $\alpha$ (so that the type of L is
$\alpha{\bf list}$).  We can imagine a function, say Map, which
applies f in turn to each member of L and makes a list of the
results. Thus Map[f,L] will produce a $\beta{\bf list}$.  We would
like Map to work on all types of list provided f was a suitable
function, so that Map would have to be polymorphic. However its
polymorphism is of a particularly simple parametric type which could
be written $(\alpha \longrightarrow \beta, \alpha{\bf list})
\longrightarrow \beta{\bf list}$, where $\alpha$ and $\beta$ stand for
any types.  

\end{quotation}

A widely accepted approach to parametric polymorphism is
the Hindley-Milner type system \cite{Hind69},
\cite{Miln78}, \cite{Dama82},
which is used in Standard ML \cite{Miln90}, \cite{Miln91},
Miranda \cite{Turn85}, \cite{Turn86} and other languages.

We will use the term parametric polymorphism in this sense.

There is no widely accepted approach to ad-hoc polymorphism.  In its
general form, we will use the word ad-hoc polymorphism and overloading
quite synonymously indicating that no restriction is imposed on the
possibility to overload an operator symbol.

However, there is a third form of polymorphism which will play a
central role in this thesis and for which an appropriate name is
missing.  It is the polymorphism which occurs when {\em categories} in
the {\sf Axiom}-terminology resp.\ {\em type classes} in the {\sf
Haskell}-terminology are used.  In \cite{Wadl88} the nice
negative formulation ``How to make {\em ad-hoc} polymorphism less {\em
ad-hoc}'' is used but no proposal for a positive name is given. When
necessary we will call the polymorphism encountered by type classes
simply {\em type-class polymorphism}.\footnote{A term like {\em
categorical polymorphism} seems to be misleading, especially since we
prefer the word type class instead of category.}

Sometimes a distinction is made between {\em polymorphic functions}
and {\em generic function calls}.  The intended meaning --- e.\,g.\ in
\cite{Fode83} --- is that {\em polymorphic} refers to functions in
which the same algorithm works on a wide range of data types, whereas
{\em generic} refers to function declarations in the language which
are resolved by different pieces of code.

However, a clear distinction can only be made if there is an untyped
language to which the typed language is reduced.\footnote{This is the
case for typed-functional programming languages which are usually
translated into the untyped lambda-calculus.  It can also be put in a
precise form that the lambda-calculus is untyped.}  On the other-hand
if typing information is used by the run-time system it does not seem
to be possible to have such a distinction.  So in the book by Aho,
et~al.\ \cite{Ahox86} no distinction is made between these terms.

Nevertheless, we will sometimes use these terms with the flavor as is
given in \cite{Fode83} when it will be clear how the language
constructs in discussion can be translated into untyped ones.

\subsubsection{Coercions}

We will assume that we have a mechanism in the language to declare
some functions between types to be {\em coercions}, i.\,e.\ conversion
functions which are automatically inserted by the system if necessary.

The usage of this terminology seems to be more or less standard, as
the definition in the book by Aho, et~al.\ \cite[p.~359]{Ahox86} shows:

\begin{quote}
Conversion from one type to another is said to be {\it implicit} if it
is to be done automatically by the compiler.  Implicit type
conversions, also called {\it coercions}, are limited in many
languages to situations where no information is lost in principle;
\end{quote}

The definitions in the glossary of the book on {\sf Axiom}
\cite{Jenk92} is quite similar:

\begin{quote}
{\bf coercion}\\ an automatic transformation of an object of one {\it
type} to an object of a similar or desired target type.  In the
interpreter, coercions and {\it retractions} are done automatically by
the interpreter when a type mismatch occurs.  Compare {\bf conversion}.
\end{quote}

\begin{quote}
{\bf conversion}\\ the transformation of an object of one {\it type}
to one of another type. Conversions that can be performed
automatically by the interpreter are called {\it coercions}. These
happen when the interpreter encounters a type mismatch and a similar
or declared target type is needed.  In general, the user must use the
infix operation ``::'' to cause this transformation.
\end{quote}

However, there are some issues which have to be clarified.  In the
following a {\em coercion\/} will always be a {\em total function}.
Although we will see that it is desirable to have injective coercions
(``no information is lost in principle'') we will not require that
coercions are injective by the definition of the term.

We will use the term {\em retraction} for non-total conversion
functions.  Our usage of this term is more general than the one in
{\sf Axiom}:
\begin{quote}
{\bf retraction} \\ to move an object in a parameterized domain back
to the underlying domain, for example to move the object 7 from a
``fraction of integers'' (domain {\sf Fraction Integer}) to ``the
integers'' (domain {\sf Integer}).
\end{quote}

In several papers --- e.\,g.\ \cite{Fuhx90}, \cite{Mitc91} --- the
term {\em subtype} is used if there is a coercion from one type (the
``subtype'') into another type (the ``supertype'').  Since the term
``subtype'' has several other meanings in the literature, we will
avoid it.  Only in our notation we will be close to that terminology
and will write $t_1 \subtype t_2$ if there is a coercion $\phi: t_1
\longrightarrow t_2$.

\subsection{General Notation}

As usual we will use {\em ``iff''} for {\em ``if, and only
if''}.\index{iff|ii}

The non-negative integers will be denoted by $\NN$. \index{
N@$\NN$|ii} The integers will be denoted by $\ZZ$ and the rationals by
$\QQ$. For $n \in \NN$ we will denote the integers modulo $n$ by
$\ZZ_n$.  We will use these symbols both for the algebras (of the
usual signatures) and the underlying sets.  Since we use these
ambiguous notations only in parts exclusively written for human beings
and not for machines, there will not be any problems.  Nevertheless, a
major part of this thesis will deal with problems which arise from
ambiguities which mathematicians usually can resolve easily.  We will
show how some of them can be treated in a clean formal way accessible
to machines, sometimes causing computationally hard problems.

The set of strings over a set $L$ --- i.\,e.\ the set of finite
sequences of elements of $L$ --- will be $L^*$,\index{ Lstar@$L^*$|ii}
where $\varepsilon$ is the empty string. \index{
epsilon@$\varepsilon$|ii}

The length of a string $s \in L^*$ will be denoted by $|s|$. \index{
"| "|@$"|\cdot"|$|ii} We will also denote the cardinality of a set $A$
by $|A|$.\index{ "| "|@$"|\cdot"|$|ii}

\subsection{Partial Orders and Quasi-Lattices}

{\bf Definition 1. (Preorder)}.
\index{preorder|ii}
\index{order!preorder|ii}
{\sl A binary relation which is reflexive and
transitive is a {\em preorder}.}

A  preorder which is also antisymmetric is a partial order.
\index{partial order|ii}
\index{order!partial|ii}

{\bf Definition 2.}
{\sl Let $\langle M ,\leq \rangle$ be a partially ordered set.
Then  $c \in M$ is a {\em common lower bound\/} of $a$ and $b$
($a,b \in M$) if  $c \leq a$ and $c \leq b$.}

{\sl Moreover, $c \in M$ is a {\em common upper bound\/} of $a$ and $b$
if  $a \leq c$ and $b \leq c$.}

{\bf Definition 3.}
{\sl Let $\langle M ,\leq \rangle$ be a partially ordered set.
Then  $c \in M$ is called the {\em infimum} of $a$ and $b$
($a,b \in M$) if $c$ is a lower bound of $a$ and $b$ and}
$$\forall d \in M : d \leq a \mbox{ and } d \leq b \Longrightarrow d \leq c.$$

{\sl Furthermore, $c$ is called the {\em supremum} of
$a$ and $b$ if it is an upper bound of $a$ and $b$ and}
$$\forall d \in M : a \leq d \mbox{ and } b \leq d \Longrightarrow c \leq d.$$

It is easy to verify that infima and suprema are unique if they exist.
By induction the infimum and the supremum of any finite subset
of a partially ordered set $\langle M ,\leq \rangle$ can be defined.

{\bf Definition 4.}
{\sl A partially ordered set $ \langle M ,\leq \rangle$ is
a {\em lower quasi-lattice\/} if  for any $a, b \in M$
$a$ and $b$ have an infimum whenever they have a common lower bound.
It is a {\em lower semi-lattice\/} if any $a,b \in M$ have an infimum.}

{\sl A partially ordered set $ \langle M ,\leq \rangle$ is
an {\em upper quasi-lattice\/} if for for any $a, b \in M$
$a$ and $b$ have a supremum whenever they have a common upper bound.
It is an {\em upper semi-lattice\/} if any $a,b \in M$ have a supremum.}

{\sl A partially ordered set $ \langle M ,\leq \rangle$ is
a {\em  quasi-lattice} if it is an upper and a lower quasi-lattice.
It is a {\em lattice} if it is both a upper and a lower semi-lattice.}

{\bf Definition 5. (Free Lower Semi-Lattices)}
\label{freesemilat}
Let $\langle M ,\leq \rangle$ be a
partially ordered set.
The {\em free lower semi-lattice on $\langle M ,\leq \rangle$}
is the following  partially ordered set $\langle F, \preceq \rangle$:
\begin{enumerate}
\item $F$ is the set of all non-empty subsets of $M$
whose elements are pairwise incomparable with respect to $\leq$.
\item If $S_1, S_2 \in F$ then
      $$ S_1 \preceq S_2 \quad \Longleftrightarrow \quad
          \forall s_2 \in S_2 \: \exists s_1 \in S_1 \, . \,
           s_1 \leq s_2.$$
\end{enumerate}

{\bf Lemma 1. (Free Lower Semi-Lattices)}
\label{lefrelosela}
{\sl Let $\langle M ,\leq \rangle$ be a partially ordered set. 
Then the free lower semi-lattice on $\langle M ,\leq \rangle$
is a lower semi-lattice.}

\begin{proof}
{\sl Let $S_1, S_2 \in F$ be arbitrary. Since $S_1 \in F$ and 
$S_2 \in F$ the chains in $S_1 \cup S_2$ with respect to $\leq$ have 
length at most 2. Let}
$$H = \{ d \in S_1 \cup S_2 \mid \exists s \in S_1 \cup S_2 \, . \,
s < d \}$$
{\sl and}
$$\overline{S}= (S_1 \cup S_2) - H.$$

Since $\overline{S}$ is not empty and contains only incomparable
elements by construction we have $\overline{S} \in F$.

We claim that $\overline{S}$ is the infimum of $S_1$ and $S_2$.

We have $\overline{S} \preceq S_1$ because for any $s \in S_1$
either $s \in \overline{S}$ or there is a $s' \in
\overline{S}$ such that $s' < s$. Similarly $\overline{S} \preceq S_2$.

Let $L \in F$ be a common lower bound of $S_1$ and $S_2$
with respect to $\preceq$, i.\,e.\ $L \preceq S_1$ and
$L \preceq S_2$.
Then for any $s \in S_1 \cup S_2$  there is an
$l \in L$ such that $l \leq s$.
Since $$\overline{S} \subseteq S_1 \cup S_2$$ 
we thus have $L \preceq \overline{S}$ by the definition
of $\preceq$.
\qed
\end{proof}

\begin{remark}
The statement given in \cite[p.~9]{Nipk91} that the union of
two sets of incomparable elements is a set of incomparable elements
and is the infimum of these sets with respect to the ordering given in
Def. 5 is false in general.  The proof of Lemma~1 shows the correct 
construction.
\end{remark}

\begin{remark}
If we define semi-lattices algebraically (see e.\,g.\
\cite[\S~6]{Grae79}),
then the free lower semi-lattice on $\langle M ,\leq \rangle$
is indeed a free  semi-lattice.
\end{remark}

{\bf Lemma 2.}
{\sl Let $\langle M ,\leq \rangle$ be a  {\em finite}
partially ordered set. Then  $\langle M ,\leq \rangle$
is a lower quasi-lattice iff it is an upper quasi-lattice.}

\begin{proof}
Let $ \langle M ,\leq \rangle$ be a lower quasi-lattice
in which $a$ and $b$ have a common upper bound. We have
to show that $a$ and $b$ have a supremum. Since the set
$$I= \{c \in M : a \leq c \mbox{ and } b \leq c \}$$
is nonempty and finite and $\langle M ,\leq \rangle$ is a lower 
quasi-lattice the infimum $c$ of $I$ exists. Clearly $c$ is the supremum
of $a$ and $b$.

The other direction is shown analogously. \qed
\end{proof}

{\bf Lemma 3.}
\label{lenolat}
{\sl Let $ \langle M ,\leq \rangle$ be a finite partially ordered set. Then
$ \langle M ,\leq \rangle$ is not a quasi-lattice
iff there are $a,b,c,d \in M$ such that
\begin{enumerate}
\item $a \leq c$ and $a \leq d$,
\item $b \leq c$ and $b \leq d$,
\item $a \not \leq b$ and $b \not \leq a$,
\item $c \not \leq d$ and $d \not \leq c$,
\item \label{lacolenolat} there is no $e \in M$ which is a common upper bound
of $a$ and $b$ and a common lower bound of $c$ and $d$.
\end{enumerate}}

\begin{figure}

\begin{center}
\unitlength=1mm
\thinlines
\begin{picture}(45.00,50.00)
\put(13.00,10.00){\makebox(0,0)[cc]{$a$}}
\put(37.00,10.00){\makebox(0,0)[cc]{$b$}}
\put(13.00,40.00){\makebox(0,0)[cc]{$c$}}
\put(37.00,40.00){\makebox(0,0)[cc]{$d$}}
\put(13.00,13.00){\vector(0,1){24.00}}
\put(13.00,13.00){\vector(1,1){24.00}}
\put(37.00,13.00){\vector(0,1){24.00}}
\put(37.00,13.00){\vector(-1,1){24.00}}
\put(9.00,25.00){\makebox(0,0)[cc]{$\leq$}}
\put(41.00,25.00){\makebox(0,0)[cc]{$\leq$}}
\put(22.00,20.00){\makebox(0,0)[cc]{$\leq$}}
\put(28.00,20.00){\makebox(0,0)[cc]{$\leq$}}
\end{picture}
\end{center}

\caption{Ad Lemma 3}
\end{figure}

\begin{proof}
Assume $\langle M, \leq \rangle$ is  a finite partially ordered set having 
elements $a,b,c,d \in M$ that satisfy the conditions of the lemma.
Since $a$ and $b$ have a common upper bound, we are done
if we can show that
they do not have a supremum. Assume towards a contradiction they had
a supremum $e$. Since $c$ and $d$ are common upper bounds
of $a$ and $b$ and $e$ is the supremum of $a$ and $b$,
we had
$e \leq c$ and $e \leq d$, a contradiction  to condition~\ref{lacolenolat} 
of the lemma.

Now let $\langle M, \leq \rangle$ be  a finite partially ordered set
which is not a quasi-lattice.
Then there are $a,b \in M$ which have a common upper bound $c$
but not a supremum. Since $M$ is finite we can assume w.\,l.\,o.\,g.\
that there is no $c' \leq c$ which is also a common upper bound
of $a$ and $b$ (if there is one, take $c'$ instead of $c$).
Since $a$ and $b$ have no supremum, there is a common upper bound
$d$ of $a$ and $b$ such that $d \not \leq c$ and $c \not \leq d$.
These elements $a, b, c, d$ satisfy the conditions of the lemma.\qed
\end{proof}

\subsection{Order-Sorted Algebras}

There is a growing literature on order-sorted algebras.  Some
comprehensive sources are the thesis of Schmidt-Schau{\ss}
\cite{Schm89}, the survey by Smolka, et~al.\ \cite{Smol89}, and the
articles by Goguen \& Meseguer \cite{Gogu89} and by Waldmann
\cite{Wald92}.  In \cite{Como90} Comon shows that an order-sorted
signature can be viewed as a finite bottom-up tree automaton.

{\bf Definition 6. (Order-Sorted Signature)}
{\sl An {\em order-sorted signature}  
\index{order-sorted signature|ii}
\index{signature!order-sorted|ii}
is a triple
$(S, \leq, \Sigma)$, where $S$
is a set of sorts, $\leq$ a preorder on \index{preorder}
$S$, and $\Sigma$ a family}
$$\{ \Sigma_{\omega,\sigma} \mid \omega \in S^*, \: \sigma \in S \}$$
{\sl of not necessarily disjoint sets of operator symbols.}

{\sl If $S$ and $\Sigma$ are finite, the signature is called finite.}

{\sl For notational convenience, we often write $f: (\omega)\sigma $
instead of $f \in \Sigma_{\omega,\sigma}$; $(\omega)\sigma$ is called
an {\em arity} \index{arity|ii} and $f: (\omega)\sigma$ a {\em
declaration}. \index{declaration|ii} The signature $(S, \leq, \Sigma)$
is often identified with $\Sigma$.  If $|\omega|=n$ then $f$ is called
a $n$-ary operator symbol.  \index{operator symbol|ii} $0$-ary
operator symbols are {\em constant symbols}.  \index{constant
symbols|ii} As in \cite{Smol89} we will assume in the following
that for any $f$ there is only a single $n \in \NN$ such that $f$ is a
$n$-ary operator symbol.}

{\sl An $\sigma$-sorted variable set is a family}
$$V= \{V_\sigma \mid \sigma \in S\}$$
{\sl of disjoint, nonempty sets. For $x \in V_\sigma$ we also 
write $x:\sigma$ or $x_\sigma$.}
\index{variable set!s-sorted@$\sigma$-sorted|ii}

In \cite{Gogu89} the following monotonicity condition
must be fulfilled by any order-sorted signature.

{\bf Definition 7.}
{\sl An order-sorted signature $(S, \leq, \Sigma)$  fulfills the
{\em monotonicity condition}, 
\index{monotonicity condition|ii}
if}
$$f \in \Sigma_{\omega_1,\sigma_1} \cap \Sigma_{\omega_2,\sigma_2}
\mbox{ and } \omega_1 \leq \omega_2 \mbox{ imply } \sigma_1 \leq \sigma_2.$$

Notice that the monotonicity condition excludes multiple declarations
of constants.  This is one of the reasons why we will not assume in
general that the order-sorted signatures we will deal with will
fulfill the monotonicity condition.

{\bf Definition 8. (Order-Sorted Terms)}
\label{defordsortterm}
{\sl The set of {\em order-sorted terms} 
\index{order-sorted terms!set of|ii}
of sort $\sigma$ freely  generated by $V$,
$T_\Sigma(V)_\sigma$, is the least set satisfying}
\begin{itemize}
\item if $x \in V_{\sigma'}$ and $\sigma' \leq \sigma$,
then $x \in T_\Sigma(V)_\sigma$
\item if $f \in \Sigma_{\omega,\sigma'}$,  $\omega = \sigma_1 \ldots \sigma_n$,
 $\sigma' \leq \sigma$ and $t_i \in T_\Sigma(V)_{\sigma_i}$ then
$f(t_1, \ldots, t_n) \in T_\Sigma(V)_\sigma$.
\end{itemize}

If $t \in T_\Sigma(V)_\sigma$ we will also write $t:\sigma$.

In contrast to sort-free terms and variables, order-sorted variables
and terms always have a sort.  Terms must be sort-correct, that is,
subterms of a compound term must be of an appropriate sort as required
by the arities of the term's operator symbol.

Note that an operator symbol may have not just one arity (as in
classical homogeneous or heterogeneous term algebras), but may have
{\em several} arities.  As a consequence, each term may have several
sorts.

The set of all order-sorted terms over $\Sigma$
freely generated by $V$ will be denoted by
$$T_\Sigma(V) := \bigcup_{\sigma \in S} T_\Sigma(V)_\sigma .$$
The set of all 
{\em ground terms} over $\Sigma$ is
$T_\Sigma := T_\Sigma(\{\})$.
\index{ground term|ii}\index{term!ground|ii}

{\bf Definition 9. (Regularity)}
{\sl A signature is {\em regular}, 
\index{signature!order-sorted!regular|see{signature!regular}}
\index{signature!regular|ii}
if the subsort preorder
of $\Sigma$ is antisymmetric, and if each term
$t \in T_\Sigma(V)$ has a least sort.}

The following theorem shows that it is decidable for finite signatures
whose subsort preorders are anti-symmetric if a signature is regular.

{\bf Theorem 1.}
\label{thregulcon}
{\sl A signature  $(S, \leq, \Sigma)$ whose subsort preorder
is anti-symmetric is regular iff for 
every $f \in \Sigma$ and $\omega \in S^*$ the set}
$$\{\sigma \mid \exists \omega' \geq \omega :
 f\in \Sigma_{\omega',\sigma} \}$$
{\sl either is empty or contains a least element.}

\begin{proof}
See \cite{Smol89}. \qed
\end{proof}

As an  example  of a simple non-regular signature, consider
$$(\{ \sigma_0, \sigma_1, \sigma_2 \}, \;
 \{ \sigma_1 \leq \sigma_0, \,  \sigma_2 \leq \sigma_0\}, \;
\Sigma_{\varepsilon,\sigma_1}={a}, \Sigma_{\varepsilon,\sigma_2}={a}).$$
The constant $a$ has two sorts which are incomparable, hence it does
not have a minimal sort.

{\bf Definition 10.}
{\sl The {\em complexity\/}
\index{complexity of a term|ii}
\index{term!complexity|ii}
 of a term $t\in T_\Sigma(V)$, $\com(t)$ is inductively
defined as follows:
\begin{itemize}
\item $\com(t)=1$, if $t \in V_\sigma$ or $t \in \Sigma_{\epsilon,\sigma}$
for some $\sigma \in S$,
\item if $f \in \Sigma_{\omega,\sigma'}$,  $\omega = \sigma_1 \cdots \sigma_n$,
and $t_i \in T_\Sigma(V)_{\sigma_i}$ then
$$\com(f(t_1,\ldots,t_n))= \max(\com(t_1), \ldots, \com(t_n)) + 1.$$
\end{itemize}}

{\bf Definition 11.}
{\sl A {\em substitution} $\theta$ \index{substitution|ii}
from a variable set $Y$ into the term algebra $T_\Sigma(V)$ is a mapping from
$Y$ to $T_\Sigma(V)$, which additionally satisfies
$\theta(x) \in T_\Sigma(V)_\sigma$ if $x \in V_\sigma$
(that is, substitutions must be sort-correct). As usual,
substitutions are extended canonically to
$T_\Sigma(V)$.
If $Y=\{x_1, \ldots, x_n\}$ we write
$\theta=\{x_1 \mapsto t_1, \ldots, x_n \mapsto t_n\}$.
If $\theta=\{x_1 \mapsto t_1\}$ and 
$t \in T_\Sigma(V)$, then we will write $t[t_1/x_1]$ for
$\theta(t)$.
If, for $t, t' \in T_\Sigma(V)$, there is a substitution
$\theta$ such that
$t'=\theta(t)$, then $t'$ is called
an {\em instance} of $t$.\index{instance|ii}
Similarly, a substitution $\theta'$ is called an
instance of a substitution
$\theta$ with respect to a set of variables $W$, written
$\theta \succeq \theta'[W]$, if there is a substitution
$\gamma$ such that 
$\theta'(x)=\gamma(\theta(x))$ for all
$x \in W$.}

{\bf Definition 12.}
{\sl A {\em unifier}\index{unifier|ii} of a set of equations
$\Gamma$ is a substitution $\theta$ such that
$\theta(s)=\theta(t)$ for all equations
$s=^{\rm ?} t$ in $\Gamma$. 
A set of unifiers $U$ of $\Gamma$ is called {\em complete}
\index{complete set of unifiers|ii}\index{unifier!complete set|ii}
(and denoted by CSU),\index{CSU|ii} if for every unifier
$\theta'$ of $\Gamma$ there exists $\theta \in U$ such that
$\theta'$ is an instance of
$\theta$ with respect to the variables in 
$\Gamma$. As usual, a signature is called
{\em unitary (unifying)},
\index{unitary unifying|ii}\index{signature!unitary unifying|ii}
\index{unifying!unitary|ii}
if for all equation sets $\Gamma$ there is a complete set of
unifiers containing at most one element;
it is called {\em finitary (unifying)},
\index{finitary unifying|ii}\index{signature!finitary unifying|ii}
\index{unifying!finitary|ii}
if there is always a finite and complete set of unifiers.}

For non-regular signatures, unifications can be infinitary, 
even if the signature is finite (see e.\,g.\
\cite[p.~326]{Smol89}, \cite[p.~26]{Wald92}).

{\bf Theorem 2. (Schmidt-Schau{\ss})}
{\sl In finite and regular signatures, finite sets of equations have
finite, complete, and effectively computable sets of unifiers.}

\begin{proof}
See \cite[Theorem~15]{Smol89}.\qed
\end{proof}

{\bf Definition 13.}
{\sl A signature $(S, \leq, \Sigma)$ is {\em downward complete}
\index{downward complete|ii}
\index{signature!downward complete|ii}
if any two sorts have either no lower bound or an infimum,
and {\em coregular}
\index{coregular|ii}
\index{signature!coregular|ii}
if for any operator symbol $f$ and any sort $\sigma \in S$
the set
$$D(f,\sigma)=\{ \omega \mid \exists  \sigma'  \in S \:.\;
f: (\omega)\sigma' \wedge \sigma' \leq \sigma\}$$
either is empty or has a greatest element.}

{\bf Definition 14.}
{\sl Let $(S, \leq, \Sigma)$  be an order-sorted signature.
It is  {\em injective} if for any operator symbol $f$
the following condition holds:
$$f: (\omega) \sigma \mbox{ and } f: (\omega') \sigma
\quad \mbox{ imply }\quad \omega = \omega'.$$

It is {\em subsort reflecting} if for any operator  symbol $f$
the following condition holds:}
$$f: (\omega') \sigma' \mbox{ and } \sigma' \leq \sigma
\quad \mbox{ imply } \quad f: (\omega) \sigma \mbox{ for some }
\omega \geq \omega'.$$

{\bf Theorem 3.}
{\sl Every finite, regular, coregular, and downward complete signature is
unitary unifying.}

\begin{proof}
See \cite[Theorem~17]{Smol89}.\qed
\end{proof}

{\bf Corollary 3A.}
{\sl Every finite, regular, downward complete, injective and subsort
reflecting signature is unitary unifying.}

\subsection{Category Theory}

We will recall some basic definitions from category theory which can
be found in many books on the topic.  Some classical textbooks are
\cite{Macl91}, \cite{Schu72}.  A more recent
textbook is \cite{Frey90}.  In \cite{Ryde88}
computational aspects are elaborated.  The basic concepts of category
theory can also be found in several books which use category theory as
a tool for computer science, e.\,g.\ in \cite{Ehri85}.

{\bf Definition 15. (Category)} 
\index{category|ii}
{\sl A {\em category} $\cat{C}$  consists of a class of
{\em objects\/} \index{objects|ii}
$\obj{\cat{C}}$, for each pair
$(A,B) \in \obj{\cat{C}} \times \obj{\cat{C}}$ a  set
$\mor{\cat{C}}{A,B}$ of {\em morphisms} (or {\em arrows}), written
$f: A \longrightarrow B$ for 
$f \in \mor{\cat{C}}{A,B}$,
and a {\em  composition}}
\index{composition|ii}
\index{  composition@$\circ$|ii}
\index{arrows!composition of|ii}
$$\begin{array}{l}
\circ: \mor{C}{A,B} \times \mor{C}{B,C} \longrightarrow \mor{C}{A,C}\\
(f: A \longrightarrow B, \; g: B \longrightarrow C)
\mapsto (g \circ f : A \longrightarrow C)
\end{array}
\index{arrows|ii} \index{morphisms|ii} \index{objects|ii}
$$
{\sl (more precisely a family of functions
$\circ_{A,B,C}$ for all objects $A,B,C$) such that the following
axioms are satisfied:
\begin{enumerate}
\item $(h \circ g ) \circ f = h \circ (g \circ f)$
\hfill {\em (associativity)} \\
for all morphisms $f,g,h$, if at least one side is defined.
\item For each object $A \in \obj{C}$ there is a morphism
${\rm id}_A \in \mor{C}{A,A}$, called 
{\em identity of $A$}, such that we have for all
$f: A \longrightarrow B$ and
$g: C \longrightarrow A$ with $B,C \in \obj{C}$\\
$f \circ {\rm id}_A = f \mbox{ and } {\rm id}_A \circ g =g$ 
\hfill {\em (identity)}.
\index{identity|ii}\index{category!identity|ii}
\index{ id@${\rm id}_A$|ii}
\end{enumerate}}

{\sl Frequently we will write}
$$A \stackrel{f}{\longrightarrow} B$$
{\sl instead of}
$$f: A \longrightarrow B .$$

{\bf Definition 16. (Opposite Category)}
\index{dual category|ii}\index{category!opposite|ii}
\index{opposite category|ii}\index{category!dual|ii}
{\sl Let $\cat{C}$ be a category. Then
$\cat{C}^{\rm op}$, the {\em opposite category} of
$\cat{C}$, is the category which is
defined by 
\begin{enumerate}
\item $\obj{C^{\rm op}}= \obj{C}$,
\item $\mor{C^{\rm op}}{A,B} =\mor{C}{B,A}$.
\end{enumerate}
Sometimes we will call $\cat{C}^{\rm op}$ the
{\em dual category} of $\cat{C}$.}

Clearly, $\cat{(C^{\rm op})^{\rm op}}= \cat{C}$.

For any categories $\cat{C}$ and $\cat{D}$ 
we will write
$\cat{C} \times \cat{D}$\index{  product@$\times$|ii}
for the category which is defined by
\begin{enumerate}
\item $\obj{C \times D}= \obj{C} \times \obj{D}$,
\item $\mor{C\times D}{(A,A'),\,(B,B')} =\mor{C}{A,B} \times 
\mor{D}{A',B'}$,
\end{enumerate}
where the symbol $\times$ on the right hand side of the equations
denotes the usual set theoretic Cartesian product.

Since $\times$ is associative, we will write
unambiguously 
$\cat{C}_1 \times \cdots \times \cat{C}_n$
for  an $n$-fold iteration.
Moreover,
$$\cat{C}^n= \underbrace{\cat{C} \times \cdots \times \cat{C}}_{n}.$$  

{\bf Definition 17. (Functors)}
\index{functor|ii}
{\sl Let $\cat{C}$ and $\cat{D}$ be categories.
A mapping $F: \cat{C} \longrightarrow \cat{D}$ is
called {\em functor}, if
$F$ assigns to each object $A$ in $\cat{C}$
an object $F(A)$ in $\cat{D}$ and to each morphism
$f: A \longrightarrow B$ in $\cat{C}$ a morphism
$F(f): F(A) \longrightarrow F(B)$ in
$\cat{D}$ such that the following axioms are satisfied:
\begin{enumerate}
\item $F(g \circ f) = F(g) \circ F(f)$ for all $g \circ f$ in
$\cat{C}$,
\item $F({\rm id}_A) = {\rm id}_{F(A)}$ for all objects $A$ 
in $\cat{C}$.
\end{enumerate}}

{\sl The composition of two functors
$F: \cat{C} \longrightarrow \cat{D}$ and
$G: \cat{D} \longrightarrow \cat{E}$ 
is defined by }
$$G \circ F (A) = G(F(A))$$
{\sl and}
$$G \circ F (f) = G(F(f))$$
{\sl for objects and morphisms respectively
leading to the {\em composite functor}
$G \circ F : \cat{C} \longrightarrow \cat{E}$.}

{\sl The {\em identical functor}
${\rm ID}_{\cat{C}}: \cat{C} \longrightarrow \cat{C}$ is
defined by 
${\rm ID}_{\cat{C}}(A) = A$ and
${\rm ID}_{\cat{C}}(f) = f$.\index{ ID@${\rm ID}_{\cat{C}}$|ii}}

{\sl A functor $F: \cat{C} \longrightarrow \cat{D}$ is
also called a {\em covariant functor}
from $\cat{C}$ into $\cat{D}$.\index{covariant!functor|ii}
\index{functor!covariant|ii}
A functor $F: \cat{C}^{\rm op} \longrightarrow \cat{D}$ is
 called a {\em contravariant functor} from 
$\cat{C}$ into $\cat{D}$.}
\index{contravariant!functor|ii}
\index{functor!contravariant|ii}

{\bf Definition 18. (Natural Transformations)}
\index{natural transformation|ii}
\index{transformation!natural|ii}
{\sl Let $S, T :\cat{C} \longrightarrow \cat{D}$
be functors.
A {\em natural transformation} $\tau: S \longrightarrow T$ 
is a mapping which assigns to any
object $A$ in $\cat{C}$ an arrow
$\tau_{A}=\tau (A) : S(A) \longrightarrow T(A)$
such that for any arrow
$f: A \longrightarrow B$ in $\cat{C}$ the diagram}
\begin{center}
\square[S(A)`T(A)`S(B)`T(B);\tau(A)`S(f)`T(f)`\tau(B)]
\end{center}
{\sl is commutative.}

{\bf Defintion 19. (Initial Objects)}
{\sl Let $\cat{C}$ be a category.
An object $I \in \obj{C}$ is {\em initial in $\cat{C}$}
if for any object $A \in \cat{C}$ there is
a unique morphism $f \in \mor{C}{I,A}$.
If the category $\cat{C}$ is clear from the
context, then it is simply said that
$I$ is an {\em initial object}.}

If an initial object exists in a category, it is uniquely determined
up to isomorphism.

\subsection{The Type System of Axiom}

The type system of {\sf Axiom} consists of three levels:

\begin{enumerate}
\item elements,
\item domains,
\item categories.
\end{enumerate}

The elements belong to domains, which correspond to types in the
traditional programming terminology.

Domains are built by {\em domain constructors}, which are functions
having the following sort of parameters: elements, or domains of a
certain category.  Any domain constructor has a {\em category
assertion} part which asserts that for any possible parameters of the
domain constructor the constructed domain belongs to the categories
given in it.

\subsubsection{Categories}

Also categories are built by category constructors which are functions
having elements or domains as parameters.

An important subclass is built by the categories which are built by
category constructors having no parameters.  They are called the {\em
basic algebra hierarchy} in \cite{Jenk92} and consist up to now of 46
categories.

As is stated in \cite[p.~524]{Jenk92} the case of elements as
parameters of category constructors is rare.

In the definition of a category there is always a part in which the
categories are given the defined category extends.\footnote{The
category which is extended by all other categories and which does not
extend any other category is predefined and is called {\tt Type}.}

An important component of the definition of a category is the
documentation. There is even a special syntax for comments serving as
a documentation in contrast to other kinds of comments.  The
importance of having a language support for the documentation as well
as for the implementation of an algorithm is also clearly elaborated
in the design of the algorithm description language {\sf ALDES}
\cite{Loos72}, \cite{Loos76}, \cite{Loos92}, and in the
implementation of the SAC-2 library (see e.\,g.\ \cite{Coll90},
\cite{Buen91}).

The {\em axioms} which a member of a category has to fulfill are
stated in the comment only and there is no mechanism for an automated
verification provided yet.  There is a mechanism to declare some
equationally definable axioms as so called {\em attributes} which can
be used explicitly in the language.  However, the attributes can be
used only directly.  A theorem proving component is not included in
the system.

Some operations in a category definition can have {\em default}
declarations, i.\,e.\ algorithms for algorithmically definable
operations can be given. These default declarations can be overwritten
by special algorithms in any instance of a category.


There are two syntactic declarations which reduce the number of
category declarations which have to be given considerably.

Using the keyword {\tt Join} a category is defined which has all
operations and properties of the categories given as arguments to {\tt
Join}.

Instead of defining different categories ${\cal C}_1$ and ${\cal C}_2$
and to declare that ${\cal C}_2$ extends ${\cal C}_1$ it is possible
to define ${\cal C}_1$ and to use the so called {\em conditional
phrase} {\tt has} in the definition of ${\cal C}_1$ to give the
additional properties of ${\cal C}_2$.

\begin{figure}
\rule{\textwidth}{0.1pt}
%\begin{progverb}
\begin{footnotesize}
\begin{verbatim}
++ the class of all multiplicative semigroups
++ Axioms
++ .  associative("*":($,$)->$)            ++ (x*y)*z = x*(y*z)
++ Common Additional Axioms
++ .  commutative("*":($,$)->$)            ++ x*y = y*x
SemiGroup(): Category == SetCategory with
    --operations
      "*": ($,$) -> $
      "**": ($,PositiveInteger) -> $

    add
      import RepeatedSquaring($)
      x:$ ** n:PositiveInteger == expt(x,n)
\end{verbatim}
\end{footnotesize}
%\end{progverb}
\rule{\textwidth}{0.1pt}

\caption{An example of a category definition in {\sf Axiom}}
\label{figaxsegcat}
\end{figure}

\subsubsection{Coercions}

In {\sf Axiom} it is possible to have coercions between domains.
Syntactically, an overloaded operator symbol {\tt coerce} is used for
the definition of the coercion functions.  There seems to be no
restriction on the functions which can be coercions. So also partial
functions can be coercions in {\sf Axiom} in contrary to the usage of
the term in this chapter.

\section{Type Classes}
\label{chtycla}

In the main part of this section we will deal with language constructs
which correspond to {\em categories} of {\sf Axiom} obeying the
restriction of having no parameters.  In Sec.~\ref{chparamtycl} we
will discuss the case of categories with parameters.

The momentarily occurring examples of such categories are given as the
``basic algebra hierarchy'' on the inner cover page of the book on
{\sf Axiom} \cite{Jenk92}.  They consist of 46 categories.  The
maximal length of a chain in the induced partial order is 15.

These categories correspond to type classes of {\sf Haskell}, cf.\
Fig.~\ref{figaxord} and Fig.~\ref{fighasord}.  We will often use the
term {\em type class} --- which seems to be preferable --- instead of
non-parameterized category.

In \cite[Appendix~A]{Webe92b} the author has shown that almost all of
the examples of types occurring in the specifications of the {\sf
SAC-2} library (see e.\,g.\ \cite{Coll90}, \cite{Buch93}) can be
structured by using the language construct of type classes.

We will also assume that all domain constructors have only domains as
parameters, and not elements of other domains.  We will discuss the
extension of having elements of domains as parameters in
Sec.~\ref{chtydeel}.

\begin{figure}[h]
\rule{\textwidth}{0.1pt}
%\begin{progverb}
\begin{footnotesize}
\begin{verbatim}
class  (Eq a) => Ord a  where
  (<), (<=), (>=), (>):: a -> a -> Bool
  max, min            :: a -> a -> a

  x <  y              =  x <= y && x /= y
  x >= y              =  y <= x
  x >  y              =  y <  x

  -- The following default methods are appropriate for partial orders.
  -- Note that the second guards in each function can be replaced
  -- by "otherwise" and the error cases, eliminated for total orders.
  max x y | x >= y    =  x
          | y >= x    =  y
          |otherwise  =  error "max{PreludeCore}: no ordering relation"
  min x y | x <= y    =  x
          | y <= x    =  y
          |otherwise  =  error "min{PreludeCore}: no ordering relation"
\end{verbatim}
\end{footnotesize}
%\end{progverb}
\rule{\textwidth}{0.1pt}

\caption{Definition of partially ordered sets in the
{\sf Haskell} standard prelude}
\label{fighasord}
\end{figure}

\begin{figure}[h]
\rule{\textwidth}{0.1pt}
%\begin{progverb}
\begin{footnotesize}
\begin{verbatim}
++ Totally ordered sets
++ Axioms
++ . a<b or a=b or b<a (and only one of these!)
++ . a<b and b<c => a<c
OrderedSet(): Category == SetCategory with
  --operations
    "<": ($,$) -> Boolean    ++ The (strict) comparison operator
    max: ($,$) -> $          ++ The maximum of two objects
    min: ($,$) -> $          ++ The minimum of two objects
  add
  --declarations
    x,y: $
  --definitions
  -- These really ought to become some sort of macro
    max(x,y) ==
      x > y => x
      y
    min(x,y) ==
      x > y => y
      x
\end{verbatim}
\end{footnotesize}
%\end{progverb}
\rule{\textwidth}{0.1pt}

\caption{Definition of totally ordered sets
in {\sf Axiom}}
\label{figaxord}
\end{figure}

\subsection{Types as Terms of an Order-Sorted Signature}

The idea of describing the types of a computer algebra system as terms
of an order-sorted signature can also be found in the work of Rector
\cite{Rect89} and Comon,\ et~al.\ \cite{Como91}.  The idea of
describing the type system of {\sf Haskell} using order-sorted terms
is due to Nipkow and Snelting \cite{Nipk91}.

However, the combination of ideas found in these papers is new and
gives a solution to an important class of type inference problems
occurring in computer algebra.

In the following a {\em type} will just be an element of the set of
all order-sorted terms over a signature $(S, \leq,\Sigma)$ freely
generated by some family of infinite sets
$V=\{ V_\sigma \mid \sigma \in S\}$.

The sorts correspond to the non-parameterized categories, the basic
algebra hierarchy.  The order on the sorts reflects the inheritance
mechanism of categories.

The sets $V_\sigma$ are  the sets of {\em type variables}.

A type denoted be a ground term is called a {\em ground type}, a
non-ground type is called a {\em polymorphic type}.  Polymorphic types
correspond to the {\em modemaps} of {\sf Axiom}.

A type denoted by a constant symbol will be called a {\em base
type}. So base types correspond to domains built by domain
constructors without parameters.  (Typical examples are \tf{integer},
\tf{boolean}, \ldots)

The non-constant operator symbols are called {\em type constructors}.
The domain constructors of {\sf Axiom} which have only domains as
parameters can be described by type constructors.

We will use
$$\begin{array}{l}
\tf{list}: (\cf{any}) \cf{any}\\
\tf{list}: (\cf{ordered\_set}) \cf{ordered\_set}\\
\tf{UP}: (\cf{commutative\_ring} \; \cf{symbol}) \cf{commutative\_ring} \\
\tf{UP}: (\cf{integral\_domain} \; \cf{symbol}) \cf{integral\_domain}\\
\tf{FF}: (\cf{integral\_domain}) \cf{field}
\end{array}$$
as typical examples, where $\tf{UP}$ builds univariate polynomials in
a specified indeterminate of a commutative ring, and $\tf{FF}$ the
field of fractions of an integral domain.

Notice the use of multiple declarations, which can be achieved in
{\sf  Axiom} using the conditional phrase {\tt has}.

In the following we will sometimes assume that we have a {\em
semantics} for the {\em ground types} which satisfies the following
conditions:
\begin{itemize}
\item The ground types correspond to mathematical objects
in the sense of universal algebra or model theory
(A comprehensive reference for universal algebra is \cite{Grae79},
for model theory \cite{Chan90}).
\item Functions between ground types are set theoretical
functions. If we say that two functions $f,g: t_1 \longrightarrow t_2$
are equal ($f=g$) we mean equality between them as set theoretic
objects.
\end{itemize}

Since we only need a set theoretic semantics for {\em ground types}
and functions between ground types, the obvious interpretations of the
types as set theoretic objects will do.\footnote{All objects
corresponding to ground types one is interested in computer algebra
can be given such a set theoretic interpretation.  In other areas,
e.\,g.\ in the context of the lambda calculus \cite{Bare84} this
is not always the case.  Nevertheless, this is not a real problem for
our work, since our approach is primarily concerned with the situation
arising in computer algebra.}

Of course, equality between two functions will be in general an
undecidable property, but this will not be of importance in the
following discussion, since we will always give some particular
reasoning for the equality of two functions between two types.

We will also deal with polymorphic types in the following.  However,
it will not be necessary to have a formal semantics for the
polymorphic types in the cases we will use them.  Giving a semantics
to polymorphic types can be quite difficult.  So the one given in
\cite{Como91} applies to fewer cases than the ones we are
interested in.  In general, it is possible that no ``set-theoretic
semantics'' can be given to polymorphic types, as was shown by
Reynolds \cite{Reyn84} for the objects of the second-order
polymorphic lambda-calculus.

\subsubsection{Properties of the Order-Sorted Signature of Types}
\label{secproossty}

The possibility to have multiple declarations of type constructors is
used in {\sf Axiom} frequently.  Syntactically it is achieved by a
conditional phrase involving {\tt has}.\footnote{In {\sf Axiom}
conditional phrases are used also for other purposes. So it might be
useful to use different syntactic concepts instead of one.}

Also constant symbols, i.\,e.\ base types, have usually multiple
declarations, e.\,g.\ it is useful to declare $\tf{integer}$ to be an
$\cf{integral\_domain}$ and an $\cf{ordered\_set}$.  So the
monotonicity condition cannot be assumed in general.  However, for the
purposes of type inference (see below) this condition is not needed.

As is shown in \cite[Sec.~5]{Nipk91} it can be assumed that
the signature is regular\footnote{At least if the signature is
finite.} and downward complete if one allows to form the
``conjunction'' $\sigma_1 \wedge \sigma_2$ of sorts $\sigma_1$ and
$\sigma_2$.  This conjunction has to fulfill the following conditions:

\begin{enumerate}
\item $\sigma_1 \wedge \sigma_2$ has to be the meet of $\sigma_1$ and
$\sigma_2$ in the free lower semi-lattice on the partially ordered set
$\langle S, \leq \rangle$ (cf.\ Def. 5).
\item If a type constructor $\chi$ has declarations $\chi: (\gamma_1
\cdots \gamma_n) \gamma$ and $\chi: (\delta_1 \cdots \delta_n) \delta$
then it also has a declaration $$\chi: (\gamma_1 \wedge \delta_1 \:
\cdots \gamma_n \wedge \delta_n) \gamma \wedge \delta.$$
\end{enumerate}

Using {\tt Join} there is a possibility to form such conjunctions of
sort having the required properties in {\sf Axiom}.

\begin{remark} Maybe the choice of the name {\tt Join} in {\sf Axiom}
is somewhat misleading.  Although the {\tt Join} of two categories
gives a category having the union of their operations, this category
is nevertheless corresponding to the {\em meet} of the corresponding
sorts in the lower semi-lattice of sorts of the order-sorted signature
of types.  We cannot simply reverse the order on the sorts.  If a type
belongs to the join of two categories ${\cal A}$ and ${\cal B}$ we can
conclude that it belongs to ${\cal A}$ (or ${\cal B}$) but not vice versa!  
\end{remark}

For the purpose of type inference it would be nice if the signature is
unitary unifying.  This is the case for regular and downward complete
signatures if they are also {\em coregular}.  However, we do not know
whether a restriction implying coregularity is reasonable in the
context of a computer algebra system.

Nipkow and Snelting \cite{Nipk91} have argued that {\sf Haskell}
enforces that the order-sorted signatures are injective and subsort
reflecting which also imply that the signature is unitary unifying.

%These assumptions seem to be problematic in the context
%of computer algebra as the following example shows.
%Consider the
%type constructor
%$\tf{FF}$  building the field of fractions of an integral domain.
%Then the following declarations --- which reflect certain
%mathematical facts --- 
%would contradict the assumption that the
%signature is {\em injective}:

An example of a declaration which would prohibit that the signature is
{\em injective} is the following.  Consider the type constructor
$\tf{FF}$ building the field of fractions of an integral domain.  Then
the declarations
$$\begin{array}{l}
\tf{FF}: (\cf{integral\_domain}) \cf{field}\\
\tf{FF}: (\cf{field}) \cf{field}
\end{array}$$
correctly reflect certain mathematical facts.  Although it does not
seem to be necessary in this example to have the second declaration we
do not know whether there is an ``algebraic'' reason which implies
that declarations violating injectivity are not necessary.  So this
point might deserve further investigations.

\subsubsection{Definition of Overloaded Functions}

The formalism developed above is well suited to express the
overloading which can be performed by category definitions.

A declaration such as
%\begin{progverb}
\begin{footnotesize}
\begin{verbatim}
AbelianSemiGroup(): Category == SetCategory with
    --operations
      "+": ($,$) -> $            ++ x+y computes the sum of x and y
      "*": (PositiveInteger,$) -> $
\end{verbatim}
\end{footnotesize}
%\end{progverb}
would translate into 
$$\begin{array}{l}
\tf{+}: \forall t_{\cf{AbelianSemiGroup}} \, .\,
 t_{\cf{AbelianSemiGroup}} \times t_{\cf{AbelianSemiGroup}}
\longrightarrow t_{\cf{AbelianSemiGroup}},\\
\tf{*}: \forall t_{\cf{AbelianSemiGroup}} \, .\,
 \tf{PositiveInteger} \times t_{\cf{AbelianSemiGroup}}
\longrightarrow t_{\cf{AbelianSemiGroup}},
\end{array}
$$
where $t_{\cf{AbelianSemiGroup}}$ is a type variable of sort
$\cf{AbelianSemiGroup}$.  It is bounded by the universal quantifier
which has to be read that $t_{\cf{AbelianSemiGroup}}$ may be
instantiated by an arbitrary type of sort $\cf{AbelianSemiGroup}$.
This is just what we want.  So the definition of categories resp.\
type classes can be seen as a syntactic mechanism to give such
declarations of overloaded operators.  The mechanism to declare that a
category extends others can be simply modeled by the order relation on
the sorts in the order-sorted algebra of types --- if there are no
parameters in category definitions.\footnote{The inheritance mechanism
is certainly convenient for such a large system as {\sf Axiom} --- as
we have mentioned before, even the basic algebra hierarchy consists of
46 categories with chains of maximal length of 15 ---, although it can
be questioned whether it is really necessary, cf.\
\cite{Chen92}.}

An advantage of the syntactic form of type classes declarations is
certainly that the general declaration of the overloaded operators and
possible {\em default declarations} are collected in one piece of
code.  This collection improves readability and makes clear which
operators can have defaults and which cannot.

The value of default declarations may not be underestimated. They are
a good way to support rapid prototyping and will become more important
the bigger a system grows. They support the possibility to obtain
algorithms over new structures quite easily.  Since it is always
possible to ``overwrite'' a default operation by a more special and
efficient one their existence does not contradict the goal of having
algorithms which are as efficient as possible.

\newsavebox{\fodauxa}
\newsavebox{\fodauxb}
\newsavebox{\fodauxc}
\newsavebox{\fodauxd}
\newsavebox{\fodauxe}
\newsavebox{\fodauxf}
\newsavebox{\fodauxg}
\newsavebox{\fodauxh}
\sbox{\fodauxa}{
\begin{minipage}[l]{0.2\textwidth}
\hfil \large \sf plus \\
\it integer \\
polynomial \\
matrix 
\end{minipage}
}
\sbox{\fodauxb}{
\begin{minipage}[l]{0.2\textwidth}
\hfil \large \sf minus \\
\it integer \\
polynomial \\
matrix 
\end{minipage}
}
\sbox{\fodauxc}{
\begin{minipage}[l]{0.2\textwidth}
\hfil \large \sf times \\
\it integer \\
polynomial \\
matrix 
\end{minipage}
}
\sbox{\fodauxd}{
\begin{minipage}[l]{0.2\textwidth}
\hfil \large \sf divide \\
\it integer \\
polynomial \\
matrix 
\end{minipage}
}
\sbox{\fodauxe}{
\begin{minipage}[l]{0.2\textwidth}
\hfil \large \sf integer \\
\it plus \\
minus \\
times \\
divide 
\end{minipage}
}
\sbox{\fodauxf}{
\begin{minipage}[l]{0.2\textwidth}
\hfil \large \sf polynomial \\
\it plus \\
minus \\
times \\
divide 
\end{minipage}
}
\sbox{\fodauxg}{
\begin{minipage}[l]{0.2\textwidth}
\hfil \large \sf matrix \\
\it plus \\
minus \\
times \\
divide 
\end{minipage}
}

\sbox{\fodauxh}{
\begin{minipage}[l]{0.99\textwidth}
\begin{center}
\fbox{\usebox{\fodauxa}}\quad\quad\fbox{\usebox{\fodauxb}}\\
\vspace{1cm}
\fbox{\usebox{\fodauxc}}\quad\quad\fbox{\usebox{\fodauxd}}\\
\vspace{1cm}
{\large\it Operation-Centered}\\
\vspace{2cm}
\fbox{\usebox{\fodauxe}}\quad \fbox{\usebox{\fodauxf}}\quad
 \fbox{\usebox{\fodauxg}}\\
\vspace{1cm}
{\large\it Type-Centered}
\end{center}
\end{minipage}
}

\begin{figure}
\framebox[\textwidth][l]{
\hfil\usebox{\fodauxh}
}

\caption{Some terminology from Foderaro's thesis}
\label{figfodterm}
\end{figure}

\label{oopfod}

In his thesis \cite{Fode83} Foderaro distinguishes between an
``operation centered'' method and a ``type centered'' or
``object-oriented'' view of organizing data (cf.\
Fig.~\ref{figfodterm}) and argues why the type-centered approach has
to be preferred.

However, in our formalism these two views are essentially equivalent.
There is a translations of a declaration of a type class --- say
$\cf{Ring}$ --- and an instantiation of it --- say with $\tf{integer}$
--- with operations $\tf{integer\_plus}$ and $\tf{integer\_times}$
into declarations
$$\begin{array}{l}
\tf{+}: \forall t_{\cf{Ring}} \, .\,
 t_{\cf{Ring}} \times t_{\cf{Ring}}
\longrightarrow t_{\cf{Ring}},\\
\tf{*}: \forall t_{\cf{Ring}} \, .\,
t_{\cf{Ring}}  \times t_{\cf{Ring}}
\longrightarrow t_{\cf{AbelianSemiGroup}},
\end{array}
$$
whereas it can be deduced by a type inference algorithm that
$\tf{integer\_plus}$ has to be used for $\tf{+}$ if $t_{\cf{Ring}}$ is
instantiated with the type constant $\tf{integer}$.  We will present
this inference algorithm in the next section.

\subsection{Type Inference}
\label{sectytycl}

In the following we will show that the type inference problem is
decidable.  We will sketch the proof which is due to Nipkow and
Snelting \cite{Nipk91} because of its importance also for
computer algebra.

In Sec.~\ref{secaxhasex} we will give some examples in which the {\sf
Axiom} type inference mechanism fails whereas in {\sf Haskell} a type
can be deduced.

\subsubsection{Type Inference Rules of Mini-Haskell}

In Fig.~\ref{figtyns} the type inference rules for the language
Mini-Haskell of Nipkow and Snelting \cite{Nipk91} are given.  This
language includes the central typing concepts of {\sf Haskell} but is
well suited for theoretical investigations since it is very small.
Many useful properties of an actual programming language can be seen
as ``syntactic sugar'' for the purpose of the type inference problem.

Mini-Haskell can only handle unary functions.  However, in this
assumption there is no loss in generality.  Since Mini-Haskell has
higher order-functions, a function of type
$$\tau_1 \times \tau_2 \longrightarrow \tau_3$$
can be  expressed by a function having type
$$\tau_1 \longrightarrow (\tau_2 \longrightarrow \tau_3),$$
a technique usually called {\em currying}.\footnote{After Haskell
B. Curry who has used this technique in his work on Combinatory
logic. Historically, already Sch\"onfinkel has used it in
\cite{Scho24}.}

The language does not have explicit recursion or pattern matching.
Although these are important properties of a programming language,
there is no loss in generality in the type inference problem if we
exclude them from the language.  There are well known translations of
pattern matching into expressions of the lambda-calculus, see e.\,g.\
\cite{Jone87}.  In principle, recursion can be expressed using
fixpoint combinators which only requires to have certain appropriately
typed functional constants (see e.\,g.\ \cite{Leis87}).

\begin{remark}
Having explicit recursion and some special typing rules for recursion
gives the possibility to assign typing to some recursive programs
which would be ill-typed otherwise (see e.\,g.\ \cite{Kfou88},
\cite{Tiur90}). However, in some of these systems type inference
becomes undecidable \cite{Tiur90}, \cite{Kfou93}.
\end{remark}

\begin{remark}
The so called ``anonymous functions'' in {\sf Axiom}
 \cite[Sec.~6.17]{Jenk92}  can simply be seen as
$\lambda$-abstracted expressions.
Since recursion can be expressed by the use of fixpoint combinators,
also $\lambda$-expressions without names can be
recursive,\footnote{Their ``names'' are bound variables!}  in contrast
to the remark in \cite[p.~168]{Jenk92}: ``An anonymous function cannot
be recursive: since it does not have a name, you cannot even call it
within itself!''
\end{remark}

In the following we will use the notation of Nipkow and Snelting
\cite{Nipk91} which has some syntactic differences to our standard
notation but should be clear from the context.  Since the type of
functions between $\tau$ and $\tau'$ has a special role in the
following there is a special notation for it and it is written as
$\tau \longrightarrow \tau'$.  The meta-variable $\chi$ ranges over
type constructors, where it is assumed that a finite set of them is
given (e.\,g.\ having $\tf{int}$, $\tf{float}$, $\tf{list}(\alpha)$,
$\tf{pair}(\alpha,\beta)$ as members, as in \cite{Nipk91}).

Formally, a typing hypothesis $A$ is a mapping from a finite set of
variables to types.  We will write
$$A+[x\mapsto \tau]$$ 
for the mapping which assigns $\tau$ to $x$ and is equal to $A$ on
${\rm dom}(A) - \{x\}$.\footnote{If $x \in {\rm dom}(A)$ its value
will be ``overwritten''.}  For signatures, the notation
$$\Sigma + \chi: (\overline{\gamma_n})\gamma$$ 
just means that a declaration  $\chi: (\overline{\gamma_n})\gamma$ 
is added to $\Sigma$.

\begin{figure}[t]
\newsavebox{\fignsaux}
\sbox{\fignsaux}{
\begin{minipage}[l]{0.995\textwidth}
$$\begin{array}{ll}
{\rm TAUT} & \frac{\displaystyle A(x) \succeq_\Sigma \tau}{\displaystyle (A, \Sigma) \vdash
                       x: \tau} \\
\\
{\rm APP}           & \frac{\displaystyle (A, \Sigma) \vdash  e_0 :
\tau \longrightarrow \tau' \quad (A, \Sigma) \vdash  e_1 }{\displaystyle (A, \Sigma) \vdash 
\tau' }\\
\\
{\rm ABS}           & \frac{\displaystyle (A+[x\mapsto \tau], \Sigma) 
\vdash  e : \tau}{\displaystyle
 (A, \Sigma) \vdash \lambda x.e: \tau \longrightarrow \tau}\\
\\
{\rm LET} & \frac{\displaystyle (A, \Sigma) \vdash  e_0 : \tau \: 
FV(\tau,A)=\{ \alpha_{\gamma_1}, \ldots , \alpha_{\gamma_k} \} \: 
(A+[x\mapsto \forall \overline{\alpha_{\gamma_k}}.\tau], \Sigma)
\vdash e_1 : \tau'}{\displaystyle (A, \Sigma)\vdash {\bf let} \: x= e_0 \: {\bf in} \:
e_1 : \tau} \\
\\
{\rm CLASS} &\begin{array}{l} 
(A, \Sigma) \vdash {\bf class} \: \gamma \leq \gamma_1, \ldots, \gamma_n \:
{\bf where} \: x_1: \forall \alpha_\gamma .\tau_1, \ldots, x_k: \forall
\alpha_\gamma . \tau_k : \\
\quad\quad (A + [x \mapsto \forall \alpha_\gamma .\tau_i \mid i=1..k],
\Sigma + \{\gamma \leq \gamma_j \mid j= 1..n\} ) 
\end{array} \\
\\
{\rm INST} & \frac{\displaystyle 
A(x_i) = \forall \alpha_\gamma . \tau_i \quad\quad 
(A, \Sigma) \vdash e_i: \tau_i[\chi(
\overline{\alpha_{\gamma_n}})/\alpha_\gamma] \quad\quad i=1..k
}{\displaystyle (A, \Sigma)\vdash
 {\bf inst} \: \chi : (\overline{\gamma_n})\gamma
\: {\bf where} \: x_1 = e_1, \ldots, x_k = e_k : (A, \Sigma +
 \chi: (\overline{\gamma_n})\gamma)}\\
\\
{\rm PROG} & \frac{\displaystyle 
(A_{i-1}, \Sigma_{i-1}) \vdash d_i: (A_i, \Sigma_i) \quad i=1..n
\quad\quad (A_n,\Sigma_n) \vdash e : \tau}{\displaystyle
(A_0,\Sigma_0) \vdash d_1; \ldots : d_n; e : \tau}
\end{array}$$
\end{minipage}
}

\framebox[\textwidth][l]{\usebox{\fignsaux}
}

\caption{The type inference rules for Mini-Haskell of
Nipkow \& Snelting}
\label{figtyns}
\end{figure}

In Fig.~\ref{figtyns} the following conventions are used.
$\overline{\alpha_{\gamma_n}}$ denotes the list
$\alpha_{\gamma_1}, \ldots, \alpha_{\gamma_n}$, 
with the understanding that the
$\alpha_{\gamma_i}$ are distinct type variables.  The first four rules
in the type inference system in Fig.~\ref{figtyns} are almost
identical to the rules of Damas and Milner for {\sf ML} typing
\cite{Dama82}.  There are two differences: all inferences
depend on the signature $\Sigma$ of the type algebra as well as the
set of type assumptions $A$.  Furthermore, generic instantiation in
rule {\rm TAUT} must respect $\Sigma$.  This is written
$\sigma \succeq_\Sigma \tau$ 
meaning that $\sigma$ has the form
$\forall \overline{\alpha_{\gamma_n}}. \tau_0$, 
there are $\tau_i$ of sort
$\gamma_i$ and $\tau=\tau_0[\tau_1/\alpha_{\gamma_1}, \ldots
,\tau_n/\alpha_{\gamma_n}]$.  The notation $FV(\tau)$ denotes the set
of free type variables in $\tau$; $FV(\tau,A)$ denotes $FV(\tau) - FV(A)$.

If no class and instance declarations are present, every type
constructor has the topmost sort as arity.

For a detailed discussion of the rules we refer to \cite{Nipk91}.
Notice that rule CLASS has no premises.  The symbol ``$:$'' has two
different meanings.  On the one hand it assigns a type to an
expression or a program. On the other hand it assigns a pair
consisting of a typing hypothesis and a signature to a {\bf class}- or
{\bf inst}-declaration.

We have presented the simpler form of the type inference system as can
be found in \cite{Nipk91}.  A problem is that the obtained
order-sorted signature $\Sigma$ need not be regular. However, if we
allow the formation of the conjunction of two sorts ---- which
corresponds to the {\tt join} of two categories in {\sf Axiom} ---
then the signature can be made regular (and downward complete).  So we
can assume w.\,l.\,o.\,g. that the signature is regular, omitting for
simplicity the slightly more complicated type inference rules for the
system handling these conjunctions of sorts.  For more details we
refer to \cite{Nipk91}.

The main result of \cite{Nipk91} can be stated in the following form.

{\bf Theorem 4. (Nipkow and Snelting)}
\label{decMH}
The type inference problem for Mini-Haskell can be 
effectively reduced to the computation of order-sorted unifiers
for a regular signature.
It is thus decidable and there is a finite set
of principal typings. If the signature is unitary unifying, then
there is a unique principal type.

%One of the main points is rule
%(ABS) in which {\bf declaration} ....
%only overloaded functions as instances of type classes

\subsubsection{Types of Functions}
\label{secaxhasex}

In this section we want to show that the above results on the type
system for {\sf Haskell} would allow an extension of the type system
of {\sf Axiom}.

\begin{figure}[t]
\rule{\textwidth}{0.1pt}
%\begin{progverb}
\begin{footnotesize}
\begin{verbatim}
->fac n == if n < 3 then n else n * fac(n-1)
                                                           Type: Void
->fac 10
   (2)  3628800
                                                Type: PositiveInteger
->g x == x + 1
                                                           Type: Void
->g 9
   Compiling function g with type PositiveInteger -> PositiveInteger
   (7)  10

                                                 Type: PositiveInteger
->g (2/3)
        5
   (8)  -
        3

                                                Type: Fraction Integer
 
->mersenne i== 2**i - 1
                                                            Type: Void
->mersenne

                       i
   (2)  mersenne i == 2  - 1
                                         Type: FunctionCalled mersenne
->mersenne 3
   Compiling function mersenne with type PositiveInteger -> Integer 

   (3)  7
                                                 Type: PositiveInteger
->addx  x == ((y :Integer): Integer +-> x + y)
                                                            Type: Void
>g:=addx 10
   Compiling function addx with type PositiveInteger -> (Integer -> 
   Integer) 

   (10)  theMap(*1;anonymousFunction;0;G1048;internal,502)
                                            Type: (Integer -> Integer)
\end{verbatim}
\end{footnotesize}
%\end{progverb}
\rule{\textwidth}{0.1pt}

\caption{Typing of some user-defined functions in {\sf Axiom}}
\label{figusdeffuax}
\end{figure}

\begin{figure}[t]
\rule{\textwidth}{0.1pt}
%\begin{progverb}
\begin{footnotesize}
\begin{verbatim}
fact 0 = 1
fact (n+1) = (n+1)*fact n
Phase TYPE:
fact :: Integral m => m -> m
\end{verbatim}
\end{footnotesize}
%\end{progverb}

\bigskip
%\begin{progverb}
\begin{footnotesize}
\begin{verbatim}
square x = x * x
Phase TYPE:
square :: Num t => t -> t
\end{verbatim}
\end{footnotesize}
%\end{progverb}

\bigskip
%\begin{progverb}
\begin{footnotesize}
\begin{verbatim}
mersenne i = 2^ i - 1
addx x    =  \y -> y+x
z::Integer
z=10
g = addx  z
h = g 3

Phase TYPE:
mersenne :: (Num tv57, Integral tv58) => tv58 -> tv57
addx :: Num tv59 => tv59 -> tv59 -> tv59
z :: Integer
g :: Integer -> Integer
h :: Integer
\end{verbatim}
\end{footnotesize}
%\end{progverb}
\rule{\textwidth}{0.1pt}

\caption{Corresponding typings in {\sf Haskell}}
\label{figusdeffuha}
\end{figure}

In {\sf Axiom} it is possible to have functions as objects, see
\cite[Sec.~6]{Jenk92} and Fig.~\ref{figusdeffuax}.  Although {\sf
Axiom} has the concept of functions as objects and it can usually
infer the type of objects, it cannot infer the type of functions.

Strictly speaking the inferred types {\tt Void} or
{\tt FunctionCalled mersenne} in Fig.~\ref{figusdeffuax}
are false, since they differ from the types when the functions are
explicitly typed by the user.

The problem seems to be that {\sf Axiom} can only infer ground types
and not polymorphic types.  For most purposes in computer algebra this
might be sufficient. However, the type of functions has to be
polymorphic in many cases.

In Fig.~\ref{figusdeffuha} it is shown that {\sf Haskell} can infer a
type for such functions.  The {\sf Haskell} syntax has to be read as
follows: {\tt Integral} is a type class to which {\tt Integer}
belongs. The typing expression for {\tt fact} has to be read as the
type of {\tt fact} is a function in one argument taking arguments of a
type in type class {\tt Integral} and returning an argument of the
same type; the type variable {\tt m} is bound in the expression and is
chosen arbitrarily.

By Theorem 4 we know that it is decidable whether there is a
typing of an expression and that there are only finitely many most
principal typings in the positive case.  As is discussed in
\cite{Nipk91} the restrictions on typings in {\sf Haskell} even imply
that there is always a single principal type. However, since we do not
know to what extend these assumptions will be justified in the area of
computer algebra, we will not claim the more special result stated in
Theorem 4.

For the purpose of this thesis we can stop at this point, since we are
interested in questions of typability and not in ones of code
generation.  A certain problem in {\sf Haskell} is that of {\em
ambiguity}.  Although all valid typings of an expression are instances
of a most general type (involving type variables) it may happen that
there is not enough information to generate code in an unambiguous
way.  Some discussions and examples of ambiguity can be found e.\,g.\
in \cite{Huda99}, \cite{Faxe02}, \cite{Nipk91}, However, since
this problem arises ``below'' the typing level, some new concepts seem
to be necessary in order to treat this problem formally, and the
author of this thesis does not know of any such formal approaches.

\subsubsection{A Possible Application of
Combining Type Classes and Parametric Polymorphism}
\label{posappcom}

As we have seen, we can extend a type system supporting type classes
with parametric polymorphism and functions as first-class citizens and
the type inference problem still remains decidable.

Such an extension of an {\sf Axiom} like type system seems to be
interesting in the area of computer algebra for several reasons.
First of all lists play an important role in computer algebra and many
typing issues related to lists are connected with parametric polymorphism.

But it seems to be possible to have some much further applications.
As is shown by Rydeheard and Burstall in \cite{Ryde88} it is possible
to encode many concepts of category theory as types in {\sf ML} and to
state several constructive properties of category theory as {\sf ML}
programs.  This encoding uses heavily the concepts of parametric
polymorphism and higher-order functions.  This formalism seems to be
very useful, although there is no perfect correspondence between the
objects of category theory and the types in {\sf ML}.\footnote{For
instance, the well-formedness of composites in a category is not a
matter of type-checking, cf.\ \cite[p.~58]{Ryde88}.  Other examples
can be found in \cite[Sec.~10]{Ryde88}.}

Now there are many well-known interactions between category theoretic
concepts and algebraic concepts, see e.\,g.\
\cite[Sec.~II.7]{Macl92} or \cite{Mane76} for interactions
of equational reasoning and category theory.  Since many concepts in
category theory are constructive, it seems to be possible to use some
of these connections in a computer algebra system.

\subsubsection{Typing of ``Declared Only'' Objects}

Consider the Axiom dialogue:
%\begin{progverb} 
\begin{footnotesize}
\begin{verbatim}
->a:Integer 
                                                            Type: Void 

->a+a a is declared as being in Integer but has not been given a value.
\end{verbatim} 
\end{footnotesize}
%\end{progverb}

Although a corresponding construct leads to a program error in {\sf
Haskell}, it could be typed by the {\sf Haskell} type inference
algorithm, if a declaration such as \verb!a: Integer! would just add
the corresponding typing assumption to the set of typing hypothesis.

Thus if we add a type declaration statement to the syntax of
Mini-Haskell\footnote{We will use {\bf has\_type} as an infix
operation in the object language for the typing declaration instead of
``:'' in order to distinguish between the object and the meta level in
rule {\rm (TYPE-AS)}.}  
$$ x\: {\bf has\_type }\: \tau,$$ 
then we simply need to add the following trivial rule to the ones given in
Fig.~\ref{figtyns}: 
$$\mbox{(TYPE-AS)} \quad\quad {\displaystyle (A,
\Sigma)\vdash x \:{\bf has\_type}\: \tau : (A+[x \mapsto \tau],
\Sigma)} $$

\subsection{Complexity of Type Inference}

\subsubsection{The ML-fragment}

The type inference problem for the simply typed lambda calculus,
i.\,e.\ the {\sf ML} core language without usage of {\tt let}
constructions reduces in linear time to a (syntactic) unification
problem. Using a representation of terms as directed acyclic graphs
(dags) the unification problem is decidable in linear time
\cite{Pate78}, and so is the type inference problem.

In \cite[p.~450]{Kane90} this result is stated in the
following precise form: 
\begin{quote} 
Given a {\tt let}-free expression $M$ of length $n$ (with all bound
variables distinct), there is a linear time algorithm which computes a
dag representation of the principal typing of $M$, if it exists, and
returns {\em untypeable} otherwise.  If it exists, the principal
typing of $M$ has length at most $2^{O(n)}$ and dag size $O(n)$.
\end{quote}

Even if {\tt let}-expressions are used, the type inference problem
remains decidable and can be solved using the Damas-Milner algorithm
\cite{Dama82}.  Unfortunately, the complexity becomes dramatically
worse.  In the worst case, doubly-exponential time is required to
produce a string output of a typing. Using a dag representation the
algorithm can be modified to run in exponential time, which is also
the proven lower (time complexity) bound of the problem (see e.\,g.\
\cite{Kane90}).

Nevertheless, {\sf ML} typing appears to be efficient in practice,
although {\tt let} expressions are frequently used in actual {\sf ML}
programs.\footnote{We refer to \cite{Kane90} for further
discussions of this point.}

\subsubsection{Complexity of Type Inference for the System of Nipkow
and Snelting}

If no {\tt let} expressions are used, then the type inference problem
for the system of Nipkow and Snelting can be reduced to an unification
problem for order-sorted terms.

This reduction is linear, so the inherent complexity of the problem is
the same as the one of corresponding unification problem.

However, the resulting signature need not be regular.  By introducing
``conjunctive sorts'' Nipkow and Snelting show how the signature can
be made regular.  This process consists of building new sorts for any
finite subset of the set of sorts introduced by the {\tt class} and
{\tt inst} declaration.  This construction is thus exponential in the
number of {\tt class} and {\tt inst} declaration of the program.

The unification problem for regular order-sorted signatures is
decidable.  However, in finite and regular signatures, deciding
whether an equation is unifiable is an NP-complete problem (see
\cite[Corollary~10]{Smol89}).

The situations is much better, if the signature is also coregular and
downward complete, since in this case unification has quasi-linear
complexity \cite[Theorem~18]{Smol89}.

Since for many programs of the system the {\tt class} and {\tt inst}
declarations are the same, the type inference problem is of feasible
complexity if the obtained signature is coregular\footnote{By
construction, it is regular and downward complete.} and we view this
signature as pre-computed.

Of course, if {\tt let} statements are used, a lower bound bound for
the complexity is exponentially.  The complexity of various type
systems for {\sf Haskell}-like overloading has been investigated in
\cite{Volp91}.

\subsection{Algebraic Specifications of Type Classes}

Many important classes of objects occurring in computer algebra can be
defined by a finite set of equations, e.\,g.\ monoids, groups, Abelian
groups, or rings.

So the corresponding type class can be specified by an algebraic
specification (see e.\,g.\ \cite{Ehri85}, \cite{Wirs91}) if we use the
class of all models of the specification as the semantics of the
specification, which is usually called the {\em loose semantics}.

\begin{remark} 
Usually, an algebraic specification is thought to specify abstract
data types in the sense {\sf Axiom} or {\sf Haskell}.  So very often
the {\em initial semantics} is used, i.\,e.\ the specified object is
the initial object in the category\footnote{Category in the category
theoretic sense!} of structures being models of the specification.  A
major advantage of this view is that many structures one is interested
in --- e.\,g.\ the rational numbers, stacks, queues, \ldots --- can be
specified by (sorted or order-sorted) equations.  A characterization
of structures which can be specified by the initial semantics can be
found in \cite{Hodg95}.
\end{remark}

So much of the work on algebraic specifications using the loose
semantics are relevant for the specification type classes.  Many
references to such work are given in the survey of
Wirsing~\cite{Wirs91}.

\subsubsection{Some Hard-to-Specify Structures}

Unfortunately, some very basic structures, namely integral domains
(and fields) cannot be specified by equations, even if we allow
equational implications.  This is a consequence of the following
simple fact.

{\bf Lemma 4.}
{\sl The class of integral domains is not closed under the formation of
products.}

\begin{proof} 
Let $A$, $B$ be two arbitrary integral domains (of
cardinality $\geq 2$).  Let $0 \neq a \in A$ and $0 \neq b \in B$.
Then $(a,0) \cdot (0,b) = (0,0)=0_{A \times B}$, i.\,e.\ the product
$A \times B$ has zero divisors.  \qed 
\end{proof}

The following well known theorem shows the problem.

{\bf Theorem 5.}
{\sl A class $V$ of algebras\footnote{Algebra in the sense
of universal algebra.} is definable by equational implications iff $V$
is closed under the formation of isomorphic images, products,
subalgebras, and direct limits.}

\begin{proof} 
See \cite[p.~379]{Grae79}. \qed 
\end{proof}

Combining these results we obtain our claim.

{\bf Corollary 5A}
{\sl The class of integral domains is not definable by equational
implications.}

Since the technique of conditional term rewriting systems handles
reasoning for equational implications (cf.\ \cite[Sec.~11]{Klop90},
\cite{Ders89}) even this powerful technique is to weak to be used as a
mechanical tool for the specification of these examples.\footnote{At
least, if we do not allow some coding of information.}

Clearly, integral domains or fields can be defined by a finite set of
first-order formulas.  Unfortunately, it is not possible to define
them by Horn clauses, which would be one of the next classes of more
powerful specification formalisms which are well known (cf.\
\cite{Wirs91}) and have a much better computational behavior than
arbitrary first-order formulas.\footnote{The success of {\sf PROLOG}
as a programming language is partly due to this fact.}

{\bf Proposition 1.}
{\sl Let ${\cal M}$ be a model-class of a first-order theory.  If ${\cal
M}$ is not closed under products, then the first-order theory of
${\cal M}$ cannot be axiomatized by a set of Horn sentences.}

\begin{proof} 
The claim follows immediately from the fact that Horn
sentences are preserved under direct products (see e.\,g.\
\cite[Prop.~6.2.2]{Chan90}).  \qed 
\end{proof}

Though most of the examples given as the ``Basic Algebra Hierarchy''
in \cite{Jenk92} can be seen as model classes of finite sets of
first-order sentences, there are some which are model classes of a set
of first-order sentences --- even if we allow infinite sets.  An
example is the category $\cf{Finite}$.

{\bf Lemma 5.}
{\sl There is no set of first-order sentences whose model
class is the class of all finite sets.}

\begin{proof} 
If a set of first-order sentences has finite models of
arbitrary large finite cardinality, then it also has an infinite
model.  \qed 
\end{proof}

\begin{remark} 
In \cite{Dave90} it is shown that there are several quite simple
operations in basic classes (such as integral domains) which cannot be
defined constructively although they can be easily specified.  So the
meaning of a certain type class given there is that of a collection of
all domains in which all the specified operations can be interpreted
constructively.  In \cite{Dave91} the technique of introducing classes
in which a operation can be defined constructively is applied to the
problem of factorization of polynomials.
\end{remark}

\subsubsection{Algebraic Theories}

So it seems to be a wise decision in the design of {\sf Axiom} to
distinguish between ``axioms'' which are only stated in comments and
give the intended meaning of an {\sf Axiom} category as a class of
algebraic structures and ``attributes'' that can be ``explicitly
expressed'' \cite[p.~522]{Jenk92}.

The parts which can be explicitly expressed by the {\sf Axiom} system
consists of equational properties only and are even a small subset of
them.  Applying the rich machinery of algebraic specifications
techniques seems to be a possibility to extend the properties that are
``explicitly expressed'' considerably.

Moreover, there are many well known specifications of structures which
are present as domains in {\sf Axiom}.  It seems to be an interesting
field of further research to clarify the interaction between
algebraically specified categories and algebraically specified
domains.

The following extension of the work of Rector \cite{Rect89} is a
first approach in this direction: Assume that only finitely many sorts
and operation symbols are used for the specification ${\cal D}$ of a
certain domain and of the specification ${\cal C}$ of a certain type
class.  We can use different semantics as the initial semantics for
the specification of the domain and the loose semantics for the
specification of the type class.  Then it can be deduced automatically
whether the domain is a member of the type class in the following way:
Generate the finitely many mappings which are potentially a view of
${\cal D}$ as ${\cal C}$ and check algorithmically whether this
mapping is a view.\footnote{We refer to \cite[p.~303]{Rect89} for
the precise definitions of the used terms.}  The possibility of giving
certain specifications an initial semantics and of giving others a
loose semantics is also built in {\sf OBJ} (cf.\ \cite{Wirs91},
\cite{Gogu92}).  The former are called {\em objects}, the
latter {\em theories} and there is the possibility to define certain
mappings as views quite in the sense of above. However, the definition
of views has ``documentation aspect''.  A verification that a given
mapping is a view is not implemented (cf.\ \cite[Sec.~4.3]{Gogu92}).

As we have seen it is not possible to specify all structures used in a
computer algebra system by equations.  Their are several possibilities
to overcome this problem: 
\begin{enumerate} 
\item Use more powerful specification techniques.  
\item Do not specify all structures 
{\em ab initio}, but take some of the structures as given.  
\end{enumerate}

The first possibility is used in \cite{Limo92}.  There the
framework of first-order logic was chosen for the specification of
structures arising in computer algebra.  However, as we have shortly
discussed, even this framework cannot handle all interesting cases.

Moreover, for an efficient system it is necessary that certain parts
of a system have to be implemented by algorithms which are not the
result of a formal specification.  So the combination of taking
certain parts as given and using equational reasoning for the formal
part whose computational behavior is much better than the one of more
powerful techniques seems to be a promising compromise between two
contradicting requirements.

Another advantage of this approach is that already much is known about
mathematical structures which can be specified in this way as e.\,g.\
the book by Manes on ``Algebraic Theories'' \cite{Mane76} shows:
\begin{quote} The program of this book is to define for a ``base
category'' ${\cal K}$ --- a system of mathematical discourse
consisting of objects whose structure we ``take for granted'' ---
categories of ${\cal K}$-objects with ``additional structure,'' to
prove general theorems about such algebraic\footnote{Here
``algebraic'' means equationally definable.}  situations, and to
present examples and applications of the resulting theory in diverse
areas of mathematics.  \end{quote}

\subsubsection{Type Classes with Higher-Order Functions}

Type inference remains decidable for a system with type classes even
if higher-order functions are allowed in the way they are in {\sf
Haskell}.  As we have shown in Sec.~\ref{posappcom} such a combination
is interesting for computer algebra systems.

In order to specify such a system algebraically it is necessary to
extend the concepts of first-order algebraic specifications techniques
with higher-order constructs.  Some investigations of such
combinations are done in \cite{Brea89a} and in \cite{Joua91}.  The
results given there show that such a combination has feasible
properties, e.\,g.\ confluence and termination properties of the
first-order part are preserved when some reasonable conditions are
fulfilled.

\subsection{Parameterized Type Classes} 
\label{chparamtycl}

In {\sf Axiom} categories can be parameterized.  The occurring
examples can be distinguished in several ways.  On the one hand there
is the distinction between domains and elements as parameters.  On the
other hand there are several other distinctions based on more
``semantical'' considerations.

Some parameterized type classes simply arise because the classes of
algebraic objects should be described as being parameterized, e.\,g.\
vector spaces over a field $K$, or more generally, left- or
right-modules over a ring $R$.

An example of a category having an element as a parameter is
%\begin{progverb} 
\begin{footnotesize}
\begin{verbatim} 
PAdicIntegerCategory(p): Category == Definition where
  ++ This is the category of stream-based representations of
  ++ the p-adic integers.
\end{verbatim}
\end{footnotesize}
%\end{progverb} 
It describes all domains implementing the $p$-adic integers for a
given integer $p$.

\label{paramtyiso} 
\sloppy This is an example of a class of categories
used quite frequently in {\sf Axiom}.  The mathematical structures
corresponding to the domains which belong to the category {\tt
PAdicIntegerCategory(p)} are all isomorphic!  The reason for
introducing such a category seems to be the following.  For different
computations it is useful to have different representations of the
$p$-adic integers in a system.  %\fussy

\label{secisomor} 
The occurrence of categories in which all members are isomorphic (seen
as mathematical structures) are not limited to categories having
elements as parameters at all.  Examples of others are

\begin{center} {\tt \begin{tabular}{l} UnivariatePolynomialCategory(R:
Ring) \\ QuotientFieldCategory(D: IntegralDomain)\\
UnivariateTaylorSeriesCategory(Coef)\\
UnivariateLaurentSeriesCategory(Coef)\\
SquareMatrixCategory(ndim,R,Row,Col) \end{tabular} } 
\end{center}

However, the case of elements as parameters for categories --- which
is claimed to be rare in \cite[p.~524]{Jenk92} --- seems to be
restricted to such categories.\footnote{This was the result of an
incomplete check of the source code of {\sf Axiom} by the author.}

It seems to be useful to treat this class of type classes by a new
concept and not only as a special case of the general one of type
classes.  The reason is the following: Formally, these type classes
correspond exactly to the concept of abstract data type in the sense
of algebraic specification as is e.\,g.\ defined by Wirsing
\cite{Wirs91}.  Since the initial and the loose semantics
coincide\footnote{We will assume that there are only at most countable
structures as members of a certain class.  Most properties we are
interested in are still valid if we look at the subclasses of classes
which consist of at most countable structures, cf.\ \cite{Hodg95}.}
the distinction between first-order and second-order types becomes a
problem.  However, such a distinction is very desirable, as we will
show below.

\subsubsection{Sequences}
\label{chapseq}

In {\sf Axiom} the operator $\tf{map}$ is defined by a simple
overloading for several cases, such as matrices, vectors, quotient
fields, \ldots

Using a parameterized type constructor $\tf{sequence}$ as in
\cite{Chen92} this form of ad-hoc polymorphism in {\sf Axiom}
could be changed to a form of type-class polymorphism.  A
parameterized category such as $\tf{HomogeneousAggregate}$ of the
``data structure hierarchy'' of {\sf Axiom} seems to have almost the
same intended meaning as $\tf{sequence}$. So it seems to be possible
even in {\sf Axiom} to define $\tf{map}$ in
$\tf{HomogeneousAggregate}$ and to have the algebraic examples as
instances.  In Sec.~\ref{s43} we will use this view in order to show
that many coercions will fulfill a condition that leads to a coherent
type system.

\subsubsection{Type Inference}

In \cite{Chen92} an extension of the type system of {\sf Haskell}
is given allowing {\em types} as arguments in type classes.  It is
then proved that the type inference problem for parameterized type
classes is decidable.

As we have argued above a restriction of category constructors to have
domains as parameters only in {\sf Axiom} does not seem to be a severe
restriction for the type system of {\sf Axiom}.  In
Sec.~\ref{undetychtydeel} we will show that not only type inference
but even type checking for a system having types depending on elements
is undecidable.  The proof of undecidability given there can be easily
applied to the case of categories having elements as parameters.  So
it seems to be useful not to allow elements as parameters for category
constructors.

A certain problem in the proof given in \cite{Chen92} is that an
entirely new technique is used which cannot be seen as an extension of
the approach of Nipkow and Snelting using order-sorted unification.
However, such an extension would be desirable.  Since we have to add
other typing constructs to the language, it is desirable to have a
well understood theory behind one aspect of the typing problem instead
of using ad-hoc approaches.

Smolka \cite{Smol88}, \cite{Smol89a} extends the framework of
order-sorted algebras by introducing functions having sorts as
parameters.  So if we were looking at category constructors which take
categories as arguments we could directly apply the results of Smolka.
However, it is not clear whether these results are also useful for the
cases we are interested in.

\subsubsection{Algebraic Specifications of Parameterized Type Classes}

As in the case of type classes, any specifications using the loose
approach can be seen as specifications of parameterized type classes.
In the survey of Wirsing \cite{Wirs91} the relevant literature is
cited.  Especially, in \cite{Wirs82} the important {\em pushout
construction} for parameterized specifications has been studied.

\subsection{Type Classes as First-Order Types}

Categories in the type system of {\sf Axiom} resp.\ type classes in
the one of {\sf Haskell} are second-order types.

By our general assumption first-order types have to correspond to
structures in the sense of model theory or universal algebra.

We will briefly discuss to what extend this assumption is justified in
various areas.

\subsubsection{Group Theory} 
\label{sgroupth} 

As the {\sf Axiom} library shows the assumption of types corresponding
to mathematical structures makes good sense for many objects of
computer algebra with the exception of group theory programs.  In a
group theory program many algorithms take certain groups as input and
return other groups --- very often subgroups --- as output.  So it is
reasonable to have the groups an algorithm works on as objects and not
as types in a program.  In this cases it seems to be more natural to
treat certain classes of groups, such as the finitely presented
groups, as a type, and not the groups themselves.  Many of the
algorithms of group theory depend on such a view of groups as objects.
In this way groups are implemented in the group theory program
{\sf GAP} \cite{GAPx17}.

Some group theoretical functions can be found in general purpose
computer algebra programs such as {\sf MAPLE} (see e.\,g.\
\cite[Sec.~4.2]{Char91a}) or {\sf Axiom} (see e.\,g.\
\cite[App.~E]{Jenk92}).  However, these are rather limited in power
and coverage compared to the special group theory programs which have
been developed in the last years ({\sf Cayley} \cite{Butl90},
{\sf GAP} \cite{GAPx17}).

The observation above shows that it is difficult to come up with a
design which can really integrate group theoretical algorithms and the
ones of other areas of computer algebra.  This problem can even be
seen within {\sf Axiom}. For instance, there are domains of
permutation groups defined in {\sf Axiom}. However, these domains are
not members of the {\sf Axiom} category $\cf{group}$!

On the other hand it would be very desirable if some results of such
group theoretic computations can be seen as types for other
computations --- such as the group of integers $\langle \ZZ, +
\rangle$ or the finite cyclic groups $\langle \ZZ_m, + \rangle$.

Of course, if types become objects, then second-order types become
first-order types.  Nevertheless, the problem which has to be solved
is that of the relationship between objects and types, and not that of
the relationship between types and type classes!\footnote{See
Chapter~\ref{chtydeel} of this thesis for further discussions.}

\subsubsection{Requirements of a System}

If types are structures, then the type classes correspond to model
classes of certain theories.  Can we assume that such model classes do
not appear as objects we will deal with?

Of course, as we have shown it makes good sense to view a type class
as an algebraic object, namely the free term-algebra of order-sorted
terms of the sort of the type class.

However, even if we model those order-sorted algebras within our
system there is no need to view type classes as first-order types, as
long as we use ``isomorphic copies'' of them.  So we can even write
e.\,g.\ a compiler or a type inference algorithm in our system using
functions defined for those algebras.

The only thing we cannot model type safe are ``run-time'' interactions
between such a compiler and an algebraic algorithm.  But having
systems which use self-modifying code is anyway contradicting the
software-engineering principles we want to support by a type system.

As we have shown in Sec.~\ref{secisomor} there are several type
classes whose members are all isomorphic.  For reasons of efficiency
it is certainly necessary to distinguish these different members and
to provide different type constructors for them, such as having a type
constructor for univariate polynomials in sparse representation and
another one for univariate polynomials in dense representation.

However, it might be useful on the level of a user interface to have
only a {\em type constructor} ``univariate polynomial'' available for
the user without forcing him to choose a particular
representation.\footnote{Contrary to a person implementing algorithms
a user may be uncertain about the advantages of a particular
representation so that the choice be the system might be better than
the one of the user.}  In this case a {\em category constructor}
univariate polynomial would become a {\em type constructor} inducing
that certain type classes become first-order types.

Nevertheless, this seems to be useful only on the level of a user
interface and seems to be restricted to cases in which the isomorphism
between the types can be implemented in the system. Since such
categories can be seen as (finite) equivalence classes in the coercion
preorder (cf.\ Sec.~\ref{chtyiso}), these equivalence classes could be
easily implemented by a new special concept. Then there would still be
a clear distinction between first-order types (which would include the
constructs describing the equivalence classes) and the second-order
types of type classes.

\subsubsection{Universal Algebra}

In universal algebra, there are constructions which would imply the
view of type classes as first-order objects.  Namely, as in
\cite[Sec.~24]{Monk76}, one can construct for a class {\bf K} of
algebras the class {\bf S\,K} of substructures, or the class {\bf
P\,K} of products or the class {\bf H\,K} of homomorphic images of
{\bf K}.\footnote{More precisely, the class of structures which are
{\em isomorphic} to substructures (or products, or homomorphic images)
of elements of {\bf K}.}  Then many theorems can be stated as an
equation, e.\,g.\ Birkhoff's theorem has the form $$\mbox{{\bf K} is a
variety iff } {\bf K} = {\bf HSP\,K}.$$ Although such a formulation is
certainly elegant, it does not seem to be really necessary.  So the
additional difficulties which arise if one has to allow that type
classes are members of the ``equality type class'' do not seem to be
justified by the practical importance of such a construction.

In model theory the possibility of imposing an algebraic structure ---
e.\,g.\ the Lindenbaum algebra --- or a topological structure on sets
of formulas is used frequently. Via the correspondence between sets of
formulas and model classes such a structure can also be imposed on a
model class making it to an algebra or a topological space. However,
since the properties on the side of the set of formulas are more
useful people work with them and not with the model classes.  Many
books on model theory can serve as references for these remarks, some
comprehensive ones are \cite{Chan90}, \cite{Poiz85}.

\subsubsection{Category Theory}

The situation is different for category theory.  An important tool for
category theory is the possibility to have a category of all
(small)\footnote{Small means that the categories are sets in a set
theory and not proper classes.} categories as objects and the functors
as arrows, or having functor categories, etc.


In this case it is not possible to have a perfect correspondence
between types and type classes in our system and the objects of
category theory.  More generally, it is not possible to have such a
perfect correspondence between the concepts of category theory and a
{\em predicative}\footnote{The word ``predicative'' refers to the fact
that a universe of types is introduced only after all of its members
are introduced.}  type-theory such as Martin-L\"of's type theory
\cite{Mart80}, as is also discussed in \cite[Sec.~10]{Ryde88}.
This is certainly a problem since impredicative
type theories might have unwanted properties.
Impredicative variants of Martin-L\"of's system 
can have an undesirable computational behavior,
as is discussed e.\,g.\ in \cite{Meye86},  
\cite{Howe87}, \cite{Coqu86}.\footnote{This problem
is discussed in the literature under the names
{\em ``Type: Type''} --- referring to the problem
whether the collection of all types is a type ---
or {\em Girard's Paradox}, since Girard has shown in his thesis
\cite{Gira72} that the original version of Martin-L\"of's type theory
allowing such constructs is inconsistent with intuitionistic
mathematics which it was supposed to model.}

So it might be preferable to have a type system which allows some
modeling of category theory but not a perfect correspondence.

\subsubsection{Bounded Polymorphism}

So in the main area of computer algebra there seems to be no need for
a concept of type classes as first-order types. So we will only sketch
some language proposals in which such a concept could be modeled.  The
main idea is to have first-order types as ``bounds'' to polymorphic
constructs.

The notion of {\em bounded quantification} was introduced by Cardelli
and Wegner \cite{Card85} in the language Fun. This proposed
language integrated Girard-Reynolds polymorphism \cite{Gira72},
\cite{Reyn74} with Cardelli's first-order calculus of subtyping
\cite{Card88}.

\begin{remark}
The so called ``second-order polymorphic $\lambda$-calculus'' was
rediscovered independently by Reynolds \cite{Reyn74} as a formalism to
express ``polymorphism'' in programming languages.  Girard has
introduced his system $F$ as a proof theoretic tool to give a
consistency proof for second-order Peano arithmetic along a line of
proof theoretic research which has originated with G\"odel
\cite{Gode58}.  A proof that all $\lambda$-terms typeable in system
$F$ are strongly normalizable and that this theorem implies the
consistency of second-order Peano arithmetic can be found in the book
by Girard, et~al.\ \cite{Gira89}.
\end{remark}

Fun and its relatives have been studied extensively by programming
language theorists and designers.  A slight modification of this
language --- called {\em minimal Bounded Fun} or $F_\leq$ --- by
Curien and Ghelli was extensively studied by Pierce in his thesis
\cite{Pier91}.  Unfortunately, the type checking problem for this
language was proven to be undecidable by Pierce \cite{Pier91},
\cite{Pier91a}.

Syntactically, types can have the form
$$\forall \alpha \leq \sigma_1 \, . \, \sigma_2,$$
where $\alpha$ is a type variable and $\sigma_1$ and $\sigma_2$  are types.
Besides the usual rules asserting reflexivity and transitivity of
$\leq$ the following rule is essential:\footnote{For a detailed
discussion of the rules we refer to the thesis of Pierce
\cite{Pier91}.}
$$\frac{\Gamma \vdash \tau_1 \leq \sigma_1
\quad\quad
\Gamma, \alpha \leq \tau_1 \vdash \sigma_2 \leq \tau_2
}{\Gamma \vdash \forall \alpha \leq \sigma_1 \, . \, \sigma_2 \:
\leq \:
\forall \alpha \leq \tau_1 \, . \, \tau_2
} \eqno\mbox{(\sc Sub-All)}$$
The expressiveness of the language\footnote{Since type checking is
undecidable, it might be too expressive.}  comes from the fact that
first-order types are bounds for type variables.  The rule
$$x \in V_{\sigma'} \mbox{ and }\sigma' \leq \sigma
\: \Longrightarrow \: x \in T_\Sigma(V)_\sigma$$
constituting a part of the definition of order-sorted terms (cf.\
Def. 8) can be seen as a special form of rule ({\sc
Sub-All}) if one would restrict the system $F_\leq$ to cases which
distinguish between two kinds of types where only one kind is allowed
to be a bound.  The typing rules for Mini-Haskell (cf.\
Fig.~\ref{figtyns}) could be simulated by the typing rules for
$F_\leq$ using a similar distinction between types.

We will not develop a formal interpretation of Mini-Haskell in
$F_\leq$ which could be done along the lines sketched above because it
is not clear yet whether the additional expressiveness of $F_\leq$ is
useful for a computer algebra system or an extension by another system
would be more appropriate.

{\bf Relation to Object-Oriented Programming} 

There has been a lot of work in the last years to show how the notions
of {\em object-oriented programming}\footnote{Some books on
object-oriented programming and languages are \cite{Meye88},
\cite{Gold83}, \cite{Kirk89}, \cite{Birt80},
\cite{Stro95}.}  can be modeled in a type safe way by using
$F_\leq$ or a related system like the so called $F$-bounded
polymorphic second-order lambda calculus \cite{Cann89}.  Some
experimental languages based on such principles are {\sf TOOPL}
\cite{Bruc93} and {\sf Quest} \cite{Card91}.

As is argued e.\,g.\ in \cite{Limo92}, \cite{Temp92} and can
be seen by a language for symbolic computation as {\sf VIEWS}
\cite{Abda86} the principles of object-oriented programming
are important tools for the design of a computer algebra system.

However, as we have shown in Sec.~\ref{oopfod} and is discussed in
more detail in \cite{Huda92}, \cite{Berg92} some important principles
of object-oriented programming already come with the use of type classes.

There are some examples --- e.\,g.\ ones related to problems of strict
versus non-strict inheritance (see e.\,g.\ \cite{Limo92},\cite{Temp92}
--- which
cannot be expressed in the type system of {\sf Axiom} and which could
be expressed using more sophisticated techniques of object-oriented
programming.  However, as we will show in Sec.~\ref{coerinstr} there
are properties of a type system which cannot be expressed by
mechanisms of object-oriented programming alone but require an
additional concept.  So it may be preferable to use a system which is
as simple as possible, even if not every example can be expressed in
it.\footnote{There seems to be one single example which is used by
several authors --- e.\,g.\ in \cite{Limo92} and in \cite{Baum95} ---
implying the need of non-strict inheritance in a computer algebra
system!}

\section{Coercions}
\label{chapcoer}

In mathematics the convention to identify an object with its image
under an embedding is used frequently.  It is certainly one of sources
of strength of mathematical notation.  Very often certain structures
are constructed as being of quite different shape and then this
convention is used to identify one with a certain subset of another
one.  Some examples which are explained in many textbooks are the
``subset relationship''
$$\NN\subseteq\ZZ\subseteq\QQ\subseteq\RR\subseteq\CC,$$ embeddings of
elements of $\QQ$ in algebraic extensions of $\QQ$ or in a $p$-adic
completion, or the embeddings of elements of a commutative ring $R$ in
$R[x]$, \ldots

If these mathematical structures correspond to types in a system and
the embeddings are computable functions, then this convention can be
modeled by the use of {\em coercions}.

While the use of implicit conversions instead of explicit conversions
might be debatable for parts of a system in which new efficient
algorithms have to be written, it is certainly necessary for a user
interface.

\subsection{General Remarks}

We will assume that we have a mechanism to declare some functions
between types to be {\em implicit coercions\/} between these types (or
simply {\em coercions}).  If there is a coercion $\phi: t_1
\longrightarrow t_2$ we will write $t_1 \subtype t_2$.

\begin{remark} The requirement of set theoretic ground types and
coercion functions excludes some constructions --- if we gave all
types the ``obvious'' set theoretic interpretation ---, as the one
used in in \cite[Lemma~2]{Mitc91}, which assumes a coercion from the
space of functions $\tf{FS}(D,D)$ over some domain $D$ into this
domain.  Such coercions which correspond to certain constructions of
models of the $\lambda$-calculus (see e.\,g.\ \cite{Bare84}) seem to
be of theoretical interest only.  At least for the purpose of a
computer algebra system the requirement of set theoretic coercion
functions does not seem to be a restriction at all!  
\end{remark}

\subsection{Coherence}
\label{seccoh}

In a larger system, it is possible that there are different ways to
have a coercion from one type into another.  Following \cite{Brea91}
and \cite{Reyn91} we will call a type system {\em coherent}, if the
coercions are independent of the way they are deduced in the
system.\footnote{Notice that the term ``coherence'' is used similarly
in category theory (see e.\,g.\ \cite{Macl91}) but is used quite
differently in connection with order-sorted algebras (e.\,g.\ in
\cite{Wald92}, \cite{Gogu92}, \cite{Rect89}).}

In the following we will look at different kinds of coercions which
occur and we will state some conditions which will yield the coherence
of the system.  Besides the technical proof of the coherence theorem
we will give some informal discussions about the significance of these
conditions.

\subsubsection{Motivating Examples}

Consider the expression $$ \tf{t} - \left( \begin{array}{cc} 1 & 0 \\
3 & \frac{1}{2} \end{array} \right) $$ which --- as a mathematician
would conclude --- denotes a $2 \times 2$-matrix over $\QQ[\tf{t}]$
where $\tf{t}$ is the usual shorthand for $\tf{t}$ times the identity
matrix.  In an {\sf Axiom} like type system, this expression involves
the following types and type constructors: The integral domain \tf{I}
of integers, the unary type constructor \tf{FF} which forms the
quotient field of an integral domain, the binary type constructor
\tf{UP} which forms the ring of univariate polynomials over some ring
in a specified indeterminate, and the type constructor $\tf{M}_{2,2}$
building the $2 \times 2$-matrices over a commutative ring.


In order to type this expression correctly several of the following
coercions have to be used.

\begin{center} \xext=1600 \yext=1200
\begin{picture}(\xext,\yext)(\xoff,\yoff) \resetparms
\setsqparms[1`-1`-1`1;1100`700]
\putsquare(0,0)[\tf{UP}(\tf{I},\tf{\tf{t}})`\tf{UP}(\tf{FF}(\tf{I}),\tf{\tf{t}})
 `\tf{I}`\tf{FF}(\tf{I});```]
\putsquare(400,400)[\tf{M}_{2,2}(\tf{UP}(\tf{I},\tf{\tf{t}}))
`\tf{M}_{2,2}(\tf{UP}(\tf{FF}(\tf{I}),\tf{\tf{t}}))
`\tf{M}_{2,2}(\tf{I})`\tf{M}_{2,2}(\tf{FF}(\tf{I}));```]
\putmorphism(110,110)(1,1)[``]{140}1b
\putmorphism(110,810)(1,1)[``]{140}1b
\putmorphism(1210,110)(1,1)[``]{140}1b
\putmorphism(1210,810)(1,1)[``]{140}1b 
\end{picture} 
\end{center}

There are different ways to coerce $\tf{I}$ to
$\tf{M}_{2,2}(\tf{UP}(\tf{FF}(\tf{I}),\tf{\tf{t}}))$.  Of course one
wants the embedding of $\tf{I}$ in
$\tf{M}_{2,2}(\tf{UP}(\tf{FF}(\tf{I}),\tf{\tf{t}}))$ to be independent
of the particular choice of the coercion functions.

In this example this independence seems to be the case, but how can we
{\em prove} it?  Moreover, not all coercions which would be desirable
for a user share this property.  Consider e.\,g.\ the binary type
constructor ``direct sum'' $\oplus$ defined for Abelian groups.  One
could coerce $A$ into $A \oplus B$ via a coercion $\phi_1$ and $B$
into $A \oplus B$ via a coercion $\phi_2$. But then the image of $A$
in $A \oplus A$ depends on the choice of the coercion function!

\subsubsection{Definition}


Relying on the set theoretic semantics for our types and coercion
functions we can give the following definition of coherence.

{\bf Definition 20. (Coherence)}
\index{coherence|ii} 
\label{defcoh} 
{\sl A type system is {\em coherent} if the following condition is satisfied:

\begin{itemize} \item[] For any ground types $t_1$ and $t_2$ of the
type system, if $\phi,\psi: t_1 \longrightarrow t_2$ are coercions
then $\phi=\psi$.  
\end{itemize}}

\subsubsection{General Assumptions}

It will be convenient to declare each identity function on a type to
be an implicit coercion.

{\bf Assumption 1.}
\label{A1} 
{\sl For any ground type $t$ the identity on
$t$ will be a coercion.  If $\phi: t_1 \longrightarrow t_2$ and $\psi:
t_2 \longrightarrow t_3$ are coercions, then the composition $\phi
\circ \psi : t_1 \longrightarrow t_3$ of $\phi$ and $\psi$ is a
coercion.}

{\bf Lemma 6.}
\label{lem1} 
{\sl If assumption 1 holds, then the set of ground types as objects
together with the coercion functions as arrows form a category.}

\begin{proof} 
Since composition of functions is associative and the identity
function is a coercion, all axioms of a category are fulfilled.\qed
\end{proof}

In the following we will always assume that assumption 1 holds
even if we do not mention it explicitly.

\subsubsection{Base Types}

It is a good instrument for structuring data types to have only as few
types as possible as base types but to construct them by a type
constructor whenever possible.\footnote{As an example consider the
field of rational numbers, which can be constructed as the quotient
field of the integers.}

Since there are only very few coercions between base types the
following assumption seems to be easily satisfiable.

{\bf Assumption 2. (Base Types)}
\label{abasety} 
The subcategory of base types and coercions between base types forms a
preorder, i.\,e.\ if $t_1$ and $t_2$ are base types and $\phi,\psi:
t_1 \longrightarrow t_2$ are coercions then $\phi=\psi$.

\subsubsection{Structural Coercions}

{\bf Definition 21. (Structural Coercions)} 
{\sl The $n$-ary type
constructor ($n \geq 1$) $f$ induces a {\em structural coercion\/}, if
there are sets ${\cal A}_f \subseteq \{1, \ldots, n \}$ and ${\cal
M}_f \subseteq \{1, \ldots, n \}$ such that \index{ Af@${\cal
A}_f$|ii}\index{ Mf@${\cal M}_f$|ii} the following condition is
satisfied:}

{\sl Whenever there are declarations $f: (\sigma_1 \cdots \sigma_n)\sigma$
and $f: (\sigma'_1 \cdots \sigma'_n)\sigma'$ and ground types
$t_1:\sigma_1, \ldots, t_n:\sigma_n$ and $t'_1:\sigma'_1, \ldots,
t'_n:\sigma'_n$ such that $t_i=t'_i$ if $i \notin {\cal A}_f \cup
{\cal M}_f$ and there are coercions $$\begin{array}{ll} \phi_i: t_i
\longrightarrow t'_i, & \mbox{if }i \in {\cal M}_f,\\ \phi_i: t'_i
\longrightarrow t_i, & \mbox{if }i \in {\cal A}_f, \\ \phi_i = {\rm
id}_{t_i} = {\rm id}_{t'_i} , & \mbox{if }i \notin {\cal A}_f \cup
{\cal M}_f, \end{array}$$ then there is a {\em uniquely defined\/}
coercion }
$${\cal F}_f(t_1,\ldots,t_n,t'_1, \ldots, t'_n, \phi_1,
\ldots, \phi_n) : f(t_1,\ldots,t_n) \longrightarrow
f(t'_1,\ldots,t'_n).$$

{\sl The type constructor $f$ is {\em covariant in its $i$-th argument}, if
$i \in {\cal M}_f$.  \index{covariant!type constructor|ii}\index{type
constructor!covariant|ii} It is {\em contravariant in its $i$-th
argument}, if $i \in {\cal A}_f$.  \index{contravariant!type
constructor|ii} \index{type constructor!contravariant|ii}}

Instead of the adjective ``covariant'' we will sometimes use the
adjective ``monotonic'', and instead of ``contravariant'' we will
sometimes use ``antimonotonic'', because both terminologies are used
in the literature and reflect different intuitions which are useful in
different contexts.

{\bf Assumption 3. (Structural Coercions)}
\label{Astruct} 
{\sl Let $f$ be $n$-ary type constructor which induces a structural
coercion and let $f(t_1,\ldots,t_n)$, $f(t'_1,\ldots,t'_n)$, and
$f(t''_1,\ldots,t''_n)$ be ground types.  Assume that}
$$
\begin{array}{ll} t_i \subtype t'_i \subtype t''_i, & \mbox{if }i
\in {\cal M}_f,\\ t''_i \subtype t'_i \subtype t_i, & \mbox{if }i \in
{\cal A}_f, \\ t_i = t'_i=t''_i, & \mbox{if }i \notin {\cal A}_f \cup
{\cal M}_f.  
\end{array}$$ 
{\sl and let $\phi_i : t_i \longrightarrow
t'_i$, $\phi'_i : t'_i \longrightarrow t''_i$ (if $i \in {\cal M}_f$),
and $\phi'_i : t''_i \longrightarrow t'_i$, $\phi_i : t'_i
\longrightarrow t_i$ (if $i \in {\cal A}_f$) be coercion functions.
For $i \notin {\cal A}_f \cup {\cal M}_f$ let $\phi_i$ and $\phi'_i$
be the appropriate identities.}

{\sl Then the following conditions are satisfied: 
\begin{enumerate} 
\item ${\cal F}_f(t_1,\ldots,t_n,t_1, \ldots, t_n, {\rm id}_{t_1}, \ldots,
{\rm id}_{t_n})$ is the identity on $f(t_1,\ldots,t_n)$, 
\item ${\cal F}_f(t_1,\ldots,t_n,t''_1, \ldots, t''_n, \phi_1 \circ \phi'_1,
\ldots, \phi_n \circ \phi'_n) =$ \\ ${\cal F}_f(t_1,\ldots,t_n,t'_1,
\ldots, t'_n, \phi_1, \ldots, \phi_n) \circ {\cal
F}_f(t'_1,\ldots,t'_n,t''_1, \ldots, t''_n, \phi'_1, \ldots, \phi'_n).$ 
\end{enumerate} }

Let $f: (\sigma_1 \cdots \sigma_n) \sigma$ be an $n$-ary type
constructor which induces a structural coercion.  
\label{defcatsigmai}
Let $\cat{C}_{\sigma_i}$ be the category of ground types of sort
$\sigma_i$ as objects and the coercions as arrows, let
$\cat{C}_{\sigma_i}^{\rm op}$ be the dual category of
$\cat{C}_{\sigma_i}$ and let $\cat{C}_{\sigma_i}^{\rm triv}$ be the
discrete subcategory of the objects of $\cat{C}_{\sigma_i}$.  Define
$$ \cat{C}_i=\left\{ \begin{array}{ll} \cat{C}_{\sigma_i}, & \mbox{if
}i \in {\cal M}_f,\\ \cat{C}_{\sigma_i}^{\rm op}, & \mbox{if }i \in
{\cal A}_f,\\ \cat{C}_{\sigma_i}^{\rm triv}, & \mbox{if }i \notin
{\cal A} \cup {\cal M}_f.  \end{array} \right.  $$ Then
assumption 3 means that the mapping assigning $f(t_1,
\ldots, t_n)$ to the $n$-tuple $(t_1, \ldots, t_n)$ and assigning the
coercion $${\cal F}_f(t_1,\ldots,t_n,t'_1, \ldots, t'_n, \phi_1,
\ldots, \phi_n)$$ to the $n$-tuple $(\phi_1, \ldots, \phi_n)$ of
coercions is a {\em functor} from $$\cat{C}_1 \times \cdots \times
\cat{C}_n$$ into $\cat{C}_\sigma$.

\label{s43} 
Typical examples of type constructors which induce a
structural coercion are \tf{list}, \tf{UP}, $\tf{M}_{n,n}$, $\tf{FF}$.
These examples give rise to structural coercions, because the
constructed type can be seen as an instance of the parameterized type
class $\tf{sequence}$ (cf.\ Sec.~\ref{chapseq}).\footnote{The
sequences can be of fixed finite length, as in the case $\tf{FF}$
where it consists of two elements only, the numerator and the
denominator.}  The coercions between the constructed types are then
obtained by {\em mapping} the coercions between the type parameter
into the sequence.  Since a mapping of functions distributes with
function composition, assumption 3 will be satisfied by
these examples.

Although many examples of structural coercions satisfying
assumption 3 can be explained by this mechanism, there are
others, which will satisfy assumption 3 because of another
reason, so that the more general framework we have chosen is
justified.  For instance, it is another mechanism which gives rise to
the structural coercion in the case of the ``function space'' type
constructor, as is well known.\footnote{See e.\,g.\
\cite{Card86}.}  It is contravariant in its first argument and
covariant in its second argument, as the following considerations
show: Let $A$ and $B$ be two types where there is an implicit coercion
$\phi$ from $A$ to $B$.  If $f$ is a function from $B$ into a type
$C$, then $f \circ \phi $ is a function from $A$ into $C$.  Thus any
function from $B$ into $C$ can be coerced into a function from $A$
into $C$.  Thus an implicit coercion from $\tf{FS}(B,C)$ into
$\tf{FS}(A,C)$ can be defined, i.\,e.\ $\tf{FS}(B,C) \subtype
\tf{FS}(A,C)$.  If $C \subtype D$ by an implicit coercion $\psi$, then
$\psi \circ f$ is a function from $A$ into $D$, i.\,e.\ an implicit
coercion from $\tf{FS}(A,C)$ into $\tf{FS}(A,D)$ can be defined.  In
this case assumption 3 is satisfied because of the
associativity of function-composition.

Although many important type constructors arising in computer algebra
are monotonic in all arguments it is not justified to assume that this
property will always hold as was done in \cite{Como91}.  We
have already seen that the type constructor for building ``function
spaces'' is antimonotonic in its first argument.  Constructions like
the fixpoint field of a certain algebraic extension of $\QQ$ under a
group of automorphisms in Galois theory (see e.\,g.\
\cite{Zari75}, \cite{Marc77}, \cite{Lang05}) would give
other --- more algebraic examples --- of type constructors which are
antimonotonic.\footnote{In {\sf GAP} \cite{GAPx17} such constructs
are implemented as functions and not as type constructors, cf.\ the
discussion in Sec.~\ref{sgroupth}.  Nevertheless, the implementation
as type constructors seems to be a reasonably possibility.}

However, an assumption that all type constructors are monotonic or
antimonotonic in all arguments as in \cite{Fuhx90}, \cite{Mitc91}
still seems to be too restrictive for our purposes.

If one allows a type constructor building references (pointers) to
objects of a certain type as is possible in Standard ML or in the
system described by Kaes \cite{Kaes92}, then this type constructor is
neither monotonic nor antimonotonic.

There are also algebraic examples of type constructors which are
neither monotonic nor antimonotonic.  Consider e.\,g.\ the quotient
groups $G/G'$, where $G'$ is the derived subgroup of $G$ (see e.\,g.\
\cite[p.~28]{Robi96}).  Assume that $H$ can be embedded in
$G$. Then in general it is not possible to embed $H/H'$ in $G/G'$ or
vice versa.  Thus if one would have a type constructor building the
type $G/G'$ for a given group $G$, then this type constructor would be
neither monotonic nor antimonotonic.

\begin{remark} 
Of course, one has to restrict the groups in consideration to ones for
which the construction of $G/G'$ can be performed effectively.  One
such class of groups is that of the finite polycyclic groups
(cf. \cite{GAPx17}).
\end{remark}

\subsubsection{Direct Embeddings in Type Constructors}

{\bf Definition 22. (Direct Embeddings)}
\label{defdiem}
\index{direct embedding|ii}
\index{embedding!direct|ii} 
{\sl Let $f:(\sigma_1, \ldots, \sigma_n)\sigma$ be a $n$-ary type
constructor.  If for some ground types $t_1:\sigma_1, \ldots,
t_n:\sigma_n$ there is a coercion
function $$\Phi^{i}_{f,t_1,\ldots,t_n}: t_i \longrightarrow
f(t_1,\ldots,t_n),$$ then we say that {\em $f$ has a direct embedding
at its $i$-th position}.}

{\sl Moreover, let $${\cal D}_f= \{i \mid \mbox{$f$ has a direct embedding
at its $i$-th position}\}$$ \index{ Df@${\cal D}_f$|ii} be the {\em
set of direct embedding positions of $f$}.}

\begin{remark} 
In {\sf Axiom} the inverses of direct embeddings are
called {\em retractions} (cf.\ \cite[p.~713]{Jenk92}) assuming
that the direct embeddings are always injective.  Thus the usage of
the term in {\sf Axiom} is a special case of our usage of that term,
since in our terminology any partial function which is an inverse of
any injective coercion can be a retraction.

On the other hand the {\sf Axiom} terminology shows that the designers
of {\sf Axiom} have seen the importance of direct embeddings, even if
there is no special terminology for direct embeddings themselves but
only for their inverses!  
\end{remark}

\begin{remark} 
In a system, a type constructor represents a
parameterized abstract data type which is usually built uniformly from
its parameters.  So the family of coercion functions
$$\{\Phi^{i}_{f,t_1,\ldots,t_n} \mid t_i \in T_\Sigma(\{\})_{\sigma_i}
\}$$ will very often be just one ({\em polymorphic\/}) function.  In
this respect the situation is similar to the one in Sec.~\ref{s43}.
\end{remark}

{\bf Assumption 4. (Direct Embeddings)}
\label{Aemb} 
{\sl Let $f:(\sigma_1 \cdots \sigma_n)\sigma$ be a $n$-ary type constructor.

Then the following conditions hold: 
\begin{enumerate} 
\item $|{\cal D}_f|\leq 1$.  
\item The coercion functions which give rise to the
direct embedding are unique, i.\,e.\ if $\Phi^{i}_{f,t_1,\ldots,t_n}:
t_i \longrightarrow f(t_1,\ldots,t_n)$ and
$\Psi^{i}_{f,t_1,\ldots,t_n}: t_i \longrightarrow f(t_1,\ldots,t_n)$,
then $$\Phi^{i}_{f,t_1,\ldots,t_n}=\Psi^{i}_{f,t_1,\ldots,t_n}.$$
\end{enumerate} }

Many important type constructors such as $\tf{list}$, $\tf{M}_{n,n}$,
$\tf{FF}$, and in general the ones describing a ``closure'' or a
``completion'' of a structure --- such as the $p$-adic completions or
an algebraic closure of a field --- are unary.  Since for unary type
constructors the condition $|{\cal D}_f| \leq 1$ is trivial and the
second condition in assumption 4 should be always fulfilled,
the assumption holds in this cases.

For $n$-ary type constructors ($n \geq 2$) the requirement $|{\cal
D}_f| \leq 1$ might restrict the possible coercions.  Consider the
``direct sum'' type constructor for Abelian groups which we have
already seen that it could lead to a type system that is not coherent
if we do not restrict the possible coercions.  For a type constructor
$$\oplus: (\cf{Abelian\_group} \; \cf{Abelian\_group})
\cf{Abelian\_group}$$ the requirement $|{\cal D}_f| \leq 1$ means that
it is only possible to have either an embedding at the first position
or at the second position.

In the framework that we have used the types $A \oplus B$ and $B
\oplus A$ will be different.  However, the corresponding mathematical
objects are {\em isomorphic}.  Having a mechanism in a language that
represents certain isomorphic mathematical objects by the same type
(cf.\ Sec.~\ref{chtyiso}) the declaration of both natural embeddings
to be coercions would not lead to an incoherent type system.  Notice
that such an additional mechanism, which corresponds to factoring the
free term-algebra of types we regard by some congruence relation, will
be a conservative extension for a coherent type system. If a type
system was coherent, it will remain coherent. It is only possible that
a type system being incoherent otherwise becomes coherent.

Let $f:(\sigma \sigma')\sigma$ be a binary type constructor with
$\sigma$ and $\sigma'$ incomparable having direct embeddings at the
first and second position, and let $t : \sigma$ and $t' : \sigma'$ be
ground types such that $$t' \subtype f(f(t,t'),t').$$ Then there are
two possibilities to coerce $t'$ into $f(f(t,t'),t')$ which might be
different in general.  In the case of types $\tf{R} : \cf{c\_ring}$
and $\tf{x} : \cf{symbol}$ the coercions of $\tf{x}$ into
$\tf{UP}(\tf{UP}(\tf{R},\tf{x}),\tf{x})$ are unambiguous, if
$\tf{UP}(\tf{UP}(\tf{R},\tf{x}),\tf{x})$ and $\tf{UP}(\tf{R},\tf{x})$
are the same type.  However, it does not seem to be generally possible
to avoid the condition $|{\cal D}_f| \leq 1$ even in cases where a
type constructor is defined for types belonging to incomparable type
classes.

The naturally occurring direct embeddings for types built by the type
constructors $\tf{FF}$ and $\tf{UP}$ show that in the context of
computer algebra there are cases in which a coercion is defined into a
type belonging to an incomparable type class, into a type belonging to
a more general type class, into a type belonging to a less general
type class, or into a type belonging to the same type class.  So
coercions occur quite ``orthogonal'' to the inheritance hierarchy on
the type classes showing an important difference between the coercions
in computer algebra and the ``subtypes'' occurring in object oriented
programming (cf.\ Sec.~\ref{coerinstr}).

The next assumption will guarantee that structural coercions and
direct embeddings will interchange nicely.

{\bf Assumption 5. (Structural Coercions and Embeddings)}
\label{Accemb} 
{\sl Let $f$ be a $n$-ary type constructor which induces a
structural coercion and has a direct embedding at its $i$-th position.
Assume that $f:(\sigma_1 \cdots \sigma_n)\sigma$ and $f:(\sigma'_1
\cdots \sigma'_n)\sigma$, $t_1:\sigma_1, \ldots, t_n:\sigma_n$, and
$t'_1:\sigma'_1, \ldots, t'_n:\sigma'_n$.  If there are coercions
$\psi_i: t_i \longrightarrow t'_i$, if the coercions
$\Phi^{i}_{f,t_1,\ldots,t_n}$ and $\Phi^{i}_{f,t'_1,\ldots,t'_n}$ are
defined, and if $f$ is covariant at its $i$-th argument, then the
following diagram is commutative:}

\begin{center}
\setsqparms[1`1`1`1;2600`600]
\square[t_i`t'_i`f(t_1,\ldots,t_n)`f(t'_1,\ldots,t'_n);
\psi_i`
{\Phi^{i}_{f,t_1,\ldots,t_n}}`
{\Phi^{i}_{f,t'_1,\ldots,t'_n}}`
{{\cal F}_f(t_1,\ldots,t_n,t'_1,\ldots,t'_n,
\psi_1, \ldots, \psi_n)}]
\end{center}

{\sl If $f$ is contravariant at its $i$-th argument,
then  the following diagram is commutative:}
\begin{center}
\setsqparms[1`1`1`-1;2600`600]
\square[t_i`t'_i`f(t_1,\ldots,t_n)`f(t'_1,\ldots,t'_n);
\psi_i`
{\Phi^{i}_{f,t_1,\ldots,t_n}}`
{\Phi^{i}_{f,t'_1,\ldots,t'_n}}`
{{\cal F}_f(t_1,\ldots,t_n,t'_1,\ldots,t'_n,
\psi_1, \ldots, \psi_n)}]
\end{center}

The type constructors \tf{list}, \tf{UP}, $\tf{M}_{n,n}$ may serve as
examples of constructors which induce structural coercions and can
also have direct embeddings: It might be useful to have coercions from
elements into one element lists, from elements of a ring into a
constant polynomial or to identify a scalar with its multiple with the
identity matrix.

As was already discussed
in Sec.~\ref{s43}, in all these examples the
parameterized data types can be seen as sequences  and the
structural coercions ---
i.\,e.\ ${\cal F}_\tf{UP}(\tf{I},\tf{x},
\tf{FF}(\tf{I}),\tf{x},
\psi, {\rm id}_{\tf{x}})$ ---
can be seen as a kind of ``mapping'' operators.

The direct embeddings are  ``inclusions'' of
elements in these sequences.
Since applying a coercion function to such  an elements
and then ``including''  the result in a sequence
will yield the same result as first including
the element in the sequence and then ``mapping'' the
coercion function into the sequence,
assumption 5 will be satisfied by these examples.
For instance,
$${\cal F}_\tf{UP}(\tf{I},\tf{x},\tf{FF}(\tf{I}),\tf{x},
\Phi^{1}_{\tf{FF},\tf{I}}, {\rm id}_{\tf{x}})$$
is  the function which maps the coercion function
$\Phi^{1}_{\tf{FF},\tf{I}}$
to the sequence of elements of $\tf{I}$ in $\tf{UP}(\tf{I},\tf{x})$
which represents the polynomial.

Thus the diagrams
\begin{center}
\resetparms
\setsqparms[1`1`1`1;1000`500]
\square[\tf{I}`\tf{FF}(\tf{I})`
\tf{UP}(\tf{I},\tf{\tf{t}})`\tf{UP}(\tf{FF}(\tf{I}),\tf{\tf{t}});```]
\end{center}
and
\begin{center}
\resetparms
\setsqparms[1`1`1`1;1000`500]
\square[\tf{I}`\tf{UP}(\tf{I},\tf{\tf{t}})`
\tf{M}_{2,2}(\tf{I})`\tf{M}_{2,2}(\tf{UP}(\tf{I},\tf{\tf{t}}));```]
\end{center}
and
\begin{center}
\resetparms
\setsqparms[1`1`1`1;1000`500]
\square[\tf{I}`\tf{FF}(\tf{I})`
\tf{M}_{2,2}(\tf{I})`\tf{M}_{2,2}(\tf{FF}(\tf{I}));```]
\end{center}
which are instances of the diagrams
in assumption 5 are commutative.\footnote{The
first of these diagrams can also be found in
\cite{Fort90}.}

\bigskip
If the mathematical structure
represented by a type $t_i$
in assumption 5 has non-trivial
automorphisms, then it is  possible to
construct the structural coercion
$${{\cal F}_f(t_1,\ldots,t_n,t'_1,\ldots,t'_n,
\psi_1, \ldots, \psi_n)}$$
in a way such that the assumption is
violated: just apply a non-trivial automorphism
to $t_i$!
However, such a construction seems to be artificial.
Moreover, the argument shows that 
a possible violation of assumption 5
``up to an automorphism'' can be avoided by an
appropriate definition of 
$${{\cal F}_f(t_1,\ldots,t_n,t'_1,\ldots,t'_n,
\psi_1, \ldots, \psi_n)}.$$

\subsubsection{A Coherence Theorem}

We are now ready to state the main result of this section.
The assumptions 1, 2, 3, 4, and 5 are
 ``local'' coherence conditions
imposed on the coercions of the type system.
In the following theorem we will prove
that the type system is ``globally'' coherent,
if these local conditions are satisfied.

{\bf Theorem 6. (Coherence)}
\label{thmain}
{\sl Assume  that all coercions between ground types
are only built by one of the following mechanisms:
\begin{enumerate}
\item coercions between base types;
\item coercions induced by structural coercions;
\item direct embeddings in a type constructor;
\item composition of coercions;
\item identity function on ground types as coercions.
\end{enumerate}
If the assumptions 1, 2, 3, 4, and 5 are satisfied,
then the set of  ground types as objects and the coercions
between them as arrows form a category which is a preorder.}

\begin{proof}

By assumption 1 and lemma 6 the
set of  ground types as objects and the coercions
between them as arrows form a category.

For any two ground types $t$ and $t'$ we will
prove by induction on the complexity
of $t'$ that if
$\phi, \psi : t \longrightarrow t'$ are coercions
then $\phi=\psi$ which will establish the theorem.

If $\com(t')=1$ then we  have $\com(t)=1$ because of 
the assumption on the possible mechanisms for building coercions.
Since $\com(t)=1$ and $\com(t')=1$  the claim 
follows from assumption 2.

Now assume  that the induction hypothesis holds
for $k$
and  let $\com(t')=k+1$.
Thus we can assume that
$t'=f(u_1,\ldots,u_n)$ for some $n$-ary type
constructor $f$. 

Let $\phi, \psi : t \longrightarrow t'$ be coercions.


The coercions $\phi$ and $\psi$ are
compositions of coercions
between  base types, direct embeddings in type
constructors and structural coercions.
Because of assumption 3
and the induction hypothesis we
can assume that there are ground
types $s_1$ and $s_2$ and
unique coercions $\psi_1: t \longrightarrow s_1$
and  $\psi_2: t \longrightarrow s_2$
such that
\begin{equation}
\label{e1}
\phi =  {\cal F}_f(\ldots, t, \ldots , s_1,
\ldots , \psi_1, \ldots )
\end{equation}
or
\begin{equation}
\label{e2}
\phi= \psi_1 \circ \Phi^i_{f, \ldots, s_1, \ldots}
\end{equation}
Similarly,
\begin{equation}
\label{e3}
\psi =  {\cal F}_f(\ldots, t, \ldots , s_2,
\ldots , \psi_2, \ldots )
\end{equation}
or
\begin{equation}
\label{e4}
\psi= \psi_2 \circ \Phi^j_{f, \ldots, s_2, \ldots}
\end{equation}
If $\phi$ is  of form \ref{e1} and $\psi$ is
of form \ref{e3}, then $\phi=\psi$
because of assumption 3
and the uniqueness of
${\cal F}_f$.
If $\phi$ is  of form \ref{e2} and $\psi$ is
of form \ref{e3}, then $\phi=\psi$
because of assumption 5.
Analogously for $\phi$ of form \ref{e1}
and $\psi$ of form \ref{e4}.

If $\phi$ is of form \ref{e2}  and $\psi$ is of form \ref{e3}
then assumption 4 implies that
$i=j$ and $s_1=s_2$. Because of the induction
hypothesis we have $\psi_1 = \psi_2$ and
hence $\phi=\psi$ again by assumption 4.
\qed
\end{proof}

\subsection{Type Isomorphisms}
\label{chtyiso}

In several important cases there is
not only a coercion from a type 
$A$ into a type $B$ but also one
from $B$ into $A$.
So there are coercions from univariate polynomials
in sparse representation over some ring
to ones in dense representation and vice versa.
Or we have
$$\tf{FF}(t_\cf{integral\_domain}) \subtype
\tf{FF}(\tf{FF}(t_\cf{integral\_domain}))$$
and
$$\tf{FF}(\tf{FF}(t_\cf{integral\_domain}))
\subtype \tf{FF}(t_\cf{integral\_domain}).$$
Other examples can be found in Sec.~\ref{paramtyiso}.
If $A \subtype B$ and $B \subtype A$ then we will
write $A \typeiso B$.

If we require that for coercions
$$\begin{array}{l}
\phi: A \longrightarrow B, \\
\psi: B \longrightarrow A
\end{array}
$$
the compositions $\phi \circ \psi$ and $\psi \circ \phi$
are the  identities  on $A$ resp.\ $B$, then
the coherence theorem 6 can be extended
to the case of type isomorphisms.\footnote{Obviously,
the conditions that $\phi$ and $\psi$ are true inverses
of each other is also a necessary condition
for coherence.}

So type isomorphisms can be seen as equivalence classes in the
preorder on types induced by the coercions.  However, there are
several reasons to treat type isomorphisms by a new typing construct
independent from the concept of coercions.  As we have shown in
Sec.~\ref{chparamtycl} there is usually the second-order type of a
category present in {\sf Axiom} for a class of equivalent types.  On
the one hand if coercions are present in the system the equivalence
classes in the coercion preorder can be deduced by a system so that it
is not necessary to define them by the programmer.\footnote{In 
{\sf Axiom} the isomorphic types are treated independently of the
 coercions.}  On the other hand --- at least for the purpose of a
user interface --- it seems to be useful to have a class of isomorphic
types present as a first-order type.  Since all equivalence classes in
the coercion preorder are finite --- only finitely many (possibly
polymorphic) functions can be defined to be coercions --- the type of
finite disjoint unions --- variant record types --- can serve as a
well known first-order type for that purpose
(cf.\ \cite[p.~46]{That91}).

Moreover, it is reasonable to assume that type isomorphisms have the
following properties which cannot be deduced from the properties of
general coercion functions.
\begin{enumerate}
\item Isomorphic types belong to the same type class, i.\,e.\ if $t_1
  : \sigma$ and $t_1 \typeiso t_2$ then $t_2 : \sigma$.
\item If $f: (\sigma_1 \cdots \sigma_n)\sigma$ is an $n$-ary type
  constructor, $t_1:\sigma_1, \ldots, t_n:\sigma_n$, $t'_1:\sigma_1,
  \ldots, t'_n:\sigma_n$, such that
$$ t_i \typeiso t'_i \quad \forall i$$ then
$$f(t_1, \ldots, t_n ) \typeiso f(t'_1, \ldots, t'_n ).$$
\end{enumerate}

The second condition is only implied by the rules for structural
coercions if $f$ would be monotonic or antimonotonic in all arguments.
Because of the second condition a {\em congruence relation} is defined
by $\typeiso$ on the term-algebra of types.\footnote{It follows from
the properties of $\subtype$ alone that $\typeiso$ defines an
equivalence relation.}  Thus we can built the factor algebra modulo
this congruence relation.  This factor algebra is isomorphic to the
factor algebra modulo some equational theory, the equational theory
which is obtained if we interpret $\typeiso$ as equality.  We will
call this equational theory {\em the equational theory corresponding
to the type isomorphism.}

For simplicity we will often neglect the sort constraints but will
only write the unsorted part. Since for many examples in consideration
the sort is always the same, these slightly sloppy view can be
justified even formally.

While it is useful to know that certain {\em different types} are
isomorphic --- such as the sparse and dense representations of
polynomials --- there are other cases where it seems to be more
appropriate to have a semantics of the type system implying that
certain types are actually {\em equal}.

So the type system is not coherent if we define all naturally
occurring embedding functions to be coercions and if we regard two
types
$$\tf{direct\_sum}(t_1,t_2) \mbox{ and } \tf{direct\_sum}(t_2,t_1)$$
as being different.  This example would not violate the coherence of
the type system if we had not only two possible coercion functions
implying that these types are isomorphic but if these types are
actually {\em equal\/} in the system.  Notice that an implementation
of this type constructor having these properties is possible.  One
just has to use the same techniques as are used for the representation
of general associative and commutative operators in certain
term-rewriting systems (see e.\,g.\ \cite[Sec.~10]{Bund93},
\cite{Bund93a}), i.\,e.\ a certain ordering on terms has to be
given and the terms have to be represented in a {\em flattened form}.

In Sec.~\ref{scoercprbl} we will give a family of type isomorphisms
whose corresponding equational theory is not finitely axiomatizable.
Thus all of these isomorphisms cannot be modeled by declaring finitely
many functions to be coercions between types (even if we allow
``polymorphic'' coercion functions between polymorphic types).  So
these type isomorphisms could be only modeled in the system by a
direct mechanism implying that certain types are equal.

\subsubsection{Independence of the Coercion Preorder
from the Hierarchy of Type Classes}
\label{coerinstr}

If two types are isomorphic, then they belong to the same type class.

Such a conclusion is not justified if there is only a coercion form
$A$ into $B$.  Consider for instance a field $K$. Its elements can be
coerced to the constant polynomials in $K[x]$. Of course, the ring of
polynomials over some field is no longer a field.

However, it cannot be concluded in general that $A \subtype B$ and $A:
\sigma$ implies $B: \tau$ for some $\sigma \leq \tau$.  Just the
opposite holds for many important examples!

Consider e.\,g.\ the coercion from an integral domain into its field
of fractions which is not only an integral domain but even a field.
Similarly, any field can be embedded in its algebraic closure,
i.\,e.\ in a structure which has additional ``nice'' properties,
namely that it is an algebraically closed field.  The constructions of
the real numbers $\RR$ or of $p$-adic completions of $\QQ$ can be seen
similarly.  The field of rational numbers $\QQ$ can be embedded in
these structures --- and is usually identified with its image under
this embedding --- which are complete metric spaces, a property that
the original structure did not have.

The construction of structures which have additional ``nice''
properties and in which the original structure can be embedded is an
important tool for mathematical reasoning.\footnote{The author could
  easily list several examples of such constructions from the area of
  mathematics he has worked on.  Since this area is non-constructive
  we will omit them.  However, it seems to be possible to find some
  examples in almost {\em any} area of mathematics.}  Usually, the
original structures and their images under this embedding are not
distinguished notationally.

So the possibility to have coercions which induce a preorder on types
that is quite independent on the preorder on types induced by the
inheritance hierarchy on type classes seems to be important.  Notice
that these preorders would still differ even if we had allowed more
sophisticated inheritance possibilities on type classes than the ones
given in {\sf Axiom} or {\sf Haskell}.  There have to be (at least)
two hierarchies.  The one corresponding to some form of
``inheritance'': more special structures (such as a ``rings'') inherit
all properties of more general ones (such as ``groups''), and another
one reflecting possible embeddings of a structure into another that
might have stronger properties.

\begin{remark}
Of course, it is desirable to have some form of control over the
possibilities how coercions behave with respect to the hierarchy on
type classes.  This seems to be possible.

All of the examples given above can be described by an unary type
constructor $F$ such that for any types $A$ and $B$ of an appropriate
sort the following holds:
$$\begin{array}{l} \mbox{If }A \subtype B, \mbox{ then } F(A) \subtype
  F(B),\\ A \subtype F(A), \\ F(F(A)) \typeiso F(A) .
\end{array}$$
Thus --- if we interpret $\subtype$ as $\subseteq$ and $\typeiso$
as equality --- the type constructor $F$ has the properties of a 
{\em closure operator} (see e.\,g.\ \cite{Dave90},\cite{Laue82}).

So the requirement that a type unary constructor which has a direct
embedding and whose constructed type belongs to a type class with
stronger properties then the type parameter has to be a closure
operator in the sense of above would be fulfilled by many important
examples.  On the other hand such a restriction might allow much more
efficient type inference algorithms so that it might be a reasonable
requirement for a system.
\end{remark}

\subsubsection{Some Problematic Examples of Type Isomorphisms}

In this section we will collect some natural examples of type
isomorphisms which arise in the context of computer algebra.  We will
show that their corresponding equational theories are not unitary or
even not finitary unifying or that the unification problem is even
undecidable.

In Sec.~\ref{sectypinfcoer} we will show why these properties of the
corresponding equational theory are problematic in the context of type
inference.

We have already shown that a family of type isomorphisms whose
corresponding equational theory is not finitely axiomatizable cannot
be modeled by means of finitely many coercion functions and thus
requires another concept.  The presentation of a family of type
isomorphisms having this property will be given in the next section
because the proof of this property will need a little technical
machinery.

{\bf Example 1.}
\label{isomac}
As was mentioned above for the type constructor $\tf{direct\_sum}$ on
Abelian groups the type isomorphisms
$$\tf{direct\_sum}(t_1,t_2) \typeiso \tf{direct\_sum}(t_2,t_1),$$ and
$$\tf{direct\_sum}(t_1,\tf{direct\_sum}(t_2,t_3)) \typeiso
\tf{direct\_sum}(\tf{direct\_sum}(t_1,t_2),t_3)$$ hold.

Thus $\tf{direct\_sum}$ would give rise to an equational theory modulo
an associate and commutative operator.  The unification problem for
such an equational theory is decidable, but not unitary unifying.
However, it is finitary unifying (cf.\ \cite{Siek89},
\cite{Joua90}).

{\bf Example 2.}
\label{isomass}
For the binary type constructor $\tf{pair}$ which builds the type of
ordered pairs of elements of arbitrary types the following type
isomorphisms hold:
$$\tf{pair}(\tf{pair}(A,B),C) \typeiso \tf{pair}(A, \tf{pair}(B,C)),$$
i.\,e.\ it corresponds to an associative equational theory.
Unification for such theories is decidable but not finitary unifying
\cite{Siek89}.

{\bf Example 3.}
\label{isounde}
Let $A, B, C$ be vector spaces over some fixed field $K$ and let
$\oplus$ denote the direct sum of vector spaces and $\otimes$ denote
the tensor product of two vector spaces.  Then we have
$$ (A \oplus B) \otimes C \cong (A \otimes C) \oplus (B \otimes C)
$$ (see e.\,g.\ \cite[p.~293]{Kowa63}.)  Thus if we had two binary
type constructors over vector spaces building direct sums and tensor
products respectively, then the ``distributivity law'' gives rise to
type isomorphisms.  Since associativity and commutativity also hold
for the type constructor building direct sums of vector spaces alone
--- any vector space is an Abelian group --- we have the case of an
equational theory having two operators obeying associativity,
commutativity, and distributivity but no other equations.

Unfortunately, unification for such theories is undecidable
\cite{Siek89}, \cite{Szab82}.

\begin{figure}[t]
\begin{center}
\begin{tabular}{|l|r|}
\hline Type isomorphisms whose & Example given \\ corresponding
equational theory & on page \\ \hline \hline is not unitary unifying &
\pageref{isomac} \\ is not finitary unifying & \pageref{isomass}
\\ has an undecidable unification problem & \pageref{isounde} \\ is
not finitely axiomatizable &
\pageref{begincoerpr}--\pageref{endcoerpro}\\ \hline
\end{tabular}
\end{center}
\caption{Some problematic examples of type isomorphisms}
\end{figure}

\subsection{A Type Coercion Problem}
\label{scoercprbl}
\label{begincoerpr}

In this section we want to present an example of a family of types
which allow type-isomorphisms which correspond to an equational theory
that is not finitely axiomatizable. In order to set up the example we
first need a technical result.

\subsubsection{A  Technical Result}
\label{s3}

{\bf Definition 23.}
{\sl Let $f : \{P,F\}^* \longrightarrow \{P,F\}^*$ be the function, which
is defined by the following algorithm:
\begin{itemize}
\item[] If no $F$ is occurring in the input string, then return the
  input string as output string.

Otherwise, remove any $F$ except the leftmost occurrence from the
input string and return the result as output string.
\end{itemize}}

{\sl Let $\equiv$ be the binary relation on $\{P,F\}^*$ which is defined by}
$$\forall v,w \in \{P,F\}^*: \; v \equiv w \iff f(v)=f(w).$$

Obviously, the function $f$ can be computed in linear time and the
relation $\equiv$ is an equivalence relation on $\{P,F\}^*$.

Let $\Sigma$ be the first-order signature consisting of the two unary
function Symbols $F$ and $P$.  We will now lift the equivalence
relation $\equiv$ to a set of equations over $\Sigma$.

{\bf Definition 24.}
\label{deeqe}
{\sl Let ${\cal E}$ be the following set of equations:}
$$\begin{array}{lll} {\cal E} = \{ & S_1(S_2(\cdots S_k(x)\cdots) =
  S_{k+1}(S_{k+2}(\cdots S_r(x)\cdots )) \mid \\ & \;\; S_i \in
  \{F,P\} \:(1 \leq i \leq r) \mbox{ and } S_1 S_2 \cdots S_k \equiv
  S_{k+1} S_{k+2} \cdots S_r & \} \\
\end{array}$$

{\bf Theorem 7.}
\label{thmtr}
${\cal E}$ is not finitely based, i.\,e.\ there is no finite set of
axioms for ${\cal E}$.

\begin{proof} Assume towards a contradiction that there
is such a finite set ${\cal E}_0$.  Let ${\cal M}$ be the free model
of $\aleph_0$ generators over ${\cal E}$ and let ${\cal M}_0$ be the
free model of one generator over ${\cal E}_0$.

Except for a possible renaming of the variable symbol $x$, ${\cal
  E}_0$ has to be a subset of ${\cal E}$.  Otherwise, ${\cal E}_0$
would contain an equation of the form
$$S_1(S_2(\cdots S_k(x)\cdots) = S_{k+1}(S_{k+2}(\cdots S_r(y)\cdots
)), $$ or of the form
$$S_1(S_2(\cdots S_k(x)\cdots) = S_{k+1}(S_{k+2}(\cdots S_r(x)\cdots
)), \;\: S_1 S_2 \cdots S_k \not \equiv S_{k+1} S_{k+2} \cdots S_r.$$
However, none of these equations holds in ${\cal M}$.

Now let $n \in \NN$ be the maximal size of a term in ${\cal E}_0$.
Then the equation
$$F(\underbrace{P(P(\cdots (P}_{n}(x) ) \cdots ))) =
F(P(F(\underbrace{P(P(\cdots (P}_{n-1}(x) ) \cdots )))))$$ holds in
${\cal M}$, but it does not hold in ${\cal M}_0$.  \qed
\end{proof}

\subsubsection{The Problem}
\label{s4}

If $R$ is an integral domain, we can form the field of fractions
$\tf{FF}(R)$.  We can also built the ring of univariate polynomials in
the indeterminate $x$ which we will denote by $\tf{UP}(R,x)$ --- the
ring of polynomials $R[x]$ in the standard mathematical notation ---
which is again an integral domain by a Lemma of Gau{\ss}.  Thus we can
also built the field of fractions of $\tf{UP}(R,x)$,
$\tf{FF}(\tf{UP}(R,x))$ --- the field of rational functions $R(x)$.

Starting from an integral domain $R$ we will always get an integral
domain and can repeatedly built the field of fractions and the ring of
polynomials in a ``new'' indeterminate.


Thus if a computer algebra system has a fixed integral domain $\tf{R}$
and names for symbols $\tf{x}_0, \tf{x}_1, \tf{x}_2 \ldots$, it should
also provide types of the form
\begin{enumerate}
\item $\tf{R}$, \label{l11}
\item $\tf{FF}(\tf{R})$, \label{l12}
\item $\tf{UP}(\tf{R},\tf{x}_0)$, \label{l13}
\item $\tf{UP}(\tf{FF}(\tf{R}),\tf{x}_0)$, \label{l14}
\item $\tf{FF}(\tf{UP}(\tf{R},\tf{x}_0))$, \label{l15}
\item $\tf{UP}(\tf{UP}(\tf{R},\tf{x}_0),\tf{x}_1)$, \label{l16}
\item
  $\tf{UP}(\tf{FF}(\tf{UP}(\tf{R},\tf{x}_0)),\tf{x}_1)$, \label{l17}
\item
  $\tf{FF}(\tf{UP}(\tf{UP}(\tf{R},\tf{x}_0),\tf{x}_1))$, \label{l18}
\item
  $\tf{FF}(\tf{UP}(\tf{FF}(\tf{UP}(\tf{R},\tf{x}_0)),\tf{x}_1)$, \label{l19}
\item
  $\tf{UP}(\tf{UP}(\tf{UP}(\tf{R},\tf{x}_0),\tf{x}_1),\tf{x}_2)$, \label{l1
  10}
\item[] \rule{0mm}{0mm} \vdots
\end{enumerate}

It is convenient to use the same symbols for a mathematical object and
the symbolic expression which denotes the object.  In order to clarify
things we will sometimes use additional $\lsb \cdot \rsb$ for the
mathematical objects.

There are canonical embeddings from an integral domain into its field
of fractions and into the ring of polynomials in one indeterminate (an
element is mapped to the corresponding constant polynomial).

It is common mathematical practice to identify the integral domain
with its image under these embeddings.  Thus the type system should
also provide a coercion between these types, i.\,e.\ if $t$ is a type
variable of sort $\cf{integral\_ domains}$ and $x$ is of sort
$\cf{symbol}$, then
$$t \subtype \tf{FF}(t)$$ and
$$t \subtype \tf{UP}(t,x).$$

However, not all of the types built by the type constructors $\tf{FF}$
and $\tf{UP}$ should be regarded to be different.  If the integral
domain $R$ happens to be a field, then $R$ will be isomorphic to its
field of fractions.  Especially, for any integral domain $R$, $\lsb
\tf{FF}(R)\rsb$ and $\lsb \tf{FF}(\tf{FF}(R)) \rsb$ are isomorphic.

The fact that also $\lsb \tf{FF}(\tf{FF}(R)) \rsb$ can be embedded in
$\lsb \tf{FF}(R) \rsb$ can be expressed by
$$\tf{FF}(\tf{FF}(t)) \tf \subtype{FF}(t),$$ which is one of the
examples given in \cite[p.~354]{Como91}.

But there are more isomorphisms which govern the relations of this
family of types.

If we assume that an application of the type constructor $\tf{UP}$
always uses a ``new'' indeterminate as its second argument, any
application of the type constructor $\tf{FF}$ except the outermost one
application is redundant.

This observation will be captured by the following formal treatment.
In order to avoid the technical difficulty of introducing ``new''
indeterminates, we will use an unary type constructor $\tf{up}$
instead the binary $\tf{UP}$.  The intended meaning of $\tf{up}(t)$ is
$\tf{UP}(t,\tf{x}_n)$, where $\tf{x}_n$ is a new symbol, i.\,e.\ not
occurring in $t$.

{\bf Definition 25.}
{\sl Define a function {\sf trans} from $\{F,P\}^*$ into the set of types
recursively by the following equations.  For $w \in \{F,P\}^*$,
\begin{itemize}
\item ${\sf trans}(\varepsilon) = \tf{R}$,
\item ${\sf trans}(Fw)=\tf{FF}({\sf trans}(w))$,
\item ${\sf trans}(Pw)=\tf{up}({\sf trans}(w))$.
\end{itemize}}

If we take $\lsb \tf{R} \rsb$ to be the ring of integers, the
following lemma will be an exercise in elementary
calculus.\footnote{If we started with the ring of polynomials in
  infinitely many indeterminates over some domain, then there would be
  additional isomorphisms.}

{\bf Lemma 7.}
\label{letrans}
{\sl Let $\lsb \tf{R} \rsb$ be the ring of integers.  For any $v, w \in
\{F,P\}^*$, the integral domains $\lsb {\sf trans}(v) \rsb$ and $\lsb
{\sf trans}(w)\rsb$ are isomorphic iff $v \equiv w$.}

{\sl Moreover, $\lsb {\sf trans}(v)\rsb$ can be embedded in $\lsb {\sf
  trans}(w)\rsb$ and $\lsb {\sf trans}(w)\rsb$ can be embedded in
$\lsb{\sf trans}(v)\rsb$ iff $\lsb{\sf trans}(v)\rsb$ and $\lsb{\sf
  trans}(w)\rsb$ are isomorphic.}

{\bf Theorem 8.}
{\sl Let $\Sigma$ be the signature consisting of the unary function symbols
$\tf{FF}$ and $\tf{up}$ and the constant $\tf{R}$.  Let $\lsb \tf{R}
\rsb$ be the ring of integers.}

{\sl Then there is no finite set of Equations ${\cal E}' $ over $\Sigma$,
such that for ground terms $t_1$ and $t_2$ the following holds.}
$$ {\cal E}' \models \{ t_1 = t_2 \} \iff \mbox{$\lsb t_1\rsb$ and
$\lsb t_2\rsb$ are isomorphic.}$$

\begin{proof}
If $t_1$ and $t_2$ are ground terms, then there are $v, w \in \{F,
P\}^*$ such that $t_1={\sf trans}(v)$ and $t_2={\sf trans}(w)$.  Now
we are done by Lemma 7 and Theorem 7.  \qed
\end{proof}

The problem is that the equational theory which describes the coercion
relations in the example we gave is not finitely based.  Since this
property of an equational theory is {\em equivalence-invariant} in the
sense of \cite[p.~382]{Grae79}, the use of another signature for
describing the types does not help.
\label{endcoerpro}

\subsection{Properties of the Coercion Preorder}
\label{secpropcoerpreord}

If the type system is coherent, then the category of ground types as
objects and the coercions as arrows is a preorder.  Even if the type
system is not coherent, a reflexive and transitive relation on the
ground types (and even on the polymorphic types) is defined by
``$\subtype$'', i.\,e.\ a preorder.\footnote{Notice the difference
between {\em a category which is a preorder} and {\em a relation
which is a preorder}.}

Factoring out the equivalence classes of this reflexive and transitive
relation we will obtain a partial order on the types.

In general this order on the types will not be a lattice if we
consider some typical examples occurring in a computer algebra system.
Take e.\,g.\ the types $\tf{integer}$ and $\tf{boolean}$. There is no
type which can be coerced to both of these types (unless an additional
``empty type'' is present in the system).

For many purposes, especially type inference (see
Sec.~\ref{secaltyincoer}), it would be convenient if this partial
ordering on the types were a quasi-lattice.  In the following we will
show that in general this will not be the case.

{\bf Example 4.}
\label{exnolatds}
Let $\tf{I}$ be the ring of integers and let $\oplus$ denote the
direct sum of two Abelian groups and let the direct embeddings into
the first argument and into the second argument of this type
constructor be present, i.\,e.\ ${\cal D}_{\oplus} = \{ 1, 2 \}$.
Then we have
$$\begin{array}{l} \tf{UP}(\tf{I},\tf{x}) \subtype
  \tf{UP}(\tf{FF}(\tf{I}),\tf{x}), \\ \tf{UP}(\tf{I},\tf{x}) \subtype
  \tf{UP}(\tf{I},\tf{x}) \oplus \tf{FF}(\tf{I}),\\ \tf{FF}(\tf{I})
  \subtype \tf{UP}(\tf{FF}(\tf{I}),\tf{x}), \\ \tf{FF}(\tf{I})
  \subtype \tf{UP}(\tf{I},\tf{x}) \oplus \tf{FF}(\tf{I}),
\end{array}$$
and no other coercions can be defined between these types.  There is
also no type $R$ with $R \neq \tf{UP}(\tf{I},\tf{x})$ and $R \neq
\tf{FF}(\tf{I})$ such that
$$\begin{array}{l} R \subtype \tf{UP}(\tf{FF}(\tf{I}),\tf{x}), \\ R
  \subtype \tf{UP}(\tf{I},\tf{x}) \oplus \tf{FF}(\tf{I})
\end{array}
$$ (cf.\ Fig.~\ref{fignolatmde}).  Thus in this case the partial
ordering given by $\subtype$ is not a quasi-lattice (see also
Lemma 3).

\begin{figure}
\begin{center}
\unitlength=1.2mm
\begin{picture}(45.00,50.00)
\put(13.00,10.00){\makebox(0,0)[cc]{$\tf{UP}(\tf{I},\tf{x})$}}
\put(37.00,10.00){\makebox(0,0)[cc]{$\tf{FF}(\tf{I})$}}
\put(13.00,40.00){\makebox(0,0)[cc]{$\tf{UP}(\tf{FF}(\tf{I}),\tf{x})$}}
\put(37.00,40.00){\makebox(0,0)[cc]{$\tf{UP}(\tf{I},\tf{x}) \oplus
    \tf{FF}(\tf{I})$}} \put(13.00,13.00){\vector(0,1){24.00}}
\put(13.00,13.00){\vector(1,1){24.00}}
\put(37.00,13.00){\vector(0,1){24.00}}
\put(37.00,13.00){\vector(-1,1){24.00}}
\put(9.00,25.00){\makebox(0,0)[cc]{$\subtype$}}
\put(41.00,25.00){\makebox(0,0)[cc]{$\subtype$}}
\put(22.00,20.00){\makebox(0,0)[cc]{$\subtype$}}
\put(28.00,20.00){\makebox(0,0)[cc]{$\subtype$}}
\end{picture}
\end{center}

\caption{Ad Example 4}
\label{fignolatmde}
\end{figure}

Even if we require $|{\cal D}_f| \leq 1$ for all type constructors ---
recall that this requirement is also necessary in order to ensure a
coherent type system --- and we have only direct embeddings and
structural coercions then it is still possible that the partial
ordering on types induced by ''$\subtype$'' is not a quasi-lattice.
Consider for instance two type constructors $f:(\sigma)\sigma$ and $g:
(\sigma)\sigma$ which we assume to be unary for simplicity.  If ${\cal
  D}_f \cap {\cal M}_f \neq \emptyset$ and ${\cal D}_g \cap {\cal M}_g
\neq \emptyset$ and $t: \sigma$, then
$$g(t) \subtype f(g(t)) \quad\quad\mbox{and}\quad\quad g(t) \subtype
g(f(t))$$ and similarly
$$f(t) \subtype g(f(t)) \quad\quad\mbox{and}\quad\quad f(t) \subtype
f(g(t))$$ (cf.\ Fig.~\ref{fignolatoth}).  Having only direct
embeddings and structural coercions the condition imposed in
Lemma 3 with $a = g(t)$, $b=f(t)$, $c=f(g(t))$ and
$d=g(f(t))$ are fulfilled.

\begin{figure}
\begin{center}
\unitlength=1.2mm
\begin{picture}(45.00,50.00)
\put(13.00,10.00){\makebox(0,0)[cc]{$g(t)$}}
\put(37.00,10.00){\makebox(0,0)[cc]{$f(t)$}}
\put(13.00,40.00){\makebox(0,0)[cc]{$g(f(t))$}}
\put(37.00,40.00){\makebox(0,0)[cc]{$f(g(t))$}}
\put(13.00,13.00){\vector(0,1){24.00}}
\put(13.00,13.00){\vector(1,1){24.00}}
\put(37.00,13.00){\vector(0,1){24.00}}
\put(37.00,13.00){\vector(-1,1){24.00}}
\put(9.00,25.00){\makebox(0,0)[cc]{$\subtype$}}
\put(41.00,25.00){\makebox(0,0)[cc]{$\subtype$}}
\put(22.00,20.00){\makebox(0,0)[cc]{$\subtype$}}
\put(28.00,20.00){\makebox(0,0)[cc]{$\subtype$}}
\end{picture}
\end{center}

\caption{Another counter-example for the coercion order}
\label{fignolatoth}
\end{figure}

The type constructors $\tf{FF}$ and $\tf{up}$ have such properties.
However, we can define
$$\tf{up}(\tf{FF}(R)) \subtype \tf{FF}(\tf{up}(R))$$ for any integral
domain $R$ using a coercion which is not a direct embedding nor a
structural coercion.

So in this case some ``ad hoc knowledge'' can be used to avoid that
the partial ordering induced by $\subtype$ is not a quasi-lattice.

In general, it does not seem to be justified to assume that the
partial ordering induced by $\subtype$ is a quasi-lattice.

\subsection{Combining Type Classes and Coercions}
\label{seccomtcco}

Let
$${\rm op} : \overbrace{v_\sigma \times \cdots \times v_\sigma}^{n}
\longrightarrow v_\sigma$$ be an $n$-ary operator defined on a type
class $\sigma$ and let $A \subtype B$ be types belonging to $\sigma$
and let
$$ \phi: A \longrightarrow B$$ be the coercion function.  Moreover,
let ${\rm op}_A$ and ${\rm op}_B$ be the instances of ${\rm op}$ in
$A$ resp.\ $B$.

For $a_1, \ldots, a_n \in A$ the expression
$${\rm op}(a _1, \ldots, a_n)$$ might denote different objects in $B$,
namely
$${\rm op}_B(\phi(a_1), \ldots, \phi(a_n))$$ or
$$\phi({\rm op}_A(a_1, \ldots, a_n)).$$

The requirement of a unique meaning of
$${\rm op}(a _1, \ldots, a_n)$$ just means that $\phi$ has to be a
{\em homomorphism} for $\sigma$ with respect to ${\rm op}$.

The typing of ${\rm op}$ in the example above is only one of several
possibilities.  In general if $\sigma$ is a type class having
$p_{\tau_1}, \ldots, p_{\tau_k}$ as parameters ---
i.\,e.\ $p_{\tau_i}$ is a type variable of sort $\tau_i$ --- then a
$n$-ary first-order operation ${\rm op}$ defined in $\sigma$ can have
the following types.\footnote{For simplicity, we will exclude in the
  following discussion arbitrary polymorphic types different from type
  variables.  Especially, we will not regard higher-order functions,
  which do not play a central role in computer algebra although they
  are useful, cf.\ Sec.~\ref{posappcom}.  For the other relevant cases
  of polymorphic types the following can be generalized easily.}
$${\rm op}: \xi_1 \times \cdots \times \xi_n \longrightarrow
\xi_{n+1},$$ where $\xi_i$, $1 \leq i \leq n+1$, is either $v_\sigma$,
or $p_{\tau_l}$, $l \leq k$, or a ground type $t_m$.

As on page~\pageref{defcatsigmai} let $\cat{C}_{\sigma}$ be the
category of ground types of sort $\sigma$ as objects and the coercions
as arrows.  For a ground type $t$ let $\cat{C}_t$ be the subcategory
which has $t$ as single object and has thus the identity on $t$ as
single arrow.\footnote{If the type system is not coherent this
  subcategory might have more than one arrow.}  Now let
$$ \cat{C}_i=\left\{
\begin{array}{ll}
\cat{C}_{\sigma}, & \mbox{if }\xi_i = v_\sigma,\\ \cat{C}_{\tau_l}, &
\mbox{if }\xi_i = p_{\tau_l},\\ \cat{C}_{t_m}, & \mbox{if }\xi_i = t_m
\mbox{ for a ground type $t_m$}.
\end{array}
\right.
$$

Let $\rtypeasop$ be a functor from $\cat{C}_1 \times \cdots \times
\cat{C}_n$ into $\cat{C}_{n+1}$.  If $(\zeta_1, \ldots, \zeta_n)$ is
an object of $\cat{C}_1 \times \cdots \times \cat{C}_n$, i.\,e.\
$$ \zeta_i=\left\{
\begin{array}{ll}
A_{\sigma}, & \mbox{if }\xi_i = v_\sigma \mbox{ and $A_\sigma$ is a
  ground type belonging to $\sigma$},\\ A_{\tau_l}, & \mbox{if }\xi_i
= p_{\tau_l}\mbox{ and $A_{\tau_l}$ is a ground type belonging to
  $\tau_l$},\\ t_m, & \mbox{if }\xi_i = t_m,
\end{array}
\right.
$$ then $\rtypeasop(\zeta_1, \ldots, \zeta_n)$ is an object of
$\cat{C}_{n+1}$, i.\,e.\ a ground type belonging to $\sigma$
resp.\ $\tau_{l'}$, or is a ground type $t_{m'}$ depending on the
value of $\xi_{n+1}$.

Informally $\rtypeasop$ can be used to specify the type of the range
of an instantiation of ${\rm op}$ if instantiations of $\sigma$ and
the parameters of $\sigma$ are given.  We need a functor $\rtypeasop$
because of the following reason.  Given two instantiations of the type
class which can be described by $(\zeta_1, \ldots, \zeta_n)$ and
$(\zeta'_1, \ldots, \zeta'_n)$ such that
$$\zeta_i \subtype \zeta'_i \quad \forall i \leq n$$ it is necessary
that
$$\rtypeasop(\zeta_1, \ldots, \zeta_n) \subtype \rtypeasop(\zeta'_1,
\ldots, \zeta'_n).$$ Otherwise, if $a_i$ is an object of type
$\zeta_i$, $1 \leq i \leq n$, the expression
$${\rm op}(a_1, \ldots, a_n)$$ has the types $\rtypeasop(\zeta_1,
\ldots, \zeta_n)$ and $\rtypeasop(\zeta'_1, \ldots, \zeta'_n)$ for
which a coercion has to be defined in order to give the expression a
unique meaning.

If $\sigma$ is a non-parameterized type class {\em any} mapping
assigning an appropriate type to a tuple $(\zeta_1, \ldots, \zeta_n)$
can be extended to a functor.  So the requirement that $\rtypeasop$ is
a functor is only a restriction for parameterized type classes.

Since in a coherent type system there are unique coercions between
types, we will omit the names of the coercions in the following and we
will write
$$\rtypeasop(\zeta_1 \subtype \zeta'_1, \ldots, \zeta_n \subtype
\zeta'_n)$$ for the image of the single arrow between the objects
 $$(\zeta_1, \ldots, \zeta_n) \mbox{ and } (\zeta'_1, \ldots,
\zeta'_n)$$ in the category
$$\cat{C}_1 \times \cdots \times \cat{C}_n$$ under the functor
$\rtypeasop$.  Thus $\rtypeasop(\zeta_1 \subtype \zeta'_1, \ldots,
\zeta_n \subtype \zeta'_n)$ is an arrow in $\cat{C}_{n+1}$.

Let $\catofsets$ be the category of all set as objects and functions
as arrows.\footnote{Notice that the category theoretic object
  $\catofsets$ is quite different from the {\sf Axiom} category {\tt
    SetCategory}.}

By the assumption of set theoretic ground types and coercion functions
we can assign to any object of $\cat{C}_\sigma$ an object of
$\catofsets$ and to any arrow in $\cat{C}_\sigma$ an arrow of
$\catofsets$ in a functorial way.  We will write
$\typesetinterpr{\cat{C}_\sigma}$ for the functor defined by this
mapping.

We will use the notation $\zeta_i \subtype \zeta'_i$ to denote the
single arrow between $\zeta_i$ and $\zeta'_i$ in $\cat{C}_i$.  Thus
$$ \typesetinterpr{\cat{C}_1} \times \cdots \times
\typesetinterpr{\cat{C}_n}(\zeta_1 \subtype \zeta'_1, \ldots, \zeta_n
\subtype \zeta'_n)
$$ is an arrow in the category
$$\underbrace{\catofsets \times \cdots \times \catofsets}_{n}.$$ Since
$n$-tuples of sets are sets there is a functor from $\catofsets ^n$
into $\catofsets$ which we will denote by $\flatsetn$.

If $(\zeta_1, \ldots, \zeta_n)$ is an object in $\cat{C}_1 \times
\cdots \times \cat{C}_n$ we are now ready to formalize a requirement
on the instantiation of ${\rm op}$ given by $(\zeta_1, \ldots,
\zeta_n)$.  We will not impose this condition directly on ${\rm
  op}_{(\zeta_1, \ldots, \zeta_n)}$.  It will be convenient to regard
the set-theoretic interpretation
$$\typesetinterpr{\cat{C}_1} \times \cdots \times
\typesetinterpr{\cat{C}_n}(\zeta_1, \ldots, \zeta_n)$$ of $(\zeta_1,
\ldots, \zeta_n)$ instead this $n$-tuple of types itself.  Then the
set-theoretic interpretation of ${\rm op}_{(\zeta_1, \ldots,
  \zeta_n)}$ induces a function between
$$\flatsetn(\typesetinterpr{\cat{C}_1} \times \cdots \times
\typesetinterpr{\cat{C}_n}(\zeta_1, \ldots, \zeta_n))$$ and
$$\typesetinterpr{\cat{C}_{n+1}}(\rtypeasop(\zeta_1, \ldots,
\zeta_n)),$$ which we will denote by $\opinterprset{{\rm op}}(\zeta_1,
\ldots, \zeta_n)$.

Given $(\zeta_1, \ldots, \zeta_n)$ and $(\zeta'_1, \ldots, \zeta'_n)$
such that
$$\zeta_i \subtype \zeta'_i \quad \forall i \leq n$$ we just need that
the following diagram is commutative.

\begin{center}
\resetparms \setsqparms[1`1`1`1;2000`700]
\square[\flatsetn(\typesetinterpr{\cat{C}_1} \times \cdots \times
  \typesetinterpr{\cat{C}_n}(\zeta_1, \ldots, \zeta_n))`
  \typesetinterpr{\cat{C}_{n+1}}(\rtypeasop(\zeta_1, \ldots,
  \zeta_n))` \flatsetn(\typesetinterpr{\cat{C}_1} \times \cdots \times
  \typesetinterpr{\cat{C}_n}(\zeta'_1, \ldots, \zeta'_n))`
  \typesetinterpr{\cat{C}_{n+1}}(\rtypeasop(\zeta'_1, \ldots,
  \zeta'_n)); \opinterprset{{\rm op}}(\zeta_1, \ldots, \zeta_n)` {\rm
    L}` {\rm R}` \opinterprset{{\rm op}}(\zeta'_1, \ldots, \zeta'_n)]
\end{center}

In the diagram above we have set
$${\rm L} =\flatsetn( \typesetinterpr{\cat{C}_1} \times \cdots \times
\typesetinterpr{\cat{C}_n}(\zeta_1 \subtype \zeta'_1, \ldots, \zeta_n
\subtype \zeta'_n))$$ and
$${\rm R} = \typesetinterpr{\cat{C}_{n+1}}(\rtypeasop (\zeta_1
\subtype \zeta'_1, \ldots, \zeta_n \subtype \zeta'_n)).
$$

This requirement on $\opinterprset{{\rm op}}$ can be read that
$\opinterprset{{\rm op}}$ is a {\em natural transformation} between
the functor
$$\flatsetn \circ (\typesetinterpr{\cat{C}_1} \times \cdots \times
\typesetinterpr{\cat{C}_n})$$ and the functor
$$\typesetinterpr{\cat{C}_{n+1}} \circ \rtypeasop .$$

Thus for a $n$-ary first-order operator ${\rm op}$ the requirements
that
\begin{enumerate}
\item the assignments of a range type for an operation given
  instantiations of a type class and its parameters has to be
  ``functorial'' and
\item the instantiation of the operator has to correspond to a natural
  transformation between functors giving the set-theoretic
  interpretations of the ground types and the coercions between them
\end{enumerate}
will guarantee that type classes and coercions interact nicely,
i.\,e.\ give expressions involving ${\rm op}$ a unique meaning.


A brief inspection of the examples of parameterized type classes
occurring in {\sf Axiom} by the author has suggested that there is no
example violating the first requirement which will always hold in
non-parameterized type classes.  Nevertheless, a formal requirement
for a computer algebra language seems to be useful to ensure that no
such violating will occur in future extensions.

The second requirement is formulated as one on the possible
instantiations of operators.  However, it can also be read that given
the instantiations only certain coercions between base types are
allowed, namely only coercions for which the interpretation is a
natural transformation.  We will show below that using this view we
can conclude that only ``injective'' coercion functions are allowed
between most types.\footnote{In the following we will precisely state
  what we mean by ``injective'' and ``most types.''}


\begin{remark}
Our conditions imposed on the combination of type classes and
coercions are an adaptation of the work of Reynolds \cite{Reyn80}
on {\em category-sorted algebras}.  The difference is that Reynolds
allows each operator to be generic, i.\,e.\ that it may be
instantiated with any type in any position.  We allow type-class
polymorphism at some position and do not allow polymorphism at all in
other positions which seems to be the natural way to describe many
important examples.
\end{remark}

\subsubsection{Injective Coercions} 

An important type class is the class of types on which a test for
equality of objects can be performed in the system.\footnote{It is
called {\tt Eq} in {\sf Haskell} and {\tt SetCategory} in 
{\sf Axiom}.}  In this type class the operator symbol
$$= \: : t_{\cf{Eq}} \times t_{\cf{Eq}} \longrightarrow \tf{Boolean}$$
is used to denote the system test for equality.  In order to
distinguish between the ``system equality'' and ``true equality'' we
will use
$${\tt isequal} : t_{\cf{Eq}} \times t_{\cf{Eq}} \longrightarrow
\tf{Boolean}$$ for the system equality in the following.

Then the boolean values of
$${\tt isequal}(a_1, a_2)$$ and
$${\tt isequal}(\phi(a_1),\phi(a_2))$$ have to be the same.
Especially, if the latter evaluates to {\tt true} then the former also
has to evaluate to {\tt true}.  In analogy to the definition of
injective this means that $\phi$ has to be an injective function
``modulo system equality'' (usually, the definition of injective
involves true equality).

Thus coercions between types belonging to the ``equality type class''
have to be ``injective.''

The system equality for a type might very well differ from the
equality defined on a certain data type representing it.  So very
often the rational numbers are just represented as pairs of integers.
Then different pairs of integers can represent the same rational
number, thus the system test for equality of rational numbers is
different from the equality on pairs of integers.

Of course, a non-injective coercion function would not violate our
requirements, if $A$ and $B$ do not use the same operator symbol as a
test for equality.  Thus defining two different type classes
$\cf{Eq1}$ and $\cf{Eq2}$ with operators ${\tt isequal1}$ resp.\ ${\tt
  isequal2}$ as tests for equality and having $A$ of type class
$\cf{Eq1}$ and $B$ of type class $\cf{Eq2}$ would allow to define a
non-injective function to be a coercion between $A$ and $B$.  Defining
such different type classes is also a clear indication for the user
that there are problems.  Exposing a problem seems to be preferable
than hiding it and and hoping that it will not occur.  Although
usually for two elements $a_1$ and $a_2$ of type $A$ the test for
equality in $A$ will be used and not the one in $B$ it might happen
that one of the elements is coerced to $B$. Probably, this will not
happen very frequently which makes the situation even more dangerous,
since the system will wrongly say that two elements are equal only in
situations which are rather complicated so that the behavior of the
system might not be clear for the user.\footnote{For instance, the
  situation described above arises when coercions between (arbitrary
  precision) integers and floating point numbers are defined and the
  same symbol is used as a test for equality. Then two integers $a$
  and $b$ which are not equal might be equal if they are coerced to
  floating point numbers.  Such a coercion is used in many system if
  an expression like ``$a+0.0$'' occurs and can thus happen in
  situations which are quite surprising for the user.}

So the requirement of ``injective'' coercions seems to be absolutely
necessary for a computer algebra system although it is not required by
a system like {\sf Axiom}!\footnote{Since it is an undecidable problem
  to check whether a given recursive function is injective --- which
  can be easily proved by applying Rice's Theorem --- it is not
  possible to enforce by a compiler that coercions are injective if
  functions defined by arbitrary code can be declared to be coercions.
  Nevertheless, it seems to be useful to state this requirement as a
  guideline for a programmer.}

\subsection{Type Inference}
\label{sectypinfcoer}

In Sec.~\ref{sectytycl} we have seen that the type inference problem
for a language having type classes is decidable even if we have a
language with higher-order functions and one allowing parametric
polymorphism.  Moreover, there is a finite set of types for any object
of the language such that any type of the object is a substitution
instance of one of those types.

The type inference problem for a language with coercions is much more
complicated.  So there are objects which have infinitely many types
which are not substitution instances of finitely many (polymorphic)
types.\footnote{Using the results of Sec.~\ref{secaltyincoer} it will
  be possible to assign finitely many types to an object in the
  subsystem described in that section which have ``minimal
  properties'' among all types of the object.}  Consider a type
$\tf{R}$ belonging to a type class $\cf{commutative\_ring}$ and let
$r$ be be an object of type $\tf{R}$.  Given coercions
$$v_{\cf{commutative\_ring}} \subtype
\tf{up}(v_{\cf{commutative\_ring}})$$ then $r$ also has the types
$$\tf{up}(\tf{R}), \tf{up}(\tf{up}(\tf{R})), \ldots$$

In \cite{Mitc91}, \cite{Fuhx89}, \cite{Fuhx90} type systems for
functional languages allowing coercions between base types and
structural coercions are given and type inference algorithms for them.
These systems do not allow type class polymorphism nor parametric
polymorphism.  In \cite{Brea89}, \cite{Brea91} a system having
coercions and parametric polymorphism is given; however, no type
inference for the system is provided.

In \cite{That91} a type inference system for the case of type
isomorphisms induced by coercions is given which allows parametric
polymorphism.  However, as is argued in \cite{That91} if the
equational theory corresponding to the type isomorphisms is not
unitary unifying then the semantics of an expression involving {\tt
  let} may be ambiguous.  Moreover, the type inference problem is
reduced to an unification problem over the equational theory
corresponding to the type isomorphisms.  So in the case of an
undecidable equational unification problem
(cf.\ Example 3) only a semi-decision method is available
for type inference.

Type inference algorithms for a system allowing parametric
polymorphism and records resp.\ variants are given in \cite{Wand87},
\cite{Wand88}, \cite{Wand89}, \cite{Wand91}, \cite{Stan88},
\cite{Leis87}, \cite{Remy89}.  Since variants can be used to model
classes of isomorphic types some of these results can be applied if we
model classes of isomorphic types as variants.

Kaes \cite{Kaes92} gives a system allowing type-class polymorphism
(also parametric type classes can be described) which can handle
coercions between base types and structural coercions according to our
definition.\footnote{In the systems in \cite{Mitc91}, \cite{Fuhx90},
\cite{Fuhx89} all type constructors have to be monotonic or
antimonotonic in all arguments.}  However, direct embeddings are not
allowed.

In \cite{Como91} a type inference system and a semi-decision procedure
for it are described.  However, in that system some assumptions on the
properties on coercions are imposed which are not justified for many
examples occurring in computer algebra.\footnote{The problematic
assumptions are that all type constructors have to be monotonic in
all arguments and that any polymorphic type can be coerced to its
substitution instances.}  In \cite{Como91} a proof is given that the
type inference problem for the described system becomes undecidable if
no restrictions on the coercions are imposed.

Since there are infinitely many ground types in a system usually
infinitely many coercions will be necessary.  However, with the
exception of the example stated in Sec.~\ref{scoercprbl} all examples
of coercions we have given --- such as the direct embeddings and the
structural coercions --- can be described by a finite set of Horn
clauses which will usually have variables.  The formalism of Horn
clauses is strong enough to capture type classes and even parametric
type classes and also polymorphic types can be described.  Then the
typability of an object can be stated as the question whether a
certain clause is the logical consequence of the given set of Horn
clauses.  Thus using a complete Horn clause theorem
prover\footnote{Notice that {\sf PROLOG} is not one because of the
used depth-first search strategy.}  we have a semi-decision
procedure for type inference.  The size of the search space seems to
be a problem for the practical use of this method, but not the fact
that it is only a semi-decision procedure.  If an expression cannot be
typed using certain resources --- i.\,e.\ a typing of the expression
involves too many coercions if it is typeable at all --- it does not
seem to be a practical limitation if a system rejects the expression
as possibly untypeable and asks the user to provide more typing
information if the user thinks that the expression is typeable.

It is not clear which classes of coercions in connection with which
other typing constructs are allowed such that the type inference
problem is decidable.  Coercions between polymorphic types are
certainly a problem. In the following we will shortly discuss to what
extent some restrictions are justified for a computer algebra system.

If type inference has to be performed for user defined functions, then
polymorphic types arise naturally (cf.\ Sec.~\ref{secaxhasex}).  Since
the possibility to type user defined functions is useful for a
computer algebra system but does not play the same central role as for
a functional programming language it might be reasonable to exclude
them from type inference if coercions are present in order to
facilitate the problem.

But there are also other objects than functions that can be
polymorphic. Especially there are naturally occurring examples of 
{\em polymorphic constants}.

In {\sf Haskell} integer constants are polymorphic constants.  If $n$
is a constant denoting an integer then it also denotes the
corresponding objects of the types in the type class {\tt Num}.
Having a language allowing coercions the use of polymorphic constants
can be avoided for the examples used in {\sf Haskell}, because
coercions can be defined between the types belonging to {\tt Num} in
{\sf Haskell}.\footnote{In {\sf Haskell} only explicit conversions but
no implicit coercions are allowed.}

In a computer algebra system there are more types present which have
objects usually denoted by integer constants.  A nice example showing
the use of polymorphic constants in mathematical notation is given by
Rector \cite[p.~304]{Rect89}:
\begin{quote}
Consider
$$\frac{(x+y)^{1+n}+1}{1+nx}$$
where the user wants to work with rational functions
over a finite field of $p$-elements.
This formula  presents the problem of polymorphic constants.
To a mathematician, the types of  each subexpression are immediately clear:
$n$ is an integer variable which must be reduced modulo $p$
in the denominator of the expression,  $x$ and $y$ are finite field
variables, $1$  appearing in the exponent is an integer and
the other $1$'s are the multiplicative identity of the finite field.''
\end{quote}
Since there are no embeddings from $\ZZ$ into 
$\ZZ_m$ nor from $\ZZ_m$ into $\ZZ$ ---
for $n \neq m$ there is not even one from
the ring $\ZZ_m$ into the ring $\ZZ_n$\footnote{If $n =km$
then there is an embedding  of the {\em Abelian group}
$\langle \ZZ_m,+ \rangle$ into the Abelian group
$\langle \ZZ_n,+ \rangle$, namely the one given by the mapping
$i \mapsto ki$. Notice that a declaration of this embedding to
be a coercion between the corresponding types
and to have the elements of $\ZZ$ as polymorphic constants
(in their usual interpretation) in $\langle \ZZ_m,+ \rangle$
and in $\langle \ZZ_n,+ \rangle$ would contradict
the requirements stated in Sec.~\ref{seccomtcco}.} 
the use of polymorphic constants cannot
be avoided by introducing coercions.

\subsubsection{Algorithms for Type Inference}
\label{secaltyincoer}

In the following section we will restrict the types
to the ones which can be expressed as terms
of a finite order-sorted signature.
As we have seen in Sec.~\ref{secproossty} 
we can also assume that the signature is regular.

Let ${\rm op}$ be a $n$-ary operation,
$${\rm op}: \xi_1 \times \cdots \times \xi_n \longrightarrow
           \xi_{n+1},$$
where $\xi_i$, $1 \leq i \leq n+1$,
is either  a type variable $v_{\tau_l}$, $l \leq k$,
or a ground type $\overline{t}_i$.
Given objects
$o_1, \ldots , o_n$ having types
$t_1, \ldots, t_n$ respectively,
the expression
$${\rm op}(o_1, \ldots, o_n)$$
will be well typed having type $\xi_{n+1}$
iff the following conditions are satisfied.
\begin{enumerate}
\item If $\xi=\overline{t}_i$ for some ground type
$\overline{t}_i$ then $t_i \subtype \overline{t}_i$.
\item If $\xi_i = \xi_j = v_{\tau_l}$ for some
$i \neq j$ then there is a type $t : \tau_l$
such that
$t_i \subtype t$ and $t_j \subtype t$.
\item If $\xi_i = v_{\tau_k}$ then there is a type
$t:\tau_k$ such that $t_i \subtype t$.
\end{enumerate}

Notice that if we require that all
objects have ground types then algorithms solving the problems
imposed by the above conditions can be used
to solve the type inference problem using
a bottom-up process.\footnote{Similar ideas can be found
in \cite[Sec.~4]{Como91} and in \cite{Rect89}.}

If we do not restrict the possible coercions
then determining whether for given types
$t_1$ and $t_2$ there is a type $t$ such that
$t_1 \subtype t$ and $t_2 \subtype t$ might be an
undecidable problem (cf.\ \cite{Como91}). 

In the following we will restrict the possible coercions
to coercions between base types,\footnote{By the assumption of
a finite signature there are only finitely many base types and we will
assume that the finitely many
coercions between base types are effectively given.}
  direct embeddings and structural coercions.
In Sec.~\ref{seccoh} we have defined
the coercions only between ground types, because we
have given semantic considerations on coercions 
and it is not clear how to define a semantics
for arbitrary polymorphic types.
The algorithmic problems we are dealing with
in this section can be seen as algorithmic problems
on certain terms of an order-sorted signature where
an additional relation ``$\subtype$'' is given.
It will be convenient to define 
$\subtype$ also for polymorphic types, i.\,e.\ non-ground terms.
It is clear how  the definitions given in
Sec.~\ref{seccoh} for direct embeddings and
structural coercions can be extended to
polymorphic types. 

We will assume that for any type constructor $f$
the set of direct embedding positions ${\cal D}_f$
and the sets ${\cal A}_f$ and ${\cal M}_f$
are well defined, i.\,e.\ independent
of the arguments of $f$.
Moreover, we will assume that
for any types $t_1 \subtype t_2$ and any (sort-correct)
substitution $\theta$ we also have
$\theta(t_1) \subtype \theta(t_2)$.
These assumptions
are satisfied by all examples we gave and
are natural for the formalism of describing
types we use.

The advantage of extending  
the notions of direct embeddings and
structural coercions to polymorphic types is
that there are {\em finitely} many (polymorphic) types
$$t_1^1 \subtype t_1^2 , \ldots, t_r^1 \subtype t_r^2$$
such that for any types
$t_1 \subtype t_2$ there is
a (sort-correct) substitution $\theta$ and
an $1 \leq i \leq r$ such that
$$t_1 = \theta(t_i^1) \quad\mbox{and}\quad t_2 =\theta(t_i^2).$$

{\bf Proposition 2.}
{\sl Assume that the types  are terms of a finite, regular
order-sorted signature and that there are only
coercions between base types, direct embeddings and
structural coercions. Then for any type $t$, the set}
$${\cal S}_t = \{ \sigma \mid \exists t'\, . \, t' : \sigma \mbox{ and }
t \subtype t' \}$$
{\sl is effectively computable.}

\begin{proof}
We claim that the set ${\cal S}_t$
will be computed by ${\sf CSGT}(t)$
(see Fig.~\ref{figalCSGT}).

All computations which are used in 
${\sf CSGT}$ and ${\sf CSBT}$
can be performed effectively.
Since the signature is finite
there are always only finitely many possibilities 
which have to be checked in the existential clauses of
the algorithms and so 
algorithm ${\sf CSBT}$ will terminate and so will
${\sf CSGT}$.
Algorithm ${\sf CSGT}$ 
is correct (i.\,e.\ ${\sf CSGT}(t) \subseteq {\cal S}_t$),
because only types and
the sort of  types  $t$ can be coerced to are computed.
Its completeness
(i.\,e.\ ${\sf CSGT}(t) \supseteq {\cal S}_t$)
 follows from the fact that structural
coercions cannot add new sorts to ${\cal S}_t$.
\qed

\newsavebox{\algcsbt}
\newsavebox{\algcsgt}
\newsavebox{\algcsgtandcsbt}
\sbox{\algcsbt}{\begin{minipage}[l]{0.94\textwidth}
\begin{center}
${\cal S} \leftarrow {\sf CSBT}(t)$.
\end{center}
[Sorts of types a base type $t$ is coercible to.
 ${\cal S}$ is the set
of sorts of types in which $t$ can be coerced to.
Assumes that the signature is finite, only direct embeddings
and structural coercions are present.]
\begin{deflist}{(1)}
\item[(1)] [Initialize.] ${\cal T} 
                 \leftarrow \{ t' \mid t \subtype t' \mbox{ and }
                               t' \mbox{ is a base type}
                         \}$; \\
${\cal S} \leftarrow \{ \sigma' \mid t : \sigma' \}$;
${\cal S}' \leftarrow {\cal S}$; ${\cal T}' \leftarrow {\cal T}$.
\item[(2)] [Compute Direct Embeddings.] 
{\bf for} $\overline{t} \in {\cal T}$ 
       {\bf do}
     $\aldesbegbr$ 
{\bf if} there are
   $\overline{\sigma}$,
   $f :(\sigma_1 \cdots \sigma_n) \sigma'$,
   $i \in \{ 1, \ldots, n \}$ such that
      $\overline{t} : \overline{\sigma}$ 
          and $\sigma_i=\overline{\sigma}$
          and $i \in {\cal D}_f$
          and $\sigma' \notin {\cal S}$
{\bf then}
    $\aldesbegbr$
    ${\cal S}' \leftarrow {\cal S}' \cup \{ \sigma' \}$;
    ${\cal T}' \leftarrow {\cal T}' \cup
             \{ f(v_{\sigma_1}, \ldots, v_{\sigma_n}) \}$
       $\aldesendbr$ $\aldesendbr$.
\item[(3)] [Iterate if something is added.] {\bf if}
             ${\cal S'} \neq {\cal S}$
      {\bf then} $\aldesbegbr$ \\
        ${\cal S} \leftarrow {\cal S}'$;
    ${\cal T} \leftarrow {\cal T}' $;
{\bf goto~(2)} $\aldesendbr$.
\end{deflist}
\end{minipage}
}
\sbox{\algcsgt}{\begin{minipage}[l]{0.94\textwidth}
\begin{center}
${\cal S} \leftarrow {\sf CSGT}(t)$.
\end{center}
[Sorts of types a type $t$ is coercible to.
 ${\cal S}$ is the set
of sorts of types in which $t$ can be coerced to.
Assumes that the signature is finite, only direct embeddings
and structural coercions are present.]
\begin{deflist}{(1)}
\item[(1)] [$t$ base type.] {\bf if} $\com(t)=0$ {\bf then}
                 $\aldesbegbr\,
                   {\cal S} \leftarrow {\sf CSBT}(t)$;
                  {\bf return}$\aldesendbr$. 
\item[(2)] [Recurse.] Let $t=g(t_1, \ldots, t_m)$; \\
           {\bf for} $i = 1, \ldots, m$ 
              {\bf do} ${\cal S}_i \leftarrow {\sf CSGT}(t_i)$; \\
      {\bf for} $(\sigma_1, \ldots, \sigma_m) \in
                   {\cal S}_1 \times \cdots \times {\cal S}_m$
          {\bf do} $\aldesbegbr$ \\
        {\bf if}
               there is 
           $g: (\sigma_1 \cdots \sigma_m) \overline{\sigma}$
               such that
              $\overline{\sigma} \notin {\cal S}$
        {\bf then}
           $\aldesbegbr$ \\
             ${\cal T} \leftarrow 
                {\cal T} \cup \{ g(v_{\sigma_1}, \ldots, v_{\sigma_m}) \}$; 
             ${\cal S} \leftarrow {\cal S} \cup \{ \overline{\sigma} \} $;
            ${\cal S}' \leftarrow {\cal S}$; 
             ${\cal T}' \leftarrow {\cal T}$
               $\aldesendbr$ $\aldesendbr$.
\item[(3)] [Compute Direct Embeddings.]
{\bf for} $\overline{t} \in {\cal T}$
       {\bf do}
     $\aldesbegbr$
{\bf if} there are
   $\overline{\sigma}$,
   $f :(\sigma_1 \cdots \sigma_n) \sigma'$,
   $i \in \{ 1, \ldots, n \}$ such that
      $\overline{t} : \overline{\sigma}$
          and $\sigma_i=\overline{\sigma}$
          and $i \in {\cal D}_f$
          and $\sigma' \notin {\cal S}$
{\bf then}
    $\aldesbegbr$
    ${\cal S}' \leftarrow {\cal S}' \cup \{ \sigma' \}$;
    ${\cal T}' \leftarrow {\cal T}' \cup
             \{ f(v_{\sigma_1}, \ldots, v_{\sigma_n}) \}$
       $\aldesendbr$ $\aldesendbr$.
\item[(4)] [Iterate if something is added.] {\bf if}
             ${\cal S'} \neq {\cal S}$
      {\bf then} $\aldesbegbr$ \\
        ${\cal S} \leftarrow {\cal S}'$;
    ${\cal T} \leftarrow {\cal T}' $;
{\bf goto~(3)} $\aldesendbr$.
\end{deflist}
\end{minipage}
}
\sbox{\algcsgtandcsbt}{
\begin{minipage}[l]{0.96\textwidth}
\begin{center}
\usebox{\algcsgt}
\end{center}
\vspace{2\bigskipamount}
where
\vspace{2\bigskipamount}
\begin{center}
\usebox{\algcsbt}
\end{center}
\end{minipage}
}
\begin{figure}[tbhp]
\hfil\fbox{\usebox{\algcsgtandcsbt}}
\caption{Algorithms computing sorts of 
             types a given type can be coerced to}
\label{figalCSGT} 
\end{figure}


\end{proof}

In the following we will rule out
antimonotonic structural coercions,
i.\,e.\ we will require that
${\cal A}_f = \emptyset$ for all
type constructors $f$. 

Notice that the restriction 
${\cal A}_f = \emptyset$ 
does not exclude type constructors like
$\tf{FS}$ from the framework.
Only the automatic insertion of a coercion
giving rise to the antimonotony is excluded.
For instance, instead of having $\tf{FS}$ as
a type constructor which is
antimonotonic in its first argument and monotonic in
its second, it is one which is only monotonic in its
second argument. Such a restriction does not seem
to cause a loss of too much expressiveness.
This is an important difference to the system
in \cite{Como91}, in which  all type constructors
have to be monotonic in all arguments.
Type constructors which are antimonotonic in some
argument have to be excluded from that system in general,
because it is not  possible that a type constructor being
antimonotonic in some argument can be made monotonic
in that argument without changing the
intended meaning of the type constructor.
Thus our framework is more general in this respect
than the one in \cite{Como91}.
However, direct embeddings are a special form
of the ``rewrite relations'' for coercion considered
in that paper. 
So  the following can be seen as a solution  
for one of the open problems stated in \cite{Como91},
namely finding restrictions on the system of coercions
which will yield a decidable type inference problem.

{\bf Definition 26.}
{\sl If for two types $t_1$ and $t_2$ there is a type $t$
such that $t_1 \subtype t$ and $t_2 \subtype t$
then $t$ is called a {\em common upper bound}
of $t_1$ and $t_2$.}

{\sl A {\em minimal upper bound} $\mub(t_1,t_2)$ of two types
$t_1$ and $t_2$ is a type $t$ satisfying the following
conditions.
\begin{enumerate}
\item The type $t$ is a common upper bound of $t_1$ and $t_2$.
\item If $t'$ is a type which is a common
upper bound of $t_1$ and $t_2$
 such that $t' \subtype t$,
then $t \subtype t'$.
\end{enumerate}
A {\em complete set of minimal upper bounds}
for two types $t_1$ and $t_2$
is a set $\CSMUB(t_1,t_2)$ such that
\begin{enumerate}
\item
 all $t \in \CSMUB(t_1,t_2)$
are a minimal common upper bound of $t_1$ and $t_2$, and
\item for every type $t'$
which is a common upper bound of $t_1$ and $t_2$ there is
a $t \in \CSMUB(t_1,t_2)$ such that $t \subtype t'$.
\end{enumerate}}

If two types $t_1$ and $t_2$ have no minimal
upper bound then the complete sets of minimal upper bounds
are all empty. In this case we will write
$\CSMUB(t_1,t_2)=\emptyset$.
We will write $|\CSMUB(t_1,t_2)|$ to denote the
smallest cardinality
of  a complete set of minimal upper bounds of $t_1$ and $t_2$.

If the partial order induced by $\subtype$ is
a quasi-lattice then
$|\CSMUB(t_1,t_2)| \leq 1$ for all types $t_1$ and
$t_2$.
However, as  we have seen in Sec.~\ref{secpropcoerpreord}
this partial order will not be a quasi-lattice in general.

In the following we will assume that for
any two {\em base types} $t_1^{\rm b}$ and $t_2^{\rm b}$ a
{\em finite}
complete
set of minimal upper bounds can be computed effectively,
say by ${\sf CSMUBBT}(t_1^{\rm b},t_2^{\rm b})$.
We will give an algorithm computing for any
two types $t_1$ and $t_2$ a complete set of minimal
upper bounds and will show that this set is finite.

{\bf Theorem 9.}
{\sl Assume that all coercions are coercions between base types, direct
embeddings and structural coercions.  Moreover, assume that for all
type constructors $f$ there is at most one direct embedding position,
i.\,e.\ $|{\cal D}_f| \leq 1$, and no antimonotonic coercions are
present, i.\,e.\ ${\cal A}_f=\emptyset$, and for any base types
$t_1^{\rm b}$ and $t_2^{\rm b}$ there is a finite complete set of
minimal upper bounds with respect to the set of base types which can
be effectively computed by a function ${\sf CSMUBBT}(t_1^{\rm
b},t_2^{\rm b})$.}

{\sl Then for any two types $t_1$ and $t_2$ there is a finite complete set
of minimal upper bounds which can be effectively computed.}

\begin{proof}
We claim that algorithm ${\sf CSMUBGT}$
(see Fig.~\ref{algCSMUBGT})
 terminates
for any input parameters $t_1$ and $t_2$ and
computes a complete set of minimal upper bounds
which is finite.


\newsavebox{\algcsmubgt}
\newsavebox{\algcsmubbt}
\newsavebox{\algcsmuall}
\sbox{\algcsmubgt}{
\begin{minipage}[l]{0.94\textwidth}
\begin{center}
${\cal U} \leftarrow {\sf CSMUBGT}(t_1,t_2)$
\end{center}
[${\cal U}$ is a complete set of minimal upper bounds
of two types $t_1$ and $t_2$. Requires that 
only direct embeddings and structural coercions are used,
$|{\cal D}_f| \leq 1$
and ${\cal A}_f = \emptyset$ for any type constructor
$f$. Assumes that algorithm {\sf CSMUBBT} returns a finite
set.]
\begin{deflist}{(1)}
\item[(1)] [$t_1$ and $t_2$ base types.]
{\bf if} $\com(t_1)=1$ and $\com(t_2)=1$
     {\bf then }
                $\aldesbegbr$
                 ${\cal U} \leftarrow {\sf CSMUBBT}(t_1,t_2)$;
                  {\bf return}
                $\aldesendbr$.
\item[(2)] [Ensure that $\com(t_1) \leq \com(t_2)$.]
            {\bf if} $\com(t_1) > \com(t_2)$ {\bf then}
                $\aldesbegbr$
                 $h \leftarrow t_1$; $t_1 \leftarrow t_2$;
                $t_2 \leftarrow h$
                $\aldesendbr$.
\item[(3)] [$t_1$ a base type.]
          {\bf if} $\com(t_1) = 1$ {\bf then}
             $\aldesbegbr$ \\
          let $t_2 = f(t_2^1, \ldots, t_2^n)$; \\
          {\bf if} $|{\cal D}_f| = 0$ {\bf then}
            $\aldesbegbr$
             ${\cal U} \leftarrow \emptyset$;
             {\bf return}$\aldesendbr$; \\
               let ${\cal D}_f = \{ i \}$; \\
               ${\cal U}' \leftarrow {\sf CSMUBGT}(t_1,t_2^i)$;
               \begin{deflist}{(3.1)}
               \item[(3.1)] {\bf if} ${\cal U}' = \emptyset$
                  {\bf then} $\aldesbegbr$
                           ${\cal U} \leftarrow \emptyset$;
                           {\bf return}$\aldesendbr$;
                \item[(3.2)] {\bf if} ${\cal U}' \neq \emptyset$
                                    {\bf then} $\aldesbegbr$ 
                       {\bf if} $i \in {\cal M}_f$ {\bf then}
                         $\aldesbegbr$
                             ${\cal U} \leftarrow \emptyset$; \\
                             {\bf for} $t' \in {\cal U}'$
                                     {\bf do} \\
                             ${\cal U} \leftarrow {\cal U} \cup
                                 \{ f(t_2^1, \ldots, t_2^{i-1},
                                       t', t_2^{i+1},\ldots, t_2^n) \}$
                          $\aldesendbr$; \\
                       {\bf if} $i \notin {\cal M}_f$ {\bf then}
                         $\aldesbegbr$
                       {\bf if} $t_2^i \in {\cal U}'$
                     {\bf then} ${\cal U} \leftarrow \{t_2\}$ \\
                     {\bf else} ${\cal U} \leftarrow \emptyset$$\aldesendbr$ 
                           {\bf return}
                              $\aldesendbr$ $\aldesendbr$.
                  \end{deflist}
\item[(4)] [General case.] let $t_1 = g(t_1^1, \ldots, t_1^m)$; 
                           let $t_2 = f(t_2^1, \ldots, t_2^n)$;
               ${\cal U} \leftarrow \emptyset$.
\item[(5)] [Structural coercions.] {\bf if} $f=g$ {\bf then}
                  $\aldesbegbr$ \\
                  {\bf for} $i \in {\cal M}_f$ {\bf do}
                        ${\cal U}_i \leftarrow {\sf CSMUBGT}(t_1^i,t_2^i)$; \\
                 let ${\cal M}_f = \{ j_1, \ldots, j_l \}$; \\
        {\bf if} $t_1^k = t_2^k$
           for all $k \in \{1, \ldots, n \} - {\cal M}_f$
         {\bf then} $\aldesbegbr$ \\
           {\bf for} $(t'_{j_1}, \ldots, t'_{j_l}) \in
                       {\cal U}_{j_1} \times \cdots \times {\cal U}_{j_l}$
                  {\bf do}
                     $\aldesbegbr$ \\
            {\bf for} $k \in \{1, \ldots, n \} - {\cal M}_f$
                 {\bf do} $t'_k \leftarrow t_1^k$; \\
                   ${\cal U} \leftarrow {\cal U} \cup
                         \{ f(t'_1, \ldots, t'_n) \}$
                    $\aldesendbr$ $\aldesendbr$ $\aldesendbr$. 
\item[(6)] [Direct embeddings in $g$.] 
                    {\bf if }
        $|{\cal D}_g| = 1$ {\bf then} $\aldesbegbr$ \\
let ${\cal D}_g = \{ i \}$; 
               ${\cal U}' \leftarrow {\sf CSMUBGT}(t_1^i,t_2)$; \\
                 {\bf if} ${\cal U}' \neq \emptyset$
                                    {\bf then} $\aldesbegbr$ 
                       {\bf if} $i \in {\cal M}_g$ {\bf then}
                         $\aldesbegbr$ 
                             {\bf for} $t' \in {\cal U}'$
                                     {\bf do} 
                             ${\cal U} \leftarrow {\cal U} \cup
                                 \{ g(t_2^1, \ldots, t_2^{i-1},
                                       t', t_2^{i+1},\ldots, t_2^m) \}$
                          $\aldesendbr$; \\
                       {\bf if} $i \notin {\cal M}_g$ 
                       and $t_1^i \in {\cal U}'$
                     {\bf then} ${\cal U} \leftarrow {\cal U} \cup \{t_1\}$
                              $\aldesendbr$ $\aldesendbr$.
\item[(7)] [Direct embeddings in $f$.] {\bf if} 
               $|{\cal D}_f| = 1$ {\bf then} $\aldesbegbr$ \\
let ${\cal D}_f = \{ i \}$; 
               ${\cal U}' \leftarrow {\sf CSMUBGT}(t_1,t_2^i)$; \\
                 {\bf if} ${\cal U}' \neq \emptyset$
                                    {\bf then} $\aldesbegbr$ 
                       {\bf if} $i \in {\cal M}_f$ {\bf then}
                         $\aldesbegbr$  
                             {\bf for} $t' \in {\cal U}'$
                                     {\bf do} 
                             ${\cal U} \leftarrow {\cal U} \cup
                                 \{ f(t_2^1, \ldots, t_2^{i-1},
                                       t', t_2^{i+1},\ldots, t_2^n) \}$
                          $\aldesendbr$; \\
                       {\bf if} $i \notin {\cal M}_f$ and 
                        $t_2^i \in {\cal U}'$
                     {\bf then} ${\cal U} \leftarrow {\cal U} \cup \{t_2\}$
                              $\aldesendbr$ $\aldesendbr$.
\end{deflist}
\end{minipage}
}
\sbox{\algcsmuall}{
\begin{minipage}[l]{0.96\textwidth}
\begin{center}
\usebox{\algcsmubgt}
\end{center}
\end{minipage}
}
\begin{figure}[tbhp]
\hfil\fbox{\usebox{\algcsmuall}}
\caption{An algorithm computing a complete set of
minimal upper bounds}
\label{algCSMUBGT}
\end{figure}



We will prove this claim by
induction on the complexity of $t_1$
and $t_2$ along the steps of the algorithm.

If $t_1$ and $t_2$ are base types, then
${\sf CSMUBBT}(t_1,t_2)$ is also
 a complete set of minimal
 upper bounds of $t_1$ and
$t_2$ with respect to
all types. This subclaim can be proved by
induction on the complexity of possible common upper bounds
of $t_1$ and $t_2$ using the assumption that for
any type constructor $f$ we
have $|{\cal D}_f| \leq 1$.\footnote{Without this assumption
the subclaim is false in general.}

So the algorithm terminates for the case of base types
and
returns a finite set which is a complete set of minimal upper
bounds for $t_1$ and $t_2$.

The algorithm will terminate for all other $t_1$
and $t_2$, too.
Recursive calls of the algorithm are done on arguments
of which at least one has a strictly smaller complexity.
Since any of the recursive calls returns a finite set,
only  finitely many iterations have to be performed
by the algorithm and the returned set is finite.

Since only direct embeddings and
monotonic structural coercions are present,
any element of ${\cal U}$ is a minimal upper
bound of $t_1$ and $t_2$.
The set ${\cal U}$ will be a complete set of
minimal upper bounds, because
$|{\cal D}_f| \leq 1$ for any type constructor
and all other possibilities of minimal upper bounds
for $t_1$ and $t_2$ are covered by the algorithm. 


Since ${\sf CSMUBGT}$ returns a finite set
of types, the existence of a finite set of minimal
upper bounds follows from the correctness of
the algorithm.
\qed
\end{proof}

\begin{remark}
Since algorithm ${\sf CSMUBGT}$ uses the type constructors
given by its arguments and does not have to perform
a search on all type constructors,
it is not necessary that the signature is finite.
It is only necessary that there is an effective algorithm
which computes for any type constructor $f$ the sets
${\cal D}_f$ and ${\cal M}_f$, and that
the conditions imposed on algorithm ${\sf CSMUBBT}$ 
are fulfilled.\footnote{If the signature is finite,
these conditions will  always be fulfilled if the coercions
between the base types are effectively given.}

An example of an infinite signature
with such properties is
a finite signature extended with a
type constructor $\tf{M}_{m,n}$  for any
$m,n \in \NN$ 
with the intended meaning of building the $m\times n$-matrices
over commutative rings.
It is natural to define ${\cal M}_{\tf{M}_{m,n}}=\{1\}$ for all
$m,n \in \NN$ and to have ${\cal D}_{\tf{M}_{m,n}}=\emptyset$ for
$m\neq n$ and ${\cal D}_{\tf{M}_{n,n}}=\{1\}$ for any $n \in \NN$.
\end{remark}

\subsubsection{Complexity of Type Inference}
\label{secomtycoer}

In \cite{Wand89} and \cite{Linc92}
the complexity of type inference for expressions of the $\lambda$-calculus 
which are typed by allowing various possibilities of coercions are
investigated.

In \cite{Linc92} the problem is shown to be NP-hard if the order given
by the coercions is arbitrary but fixed by reducing the following
problem on partial orders called {\sc Pol-Sat} to it.
\begin{quote}
Given a partial order $\langle P, \leq \rangle$ and a set of
inequalities $I$ of the form $p \leq w$, $w \leq w'$, where $w$ and
$w'$ are variables, and $p$ is a constant drawn from $P$, is there an
assignment from variables to members of $P$ that satisfies all
inequalities of $I$?
\end{quote}
{\sc Pol-Sat} is an NP-complete problem.  It is shown to be NP-hard by
reducing the {\sf 3-SAT}-problem to it.\footnote{A proof that 
{\sf 3-SAT} is NP-complete can be found e.\,g.\ in
\cite[p.~347]{Davi94}.}  However, if only lattices are
allowed as partial orders in {\sc Pol-Sat} then the problem is
decidable in linear time.

A quite similar problem on partial orders, called {\sc Po-Sat} is
introduced in \cite{Wand89}, which is reduced in polynomial time to a
type inference problem using polymorphic functions.  The problem {\sc
  Po-Sat} is proven to be NP-complete for arbitrary partial orders but
to be solvable in polynomial time if the partial orders are restricted
to finite quasi-lattices.

A quite systematic study of the complexity of decision problems for
various partial orders which might be relevant for type inference is
given in \cite{Tiur92}.

\section{Other Typing Constructs}
\label{chapothtyc}

\subsection{Partial Functions}
\label{secpartfunc}

Many functions arising in the area of computer algebra are only
partially defined. Some basic examples are

\begin{enumerate}
\item division in a field, which is defined for non-zero elements only;
\item matrices over fields have inverses only, if they are regular;
\item the square-root over the reals exists for non-negative
values only.
\end{enumerate}

We could make partial functions total by introducing new types --- the
type of elements, on which the function is defined.

The following examples, which are taken from \cite{Farm90},
show that there are severe problems  if we were to take this solution.

Let $f$ be the binary functions over the reals defined by
$$f(x,y) = \sqrt{x-y}.$$
The function $f$ cannot be represented as a {\em binary\/} total-function
in a many-sorted algebra since the domain of $f$ is not a set of the
form $D_x \times D_y$, where $D_x$ and $D_y$ are
subsets of the real numbers.

It makes good sense to view division in a field as a partial function
with the second argument having the type of the field.  If in the case
of the rationals we were to restrict the second argument to a type
``non-zero rationals'', we would have made this function total.
However, this solution has a severe drawback.  A term such as
$1/(2-1)$ is no longer well-formed, since ``$-$'' is a function into
the rationals and not into the non-zero rationals only.

The usual solution which is taken in connection with
many-sorted and order-sorted algebras uses the ``opposite'' way.

New elements --- ``error  elements'' --- are introduced
and new types are built by adjoining these error elements.
A partial function is made total by setting the value
of the function to be an error element if it is undefined before,
see e.\,g.\ \cite{Smol89} for a more detailed description
of this construction.

This construction is also used in universal algebra in order
to embed a partial algebra in a full algebra,
see e.\,g.\ \cite[p.~79]{Grae79}.

In the area of computer algebra this approach is taken in
the computer algebra system {\sf Axiom}.

The disadvantage of this approach is that we loose information.
If we consider terms built out of partial functions and
total functions, we have to repeat the construction.
Since the range of the partial function has increased,
a previously total function has become partial, since it is
not defined on the error value.

In the general framework of many-sorted or order-sorted computations,
it might be difficult to regain the lost information.  There are
important examples, where the set of elements on which a partial
function is defined is only recursively enumerable but not recursive
(see e.\,g.\ \cite[p.~342]{Smol89} for an example).

In connection with a computer algebra system, a better solution should
be possible. In most cases, the set of elements a partial function is
defined on can easily be decided; in our examples a simple test for
being non-zero, non-negative or calculating a determinant would have
been sufficient.

Hence, in these cases it is decidable whether a
{\em ground term\/} is well formed, i.\,e. has an error value or not.

Finding conditions and algorithms which tell the (minimal) type of an
arbitrary term is an interesting problem, whose solution would be of
practical significance.

\subsubsection{Retractions}
\label{specialize}

The sum of two polynomials is in general again a polynomial.
However, if we add the polynomials $(-x+5)$ and $(x+2)$, we obtain
the  constant polynomial $3$ as a result.
For future computations it would be useful if we {\em retract}
the type of the result from \tf{integral polynomial} to
\tf{integer}

Since retractions are partially defined implicit conversion functions
the general framework developed for other kinds of partial functions
also applies to retractions.

\subsection{Types Depending on Elements}
\label{chtydeel}

In this section we will discuss typing constructs which correspond to
the case of elements as parameters to domain constructors in {\sf
  Axiom}.  We will use the term ``types depending on elements'' to
describe these types, because it seems to be more or less standard for
type theories including such constructs.

There are some important examples of data structures whose type
depends on a non-negative integer.
\begin{itemize}
\item Elements of $\ZZ_m$.
\item Vectors of dimension $n$.
\item The $m \times n$-matrices.
\end{itemize}
However, the elements a type can depend on are not restricted to
integers.

An algebraic number $\alpha$ over $\QQ$ is usually represented by its
minimal polynomial over the rationals.  Thus, an element of the field
$\QQ[\alpha]$ has a type depending on some polynomial over the
rationals.

An example of a type which depends on a matrix (namely the matrix
defining a quadratic form) is the one which is built by the domain
constructor $\tf{CliffordAlgebra}$ (see \cite[Sec.~9.9]{Jenk92}.

In group theory programs, very often a group is represented with
respect to its generators, cf.\ \cite{GAPx17}.  So the concept of
types depending on elements is a possibility to treat certain
structures which are treated as objects of a computation in a certain
context as {\em types} in another one (cf.\ Sec.~\ref{sgroupth}).

Some of the examples given above could be reformulated such that the
concept of types depending on elements is no longer necessary in order
to describe them.  So it might be sufficient to have only a type of
matrices of arbitrary dimension (over some ring) in the system and not
a type of $m \times n$-matrices.  Then matrix-multiplication or even
addition of two matrices would be partial functions only.  A treatment
of partial functions (cf.\ Sec.~\ref{secpartfunc}) would be sufficient
and the additional concept of types depending on elements could be
avoided.

However, for the case of $\ZZ_m$ it seems to be necessary to have for
any $m \in \NN$ also a type corresponding to $\ZZ_m$ in a system which
also allows the possibility to have computations on the integer $m$.

So the concept of types depending on elements is important for many
computer algebra applications.  Unfortunately, as we will show below
it is not possible to have type-safe compile-time type-checking.

\subsubsection{Undecidability of Type Checking}

\label{undetychtydeel}

{\bf Lemma 8.}
\label{lemundetychtydeel}
{\sl Let ${\cal R}$ be the class of unary recursive functions.
Then the following questions are undecidable:
\begin{enumerate}
\item For $f \in {\cal R}$, is  $f(x)=n$
for some fixed $n \in \NN$ and for all $x$?
\item  For $f \in {\cal R}$, is $f(x)$  a prime number for all $x$?
\item  For $f \in {\cal R}$, is $\gcd(f(x),n)=1$ for some fixed $n \in \NN$ 
and for all $x$?
\end{enumerate}}

\begin{proof}
All of the questions above are equal to determining the membership of
$f$ in certain classes of partial recursive functions, which are all
non-trivial.  So the lemma is proved by applying Rice's Theorem (see
e.\,g.\ \cite[p.~150]{Odif92}).  \qed
\end{proof}

Assume that the language is universal, i.\,e.\ every partial recursive
function can be computed in the language.  Assume that there is a type
corresponding to $\NN$ present in the language and that indeed every
unary recursive function can be represented in the system as one
having type $\NN \longrightarrow \NN$.  Moreover, assume that there is
a type corresponding to $\ZZ_m$ for any $m \in \NN$.

Let $n \in \NN$ and let $f: \NN \longrightarrow \NN$ be a unary
recursive function.  By Lemma 8 it cannot be
decided by a compiler, whether $\ZZ_{f(x)}$ and $\ZZ_n$ are equal.
Thus having $a \in \ZZ_{f(x)}$ and $b \in \ZZ_n$ and having a
polymorphic operation ${\tt op}$ with type
$$\forall t \, . \, t \times t \longrightarrow \tf{Boolean}$$
like the check for equality it cannot be decided at compile time
whether 
$${\tt op}(a,b)$$
is well typed.

Determining whether $\ZZ_{f(x)}$ is a field, i.\,e.\ whether $f(x)$ is
prime is also not possible at compile time.  So it cannot be decided
whether computations requiring that $\ZZ_{f(x)}$ is a field are legal.

Since it cannot be decided by the compiler whether $\gcd(f(x),n)=1$ it
is also impossible to decide whether the lifting connected with the
Chinese remainder theorem can be applied to an element of $\ZZ_{f(x)}$
and to one of $\ZZ_n$ giving one of $\ZZ_{f(x) \cdot n}$.

In the following we will show that it is necessary to allow such
run-time computations of elements a type depends on for many important
applications in computer algebra.

\subsubsection{Necessity of Run-Time Computations
of Elements Types Depend on}
 
Frequently, computations in $\ZZ_m$\footnote{Or in the ring of
  polynomials over $\ZZ_m$, etc.  In our framework these structures
  can be all expressed as types having $\ZZ_m$ substituted for a type
  variable.}  are done in the context of computer algebra because of
the following observation:

If one wants to have the solution for a problem over the integers,
then it is often possible to compute a $b \in \NN$ (a ``bound'') such
that for all $n \geq b$ the result of the computation in $\ZZ_n$ can
easily be extended to a solution for the problem over the
integers.\footnote{Many books on computer algebra can serve as
references, e.\,g.\ \cite{Buch82a} --- especially \cite{Laue82} or
\cite{Kalt83a} --- or \cite{Dave88}, \cite{Lips81}, \cite{Gedd92}, and
also \cite{Knut71}.}

Very often, these computations are not done directly in $\ZZ_b$, but
in $\ZZ_{p_1}, \ldots, \ZZ_{p_h}$ for primes $p_1, \ldots, p_h$.  The
results are then ``lifted'' either to $\ZZ_{p_1 \cdots p_h}$ by an
application of the Chinese remainder theorem or to $\ZZ_{p^l}$ by a
Hensel lifting.  The choice of $p_1, \ldots, p_h$ resp.\ of $p$ and
$l$ are such that $p_1 \cdots p_h \geq b$ resp.\ $p^l \geq b$.

However, the class of algorithms which is used to compute the bounds
can be fairly complicated.  Technically speaking, if $f(x)$ and $g(x)$
are two functions that can be computed by the class of algorithms used
for the bound computations, then it is undecidable whether
$$f(x) \equiv g(x) \quad\quad\forall x.$$
Let us now assume that we could restrict the occurring types to the
ones corresponding to $\ZZ_{p_1 \cdots p_k}$, where $\{p_1, p_2, p_3,
\ldots \}$ is the set of prime numbers.  However, it is undecidable
whether $p_1 \cdots p_k = p_1 \cdots p_{k'}$, if $k$ and $k'$ are
minimal such that $p_1 \cdots p_k \geq f(x)$ and $p_1 \cdots p_{k'}
\geq g(x)$.  So a compiler cannot decide whether a statement involving
an element of $\ZZ_{p_1 \cdots p_k}$ and one of $\ZZ_{p_1 \cdots
p_{k'}}$ requiring both to have the same type\footnote{Simple
operations such as a test for equality or addition can serve as
examples.}  will lead to a typing error or not.

\subsubsection{Calculi Dealing with Types Depending on Elements}

The results of this section show that it is useful to distinguish
between domains and elements as parameters of domain constructors.

Having only type classes as additional typing construct a static
typechecking is possible in principle in the former case.  In the
latter case it becomes undecidable, where we have argued that this
undecidability results are relevant for many examples occurring in
practical computer algebra applications.

For a user interface it is usually sufficient to perform type
inference on expressions which do not allow recursion and which do not
form a Turing-complete language for computations on elements types
depend on.

So the problems which yield that the type inference problem and even
the type checking problem is undecidable in the case of a computer
algebra language do not apply to the case of a user interface of a
computer algebra system.

Since the type of an element another type depends on can nevertheless
be quite complicated (see the examples given above) it seems to be
useful to have some sophisticated techniques available also for this
case.

During the last years several general type theories having the concept
``types depending on elements'' have been developed.  Some are
Martin-L\"of's Type Theory \cite{Mart80}, and the {\em Calculus of
  Constructions} of Coquand and Huet \cite{Coqu86}, They have been
explored extensively, especially as ``logical frameworks''
\cite{Huet91}.  For this purpose several subcalculi and variations
such as LF \cite{Harp93}, or Elf \cite{Pfen89}, \cite{Pfen91},
\cite{Pfen92} have been defined.  Some extensions of unification
algorithms to these type theories have been given in \cite{Elli89},
\cite{Pfen91a}.  For the purpose of computer algebra probably
another variant of this theories will be more suited than the
existing.  Nevertheless, it seems to be very likely that some of the
obtained results are applicable to the type inference problem for a
user interface of a computer algebra system.


\chapter[Type Systems 2]{A Type System for Computer Algebra}

This is based on Santas \cite{Sant95}. Changes have been made to 
integrate it.

Type systems for computer algebra systems like Axiom \cite{Jenk92}
and theorem provers like 
Nuprl \cite{Cons85}, or
functional programming languages like Standard ML (SML) 
\cite{Miln90} \cite{Miln91} \cite{Harp93a},
Haskell \cite{Huda92}, etc. and 
object oriented languages \cite{Meye92} \cite{Gold83} \cite{Grog91}
are usually stratified into levels or universes in a way similar to type
theories like 
Damas and Milner's \cite{Dama82} type schemes, 
Martin-Lof's \cite{Mart73} constructive mathematics, etc.

More specifically, Axiom's types are basically divided into
{\tt domains} and {\tt categories}; Domains are both {\sl types} for
run-time values like $Integer$, $Integer\rightarrow Boolean$ and
$Fraction(Integer)$, and {\sl packages} which include definitions of
other values. Categories are {\sl type} of domains, like $Ring$ and
$Module(R)$. Categories and domains can be parameterized with other
domains, allowing the definition of category and domain
constructors. Categories specify the values defined in domains, and
can form hierarchies of specifications.

SML's types belong to two universes: $U_1$, which includes {\tt types}
like $Integer$ and $String \rightarrow Boolean$, and the universe
$U_2$ of types of {\tt modules}; this system as extended by Mitchell,
Meldal, and Madhav \cite{Mitc91b} can include existential types
($\exists$ types) in $U_1$ and dependent product and sum types
($\Pi$ and $\Sigma$ types) in $U_2$. These two levels correspond
essentially to the separation into {\tt monomorphic} and
{\tt polymorphic} type expressions in the Damas and Milner
\cite{Dama82} system. {\sl Monotypes} are arbitrary base types, an
infinite number of {\tt type variables} and the types built from other
monomorphic types through function space constructor $\rightarrow$.
{\sl Polytypes} are the types which range over monotypes, through the
universal quantifier $\forall$, like the type of the identity
function: $\forall t.t\rightarrow t$. Since every monotype $\tau$ can
be viewed as a polytype (like application of $\forall t.\tau$ to any
monotype where $t$ is a type variable which does not exist in the type
expression $\tau$), we assume that there is an injection from
$U_1$ to $U_2$.

On the other side of the camp, object oriented languages include the
notions of {\tt class} and {\tt abstract class}, resulting in (a)
implementation classes which can define {\tt objects}, and (b)
abstract classes used explicitly for inheritance and specification of
behaviour.

The two-universe approach in Axiom has been designed and proved
beneficial for the modeling of algebraic concepts, where 
{\sl categories} correspond to {\sl algebras} and {\sl domains} to
sets of values which share common representation and functionality as
described by Davenport, Trager \cite{Dave90} and Gianni
\cite{Dave91}. Similar results have been accomplished in object
oriented languages like C$++$ \cite{Stro95}, where abstract classes
play the role of categories, while implementation classes model
domains in Barton and Nackman \cite{Bart94}. On the other hand, the
two universes and their corresponding elements in SML were design
decisions for purely type-theoretic reasons. Although approaches vary,
it is possible to define a mapping of the constructs defined in typed
computer algebra systems to constructs of type theory similar to the
ones used for the definition of programming languages like SML. This
is important in two ways: it clarifies the semantics of computer
algebra systems, and allows their further improvement by means of
extensions, removal of inconsistencies, more expressiveness, and
better performance. We try to accomplish these targets with the
definition of a strong and static type system based on a powerful
subset of the Axiom system, which we extend incrementally.

We briefly examine some of the most recent concepts for object
oriented programming and abstract datatypes for computer algebra
systems. We observe that these concepts although quite powerful are
not adequate for the properly typed modeling of the relations among
simple algebraic constructs like the domains of integers and
rationals.
These concepts are simplified and integrated with the notions of
domains and categories. We provide the formal definition of domain
categories by means of existential types and the calculus for
subtyping on categories. Our type system is enriched with transparent
types which form the main construct of the sharing mechanism. Domains
are extended to form packages while their corresponding types form
dependent sums. Parameterized categories and functors are examined and
their calculus is defined with subtyping rules. Sharing constraints
are proved to handle many cases of parameterization resulting in
flattening of parameterized categories with facilities for recursive
definitions at the category level; there we establish the conditions for
successful functor application. This reduces the load of the type
system considerably, so as to afford the inference and manipulation of
type classes.

Since Axiom has the most references in the literature of typed
computer algebra systems, the notation used here is based on an
Axiom-like language which assumes:
\begin{enumerate}
\item values which consist of value variables, records, record
components, (higher order) functions, term application, value
components of packages and {\sl let} expressions
\item type expressions with type variables, base types, record types,
tagged unions, recursive types, function types, type components from
packages, type constructors, and type application
\item domain expressions with domain variables, self, domain
constructors, parameterized domains and application
\item expressions which can be used either as domains or types
\item packages with package variables, package constructors, package
components of other packages, parameterized packages and application
\item categories with category variables, sharing, category
constructors, parameterized categories and application
\item sharing constraints
\item facilities for accessing package components
\end{enumerate}

The abstract syntax of the language is given by the following grammar:
\[\begin{array}{ll}
{\rm values:} & e ::=  x~|~\{x_i=e\}_{\forall i\in I}~|~e.x~|~
(x:\delta)\mapsto e~|~e~e~|~x\$w~|~ let~x : \phi := e~in~e\\
{\rm types:} & \tau ::= t~|~T~|~\{x_i:\tau\}_{\forall i\in I}~|
+[x_i:\tau]_{\forall i\in I}~|~\mu t.\tau~|~t\$\omega~|~
\tau\rightarrow\tau~|~t\mapsto\tau~|~\tau~\tau\\
{\rm domains:} & \delta ::= d~|~\%~|~add(\overline{\delta})~|~
(d:\sigma)\mapsto\delta~|~\delta~\omega\\
{\rm both:} & \phi ::= \delta~|~\tau\\
{\rm packages:} & \pi ::= m~|~add(\overline{\pi})~|~m\$\omega~|~
(m:\sigma)\mapsto\pi~|~\pi~\pi\\
{\rm categories:} & \sigma ::= c~|~with(\overline{\sigma})~|~
(m:\sigma)\mapsto\sigma~|~c~\omega\\
\delta-{\rm comps:} & \overline{\delta} ::= \emptyset~|~
Rep:=\phi;\overline{\delta}~|~x:=e;\overline{\delta}~|~
\delta;\overline{\delta}\\
\pi-{\rm comps:} & \overline{\pi} ::= \overline{\delta}~|~
m:=\pi;~\overline{\pi}~|~t:=r;~\overline{\pi}~|~\pi;~\overline{\pi}\\
\sigma-{\rm comps:} & \overline{\sigma} ::= \emptyset~|~
m:\sigma ;~\overline{\sigma}~|~t:Type;~\overline{\sigma}~|~
s;~\overline{\sigma}~|~x:\tau;~\overline{\sigma}~|~
\sigma;~\overline{\sigma}\\
{\rm sharings:} & s  ::=  t=\tau~|~t=d~|~t=d\$w~|~m=\omega\\
{\rm access:} & \omega  ::= m~|~\omega~\omega~|~m\$\omega\\
\end{array}\]

The components inside the {\sl with} and {\sl add} constructs are
optional. {\sl let} expressions can be defined for types, domains,
packages, and categories. Declarations in value expressions are
optional, thus $let~x:=e~in~e$ is valid too. Wherever we have
$t=thing$ or $t:Type$ we can assume also that $Rep=thing$ and
$Rep:type$ is allowed, but not vice versa. Records with $n>0$
components are represented as 
$\{x_i:\tau\}_{\forall i\in\{1,\ldots,n\}}$. Similarly for unions,
replacing \{\} with $+[]$. Values can include $n$-tuples of the form
$(e,\ldots,e)$ which are implicitly viewed as records indexed by
integers from 1 to $n$; the expression $()$ is the empty tuple and is
the same as \{\}; there are no 1-tuples. Tagged unions can model
enumerated types or variations as in:

$\quad bool:=true = false$

$\quad tree(t:Type):=empty + node : (tree~t, tree~t) + leaf : t$

The syntax of this language hardly distinguishes between domains and
packages; this comes in accordance with the Axiom language where
domains are packages. Since domains can be used in a context where
types are expected, $\phi$ defines a dummy union of domains and
types. Types can be lifted to domains, and the representation of
domains can be viewed as type. The exact transformations are examined
in what follows; this allows us to have domains in union and record
types without complicating the language. Additionally packages assume
the arrow types as special cases of $\Pi$ types where there are no
dependencies between the domain and the range of the types.

The calculus of the presented type system is given by a set of
selected rules. Each rule has a number of antecedent judgments above a
horizontal line (optional) and a conclusion judgment below the
line. Each judgment has the form $E\vdash\mathcal{A}$, for an
environment $E$ and an assertion $\mathcal{A}$, depending on the
judgment. Environments contain typing assumptions for variables, type
variable declarations and subtyping assumptions. Typical judgments are
$E\vdash x$ asserting that $x$ is definable in $E$, $E\vdash x:\sigma$
meaning that $x$ has type $\sigma$ under assumptions $E$ and
$E\vdash \sigma^\prime\sqsubseteq\sigma$ asserting that
$\sigma^\prime$ is a subtype of $\sigma$ (or equivalently,
$\sigma$ is a supertype of $\sigma^\prime$) in $E$; we demand that all
the environments be well formed. An environment can be extended
with new assumptions as in $E;x:\sigma;E^\prime\vdash x:\sigma;$ this
environment is well formed provided that there is no other assumption
for $x$ in $E;E^\prime;$ variables can be renamed in case of
conflicts.

Some of the examples given have been influenced by the modeling of
algebraic concepts in the Axiom library.

Although demonstration of the simplicity of our type system is out of
the scope of this chapter, we claim that it combines the concepts of
{\tt domain} and {\tt package} and Haskell-like {\tt type classes}, it
increases the expressiveness of the programs with the propagation of
sharing constraints, it distinguishes compile time and run-time
constructs allowing for static and strong type checking and more
optimizations and reduces the need for {\tt coercions} and
{\tt retractions} (described in Sutor and Jenks \cite{Suto87} and
analyzed by Fortenbacher \cite{Fort90} in Axiom.

\subsection{Object in Computer Algebra}

Many computer algebra operations involve purely functional objects:
functions or methods operate on objects and return a new object with a
new internal state, instead of updating the state of an older
object. If the objects are big (like lists, arrays, or polynomials)
then in-place updating is more common. In object-oriented programming
however there is always an updating of the internal state of an
object: an {\tt object} must be a {\tt reference}. Due to the poor
type-theoretic properties of references and side-effects, most type
theoretic accounts of object-oriented programming by
Cardelli and Wegner \cite{Card85},
Cardelli \cite{Card86}, 
Ghelli \cite{Ghel91}, 
Pierce \cite{Pier91},
etc. deal with purely functional objects and employ techniques used
for purely functional closures, or define special constructors for
dealing with the increased complexity of object subsumption and
effects.

Object oriented languages usually take one object (e.g. a rational
number) with methods and functions for reading and updating its
numerator and denominator or performing addition among rationals - to
be an element of a recursively defined type (Bruce and Mitchell
\cite{Bruc92}) like:
\begin{verbatim}
Rational : Category := with (
  new: (Integer,Integer) -> Rep ;
  numerator: Rep -> Integer ;
  denominator: Rep -> Integer ;
  SetNumDenom: Rep - (Integer,Integer) -> Rep ;
  + : Rep -> Rep -> Rep ;
  * : Rep -> Rep -> Rep ; etc. )
\end{verbatim}
where {\sl Rep} stands for the type under scope, here {\sl Rational},
or any subtype of {\sl Rational}.

This encoding hides the fact that Rationals may have an internal state
that is shared by all its methods: the responsibility for building a
new instance of Rational in response to a call to {\sl new} is placed
within the function {\sl new} itself. Another notation identifies the
representation of instances with the implementation of the
representation, as in Pierce and Turner \cite{Pier93}
\begin{verbatim}
RationalFun := (Rep: Type) -> with (
  new: (Integer,Integer) -> Rep ;
  numerator: Rep -> Integer ;
  denominator: Rep -> Integer ;
  SetNumDenom: Rep -> (Integer,Integer) -> Rep ;
  + : Rep -> Rep -> Rep ;
  * : Rep -> Rep -> Rep, etc. )
\end{verbatim}

This method gives the caller of {\sl new} the responsibility for
transforming the value returned by {\sl new} into a new Rational
instance. Although this notation may seem strange, it offers much in
terms of {\sl simplicity}: the entire development, including the
notions of {\sl Rep}, can be carried out without the need of
{\tt recursive types}, dealt by Amadio and Cardelli \cite{Amad93}
or {\tt extensible records} (Cardelli \cite{Card88a}, Mitchell
\cite{Mitc88a}, Wirth \cite{Wirt88}) which are implementation
dependent; however, expressiveness is sacrificed. Using recursive
types the above expressions can have any desirable formulation as in
\begin{verbatim}
RationalFun := (Rep < F(Rep)) -> with (
  new: (Integer,Integer) -> Rep ;
  etc. )
\end{verbatim}
where {\bf F} can be the functor {\sl RationalFun} itself. This
formulation is closer to our intuition about object-oriented
programming with subtypes. It can be further enriched with other
second-order constructs, which lead to the definition of special
{\tt object constructors} by Abadi and Cardelli (\cite{Abad94}
\cite{Abad94a}).

These scheme are based on {\tt existential types} 
(Mitchell and Plotkin \cite{Mitc88}) 
which hide the implementation of the objects,
allowing us to deal with first class {\tt abstract types} 
(Liskov \cite{Lisk79}, 
Burstall and Lampson \cite{Lamp88}, 
Milner and Tofte \cite{Miln91}, 
Bruce and Mitchell \cite{Bruc92}) 
independently of implementation details without
any cost in type safety. In the next sections we present some examples
of the advantages and the limitations of this approach: this leads us
to the introduction of constructs which make the representations
visible in order to add more flexibility to the type system keeping it
consistent and sound while increasing the efficiency of the
implementations.

{\sl RationalFun} is actually the type of a {\tt functor}, i.e. a
{\sl function from types to types} and specifies the visible behaviour
of the functions on rationals, in terms of the
{\sl abstract representation type}. An object satisfying this
specification consists of a list of functions of type
{\sl RationalFun(Rep)} for some concrete type {\sl Rep}, paired with a
state of type {\sl Rep}: both are surrounded with an abstraction
barrier that protects the {\tt internal structure} from access except
through the above specified functions. By internal structure we mean
either the internal state, or the hidden functions that operate on
this state, or any other operation including the ones mentioned in the
specification. This encapsulation is directly expressed by a (possibly
recursively defined) existential type:
\begin{verbatim}
Rational:=((Rep: Type) +> with (Rep, RationalFun(Rep))) SomeRep
\end{verbatim}
Abstracting {\sl RationalFun} from {\sl Rational} yields a higher
order type operator that, given a specification, forms the
{\tt type} of objects that satisfy it.
\begin{verbatim}
DeclareDomain := (Fun: Type -> Category) +>
  (((Rep: Type) +> with(Rep,Fun(Rep))) SomeRep)
\end{verbatim}
The type of {\sl Rational} objects can now be expressed by applying
the {\sl DeclareDomain} constructor to the specification of
{\sl RationalFun}:
\begin{verbatim}
Rational := DeclareDomain(RationalFun)
\end{verbatim}
or the shortcut:
\begin{verbatim}
Rational : RationalFun
\end{verbatim}

In order to give proper treatment to the interaction between
representations and subtyping, it is necessary to separate 
{\sl Rational} into the specifications of its functions and the
operators which capture the common structure of all object types. This
separation is also important for the semantical construction of
categories and the definition of the internal structures of the types.

\subsection{Multiple Representations}

Rationals are created using the function {\sl box}, which captures the
semantics of dynamic objects in object oriented programming. A
rational number with {\tt representation} (x:Integer,y:Integer),
{\tt internal state} (5,2) and method implementations:
\begin{verbatim}
numerator := (r:{x:Integer, y:Integer}) +> r.x
denominator := (r:{x:Integer, y:Integer}) +> r.y
\end{verbatim}
can be created as:
\begin{verbatim}
r: := box ( coerceTo ( ( Rep := {x:=5, y:=2},
  (numerator := (r:{x:Integer,y:Integer}) +> r.x ;
   + := (r:{x:Integer,y:Integer}) +> (arg: Rational) +>
     new((r.x*denominator arg + r.y*numerator arg)/
                               (r.y * denominator arg)
   etc. )),
  Rational ))
\end{verbatim}

The {\sl coerce} function here is only a syntactic construct, which
shows the compiler how to view the introduced list; in this example it
is the identity function since we provide exactly what is necessary
for building a rational.

The {\sl box} function is helpful for the implementation of the
{\sl new} function and the $+$ and $*$ operators which return new
instances of Rational:
\begin{verbatim}
new:=(initX:Integer, initY:Integer) +>
   box ( coerceTo ( (Rep:={x:=initX,y:=initY},...), Rational))
\end{verbatim}

Unlike Axiom, in object oriented systems based on {\tt delegation}
(Ungar and Smith \cite{Unga91}) or on {\tt subtyping} the elements of an
object type have different internal representations and different
internal representations of their functions. For example, a rational
with representation type $(x:Integer)$ might be implemented as
follows:
\begin{verbatim}
r2 := box ( coerceTo ( (Rep := {x:=5},
  (numerator := (r:{x:Integer}) +> r.x ;
   denominator := (r:{x:Integer}) +> 1 ;
   + := (r:{x:Integer}) +> (arg:Rational) +> new(r.x + numerator arg) ;
   etc. )),
  Rational ))
\end{verbatim}
and the definition of {\sl new} changes accordingly. The functions
{\sl SetNumDenom, numerator, denominator} have compatible signatures
as in the previous implementation since we abstract
$s:\{x:Integer\}$ to $s:Rep$.

Static type checking can be obtained even without having all the
information about the internal structure of the objects. The constants
0 and 1 can be implemented in any of the two ways, having $x$ assigned
to 0 and 1 respectively, while the $y$ field can be 1. This
variability will be helpful for defining type classes in a later
section.

Type checking however is complicated by the evaluation
process. Suppose that we invoke the function {\sl SetNumDenom} passing
to it the object $r_2$. We have to open $r_2$, and make sure that the
only functions applicable to its {\sl Rep} are the ones declared in
{\sl Rational}. Consequently we produce a new value of type {\sl Rep},
which is {\sl reboxed} as a rational number which has access to the
methods of $r_2$, and hides {\sl Rep}. The same process happens for
functions like $+$ and $*$. The functions {\sl numerator} and
{\sl denominator} do not return any new object, so no reboxing is
necessary. The complications appear when {\tt binary operations} are
introduced in this schema. Since the two arguments of the {\sl plus}
operator may have different representations, it is {\sl necessary}
that the objects are accessed through the functions that operate on
their structure, if the compiler allows this at all. However, calling
{\sl plus} from $r_1$ does not necessarily return the same results as
{\sl plus} from $r_2$. The problem can be transferred one level
deeper, namely at binary operations on integers, which might pose
similar representation complications. This is a result of the pure
object oriented scenario with message passing we have assumed so
far. Although such asymmetries do not seem appealing for ordinary
arithmetic operations, the above constructs are quite useful for the
implementation of operations which depend on one argument or
operations on elements of heterogeneous lists. In order to handle the
above, some object oriented systems are extended with generic
functions: this results in very complex calculi for modeling their
behaviour, which do not scale up easily for applications in computer
algebra.

\subsection{Domains and Categories}

It is useful to have all the operations defined in the previous
section in the class {\sl Rational}. In such case we assume a hidden
{\sl common representation} for all objects of class {\sl Rational},
so we can assume that Rational is defined locally:
\begin{verbatim}
let Rep := {x:Integer, y:Integer}
in RationalDomain := coerce (
  (new := (m:Integer, n:Integer) +> box {x:=m, y:=n} ;
   numerator := (s:Rep) +> s.x ;
   + := (s1:Rep) +> (s2:Rep) +> box(...) ;
   etc. ),
  RationalFun Rep)
\end{verbatim}

Many simplifications can be performed. Since all the objects of
{\sl RationalDomain} have the same representation, unboxing and
reboxing is not needed for performing the operations
{\sl SetNumDenom}, $+$, etc. These functions can operate directly on
the representations of the objects. By doing this we reduce the
computational cost of the boxing operations, be we restrict the
application of the above functions to objects of the same domain. The
function {\sl coerce} replaces syntactically all the annotations which
include {\sl Rep} with annotations which include {\sl
RationalDomain}. There are two main reasons for this: since {\sl Rep}
has been defined locally, it should not escape this local scope;
additionally, we may want {\sl Rep} to be opaque,
so that other definitions do not depend on
the implementation of this particular representation.

The above construct is not unique in Axiom. It corresponds to
{\tt abstype} declarations in SML, or {\tt clusters} introduced by
Liskov, Snyder, Atkinson, and Schaffert \cite{Lisk77} \cite{Lisk79}
in CLU and, as we saw before, classes with {\tt private members} in
object oriented languages.

\subsubsection{Categories}

We have seen that the instances of a class may have various
representations, while it is desirable to have an abstraction on the
level of functions defined for the instances of a class.
Mitchell and Plotkin \cite{Mitc88}
proposed that the {\sl interface part of a data type be viewed as a
type, while the implementation part as a value of that
type}. According to this analysis a domain comprises:
\begin{enumerate}
\item the internal representation of its instances
\item operations for analyzing and manipulating this representation
\item an abstraction barrier that prevents any access to the
representation except by means of the given operations
\end{enumerate}

Some of the functions defined in the example with rationals, like
{\sl numerator}, {\sl denominator}, {\sl SetNumDenom}, and {\sl new}
deal only with the representation of its instances. These functions
form the {\sl type of the representation}. Other functions like
$+$, $*$, $/$, and the constants 0 and 1 are expected to be declared
in a transparent fashion to any form of implementation. The
second set of operations can be included in the specification of many
other types, independently of the operations which manipulate the
representation. For instance ($+$) can be defined for Matrices and
Booleans, without any need for them to include operations like
{\sl numerator} or {\sl denominator}. At the same time, a
($+$) defined to operate on rationals, should not accept matrix values
as arguments.

We can transliterate this, using {\tt categories}:
\begin{verbatim}
RationalMeta := (Rep:Type) +> (
  + : Rep -> Rep -> Rep ;
  - : Rep -> Rep ;
  * : Rep -> Rep -> Rep ;
  / : Rep -> Rep -> Rep ;
  0 : Rep ;
  1 : Rep ;
\end{verbatim}

This specification can be viewed as a coding for the
{\tt algebraic structure} {\sl Field}: we write
\begin{verbatim}
Field:=RationalMeta
\end{verbatim}

{\sl Rep} refers to the implementation of the domains declared as
instances of Field:
\begin{verbatim}
Rational : Field
\end{verbatim}

The programming language {\sl Haskell} defines similar constructs
calling them {\tt typeclasses}. The analogous constructs for object
oriented languages are {\tt abstract classes}, although they cannot
express the constraint we have posed above.

Axiom's categories and object oriented abstract classes can provide
{\tt default definitions} in categories which can be overridden in
other categories or in the domains instances of these categories. This
construct has great expressive capabilities but we do not handle it in
this paper, because its semantics are not well understood and may
cause unsoundness in the type system. We will examine an alternative,
which can be extended to support default definitions at category
level.

\subsubsection{Existential Types}

Categories are the way {\tt existential types} are coded in the type
system. The category {\sl Field} corresponds to the existential type
\footnote{The fact that the name of the operators is lost in the
translation is of minor importance. We can assume canonical
representation of the components of {\sl Field} corresponding to the
ordering in the resulting list}.
\[\begin{array}{rl}
\exists Rep & [(Rep,Rep)\rightarrow Rep, Rep \rightarrow Rep,\\
&\quad (Rep,Rep)\rightarrow Rep,(Rep,Rep)\rightarrow Rep,Rep,Rep]
\end{array}\]
This means that there exists a type {\sl Rep}, which is used for the
implementation of the operations {\sl plus}, {\sl minus}, etc. with
the types $(Rep,Rep)\rightarrow Rep$, $Rep\rightarrow Rep$,
etc. respectively.

Existential types (\cite{Mitc88},\cite{Card85}) were introduced in
constructive logic by Girard \cite{Gira72} and are related to
{\tt infinite sums} in {\sl category theory}
(Herrlich and Strecker \cite{Herr73}).
If all the domains have a hidden representation, they are all
described by types of the form: $\exists Rep.C$ where $Rep$ is free in
$C$. In the domain declaration
\[D : C := add(Rep := \tau, M)\]
the type of the components of $M$ is the result of substituting $\tau$
for $Rep$ in $M_i$ doing the appropriate $\alpha$-conversions if
needed:
\[M_i : [\tau/Rep]C_i\]
allowing the type-checking of the definition of a domain. If a domain
does not define any $Rep$, then a {\tt type variable} is assumed for
the typing process; thus, $Rep$ can be instantiated to any other type
in extensions of this domain.

By declaring a variable as an instance of a domain $D$, the following
constants are introduced:
\[f_i : [D/Rep]C_i\]
i.e. the name of the domain $D$ replaces all occurrences of $Rep$ in
the above context. The only information provided to the global context
about $D$ is its name and its exported operations. If its
representation is visible the type system can infer the $D=\tau$.

Since representations may involve recursive definitions as in the case
of lists, we introduce the notion of {\tt recursive types}. A
recursive type $\mu t.\tau$ satisfies the equation:
\[\mu t.\tau = [\mu t.\tau/t]\tau\]
In order to avoid the continuous replacement of $Rep$ in $D$ in a
recursively defined domain, we can perform the substitution only once,
and then bind the nested occurrence of $Rep$ to a fresh variable
which can be equated to $D$ when needed during the inference
process. In this way, we abstract recursive types away, by using one
level of indirection provided by the existential types. The penalty we
pay is that type equality for recursive types cannot be
structural. The same type definition in two different contexts will
introduce two different fresh variables and consequently two different
types. Although the expressiveness is not the same as in the case of
pure recursive types, since we cannot reason about type equality as in
(Amadio and Cardelli \cite{Amad93}), this notation can model all the
cases of recursive types. \cite{Mitc88}.

\begin{figure}[t]
\[\begin{array}{|ll|}
\hline
&\\
\ [{\rm sub{\rm -}refl}] & E \vdash M \sqsubseteq M \\
&\\
\ [{\rm sub{\rm -}trans}] & \displaystyle
\frac{E\vdash S\sqsubseteq T\quad E\vdash T\sqsubseteq U}
{E\vdash S\sqsubseteq U}\\
&\\
\ [{\rm thinning}] & \displaystyle
\frac{E\vdash M_a \sqsubseteq M_b}
{E\vdash[\overline{M_a},M_{n+1}] \sqsubseteq  [\overline{M_b}]} \quad
\overline{\{M_1,\ldots,M_n\}}=M_1,\ldots,M_n\\
&\\
\ [\exists{\rm -opaque}] & \displaystyle
\frac{E,Rep:Type \vdash M_a \sqsubseteq M_b}
{E\vdash\exists Rep.M_a\sqsubseteq\exists Rep.M_b} \\
&\\
\ [\exists{\rm -transparent}] & \displaystyle
\frac{E,Rep=\tau\vdash M_1\sqsubseteq M_2}
{E\vdash\exists Rep=\tau .M_1\sqsubseteq\exists Rep=\tau.M_2}\\
&\\
\ [\exists{\rm -forget}] & \displaystyle
\frac{E,Rep=\tau\vdash M_1\sqsubseteq M_2}
{E\vdash\exists Rep=\tau .M_1\sqsubseteq\exists Rep .M_2}\\
&\\
\hline
\end{array}\]
\caption{\label{figure1}Subtyping rules for $\exists$-types}
\end{figure}

Domains have the {\tt transparent type}:
\[\exists Rep = \tau.C\]
where $Rep$ is free in $C$. This corresponds to a category where the
representation of a domain is visible.

The transparent type $\exists Rep = \tau.C$ can be reduced to the
{\tt opaque type} $\exists Rep . C$ by {\sl forgetting or hiding} the
constraint $Rep=\tau$. In such case all the information about the
representation of $D$ is lost: $D$ is an {\tt abstract type}.

\subsubsection{Subtyping among Categories}

Categories build hierarchies analogous to the existing hierarchies in
algebra as described by Davenport and Trager \cite{Dave90}. Although
algebraic hierarchies are based on set theoretic terms concerning the
properties of the algebraic structures, in a type system we can assume
only type theoretic terms, which, in our case, correspond to algebraic
constructs.

Intuitively a category $B$ is a subtype of category $A$ (we write this
as: $B\sqsubseteq A$), if each domain belonging to $B$ belongs to $A$,
or satisfies the properties of $A$. The subtype relation is reflexive
and transitive. The rules for subtyping are given in Figure \ref{figure1}.

A Category $B$ extends category $A$ if its definition introduces new
components. In such case, $B$ is a subtype of $A$ [thinning]. The
subtype relation [$\exists$-opaque] between two opaque categories
reduces to the previous rule. The [$\exists$-transparent] rule for
transparent categories is slightly more complex: it requires that the
representations of their instances are the same, and then reduces to
the thinning rule. The rule [$\exists$-forget] involves forgetting the
information about the domain representations. For a category with a
constraint on the representation of its domains to be a subtype of a
category without this constraint, we check if the former can form a
subtype of the latter when the constraint is added. A category with
fields labeled $with(x_1:\sigma_1,\ldots,x_m:\sigma_m)$ is a subtype
of any category with a smaller collection of fields, where the type of
each shared field in the {\sl less informative} type is a supertype of
the one in the {\sl more informative} type.

The above scheme is more general than Axiom's. Axiom allows subtyping
among named categories only if this has been explicitly defined by the
user. However the introduction of {\tt anonymous categories} in Axiom
as described by Jenks and Sutor \cite{Jenk92}, should involve rules
like the above modulo the transparent types (the current version
forbids the declaration of transparent categories).

Another advantage of the above scheme is that representation
constraints can be specified directly in the categories: each domain
whose type is $\exists Rep=\tau.M$ should define a representation
equal to $\tau$. The reason we allow this is to make the system more
flexible while maintaining consistency, so that {\tt domain
extensions} do not lead to unsoundness.

The last rule is a special case of the rule for subtyping defined in a
new type calculus for SML by Harper and Lillibridge \cite{Harp94}: $\tau$
expressions cannot include $\exists$-types in our scheme, separating
the universe of categories from the universe of types. They can be
shared with no anonymous domains though. Without this stratification,
using the above general version would result in undecidability, since
the substitution $[\tau/Rep]M_2$ can increase the size of type
expressions as shown by Pierce \cite{Pier91a}. Additionally, sharing
with domains from given paths gives a meaning to domain equality which
would be otherwise very difficult to obtain. In this way we can always
decide if two domains share by checking their paths: if two domains
derive from the same path then they share, otherwise we have to
examine the sharing constraints in which they participate.

\subsection{Domain Sharing}

In the previous section we mentioned the presence of
{\tt transparent types}. The difference between opaque types, is that
the information about the representation of a domain is visible in the
case of transparency, and invisible otherwise. This means that in the
declaration
\[D : \exists Rep = Integer.[f : Rep\rightarrow Rep, x : Rep]\]
the domain $D$ shares the same representation as the domain $Integer$,
and the value components $x$ can be safely viewed as $Integer$. This
allows a certain level of flexibility in the manipulation of domains
and their components.

Sharings introduces a form of equational reasoning in the system,
similar to that presented by Martin-L\"of in his intuitionistic type
theory \cite{Mart73}. The behaviour of sharings is summarized in the
following rule for domains with transparent types:
\[[\exists{\rm -sharing}]\quad\displaystyle
\frac{E\vdash D:\exists Rep = \tau.M}{E\vdash D=\tau}\]
This rule comes in accordance with Axiom's behaviour where the
representation of a domain can be exchanged with the domain itself
inside the scope of its definition.

The first application of this construct is the extension of
domains. In the above example, the domain $D$ extends the domain
$Integer$ by introducing the function 
$F:Integer\rightarrow Integer$ and the integer value
$x:Integer$. Every new domain which shares representation with $D$ and
$Integer$ is again an extension of any of them.

Opaque and transparent types can be combined to control domain
extensions and ensure their safety. In the definitions
\begin{verbatim}
Semigroup := with (+ : Rep -> Rep -> Rep )
Monoid := with ( Semigroup; 0:Rep )
D1 : with (Semigroup; Rep=Integer) := add (
   Rep := Integer ;
   + := (+)$Integer )
D2 : Monoid := add (
   D1 ;
   Rep := D1 ;
   0:Rep := 0$Integer )
\end{verbatim}
the domain $D_1$ shares with $Integer$. This allows a safe extension
of $D_1$ in the body of $D_2$, which involves a value of type
$Integer$. Since the type $Monoid$ of $D_2$ does not specify any
sharing, $D_2$ is sharing with neither $Integer$ nor $D_1$. This does
not mean that $D_2$ cannot be extended:
\begin{verbatim}
D3 : with ( Monoid; Rep=D2 ; double: Rep -> Rep) := add ( D2;
            double(x:Rep):Rep := x+x )
\end{verbatim}
$D_3$ shares with $D_2$; instances of $D_2$ can be passed as arguments
to the function $double$ defined in $D_3$.

A side-effect of domain sharing is that if two domains $D_1$ and $D_2$
share common representation and define functions with the same name
$f$ such that
\[f_{D_1} : t_1,\quad f_{D_2} : t_2,\quad t_1 = [D_1/D_2]t2\]
then if both domains are imported in the same context, the system is
unable to choose the right function; explicit selections (as we did
above) are needed. This is part of a more general problem which
suggests that name overloading in the same context is not recommended,
although identifiers should be free to have different meanings in
different contexts.

\subsection{Packages and Categories}

Type systems which provide domains and abstract types can be
transformed into systems with {\tt module types}, by flattening
domains into packages. Languages like 
{\sl Modula-2} \cite{Wirt83}, 
{\sl Ada} \cite{Adax12}, and 
{\sl CAML} \cite{Lero17}
which do not use domains or classes for the implementation of objects
use the notion of {\tt package} or {\tt module} for encapsulating the
appropriate behaviour: each domain $D$ with representation $t$ and
operations $M_i$, where $t$ is free in $M$, can be viewed as a package
with a type component $t$ and value components $M_i$ which depend on
the type $t$.

In such case it is useful to consider packages with type components as
instances of categories. For example, a package which implements
rationals, and its corresponding type are:
\begin{verbatim}
RationalPack := (
  Rep := {x:Integer, y:Integer} ;
  + : Rep -> Rep -> Rep := a +> b +>
    {x := a.x*b.y + a.y*b.x, y:=a.x*b.x} ;
  ... )
\end{verbatim}
\begin{verbatim}
FieldPack := with (
  Rep: Type ;
  + : Rep -> Rep -> Rep ;
  - : Rep -> Rep ;
  * : Rep -> Rep -> Rep ;
  ... )
\end{verbatim}
\begin{verbatim}
RationalCatPack := with (FieldPack; Rep = {x:Integer, y:Integer})
\end{verbatim}

Categories which correspond to packages represent a type similar to an
existential type, namely, a {\tt dependent sum type}
(MacQueen \cite{Macq84},
Harper, Mitchell, and Moggi \cite{Harp89},
Constable et al \cite{Cons85}).
Since all the type and value components of a package can be accessed,
package categories are {\tt strong sums}
(MacQueen \cite{Macq86}) of the form
\[\Sigma t : Type . [(t,t)\rightarrow t,\quad t\rightarrow t;...]\]
For reasons of symmetry, domain categories whose type component cannot
be accessed form {\sl weak sums}
(Harper and Lillibridge \cite{Harp94}).
This approach has been influenced by 
Martin-L\"of's Constructive Mathematics \cite{Mart79}).

Packages can be nested inside other packages. {\tt Nested packages}
are accessed by accessing their parent package recursively. The
components of a package can depend on the type components of other
nested packages as in:
\begin{verbatim}
PolynomialCat := with (
  R:Ring;
  E:OrderedAbelianMonoid;
  Rep:Type;
  leadingCoefficient: Rep -> Rep$R;
  degree: Rep -> Rep$E;
  ... )
\end{verbatim}

In this way, {\sl extensions} on existing domains are easily
implemented, by flattening domains to packages, and exposing their
representations which are matched with the representation of the
extensions. {\sl Sharing} among type components are introduced
locally inside a package, allowing the definitions of constraints
which are valid only inside a certain scope and are invisible outside
of it. This information is not easily expressible with domains without
equating the two domains as we saw in a previous section.

Multiple type components are represented by
{\tt nested $\Sigma$ types}, resulting in structures which contain two
or more carrier sets (as in {\sl PolynomialCat}).  This implies that
many complex data structures can be defined in packages and make their
representations visible to the world.

The basic rules for subtyping on sum types are similar to the ones
described for existential types but more general since multiple nested
dependencies can occur in the body of a category. The rules are given
in Figure \ref{figure2}. The rule $\Sigma$-instance needs special
attention. We will see that types and values can be coerced to
packages. Thus, type sharings are propagated together through the
typing mechanism. In the absence of this mechanism, we would need to
add additional rules such as
\[[\Sigma{\rm-instance}-\tau] \quad\displaystyle
\frac{E\vdash\tau\quad E;t=\tau\vdash\overline{\delta}:\overline{\sigma}}
{E\vdash add(t=\tau,\overline{\delta}):with(t:Type,\overline{\sigma})}\]

The correspondence between $\exists$-types and $\Sigma$-types is quite
interesting and shows the set-theoretic distinction between the two
constructs
(Turner \cite[chapter 10]{Turn91},
Mitchell and Plotkin \cite{Mitc88}):
\[x\in\Sigma T : C.M \leftrightarrow \exists T : C.(x\in M)\]
This relation is of practical interest too, since it denotes that
abstract types hide their implementations and components, but packages
expose it. For instance, when a package with an opaque type
$\Sigma t.M$ is opened it introduces the constants
$x_i : M_i$ and $t : Type$, which is similar to those of domains. On
the other hand, if a transparent package of the form
$\Sigma t=\tau.M$ is opened, the introduced components are:
$x_i:[\tau/t]M_i$ and the type $t=\tau$ whose kind shows that it
shares the same implementation as $\tau$, i.e. they are the same
types. This however does not mean that the two packages are the same,
allowing us to make a distinction between type and package sharings
(something we could not do with domains alone).

One could argue that packages with multiple type components correspond
to multiple domain definitions. This is in general impossible when the
distinction in the interface (category) for the two domains is not
clear:
\begin{verbatim}
PackMultCat := with (
  R: Type ;
  S: Type ;
  x: R -> S -> R )
\end{verbatim}
In order to implement the above package as a combination of two domain
categories, each category needs to reference the representation of the
domains of the other. This involves parameterization, which we discuss
in the next section. In general $\Sigma$-types allow the typing of
more expressions than $\exists$-types, since they introduce
{\tt impredicative polymorphism} (Harper and Mitchell \cite{Harp93a}).

\begin{figure}[h]
\[\begin{array}{|ll|}
\hline
&\\
\ [\Sigma{\rm -inherit}] & \displaystyle
\frac{E\vdash\sigma\sqsubseteq \sigma^\prime}
{E\vdash with(\sigma)\sqsubseteq with(\sigma^\prime)}\\
&\\
\ [\Sigma{\rm -thinning}] & \displaystyle
\frac{\forall i\in I.\exists j\in J.\quad 
E;(;[\overline{\sigma_j}])_{\forall j\in J}\vdash \overline{\sigma_j}
\sqsubseteq\overline{\sigma^\prime_i}}
{E\vdash with(\overline{\sigma_j})_{\forall j\in J}\sqsubseteq
with(\overline{\sigma^\prime_i})_{\forall i\in I}}\\
&\\
\ [\Sigma{\rm -sub}{\rm -val}] & \displaystyle
\frac{E\vdash\tau^\prime\sqsubseteq \tau}
{E\vdash x:\tau^\prime\sqsubseteq x:\tau}\\
&\\
\ [\Sigma{\rm -transparent}] & \displaystyle
\frac{E\vdash\tau^\prime=\tau}
{E\vdash t=\tau^\prime\sqsubseteq t=\tau}
\quad\displaystyle
\frac{E\vdash\omega_1=\omega_2}
{E\vdash m=\omega_1\sqsubseteq m=\omega_2}\\
&\\
\ [\Sigma{\rm -forget}] & \displaystyle
\frac{E\vdash\tau}{E\vdash t=\tau\sqsubseteq t}\\
&\\
\ [\Sigma{\rm -opaque}] & \displaystyle
E\vdash t\sqsubseteq t\\
&\\
\ [\Sigma{\rm -decl}] & \displaystyle
\frac{E\vdash\sigma^\prime\sqsubseteq\sigma}
{E\vdash m:\sigma^\prime\sqsubseteq m:\sigma}\\
&\\
\ [\Sigma{\rm -nesting}] & \displaystyle
\frac{E\vdash\sigma^\prime\sqsubseteq\sigma\quad
E,m:\sigma^\prime\vdash\overline{\sigma^\prime}\sqsubseteq\overline{\sigma}}
{E\vdash with(m:\sigma^\prime,\overline{\sigma^\prime})\sqsubseteq
with(m:\sigma,\overline{\sigma})}\\
&\\
\ [\Sigma{\rm -sub}] & \displaystyle
\frac{E\vdash m:\sigma^\prime\quad
E\vdash\sigma^\prime\sqsubseteq\sigma}
{E\vdash m:\sigma}\\
&\\
\ [\Sigma{\rm -instance}] & \displaystyle
\frac{E\vdash\delta:\sigma\quad
E,m:\sigma\vdash\overline{\delta}:\overline{\sigma}}
{E\vdash add(m=\delta,\overline{\delta}):
with(m:\sigma,\overline{\sigma})}\\
&\\
\hline
\end{array}\]
\caption{\label{figure2}Subtyping rules for $\exists$-types}
\end{figure}

\subsection{Parameterization}

In the presentation of categories and domains in previous sections, we
assumed that they are parameterized by the representation of their
instances. This was merely a notational convenience in order to
clarify the correspondence between domains and their existential
types. Computer algebra systems like Axiom have chosen to use a
special symbol such as \$ or \% for representing the unaccessible type
component of the domain categories. We used a {\sl flattened} notation
for packages in order to specify explicitly that type components can be
accessible as if they were normal value components. Nevertheless, the
code previously given for the creation of the objects $r_1$ and $r_2$,
provides a concrete type for the representation as it is difficult to
implement the functions without knowledge of the actual hidden type.

\subsubsection{Parameterized Categories}

What is hidden behind the notation we used is that it is possible to
parameterize both domains and packages with other domains and
packages. Given the relation between $\exists$-types and
$\Sigma$-types and the conclusion that packages are more general than
domains, we can assume that $\Sigma$-types and packages can be
parameterized by other packages resulting in new $\Sigma$-types and
packages respectively as in:
\begin{verbatim}
DictionaryCat (Keys: with
                      (T:Type;
                       eq:T->T->Boolean;
                       less:T->T->Boolean)
              (Values: with (T:Type))
  := with (
    T:Type ;
    empty: T ;
    at: T -> T$Keys -> T$Values ;
    update: T -> T$Keys -> T$Values -> T ;
    ... )
\end{verbatim}

The parameterized category {\sl DictionaryCat} provides the interface
of parameterized packages which receive two arguments: a package with
ordering operations on its carrier set ({\sl Keys}) and a package
which provides a carrier set ({\sl Values}). The type of the returned
package depends on the carriers of the arguments as it can be seen by
the signatures of the functions {\sl at} and {\sl update}. In this
way, parameterized categories form {\tt dependent products} and their
instances are mappings from packages to packages: we call them
{\tt functors}.

Ideally, a parameterized category is the type of a parameterized
package. However, parameterized categories are used extensively in
systems like Axiom to code the categories of Modules or Polynomials
over a Ring, etc. \cite{Dave90} \cite{Dave91} which are actually
simple $\Sigma$-types. Given the category of Polynomials as defined
previously we can construct a parameterized package which returns
implementations of polynomial types:
\begin{verbatim}
PolynomialRing(R:Ring,E:OrderedAbelianMonoid): PolynomialCat
  := add (
    R := R;
    e := E;
    Rep := List {k:Rep$E, c:Rep$R};
    leadingCoefficient := r +> ... ;
    degree := e +> ... ;
    ...
  )
\end{verbatim}

\begin{figure}[t]
\[\begin{array}{|ll|}
\hline
&\\
\ [\Pi F{\rm -apply}] & \displaystyle
\frac{E\vdash m_1:\Pi\ m:c_1.c_2\quad E\vdash m_2:c_1}
{E\vdash m_1(m_2):[m_2/m]c_2}\\
&\\
\ [\Pi{\rm -instance}] & \displaystyle
\frac{E;m:c\vdash m^\prime:c^\prime}
{E\vdash(m:c)\mapsto m^\prime:(m:c)\mapsto c^\prime}\\
&\\
\ [\Pi{\rm -apply}] & \displaystyle
\frac{E:\vdash m_1:c_1\quad E\vdash add(M=m_1,m):\Sigma M=m_1.c_2}
{E\vdash m:(\Pi M:c_1.c_2)m_1}\\
&\\
\ [\Pi{\rm -sub}] & \displaystyle
\frac{E\vdash c^\prime_1\sqsubseteq c_1\quad 
E;m:c^\prime_1\vdash C_2\sqsubseteq c^\prime_2}
{E\vdash\Pi(m:c_1).c_2\sqsubseteq\Pi(m:c^\prime_1).c^\prime_2}\\
&\\
\ [\Pi{\rm -subapply}] & \displaystyle
\frac{E\vdash\Sigma m^\prime=m_1.c\sqsubseteq
\Sigma m^\prime=m_1.[m^\prime/m]c_2}
{E\vdash c\sqsubseteq (\Pi(m:c_1).c_2)m_1}\\
&\\
\hline
\end{array}\]
\caption{\label{figure3}Subtyping rules for $\Pi$-types}
\end{figure}

The advantage of this modeling is that the implementation of the
coefficients and the terms of polynomials are accessible from the
package which defined functionality on them. In this way, each
polynomial package propagates all the information about its
constituents: extensions of their functionality is possible by
accessing not only the structure, but also the implementation of the
types of their components.

\subsubsection{Dependent Products}

The category {\sl DictionaryCat} corresponds to the {\tt product type}
\[\Pi(K:Ord).\Pi(T:Set).\Sigma(t:Type).
[t,t\rightarrow K.t\rightarrow T.t,
t\rightarrow K.t\rightarrow T.t\rightarrow t,...]\]

The types of functors are {\tt dependent products}
(Burstall and Lampson \cite{Lamp88},
MacQueen \cite{Macq84} \cite{Macq86},
Harper and Lillibridge \cite{Harp94}
Leroy \cite{Lero94}): the type of the return package (domain) can depend on
the functor arguments. In the declarations
\[F(m:C^\prime):C=M\]
\[M^{\prime\prime}=F(M^\prime)\]
where $M$ is the instance of the category $C$,
$M^\prime$ an instance of the category $C^\prime$ and the type of the
components of $M^{\prime\prime}$ are equal to:
\[M^{\prime\prime}_i:[M^\prime/m]C_i\]
As it has been mentioned, the arguments of a functor (and of its
dependent type) are always packages; a functor application results in
a new package. The rules for typing and subtyping for dependent
products are given in figure(\ref{figure3}).

In order to pass a value $f$ as argument to a functor, a new anonymous
package with a value component $f$ is generated implicitly, resulting
in a functor with a package argument:
\[\Pi(f:t_1\rightarrow t_2).M \Rightarrow
\Pi(s:\Sigma().t_1\rightarrow
t_2).[f\$s/f]M\]
For example, consider the transformation:
\[F(x:Integer):C:=M\Rightarrow F(X:with(x:Integer)):C:=[x\$X/x]M\]
If we call $F$ with an integer argument $F(x:=3)$, then an anonymous
package is generated: $F(add(x:Integer:=3))$ and $M$ is returned. If
we pass two arguments, as in $F(x:=3,y:=4)$, then following the same
procedure we have
\[F(add(x:Integer:=3,y:Integer:=4))\]
Given the rules $\Pi$F-apply and $\Sigma$-forget, the new argument
matches the type $\Sigma()$.[$x:Integer$], and the functor application
returns $M$.

\subsubsection{Subtyping of Products}

As we have seen, the arguments of parameterized categories have sum
types, while the body is also an instance of a sum type. The case of
categories (and domains) with $N$ arguments is viewed recursively: the
body is parameterized by $N-1$ arguments, i.e. it has a product type,
and after $N$ steps the final body is not parameterized (0 arguments),
resulting in a sum type.

The subtype relation among sum types can be extended to accommodate
dependent products. The subtyping relation in this case involves
{\sl contravariance} in the argument position and {\sl covariance} in
the body type. The type $\Pi(m:C_1).C_2$ places a stronger constraint
on the behaviour of its instances than $\Pi(m:C^\prime_1).C^\prime_2$
if it demands that:
\begin{enumerate}
\item they behave properly on a larger set of inputs
$(C^\prime_1)\sqsubseteq C_1)$ or
\item their results fall within a smaller set of outputs
$(C^\prime_2\sqsubseteq C^\prime_2)$, provided that the inputs range
in $C^\prime_1$
\end{enumerate}
or possibly both ([$\Pi$-sub]).

\subsubsection{Sharing (Take Two)}

Sharing among type components is of special importance in the case of
functors and product types. The rule $\Sigma$-transparent for sharing
is powerful enough to allow the definition of functors which combine
or compose domains and packages:
\begin{verbatim}
RingFun (P1:Monoid) (P2: with (Monoid; Rep=P1.Rep)):Category :=
  with ( Monoid ;
         * : Rep - Rep -> Rep ;
         1 : Rep )
\end{verbatim}

\begin{verbatim}
BuildRing: RingFun := P1 +> P2 +>
  add ( P1;
        * := (+)$P2 ;
        1 := 0$P2 )
\end{verbatim}
In this example, two monoids with the same representation are combined
in order to form a ring. Since the category {\sl Monoid} already
exists in the system, the only additional action is to add a sharing
constraint to it for $P2$.

Without sharing the above construction is more complicated and less
natural for modeling algebraic concepts. It would require the
parameterization of algebraic categories with their representation:
\begin{verbatim}
RingFun(T:Type)(M1:Monoid(T))(M2:Monoid(T)):Category := ...
\end{verbatim}

Such constructs demand the implicit or explicit creation of new
hierarchies for parameterized monoid categories. The former increases
the load of a type system and its implementation dramatically while
the latter demands that the user create parallel hierarchies for all
possible cases of parameterization.

Sharings reduce the need for instantiation of parameterized
categories, as the above example shows. For instance, many cases which
involve parameterized categories in the Axiom library can be reduced
to simple categories with sharing constraints. The rest can be
transformed to sum types with sharing constraints on their type
components. This feature can be exploited for the implementation of
higher order categories by means of first order possibly nested
constructs and sharings, which reduce the complexity and the
computational load of the compiler.

\subsubsection{Flattening Dependent Products}

The application of dependent products for the generation of new sum
types complicates the elaboration of programs and adds considerable
load in the typing process. In the presence of subtyping, new
hierarchies have to be created since product types may extend other
products. Sums do not have such complications and their hierarchies
remain stable.

One of the properties of $\Pi$-types is that their components cannot
be accessed, thus sharing constraints cannot propagate to their
environment. Therefor it is always desirable to involve $\Sigma$-types
in the presence of static typing. On the other hand, sharing
specifications can restrict a sum to certain implementations,
emulating the results of product application to equivalent
implementations.

Another observation we made in the last sections is that it is not
always convenient to implement categories as products if we want to
access the implementations of the constituents of a package. For
instance, the code of DictionaryCat was rather artificial. A more
realistic implementation would demand the type of the keys and values
to be accessible so as to perform computations other than the ones
specified in this category, like transforming dictionaries to other
structures. The category
\begin{verbatim}
DictionaryCat(Keys:=String; Values:=Integer)
\end{verbatim}
could be defined as
\begin{verbatim}
DictionaryCat := with (
  Keys: with(Type;...);
  Values: Type;
  Rep: Type;
  at: T -> Rep$Keys -> Rep$Values
  ...
  );
  with (DictionaryCat; Keys=String; Values=Integer)
\end{verbatim}
allowing dictionaries to expose the implementation of Keys and Values
and propagate evntual sharing constraints.

However, in some cases, instantiation of dependent products can serve
modeling purposes better than dependent sums. For instance, a
{\sl Ring} is a {\sl LeftModule} over itself:
\begin{verbatim}
LeftModules(R: with(AbelianGroup; SemiGroup)) :=
  (AbelianGroup;
   * : (Rep,R) -> Rep )
Ring := with (LeftModules(%):...)
\end{verbatim}
where the construct \% refers to the package which is an instance of
the category we define (in this case {\sl Ring}). If $R$ were a nested
package inside the definition of LeftModules then it should be defined
forall the Rings too. This would cause great inconveniences in the
coding of Rings since a Ring is a package which has an $R$ component
assigned to itself, leading to recursions. The typing system should
handle these cases automatically and in a transparent way.

The application of dependent product types can be defined in terms of
already defined constructs such as nested packages and sharings as the
previous examples indicate. We distinguish two cases of interest:
\begin{enumerate}
\item Declarations of packages as instances of applications of
$\Pi$-types of the form
\[P_2:(\Pi(P:C_1).C_2)P_1:=P_3\]
\item Inheritance of applications of $\Pi$-types of the form
\[C_3:=\Sigma((\Pi P:C_1.C_2)P_1).C_x\]
the right hand side of which is essentially the expression
\[with((\Pi(P:C_1).C_2)P_1 ; C_x)\]
\end{enumerate}
The expression $(\Pi(P:C_1).C_2)P_1$ creates a sum type
$\Sigma P:C_1=P_2.C_2$ with the sharing specification $P=P_2$.

This transformation has another important consequence: it allows
products to be transformed into sums, meaning that the arguments of a
$\Pi$-type can have $\Pi$-types themselves without changing their
inital semantics. This makes possible the definitions of
{\tt higher order functors} similar to 
Cregut and MacQueen \cite{Macq94}.

The RHS of the expression $P_2:(\Pi(P:C_1).C_2)P_1:=P_3$ is handles as
above, but the typing mechanism should automatically add a $P$
component to $P_3$ so that $P_3$ will match with the resulting sum
type. Consequently, $P_2$ will have an implicit $P$ component. In case
the declaration specifies the argument of a dependent product then
$P_3$ plays the role of the argument passed during the application of
the product as in
$(\Pi(P:(\Pi(P:C_1).C_2)P_1).C_3)P_3$

The expression
\[\Sigma((\Pi P:C_1.C_2)P_1).C_x\]
is transformed to
\[((\Pi P^\prime : C_1.\Sigma[P^\prime/P]C_2.C_x)P_1)\quad
{\rm where\ }P^\prime{\rm\ fresh}\]
and then we proceed as above.

These transformations assume that such directives to the typing
mechanism should be coded directly in the $\Pi$-types and then
elaborated during their applications, so that the appropriate
components of the resulting $\Sigma$-types and their instances will be
introduced when needed. This involves the following steps:
\begin{enumerate}
\item Add a directive for introducing component $p$ in the definition
of the instances of the category resulting from applying
$\Pi(P:C_1).C_2$ to a package $P_1$. In order to avoid name conflicts,
a fresh variable $P^\prime$ can be introduced instead of $P$,
performing the appropriate $\alpha$-conversions.
\item Create a dummy package $P_{dummy}$ which satisfies $C_1$
\item In the definition of the sum
$\Sigma P^\prime:C_1.[P^\prime/P]C_2$
propagate the directive of step 1 and do type-checking by sharing
$P_{dummy}$ from step 2 with $P^\prime$. The directive from step 1
should now affect the instances of the sum.
\item If type checking succeeds, remove $P_{dummy}$. When a domain is
matched against the introduced sum, the stored directives introduce
the appropriate nested packages automatically, resulting in the
generation of a new package. Add the appropriate sharings by
generating a new category.\\
In the above example,, if a package $M$ is to be matched against
$Module(\%)$ (where \% refers to $M$), a new sum type $Module_{flat}$
is generated through steps 1-3. Consequently a package $M_{new}$ is
generated including all the components of $M$ plus a component
$P^\prime$ resulting from the directives of step 3. A sharing
specification $R^\prime=M$ is added to $Module_{flat}$. Type checking
and propagation of sharings proceed.
\item Any other operation involving sums should take into account the
directives stored in them, or inherited by other sums. In this case,
{\sl Ring} should include the directives given by
{\sl LeftModule(\%)}.
\end{enumerate}

The above result in the rules $\Pi$-apply and $\Pi$-subapply in
figure(\ref{figure3}).

The typing process in this scheme is decidable, due to the generation
of new sums and packages from existing components. However, the above
steps avoid the generation of new type components since the old
components are also members of the new packages. Thus propagation of
type information is not affected.

The rules for application of $\Pi$ types imply that a dependent
product can be applied only when the dependencies of the body of the
product to its arguments have been removed. This assumes an implicit
coercion of the {\tt dependent product type} to an {\tt arrow type} as
in
\[(m:(\Pi M:C_1.C_2))m_1 \Rightarrow (m:C_1\rightarrow [m_1/M]C_2)m_1\]
provided that $m_1$ can be shown to have the type $C_1$.

\subsection{Subtyping of Domains}

Descriptions of {\tt sets} of entities which belong to various
domains can be arranged into useful classification hierarchies. For
example, the set of integers can be seen as a subset of the rational
numbers. This supports a useful kind of reasoning: if $X$ is an
integer, then $X$ must also be a rational number, and every
interpretation of a rational number should be true for $X$ if both
domains are viewed as instances of the same category. For example, it
should be valid to make the judgement
\[\displaystyle
\frac{E\vdash Integer\sqsubseteq Rational:Ring\quad E\vdash x:Integer}
{E\vdash x:Rational}\]

The semantics of $S\sqsubseteq T$ for domains and types can be derived
by the corresponding relations in $\Sigma$ and $\Pi$-types; they are
included in the following statement:

If $S\sqsubseteq T:C$, then an instance of $S$ may safely be used in
any operation specified in $C$ where an instance of $T$ is expected.

For example the function
\begin{verbatim}
foo(x:Rational) := (x+1) * x**2
\end{verbatim}
can be safely applied to the integer argument 4 because it is possible
to view any integer as a rational in the above operations. More
important, since integers form a {\tt subring} of rationals, and the
definition of {\sl foo} includes operations defined for rings, we can
do this substitution under the assumption that both {\sl Integer} and
{\sl Rational} form a ring. This leads to the more general subrule
[sub-val] (figure \ref{figure4}).

\subsubsection{Coercions}

Two formal accounts can be given for the semantics of subtyping. In
the simpler view, the syntactic subtype relation $S\sqsubseteq T$
where $S,T$ are domains, is interpreted as asserting that the
{\tt semantic domain} denoted by $S$ is included in that denoted by
$T$. In the more general view $S\sqsubseteq T$ is interpreted as a
{\tt canonical coercion function} from the domain denoted by $S$ to
the one denoted by $T$:
\[\displaystyle
\frac{E\vdash S,T:C\quad E\vdash x:S\quad E\vdash {\bf coerce}:
S\hookrightarrow T}{E\vdash {\bf coerce}(x):T}\]
i.e.
\[\displaystyle
\frac{E\vdash x:S\quad E\vdash {}_C{\bf coerce}:S\hookrightarrow T}
{E\vdash {}_C{\bf coerce}(x):T}\]
where coercions keep information about the form of subtyping among
domains in terms of algebraic relations: if $C$ involves {\sl Ring}
structures, then the above rule describes a subring relation.

Since coercions can be composed, the subtype information carried with
them can be modified. In a type system, information cannot be
generated from inside, therefore any composition of coercions reduces
the information:
\[\displaystyle
\frac{E\vdash C_1\sqsubseteq C_2\quad
E\vdash {}_{C_1}{\bf coerce}:R\hookrightarrow S\quad
E\vdash {}_{C_2}{\bf coerce}:S\hookrightarrow T}
{E\vdash {}_{C_1}{\bf coerce}:R\hookrightarrow T}\]
\[\displaystyle
\frac{E\vdash C_1\sqsubseteq C_2\quad
E\vdash {}_{C_2}{\bf coerce}:R\hookrightarrow S\quad
E\vdash {}_{C_1}{\bf coerce}:S\hookrightarrow T}
{E\vdash {}_{C_1}{\bf coerce}:R\hookrightarrow T}\]

In this way paths can be composed with different subtyping carried
information. A useful theorem which can be proved by the above rules
is that any direct coercion between two domains, carries more
information than any other coercion path which uses an intermediate
domain. This means that direct coercions are preferable in case of
multiple paths. Assume the domains $A$, $B$, $C$, and $D$, and the
coercions
\[\begin{array}{c}
A\hookrightarrow C\hookrightarrow B\\
A\hookrightarrow D\hookrightarrow B\\
\end{array}\]
A type system cannot in general prove whether this graph commutes,
since the former path may have different semantics than the
latter. Since the path $A\hookrightarrow B$ keeps more information than the
path $A\hookrightarrow C\hookrightarrow B$ any composition of the above paths leads to
similar results, i.e. the path
\[A\hookrightarrow B\hookrightarrow D\]
is preferable for a transition from $A$ to $D$ than

\begin{figure}[h]
\[\begin{array}{|ll|}
\hline
&\\
\ [{\rm sub-val}] & \displaystyle
\frac{E\vdash x:S\quad E\vdash S\sqsubseteq T:C}
{E\vdash x:T}\\
&\\
\ [\{\}{\rm -sub}] & \displaystyle
\frac{\forall j\in\{1,\ldots,m\}\exists i\in\{1,\ldots,n\}~.~
E\vdash x_i\equiv x_j~
E\vdash \tau_i\sqsubseteq\tau_j,~m\le n}
{\{x_i:\tau_i\}_{\forall i\in\{1,\ldots,n\}}\sqsubseteq
\{x_j,\tau_j\}_{\forall j\in\{1,\ldots,m\}}}\\
&\\
\ [\rightarrow{\rm -sub}] & \displaystyle
\frac{E\vdash T_1\sqsubseteq S_1\quad E\vdash S_2\sqsubseteq T_2}
{E\vdash S_1\rightarrow S_2\sqsubseteq T_1\rightarrow T_2}\\
&\\
\ [\mu{\rm -sub}] & \displaystyle
\frac{E;T_1:Type;S_1\sqsubseteq T_1\vdash S\sqsubseteq T}
{E\vdash \mu S_1,S\sqsubseteq \mu T_1,T}\\
&\\
\ [{\rm +-sub}] & \displaystyle
\frac{\forall i\in\{1,\ldots,m\}\exists j\in\{1,\ldots,n\}~.~
E\vdash x_i\equiv x_j~E\vdash\tau_i\sqsubseteq\tau_j,~m\le n}
{+[x_i:\tau_i]_{\forall i\in\{1,\ldots,m\}}\sqsubseteq
+[x_j:\tau_j]_{\forall j\in\{1,\ldots,n\}}}\\
&\\
\hline
\end{array}\]
\caption{\label{figure4}Subtyping rules for types}
\end{figure}

\[A\hookrightarrow C\hookrightarrow B\hookrightarrow E\hookrightarrow D\]
It is important to mention that the length of the path is of no
particular importance, although the preferable path is always shorter
than any other path. For instance, if in the above paths the coercion
$C\hookrightarrow D$ is added, then there is no way to choose any particular
path.

\subsubsection{Subtyping of Representations}

In this section we restrict ourselves to natural subtyping between
types of run-time objects. The subtyping rules for records, arrow
types, and recursive types correspond to the subtyping rules for
packages ($\Sigma$-thinning), dependent products ($\Pi$-sub), and
domains ($\exists$-forget); a similar rule for {\tt disjoint unions}
is added (figure \ref{figure4}).

Records can be viewed as degenerated forms of packages where there are
no type components, and no dependencies among the components and their
types. The above rule for records supports the subtyping of tuples if
we view tuples as records with integer fields.

In a similar way, an arrow type is a $\Pi$-type where there is no
dependency between the input type and the output:
$\Pi(x:\tau_1).\tau_2$ reduces to $\tau_1\rightarrow\tau_2$ if $x$ is
not in the expression $\tau_2$.

The rule for function types interprets the rule for records in the
following way: If we think of a record as a function from labels to
values, a record $\tau$ represents a stronger constraints than
$\tau^\prime$ on the behaviour of such function, if $\tau$ describes
the function's behaviour on a larger set of labels or gives a stronger
description of its behaviour on some of the labels also mentioned by
$\tau^\prime$ (Pierce \cite{Pier91a}).

By means of [+-sub] and [$\mu$-sub] our type system can assure that
\[Tree(Integer)\sqsubseteq Tree(Rational)\].

For continuing our analysis we introduce the category $Ring$ and
repeat the definition of the category $Field$:
\begin{verbatim}
Ring := (Rep:Type) +> with (
  + : Rep -> Rep -> Rep ;
  - : Rep -> Rep ;
  * : Rep -> Rep -> Rep ;
  0 : Rep ;
  1 : Rep )
\end{verbatim}
\begin{verbatim}
Field := (Rep: Type) +> Ring(Rep) with (
  / : Rep -> Rep -> Rep )
\end{verbatim}
and $Field\sqsubseteq Ring$ given the subtyping rules for existential
types above. This means that {\sl every instance of the category Field
can be viewed as an instance of Ring} or {\sl every instance of an
instance of Field is also an instance of an instance of
Ring}. Unfortunately the rules for subtyping among categories cannot
be easily extended to rules for their instances.

Given the definition of the type {\sl RationalFun} from above we can
introduce in a similar way the type of integers which form a Ring. Our
notation allows us to abstract the implementations away:
\begin{verbatim}
IntegerFun := (Rep:Type) +> (
  new: Integer -> Rep ;
  numerator: Rep -> Integer ;
  denominator: Rep -> Integer ;
  SetNumDenom: Rep -> Integer -> Rep ;
  + : Rep -> Rep -> Rep ;
  - : Rep -> Rep ;
  * : Rep -> Rep -> Rep ;
  0 : Rep ;
  1 : Rep ; )
\end{verbatim}
It is not the case that $IntegerFun\sqsubseteq RationalFun$ since the
specification of integers does not include the record field $/$,
leaving $IntegerFun$ with one field less than $RationalFun$.

The obvious solution to cope with the above problem is the use of
coercions, as we saw in the previous section. The problem is that
coercions are expensive in computational resources and in some cases
they can introduce inconsistencies.

\subsection{Type Classes}

In this section, we are concerned with modeling subtyping without use
of coercions. We introduce the concept of type classes, which has some
similarities with the homonymous concept in Haskell
(Nipkow and Prehofer \cite{Nipk95},
Wadler and Blott \cite{Wadl88}). The semantics and the formal definitions
for type classes are provided in the next section. Type classes are
distinct from categories in the sense that they are not part of the
user defined types. The main difference with Haskell's type classes is
that our type classes are not syntactically defined in the language
but are inferred by its type system, reducing the complexity of the
former.

It is clear now that we are not interested in a pure subtype
relationship, but in an algebraic relationship based in terms of
{\tt subrings}, {\tt subfields}, etc. We introduce type classes in
order to provide the facility of viewing Integer as a {\tt subring} of
Rational:
\begin{verbatim}
RationalRing := (Rep:Type) +> with (
  + : Rep -> Rep -> Rep ;
  - : Rep -> Rep ;
  * : Rep -> Rep -> Rep ;
  0 : Rep ;
  1 : Rep )
\end{verbatim}

This declares that any eventual subtype of rational which happens to
be an instance of {\sl Ring} implements the operations declared in the
type class {\sl RationalRing}. Since {\sl Integer} forms a subring of
{\sl Rational}, from the definition
\begin{verbatim}
IntegerRing := (Rep:Type) +> RationalRing(Rep)
\end{verbatim}
we may conclude:
\[IntegerRing\sqsubseteq RationalRing\]
The declaration of $Integer$ as instance of the type class
$IntegerRing$ is straightforward:
\begin{verbatim}
Integer :: IntegerRing
\end{verbatim}
while for Rationals we need one additional type class:
\begin{verbatim}
RationalField := (Rep:Type) +> RationalRing(Rep) with (
  / : Rep -> Rep -> Rep )

Rational :: RationalField
\end{verbatim}

Given the function definition
\begin{verbatim}
dblSqrd(x::Rational) := (x+x)*(x+x)
\end{verbatim}
by means of type classes the system infers the most general signature
for $dblSqrd$:
\[\Gamma,\forall a::RationalRing \vdash dblSqrd :
a\rightarrow a\rightarrow a\]
$dblSqrd$ can receive as argument an instance of any the types of
$RationalRing$, including $Integer$, without any need to coerce it to
$Rational$: we have managed to define a natural subtype relationship
between integers and rationals, which comes in accordance with the
algebraic semantics of the terms.

\subsubsection{Definition of Type Classes}

For the definition of type classes we assume that operations do not
belong to a type but to an algebra (that is, a particular collection
of types). A class combines one or more types for the implementation
of its instances. A class's instances do not need to have a common
internal structure but they are elements of the types which a class
assumes. Herewith we can define type classes.

Formally a {\tt type class} has the following structure:
$Class[T,B]$ in which $T$ is the set of types and $B$ is the behaviour
of the class's instances. An instance of a type is by definition an
instance of any of the classes in which this type belongs. The
{\tt instanceOf relation} (denoted by ::) represents membership in a
set of instances and as such it is {\sl irreflexive} and
{\sl non-transitive}.

The {\tt subclass relation} (denoted by $\sqsubseteq$) is a
{\sl reflexive}, {\sl antisymmetric}, and {\sl transitive} binary
ordering relation in the partial ordered set of classes.

{\tt Subtyping} can be seen in two ways (which are consistent with the
definitions given in the previous sections): subtyping by means of
subclassing or coercions.

\begin{figure}[h]
\[\begin{array}{|ll|}
\hline
&\\
\ [\bigcap{\rm -sub}] & \displaystyle
\frac{\forall j\in\{1,\ldots,m\}\exists i\in\{1,\ldots,n\}~.~
E\vdash\tau_i\sqsubseteq\tau_j,~m\le n}
{\bigcap(\tau_i)_{\forall i\in\{1,\ldots,n\}}\sqsubseteq
 \bigcap(\tau_j)_{\forall j\in\{1,\ldots,m\}}}\\
&\\
\hline
\end{array}\]
\caption{\label{figure5}Subtyping rules for intersection types}
\end{figure}

\[\begin{array}{cc}
\displaystyle
\frac{E\vdash x::C\quad E\vdash C\sqsubseteq C_i}{E\vdash x::C_i} &
\displaystyle
\frac{E\vdash x::C\quad E\vdash{\bf coerce}:C\hookrightarrow C_i}
{E\vdash{\bf coerce}(x)::C_i}
\end{array}\]
The above rules handle the case of multiple subclassing for they
are applied $\forall i\in[1,\ldots,n]$.

Type classes $C_1$, $C_2$ are said to belong to the same
{\tt inheritance path} when one can derive through $\sqsubseteq$ or
$\hookrightarrow$ relationships that $C_1\sqsubseteq C_2$ or
$C_1\hookrightarrow C_2$ respectively.

The above inductive definitions can be seen as the definition of
{\tt class}\\
{\tt intersection} which differs from the classical definition
of set intersection in the sense that equality for instances of
different classes cannot be established. Such a relation is possible
only between members of the same equality type. Identity is only
possible between members of the same type. Class intersection
corresponds to intersection of the sets of types implementing the
classes. Classes define behaviour by means of a set of axioms and
operations among each class's instances, therefore a class
intersection produces behavioural {\sl union}. This definition has
constructive power since an instance must be an element of a
particular type. It corresponds to the subtyping rules for
{\tt records} (union), and {\tt intersection types} (figure \ref{figure5}).

{\tt Class Intersection:}
\[x::(C_1\sqcap\ldots\sqcap C_n)\Leftrightarrow\forall_i x::C_i\]
and
\[[T_1,B_1]\sqcap\ldots\sqcap[T_n,B_n]=[\bigcap_iT_i\bigcup_iB_i]\]
where we write [$T,B$] for the class implemented by each of the types
$t\in T$ and supporting behaviours $b\in B$.

In the case that $T_1\cap\ldots\cap T_n=\{\}$, the class intersection
is $\bot$. Similarly, we can define the union of classes as their
superclass:

{\tt Class Union:}
\[x::(C_1\sqcup\ldots\sqcup C_nn)\Leftrightarrow\exists_i x::C_i\]
\[[T_1,B_1]\sqcup\ldots\sqcup[T_n,B_n]=[\bigcup_iT_i,\bigcap_iB_i]\]

The rules we use for typing values with type-classes are similar to
those of the Haskell system. We avoid repeating them here since they
can be found in (Nipkow \cite{Nipk95}). Every new value declaration which
involves instances of a domain which is a member of a type class is
added to the value set of this type-class. Since a domain can be a
member of many type classes the most general one which ensures the
typing of the value is chosen. For every new value the same process is
followed. This means that although the initial constructions of type
classes depends on the category hierarchy, after the introduction of
new values they can include more declarations.

\subsubsection{Coercions vs. Typeclasses}

Typeclasses allow a different kind of behaviour than coercions. If
there is a coercion from $A$ to $B$, but we can perform the operations
defined in $B$ directly in $A$ (assuming that instances of type $A$
are passed as arguments) then we do not need to consider the
possibility of multiple paths or computational effects, but we get the
behaviour defined in $A$ (instead of that in $B$). The former is an
advantage of typeclasses over coercions, the latter allows for more
options in the design.

It is worth noting that none of the forgoing would have been possible
if the type variable $Rep$ was not introduced in the definition of
domains and categories. Suppose that Rational and Integer had the
definitions
\begin{verbatim}
Rational := ( ...
  + : Rational -> Rational -> Rational
  etc. )

Integer := ( ...
  + : Integer -> Integer -> Integer
  etc. )
\end{verbatim}

This would influence the definitions of RationalFun and IntegerFun
respectively, and finally the definitions of RationalRing and
IntegerRing. There would have been no simple way to derive a subtype
relationship among types which would include the terms
\[Rational\rightarrow Rational\rightarrow Rational\]
and
\[Integer\rightarrow Integer\rightarrow Integer\]
since these two cannot form a subtype relationship due to the
introduction of the {\tt contravariance} rule for function types.

For the same reason it is also difficult to use packages for defining
the above form of subtyping. Domains have only one carrier set, making
it simple to reason and infer type hierarchies. In the presence of
more than one combined sets this task is more difficult since it can
involve recursive domain definitions. Additionally, the form of
subtyping proposed can be easily implemented with dictionaries
specific to each type class. In the presence of multiple carriers this
implementation requires multiple dictionaries, most of which may never
be used. This approach would decrease the efficiency of the system
without acutally bringing any substantial benefit.

\subsection{Comparison with Related Work}

The type system we have presented is an extension of Axiom's type
system with the introduction of strong sums, higher order functors,
transparent category types, structural type equivalence, sharing
specifications and type classes. It can also be viewed as an extension
of SML's module system by transforming abstract types (domains) to
modules (packages), and introducing recursive higher order dependent
products.

Work of considerable value on type systems for computer algebra was
done during the 90's with examples such as {\sl Newspeak}
(Foderado \cite{Fode83}). A comparative review of systems of that generation
is given by Fateman \cite{Fate90}. An implementation which increased our
understanding about the relation between abstract types and
existential types was the {\sl SOL} language by
Mitchell and Plotkin \cite{Mitc88}
and an extension of {\sl SML} with subtyping
(Mitchell, Meldal, and Madhav \cite{}). 
Also the theorem prover {\sl Nuprl} based on predicative logic can
elaborate many parts of our type system. $\Sigma$-types are part of
the {\sl Nuprl} system and facts about abstract theoretical levels can
be proved directly.

Axiom's new language (Watt \cite{Watt94a}) includes the two first extensions
but not the last three. Its full type system unifies the concept of
domains and packages into the concept {\tt type}, allowing objects to
be instances of packages. This introduces syntactic sugar for
expressions of the form $Rep\$P$ we described above and is easily
handled by a compiler. It views packages as records, something which
needs delicate handling for avoiding unsoundness. Thus categories
($\exists$ or $\Sigma$ types) are considered as types too, disallowing
the use of the [forget] rule, since this would lead to undecidability.
In fact, sharings are not supported.

Various systems handle polymorphism in different ways. Axiom allows
parameterization of domain definitions. In addition, its new language
uses types as first class values (as in the language {\sl Pebble} by
Burstall and Lampson \cite{Lamp88}) and adds run-time type tests, meaning
that types can be passed as function parameters or returned by
functions dynamically. This involves loss of static typing information
as in (Mitchell, Meldal, and Madhav \cite{Mitc88})
where $\Sigma$-types are coerced to $\exists$-types. In a language
which does not allow overloaded identifiers for values in a given
context, type variable can be inferred automatically as in the
Hindley-Milner type system or by means of type classes as in
{\sl Haskell's} typing mechanism, allowing for implicit type
parameterization of functions. In {\sl XFun} by
Dalmas \cite{Dalm92}
everything can be a first class value as in the new Axiom, even
categories (called {\tt signatures}). {\sl XFun} relies on run-time
type checks. The same is valid about the {\sl Magma} system by
Cannon and Playoust \cite{Cann01}, which, however, does not include many of
the features we presented. {\sl Magma} does not allow user defined
types.

{\sl SML} supports functors with {\tt transparent signatures} and
{\sl SML/NJ} extends their definition to higher order functors without
mixing them with values. Recent extensions of the {\sl SML} module
system with {\tt translucent sums} by
Harper and Lillibridge \cite{Harp94} involve the definition of
{\tt first class higher order modules} allowed to be defined as
values. Type-checking restricts their flexibility in type contexts in
order to maintain static and strong typing. Our system does not
support the use of types as values but allows functors and products to
be applied to them. The concept of type classes has been inspired by
{\sl Haskell} in order to allow overloading of user defined functions
on arbitrary types. The extension to support modules as values is
possible since modules consist of a compile-type and run-time part as
described in Harper, Mitchell, and Moggi \cite{Harp89}.
All type information is resolved at compile time but implementations
can be computed at run-time, thus type components cannot be used as
values. For instance, if the body of a function $f$ with argument $x$
involves code which applies a functor $F$ to a package $D$ the value
components of which may depend on $x$, then the part of $F$ which
involves type components (static part) can be applied to the type
components of $D$ out of the body of the function $f$, and the value
part of $F$ can be applied to the value components of $D$ inside the
body of $f$, assuming type information derived from the static part. If
the types depend on $x$ then only abstract types can be created,
i.e. only minimal typing information can be propagated. Use of
categories (or types) as values in a static type system is reasonable
only under restrictions which allow for dynamic typing.

An advanced concept which is part of the {\sl Nuprl} inferencer is the
use of {\tt propositions as types}. Using propositions, properties
like associativity can be expressed and proved at the category level
but operations like equality demand special treatment. We have not
addressed this issue in our presentation because we want first to
establish basic type theoretic properties and examine the limits of
the extensions analyzed. Additionally, the application of propositions
is not so clear for modeling algebraic structures. Expressing category
properties in a more limited form as in Axiom can be easily
incorporated in our system by introducing additional (type or value)
components into categories.

The sharing construct we presented is an extension of {\sl SML}'s
sharing constraints
(MacQueen \cite{Macq88},
Milner and Tofte \cite{Miln91}) and their extensions by
Leroy \cite{Lero94},
and Harper and Lillibridge \cite{Harp94},
since we allow arbitrary type constructors to be shared, to the point
this is allowed by the syntactic rules we defined in the
introduction. Domains are implemented by the construct {\sl abstype}
in the core {\sl SML} language, or by using packages (abstractions)
with opaque types in the {\sl SML} module system. This is not possible
for functors which always have transparent types.

A difference between {\sl SML} and the system presented in this paper
is that functors in {\sl SML} are {\tt generative}, i.e. two
instantiations of a functor do not necessarily result in the same type
or package. This implies that two occurrences of
{\sl Polynomial(Integer,x)} in the same context may not have
compatible carrier sets.

\subsubsection{Conditional Categories}

A feature of Axiom which we have not elaborated in this paper are
{\tt conditional categories}, that is, parameterized categories whose
body size and kind depends on their arguments. The introduction of
dependent sums would allow conditional dependencies on the body of the
category which carries them.

Although the type theoretic properties of conditional categories are
not well understood, it is easy to show that without certain
restrictions imposed on them they result in undecidability quite
easily, even if we assume that domains are not first class objects and
the arguments of a category are not first class values (the
undecidability in the case of value arguments is obvious and has been
examined by Weber \cite{Webe93b}). This happens because the type of the
argument of a conditional category (and thus its body) can increase in
the presence of recursive definitions in a way similar to that
encountered at the $F_{\le}$ type system which has been shown by
Pierce \cite{Pier91a} to be undecidable. For example, given the definitions:
\begin{verbatim}
C():Category == with (if % has Ring then Ring)
t: C() == add ...
\end{verbatim}
it is not clear whether $t$ is a $Ring$, and the system may loop. The
above situation is handled in {\sl Gauss} (Monagan \cite{Mona93}) purely
syntactially by viewing \% as the category in scope and checking if
the defined category (here C) includes code for Rings up to the point
where the conditional is found: this would add $Ring$ to $C$.

Despite such inconveniences, conditional categories are a very useful
and powerful construct, which is able to simulate examples supported
by other higher order systems (like $F_{\le}$), and can implement in a
unique way many applications from computer algebra where it can
increase our understanding about the interaction of types.

\subsection{Conclusions}

We have presented selected parts of a type system for symbolic
computation which is sound and consistent, and powerful enough for
algebraic applications since it combines and extends commonly used
systems. The issues of abstract representations, abstract and concrete
implementations, subtyping without coercions and sharing of domains
have been handled, solving many problems of existing approaches. For
the purposes of our analysis we have extended the concept of
categories to include strong sum types and sharings, and used
existential types for defining subtyping among domains without need of
coercions.

A type inference mechanism transforms domains into packages and
flattens parameterized categories with package instances into sum
types with sharings. In this way we avoid the combinatorial explosion
of subtyping hierarchies among categories. The information about
domain structures is saved for later stages of the inference
process. Another mechanism constructs type classes as a combination of
categories and domains, in order to resolve the conflicts introduced by
the different definitions of subclass and subtype in algebra and type
theory accordingly. However, in cases in which a subdomain does not
define operations of its supertype, coercions can be called.

We tried to concentrate on typing issues with respect to computer
algebra rather than presenting a complete language or system
design. Therefore we left aside other important topics such as
exception handling, pattern matching, etc. although these constructs
are subject to typing in our scheme.

Additional research is of interest in the following subjects:
\begin{enumerate}
\item Introduction of {\tt first class packages} in the system without
sacrificing the nice properties of the presented stratification. This
should split packages and functors into a static and dynamic part. In
such case, records and functions can be removed as special cases of
higher order constructs. The difficulties here involve recursive
definitions and effects. We need a clear semantics for recursive
package definitions.
\item Examination of the appropriate constraints for the introduction
of conditional categories in order to maintain soundness.
\item An efficient implementation of type classes without sacrificing
space
\item A scheme for coercions which involes algebraic relations as in
the case of the {\sl Weyl} system by Zippel \cite{Zipp93} instead of simple
coercions from one type to another.
\item A more remote target is the introduction of propositions at the
category level and the integration of an environment for theorem
proving at the top level.
\end{enumerate}

This introduced type system is currently implemented for the
$\lambda{}A$ language designed and implemented by the author at ETH
Zurich. The concepts presented have been tested in our implementation
by translating them to SML constructs.

\chapter[Doye's Coercion Algorithm]{Doye's Coercion Algorithm by Nicolas Doye}

This chapter is based on a PhD thesis by Nicolas James Doye
\cite{Doye97}. Changes have been made to integrate it.

\section{Introduction}
\label{DoyeChap1}
\subsection{Abstract Datatypes in General}
\label{DoyeSec1.4}

Abstract datatypes provide a way of specifying a type. As a (useful)
side effect an abstract datatype can express how a whole collection
of types may act.

An incredibly specific specification will only specify types which are
all ``the same'' (isomorphic). A less exacting specification may
result in types which are similar, but not the same.

This ``side-effect'' of using a less exacting specification to collect
similar types together forms the basis of strict categorical type
systems.

For example, we may say which operations are available on a certain
family of types. We may also state certain facts about the structure
of all types in a certain family. Most importantly of all, we can
enforce relationships between various types (of different families),
how they interact, and how they may depend upon each other (or not).

See section \ref{DoyeSec4.2} for how we may specify types and what
operations are available to them. Look at section \ref{DoyeSec4.7} for
the relationships between different types. Section \ref{DoyeSec4.6}
details how we may restrict the structure of certain types using sets
of equations.

As a simple example, a polynomial ring depends for its specification
on the underlying ring (from which the coefficients are taken).

As a more complicated example, a polynomial ring could also depend
upon: the type from which it takes its variables; the type of the
exponents; a boolean algebra; an ordered free monoid with one
generator (the natural numbers adjoin \{0\} is often a good choice);
and maybe a few others.

Another factor is the actual relationship between the various
types. In our above example, of the polynomial ring, we know that:
\begin{enumerate}
\item there is a ring monomorphism from the underlying ring to the
polynomial ring
\item there is an injection from the variable type\footnote{The
type from which all the variables are taken. In Axiom, the
variables are usually elements such as X, Y, and Z which are
from the type Symbol} to the polynomial ring. (In fact, an
ordered-set monomorphism\footnote{This is an injective function,
$\phi$, which is both a set-homomorphism (which is just a total
function) and preserves order. So if $a<b$ then $\phi(a) < \phi(b)$.
Mathematicians often utilise the order on the variables and extend it
to an order on polynomials}.)
\end{enumerate}

Other types often have more complicated lattices of
relationships. These relationships are often able to be abstracted out
to apply all the instance types of a particular abstract data type.

\subsection{The Problem}
\label{DoyeSec1.5}

A commonly encountered difficulty in languages which utilise types is
the following:
\begin{enumerate}
\item Given {\tt thing}, of type {\tt Type1}, can we change the type
of {\tt thing} to be that of {\tt Type2}? More accurately, can we
create an element of {\tt Type2}, which corresponds, in some natural
way to {\tt thing} of type {\tt Type1}?
\item If so, how do we go about performing such an operation? Can we
perform this task algorithmically?
\item Does there exist a way of abstracting this question or must
every ({\tt Type1}, {\tt Type2}) pair be considered? Moreover, can we
abstract such an algorithm (as mooted in 2, above) out to cover all
cases? 
\end{enumerate}

Such type changes are often called {\sl coercions}, {\sl conversions},
or {\sl castings}, with all three words having slightly different
meanings.

In this work, we will consider castings and conversions to mean any
type change, regardless of mathematical rigour. We will call a type
change a coercion if it is in some way, ``natural''. We will define
this more rigourously later.

Computer algebraists often need to use coercions since informally most
mathematicans alter the domain of computation without saying phrases
like, ``in the monomorphic image of $A$ to $B$'' or, ``forgetting that
$x$ is a $T$ and viewing it as a $U$''.

For example, we often view the integers ($\mathbb{Z}$) as polynomials (in
$\mathbb{Z}[x]$) rather than considering the constant polynomials of
$\mathbb{Z}[x]$ as being a monomorphic copy of $\mathbb{Z}$. This is
the coercion $\mathbb{Z}\rightarrow\mathbb{Z}[x]$.

Other simple examples of this sort of coercion are:
\[\mathbb{Z}\rightarrow\mathbb{Q},\quad
\mathbb{Z}[x]\rightarrow\mathbb{Z}[y,z],\quad
\mathbb{Z}\rightarrow\mathbb{Q}[x,y],\quad
S(2)\rightarrow S(5),\quad
\mathbb{Z}[x]\rightarrow\mathbb{Z}(x)\]
where $S(n)$ is the symmetric group on a set of $n$ symbols.

Coercions are useful when creating elements of quotient structures,
such as $\mathbb{Z}\rightarrow\mathbb{Z}_n\cong\mathbb{Z}/n\mathbb{Z}$
where ($n \in \mathbb{N}$). Other epimorphic (surjective) examples
include,
\[\mathbb{Z}_{25}\rightarrow\mathbb{Z}_5,\quad
G\rightarrow G/G^\prime\]
where $G$ is any group and $G^\prime$ is the commutator subgroup of
$G$. 

Under certain circumstances, coercions can be ``lifted'' into other
constructors. We refer to these as ``structural coercions'' later in
this work. As examples consider
\[\begin{array}{c}
List(\mathbb{Z})\rightarrow List(\mathbb{Q}),\quad
List(\mathbb{Z}_{25})\rightarrow List(\mathbb{Z}_5),\quad
\mathbb{Z}_{25}[x]\rightarrow\mathbb{Z}_5[x],\\
\mathcal{M}_{2,2}(\mathbb{Z})\rightarrow\mathcal{M}_{2,2}(\mathbb{Z}(x))
\end{array}\]

Examples of type-conversions which are {\sl not} coercions include,
\[\mathbb{Q}\rightarrow{\rm Float},\quad
{\rm List}(\mathbb{N})\rightarrow S(n),\quad
\mathbb{Z}_5\rightarrow\mathbb{Z},\quad
\mathbb{Z}_5\rightarrow\mathbb{Z}_3,\quad
\mathbb{Z}[x]\rightarrow\mathbb{Z}[y]\]

Natural type changes (coercions) can be created in the interpreter at
the moment using a series of {\sl ad hoc} measures. All of this occurs
transparently to the user and works for most common types. However, it
is {\sl not} algorithmic. One of the aims is to provide
an algorithm (algorithm \ref{Doye7.4.1}) to create unique
(theorems \ref{Doye6.5.4}, \ref{Doye7.6.7}) coercions.

\subsection{Examples of how Axiom coerces}
\label{DoyeSec1.6}

Axiom knows (or believes) that all functions called {\tt coerce} are
coercions and will use them to ``build'' other coercions using certain
rules, and special cases defined in the interpreter. Here we give
examples of how Axiom coerces between certain common types.

\subsubsection{Simple examples}
\label{DoyeSubSec1.6.1}

\begin{DoyeExample}
\label{Doye1.6.1}
$\mathbb{Z}\rightarrow\mathbb{Q}$
\end{DoyeExample}

In Axiom, $\mathbb{Z}$ is of type {\tt Integer} and $\mathbb{Q}$ is of
type {\tt Fraction(Integer)}. The type constructor 
{\tt Fraction(R : Ring)} exports a function\footnote{Actually, defined
in a {\tt Category} to which {\tt Fraction} belongs.}
\[{\rm coerce :  R} \rightarrow \%\]
which is the natural monomorphism including the ring {\tt R} in its
fractional field {\tt Fraction(R)}.

\begin{DoyeExample}
\label{Doye1.6.2}
$\mathbb{Z}\rightarrow\mathbb{Z}[X]$:
\end{DoyeExample}

In Axiom. the type {\tt UnivariatePolynomial(X,Integer)} typically
represents $\mathbb{Z}[X]$. This is constructed by the type
constructor,
\begin{verbatim}
   UnivariatePolynomial:(S:Symbol,R:Ring) ->
      PolynomialCategory(R,NonNegativeInteger,Symbol)
\end{verbatim}
{\bf UnivariatePolynomial} exports a function
\[{\rm coerce : R} \rightarrow \%\]
which is the natural monomorphism including the ring {\tt R} in the
polynomial ring as the constant polynomials.

\begin{DoyeExample}
\label{Doye1.6.3}
$\mathbb{Z}[X]\rightarrow\mathbb{Z}[X,Y]$
({\rm where} $\mathbb{Z}[X,Y]=\mathbb{Z}[X][Y]$)
\end{DoyeExample}

This is the same as the previous example, $[Y]$ is acting as a
morphism\\
$\mathbb{Z}[X]\rightarrow\mathbb{Z}[X][Y]$.

\begin{DoyeExample}
\label{Doye1.6.4}
$\mathbb{Z}[X]\rightarrow\mathbb{Z}[X,Y]$
({\rm where} $\mathbb{Z}[Y,Z]=\mathbb{Z}[Y][X]$)
\end{DoyeExample}

Axiom knows that $\mathbb{Z}\rightarrow\mathbb{Z}[Y]$ is a coercion,
and that the map\\
$[X]:\mathbb{Z}[Y]\rightarrow\mathbb{Z}[Y][X]$
(i.e. {\tt UnivariatePolynomial}) lifts this coercion.

\begin{DoyeExample}
\label{Doye1.6.5}
$\mathbb{Z}\rightarrow\mathbb{Q}[X,Y]$
\end{DoyeExample}

This is built by the chain of coercions
\[\mathbb{Z}\rightarrow\mathbb{Q}\rightarrow\mathbb{Q}[X]\rightarrow
\mathbb{Q}[X,Y]\]

\begin{DoyeExample}
\label{Doye1.6.7}
$\mathbb{Z}[X]\rightarrow\mathbb{Z}(X)$
\end{DoyeExample}

This uses the coercion from {\tt Fraction} discussed above, since
$\mathbb[Z](X)$ is represented by 
{\tt Fraction(UnivariatePolynomial(X,Integer))}

\begin{DoyeExample}
\label{Doye1.6.8}
$\mathbb{Z}\rightarrow\mathbb{Z}_5$
\end{DoyeExample}

This uses the function
\[{\rm coerce : \mathbb{Z}\rightarrow\%}\]
defined in {\tt IntegerMod(n:PositiveInteger)}

\begin{DoyeExample}
\label{Doye1.6.9}
$\mathbb{Z}_{25}\rightarrow\mathbb{Z}_5$
\end{DoyeExample}

Axiom cannot coerce from {\tt IntegerMod(25)} to {\tt IntegerMod(5)}.

\subsubsection{Structural coercions}
\label{DoyeSubSec1.6.9}

The structural coercions are ``lifts'' of other coercions. In the case
of {\tt Lists}, one may perform a structural coercion as follows:
\begin{verbatim}
   coerce(x:List(A)):List(B) == map(coerce,x)@List(B)
\end{verbatim}
\begin{verbatim}
   map(f:List(A) -> List(B),x:List(A)):List(B) ==
     nullp(x) => nil()$List(B)
     cons(f(car(x)),map(f,cdr(x)))
\end{verbatim}

In the case of {\tt Polynomials}, a similar {\tt map} function exists
which uses $+$ instead of {\tt cons}

In the case of {\tt Matrix} a similar map function can be used (though
since matrices are of fixed sizes -- unlike sets, lists, and
polynomials -- a default valued matrix $(0)$ has to be created, then
all the values substituted into (added to) it).

\subsection{Mathematical solution overview}
\label{DoyeSec1.7}

The solution to the questions raised in section \ref{DoyeSec1.5} rely
on abstract datatypes. These ``collect'' large numbers of types into
collections of similar types. We will be considering abstract
datatypes in the language of universal algebra; specifically,
order-sorted algebra (section \ref{DoyeSec4.4}).

Universal algebra has close links with the ideas of category
theory. Indeed, (see chapter \ref{DoyeChap3} for category theory
and section \ref{DoyeSec5.4} for the links between the two theories).
Axiom uses the language of category theory to describe
its algebraic design. Thus we will also discuss the fundamentals of
category theory in relation to this work.

Using the order-sorted algebra framework we will show that there is
indeed a strict mathematical definition
(definition \ref{Doye5.5.2})
which we may use to tell us
which type changes are natural, and therefore, in our definition,
coercions.

Moreover we will show that this approach allows us to create such
coercions algorithmically. (algorithm \ref{Doye7.4.1})

In section \ref{DoyeSec1.8} we will show an analogy with moving
buildings to the solution of changing the type of (i.e. coercing)
something. 

\subsection{Constructing coercions algorithmically}
\label{DoyeSec1.8}

Consider the following analogy.

Suppose you have been given the job of moving London Bridge from one
location to another. The bridge is too large to move in one piece.

You will merely be supervising the work and will not have to carry any
of the bricks yourself, hence efficiency is not your concern.

You are also planning to move other different bridges in the future,
and once you have a method for moving one bridge, you would like to be
able to apply that method to the next. This means that next time a
bridge needs moving, you won't have to do much work at all.

Lucky for you someone has already worked out a way of moving any of
the bricks at the very bottom of the bridge to their new location,
regardless of any obstruction, which would normally leave civil
engineers crying. This person was clearly a magician.

So here is your algorithm for moving London bridge, and hence any
future bridge.
\begin{verbatim}
  If the original bridge has no bricks left then finished.
  Else,
    take a brick from the top of the original bridge.
    If this brick is a bottom brick then,
      use the magical bottom brick mover, to place
      the brick in the corresponding location of
      the new bridge.
    Else
      hold the brick in place, in the corresponding
      location in the new bridge.
    Endif.
  Endif.
  Repeat.
\end{verbatim}

Indeed, you realize that this method will work for other brick-based
constructions, or indeed anything that is ``built''. (Obviously, there
are other considerations, is the new location for the bridge a
``natural'' one? For example, is it long or tall enough?)

The analogue of the bridge is a ``thing'' or ``item'' in our computer
algebra system. The original location of the bridge is the original
type of the ``item'' and the new location for the bridge is the
analogue of the type to which we are coercing the ``item''.

Thus what we need to know are:
\begin{enumerate}
\item what are the foundations of the bridge?\\
({\bf Analogue}: what are the constants of our type?)
\item what is the length, width, strength of the bridge?\\
({\bf Analogue}: what are the parameters of the type?)
\item how do you alter the bricks to be of a new shape/material?\\
({\bf Analogue}: we need to be able to recursively call the coercion
algorithm on types which are ``recursively required for
coercion''. That is types which form the bits of the ``item'' once we
have chopped it up.)
\end{enumerate}

This is our analogy and we may consider many data types to be
constructed (or built) similarly. For example, lists (in the
Lisp-sense) are always constructed either by being the empty list, (),
or by being the {\tt cons} of something to the front of another list.

As a more complicated example, polynomials are constructed via one of
the following mechanisms:
\begin{enumerate}
\item by being a member of (the included monomorphic copy of) the
underlying ring (a constant polynomial)
\item by being a symbol exponentiated to some positive power (a
univariate monic monomial)
\item by being the product of a monic monomial and a univariate monic
monomial (a monic monomial)
\item by being the product of a monic monomial and a member of the
underlying ring (a monomial)
\item by adding a monomial to a polynomial
\end{enumerate}

So we see that at least two of the most basic types in computer
algebra are constructed in this way.

\section{Types in Computer Algebra}
\label{DoyeChap2}

\subsection{Introduction}
\label{DoyeSec2.1}

Axiom's main view of things is that every object has a type, and that
there are ostensibly four layers.
\[{\rm items} \in {\rm Domains} \in {\rm Categories} \in {\rm
Category}\]
For example,
\[3 \in Integer \in Ring \in Category\]
and the top layer is the unique distinguished symbol, ``Category''. In
general, this top layer is never referred to, and the three lower
layers are all that are ever considered.

Strictly speaking, if one considers Axiom's {\tt Categories} to be
categories, and Axiom's {\tt Category} symbol, to signify the category
of all the {\tt Categories}\footnote{{\tt Category} is not one of the
{\tt Categories} in Axiom}, then one should write,
\[3 \in {\rm Integer},\quad {\rm Integer} \in {\rm Obj(Ring)},\quad
{\rm Ring} \in {\rm Obj(Category)}\]
where $3 \in {\rm Integer}$ means 3 is an element of the carrier of
the principal sort of the signature to which {\rm Integer} belongs.

Users may extend the Axiom system by writing new {\tt Domains} and
{\tt Categories} in the Axiom language called Spad.

A category is a collection of domains all of which are ``similar''. In
\cite{Dave90} the authors state that they designed the Scratchpad system
of categories or ``abstract algebras''\footnote{Axiom's 
{\tt Categories} can also be thought of as order-sorted algebras, but
the order on sorts is never explicitly defined and there are some
difficulties in the definition of the operator symbols.} for the
following reasons:
\begin{enumerate}
\item economy of effort (section \ref{DoyeSubSec2.5.1})
\item interest (section \ref{DoyeSubSec2.5.2})
\item functoriality (section \ref{DoyeSubSec2.5.3})
\end{enumerate}

\subsubsection{Economy of effort}
\label{DoyeSubSec2.5.1}

The most obvious reason for this is the view of inheritance. A
category (in Axiom) is said to extend another if it inherits all the
other's functions, attributes, and equations. This allows for a
significant amount of re-use.

A more important issue in an algebra system is that if one can prove a
theorem in some generality, i.e. for all the objects in a category,
then one does not have to go around proving the theorem for each
object of the category.

\subsubsection{Interest}
\label{DoyeSubSec2.5.2}

Some categories are interesting and some are not. For example,
{\tt Ring} and {\tt Field} are particularly interesting. Many theorems
can be proved {\sl for all} {\tt Fields}.

However (to borrow an example from \cite{Dave90}), the category of all
rings which when viewed as Abelian groups, have an involution with
precisely one fixed point, is not particularly interesting.

One may think of the designer's concept of ``interest'' as congruent
to ``useful'' (usually to the user, but occasionally to the
designers).

\subsubsection{Functoriality}
\label{DoyeSubSec2.5.3}

This is called ``higher order Polymorphism'' in \cite{Crol93}. There
are operators which given objects of a category can create new objects
of a given (potentially different) category. For example, {\tt List}
is a functor from {\tt SetCategory} to {\tt ListAggregate}
\[\begin{array}{rcl}
{\rm List:SetCategory} & \rightarrow & {\rm ListAggregate}\\
{\rm S} & \mapsto & {\rm List(S)}
\end{array}\]

Indeed, this target category may even be the distinguished symbol
{\tt Category}.

In Axiom, the {\tt Category} constructors (often called functors) are
functors from
\[\prod_{n\in {\rm N}\cup 0}{{\rm Category} \rightarrow {\rm Category}}\]

For example, the {\tt Ring} functor takes no arguments and returns the
{\tt Category} of {\tt Ring}.
\[\begin{array}{rcl}
{\tt Ring} : () & \rightarrow & {\rm Category}\\
() & \mapsto & {\tt Ring}
\end{array}\]

The special {\tt Category} creation operation, {\tt Join} is also a
functor. {\tt Join} is used to declare a new {\tt Category} to be a
subcategory of the intersection of two or more {\tt Categories}. Here
we illustrate the case of {\tt Join} acting on two {\tt Categories}.
\[\begin{array}{rcl}
{\rm Join : (Category,Category)} & \rightarrow & {\rm Category}\\
{\rm (A,B)} & \mapsto & {\rm A}\cap{\rm B}
\end{array}\]

More commonly in Axiom, and Axiom's own use of the word 
``{\sl functor}'' covers cases as follows. This is similar to the case
of {\tt Ring}, only less trivial. {\tt ListAggregate} in the
{\tt Category} of all types of finite lists.
\[\begin{array}{rcl}
{\rm ListAggregate:(SetCategory)} & \rightarrow & {\rm Category}\\
{\rm S} & \mapsto & {\rm ListAggregate(S)}
\end{array}\]

Here we see the subcategory {\rm ListAggregate(S)} of the
{\tt Category} {\tt ListAggregate}. When using Axiom, one usually
thinks of one's type as belonging to the smaller, concrete
(sub){\rm Category}, {\rm ListAggregate(S)}. Mathematically, one
usually thinks of one's type as being in the larger, more abstract
{\tt ListAggregate}.

\section{Category Theory}
\label{DoyeChap3}

As we have already hinted at, category theory is one of the main
foundations of Axiom. In this chapter, we will give the necessary
definitions to investigate this claim. We will also investigate the
claim in itself.

The definitive text on Category Theory is Mac Lane \cite{Macl91}. The
amount of theory we require is relative small. We define categories
without worrying about Russell's Paradox \cite{Mend87}.

\subsection{About Category Theory}
\label{DoyeSec3.2}

\noindent
\begin{DoyeDefinition}
\label{Doye3.2.1}
A category, C, consists of two collections. The
first collection is called the {\tt objects} of the category, or
{\rm Obj}(C). The second collection is called the arrows of category,
or {\rm Arr}(c).

Also, for each arrow, f, there exists two special objects with
which it is associated. The first is the source of f, called
{\tt source}(f). The second is called the target of f, called
{\tt target}(f).

There also exists a ``{\rm law of composition}'' for arrows:
\[\begin{array}{l}
(\forall g,f {\rm arrows})(({\rm source}(g) = {\rm target}(f))
\Rightarrow\\
(\exists g \circ f {\rm arrow})(({\rm source}(g \circ f) = {\rm
source}(f)) \land ({\rm target}(g \circ f) = {\rm target}(g))))
\end{array}\]

Next, for each object, c, there exists a unique arrow, called the
{\rm identity arrow} on c, or ${\rm id}_c$.

Finally the following two axioms must hold:
\[\begin{array}{l}
(\forall k,g,f~{\rm arrows})
((g\circ f,k\circ g~{\rm arrows})\Rightarrow\\
\quad((k\circ(g\circ f),(k\circ g)\circ f~{\rm arrows})\land\\
\quad\quad(k\circ(g\circ f) = (k\circ g)\circ f)))\\
\end{array}\]
\[\begin{array}{l}
(\forall f~{\rm arrows})(({\rm id}_{{\rm target}(f)}\circ f = f)\land
(f\circ {\rm id}_{{\rm source}(f)} = f))
\end{array}\]
\end{DoyeDefinition}

To introduce the concept, here are some simple finite categories.

\begin{DoyeExample}
\label{Doye3.2.2}
{\bf 0} is the empty category, it has no objects and no
arrows
\end{DoyeExample}

\begin{DoyeExample}
\label{Doye3.2.3}
{\bf 1} is the category with one object and one arrow.
\end{DoyeExample}

\begin{DoyeExample}
\label{Doye3.2.4}
{\bf 2} is the category with two objects, $a,b$ and one
non-identity arrow, $f:a\rightarrow b$
\end{DoyeExample}

\begin{DoyeExample}
\label{Doye3.2.5}
$\downarrow\downarrow$ is the category with two objects,
$a,b$ and two non-identity arrows, $f,g:a\rightarrow b$
\end{DoyeExample}

Now let us consider some more useful categories. The collection of
objects in the following examples do not always form a set
\cite{Macl91}\cite{Bern91}\cite{Devl79}\cite{Fran73}

\begin{DoyeExample}
\label{Doye3.2.6}
{\bf Set} is the category which has as objects, all
sets, and has as arrows, all total functions between sets.
\end{DoyeExample}

\begin{DoyeExample}
\label{Doye3.2.7}
{\bf Grp} is the category which has as objects, all
groups, and has as arrows, all group homomorphisms between them.
\end{DoyeExample}

\begin{DoyeExample}
\label{Doye3.2.8}
{\bf Ring} is the category which has as objects, all
rings, and has as arrows, all ring homomorphisms between them.
\end{DoyeExample}

\begin{DoyeExample}
\label{Doye3.2.9}
{\bf Poly} is the following category. An object of
{\bf Poly} is a set of all polynomials with: variables from a fixed
ordered set $V$; coefficients from a fixed ring $R$; and the exponents
of the variables from an ordered free monoid with one generator $E$.
\end{DoyeExample}

An arrow of {\bf Poly} is a Polynomial homomorphism. That is, a Ring
homomorphism which also acts homomorphically on a certain set of
functions which act on polynomials.

Some texts call the arrows of a category the {\sl morphisms} of a
category. Examples Set, Grp, Ring, and Poly show us that for some
naturally occuring categories, the arrows (or morphisms) are just the
homomorphisms of the category. Indeed, an arrow of {\bf Set} (a total
function) is really just a homomorphism between sets.

So we see that the arrows preserve a certain set of properties for
each element of the object.

We also see that every object of {\bf Grp} is an object of {\bf Set};
every object of {\bf Ring} is an object of {\bf Grp}; and every object
of {\bf Poly} is an object of {\bf Ring}.

Now similarly, replace the word ``object'' with the word ``arrow'' in
the previous paragraph and it still holds true.

Hence we see that in an algebra system, there is a certain amount of
``inheritance'' amongst the categories. The categories are the
so-called {\sl abstract datatypes} since they type the usual
datatypes.

We can also see how the categories can collect together all similar
types. This functionality can be used for the three design goals of
Axiom: economy of effort -- the code for many similar types need only
be written once; interest -- collecting similar types together gives
the user an identiical interface to similar types; and functoriality
which is best left to be discussed after the following definition.

\begin{DoyeDefinition}
\label{Doye3.2.10}
 Let B, C be categories, then a functor,
$T : C \rightarrow B$ consists of two functions
\begin{enumerate}
\item $T:{\rm Obj}(C) \rightarrow {\rm Obj}(B)$
(the {\rm object} function)
\item $T:{\rm Arr}{C} \rightarrow {\rm Arr}{B}$
(the {\rm arrow} function)
\end{enumerate}

These must obey the following:

\[\begin{array}{rcl}
(\forall f~{\rm arrow})(T({\rm source}(f)) & = & {\rm source}(T(f)));\\
(\forall f~{\rm arrow})(T({\rm target}(f)) & = & {\rm target}(T(f)));\\
(\forall c~{\rm object})&=&({\rm id}_{T(c)})\\
(\forall g,f~{\rm arrows})(({\rm source}(g) = {\rm target}(f)) &\Rightarrow&
(T(g\circ f) = T(g)\circ T(f)))
\end{array}\]
\end{DoyeDefinition}

This is a very useful definition. It shows us that when we have two
objects and an arrow between them in one category, then a ``sensible''
map of these objects to another category will induce the obvious map
between these image objects.

In fact, one can see that if one were to define a category which has
as objects ``all categories'', and as arrows ``all functors'' then we
would (set theoretical concerns aside) have a well-defined category.

Categories, just like many other mathematical constructs, may form
products. 

\begin{DoyeDefinition}
\label{Doye3.2.11}
For two categories $B$ and $C$ we may construct a
new category denoted $B\times C$ called the product of $B$ and $C$.

An object of $B\times C$ is a pair $\langle b,c\rangle$ where $b$
is an object of $B$ and $c$ an object of $C$

An arrow of $B\times C$ is a pair $\langle f,g\rangle$ where
$f:b\rightarrow b^\prime$ is an arrow of $B$,
$g:c\rightarrow c^\prime$ is an arrow of $C$,
the source of $\langle f,g\rangle$ is $\langle b,c\rangle$
and the target of $\langle f,g\rangle$ is
$\langle b^\prime,c^\prime\rangle$.

Composition of arrows is
$\langle f^\prime,g^\prime\rangle : \langle b^\prime,c^\prime\rangle
\rightarrow\langle b^{\prime\prime}, c^{\prime\prime}\rangle$ and
$\langle f,g\rangle : \langle b,c\rangle\rightarrow
\langle b^\prime, c^\prime\rangle$ is defined via
\[\langle f^\prime,g^\prime\rangle \circ \langle f\circ g\rangle =
\langle f^\prime\circ f, g^\prime\circ g\rangle\]
\end{DoyeDefinition}

Axiom makes use of such products implicitly in its functor
definitions. For example, see the discussion of
{\tt PolynomialCategory} below.

``Natural transformations'' are another important part of category
theory. They are to functors as functors are to categories. Here is a
more formal definition.

\begin{DoyeDefinition}
\label{Doye3.2.12}
For two functors $S,T: C \rightarrow B$ a
natural transformation $\tau : S\dot\rightarrow T$ is a function which
assigns to every $c \in {\rm Obj}(C)$ an arrow
$\tau_c = \tau c : S c\rightarrow Tc$ of $B$ such that
\[(\forall c~\in {\rm Obj}(C))
(\forall f :c\rightarrow c^\prime \in {\rm Arr}(C))
(T f\circ \tau c = \tau c^\prime \circ S f)\]
\end{DoyeDefinition}

Now we may define the following interesting category.

\begin{DoyeDefinition}
\label{Doye3.2.13}
For two categories, $B,C$, the {\rm functor
category} $B^C$ is the category with objects the functors
$T:C\rightarrow B$ and arrows, the natural transformations between two
such functors.
\end{DoyeDefinition}

As another fairly abstract definition, consider the following.

\begin{DoyeDefinition}
\label{Doye3.2.14}
Let $S:D\rightarrow C$ be a functor and
$c \in {\rm Obj}(C)$. A universal arrow $c\rightarrow S$ is a pair
$\langle r,u\rangle \in {\rm Obj}(D)\times{\rm Arr}(C)$ where
$u:c \rightarrow Sr$ such that
\[(\forall(d,f)\in{\rm Obj}(D) \times {\rm Arr}(C) {\rm where}
f : c \rightarrow Sd)
(\exists! f^\prime:r\rightarrow d\in{\rm Arr}(D))(Sf^\prime\circ
~u=f)\]
\end{DoyeDefinition}

Functoriality in a computer algebra system allows us to view objects
of one category as objects of another.

As we have already seen, an object of {\tt Poly} is an object of
{\tt Ring} and hence an object of {\tt Grp} and thus an object of
{\tt Set}. We have also seen this is true for their arrows. This
relationship is called the subcategory relationship, defined formally
as follows.

\begin{DoyeDefinition}
\label{Doye3.1.15}
A category $C$ is a subcategory of a category
$B$ iff every object of $C$ is an object of $B$ and every arrow of $C$
is an arrow of $B$.
\end{DoyeDefinition}

Functors from subcategories to the categories of which they are
subcategories are often called ``forgetful functors''. This is more
often true when the target of the functor is {\sl Set}.

Axiom's designers also use functors to create instances of abstract
datatypes. {\tt PolynomialCategory(.,.,.)} (Axiom's equivalent of
{\tt Poly}) is in Axiom's view a functor.

\[\begin{array}{c}
{\rm Ring}\times {\rm OrderedAbelianMonoid}\times {\rm OrderedSet}\rightarrow
{\rm PolynomialCategory(.,.,.)}\\
{\rm (R,E,V)}\mapsto {\rm PolynomialCategory(R,E,V)}
\end{array}\]

This functoriality provides the ``glue'' for Axiom's type mechanism.

Now, trivially for categories $A,B$ if $\exists F:A\rightarrow B$ a
forgetful functor, the $B$ is a subcategory of $A$. Equally trivially,
a concrete instance of {\rm PolynomialCategory(R,E,)} is a subcategory
of {\tt PolynomialCategory(R,.,.)} which is a subcategory of
{\rm Poly}.

It is this first form of subcategory relation that provides Axiom's
inheritance mechanism.

\subsection{Categories and Axiom}
\label{DoyeSec3.3}

\begin{DoyeNotation}
\label{Doye3.3.1}
To distinguish between Axiom's internal structures
and those commonly used in mathematics, things which belong to Axiom
will be writtin in {\bf this font}. Specifically,
\begin{itemize}
\item {\bf Category} will always refer to Axiom's distinguished
symbol, to which all Axiom's {\bf Categories} belong.
\item Hence, a {\bf Category} is an Axiom object declared to be
such an object, e.g. {\bf Ring}, {\bf PolynomialCategory(R,E,V)}
\item A {\bf Domain} is an Axiom object declared to be a member
of a particular {\bf Category}, e.g. {\bf Integer},
{\bf Polynomial(Integer)}
\item An {\bf item} is an element of a {\bf Domain}, e.g. 1, 5*x**2+1
\end{itemize}
\end{DoyeNotation}

Axiom's main view of things is that every object
has a type, and that there are four layers.
\[{\bf items}\in{\bf Domains}\in{\bf Categories}\in{\bf Category}\]

{\bf Categories} also may inherit from or extend other {\bf
Categories}, forming an inheritance lattice. Thanks to the higher
order polymorphism available in Axiom, {\bf Categories} may also be
parameterized by {\bf items}\footnote{One may view a domain as a
category, whose objects are the items, and whose arrows are either
trivial (i.e. solely the identity arrows) or some other natural
occurring meaning, e.g. in {\bf PositiveInteger} one may think that
there is a natural map $35\rightarrow 5$, since $5\vert 35$. This
would then mean that this map could be ``lifted'' to
{\bf IntegerMod 35 $\rightarrow$ IntegerMod 5}} or {\bf Domains}.

This parameterisation may cause some confusion. For example,
\[{\bf List(S) : ListAggregate(S)}\]
declares for each and every {\bf S}, {\bf List(S)} is in the category
{\bf ListAggregate(S)}. For example, {\bf List(Integer)} is an object
of {\bf ListAggregate(Integer)}. However, {\bf ListAggregate(Integer)}
is a mere subcategory of ``the category of all objects which are
domains of linked lists''. This is {\bf ListAggregate(S)}.

So {\bf ListAggregate(S)} contains, for example, both
{\bf List(Integer)} and\\
{\bf List(Fraction(Integer))} as objects, but
what are the arrows of this category? This is something which is not
made explicit in the Axiom literature, and is indeed the core of this
work.

We will discuss what it means to be a ``natural map'' and how this
related to categorical arrows.

\subsection{Functors and Axiom}
\label{DoyeSec3.4}

Axiom describes its domain constructors as functors, and this is
true. After all, (neglecting difficulties with constructors which take
domain elements for arguments) these constructors are maps from a
cross product of categories to another category.

The difficulties with constructors which take domain elements for
arguments disappear when considering the argument use in the 
footnote in section \ref{DoyeSec3.3}.

\subsection{Coercion and category theory}
\label{DoyeSec3.5}

If {\bf coerce : A $\rightarrow$ B}, then we are going to have that
{\bf A} and {\bf B} are objects of the same category {\bf ACAT}, and
that {\bf coerce} is an arrow of that category. (See definition
\ref{Doye5.5.2})

Also, if {\bf T} is a functor from {\bf ACat} to {\bf TCat}, which in
Axiom would look like
\[{\bf T(A:ACat) : TCat}\]
then {\bf T} lifts the arrows of {\bf ACat} to {\bf TCat}, and in
particular
\[{\bf T : coerce : A \rightarrow B \mapsto
coerce:TCat(A)\rightarrow TCat(B)}\]

In many types (and some other languages) {\bf T} is acting like the
familiar ``map'' operator on the {\bf coerce} function.

\subsection{Conclusion}
\label{DoyeSec3.6}

We have seen in this section how category theory and abstract
datatyping especially with respect to Axiom's {\bf Category} mechanism
are ideologically similar.

We have also shown how Axiom's functors interact with coercions from a
category theoretical perspective.

\section{Order sorted algebra}
\label{DoyeChap4}

In this section we will introduce the concepts of universal
algebra. We will look at the unsorted case to start with and then move
on to the sorted case.

We then follow up with the equational calculus which allows us to
consider sets of equations which must hold in a concrete instance of
an algebra.

All the work in this section is taken from \cite{Dave93a} except for a
couple examples.

\subsection{Universal Algebra}
\label{DoyeSec4.2}

Universal algebra will provide us with a natural way of representing
categories of types which possess certain functions. In the following
lengthy section of definitions, keep in mind that what we will define
as a ``signature'' will be equivalent (in some sense) to our notion of
a category (or abstract datatype). An algebra will be the ideological
equivalent of an object (or type).

\begin{DoyeDefinition}
\label{Doye4.2.1}
A sort-list $S$, is a (finite) sequence of
symbols (called sorts) normally denoted ($s_1,\ldots,s_m$).
\end{DoyeDefinition}

\begin{DoyeDefinition}
\label{Doye4.2.2}
Given a sort-list of size $m$, a set $A$ of
$S$-carriers is an ordered $m$-tuple of sets $A_{s_i}$, indexed by
$S$.
\end{DoyeDefinition}

\begin{DoyeExample}
\label{Doye4.2.3}
One may consider the sort-list of a vector space to
be ($K$,$V$) where $K$ is to be identified with the underlying field
and $V$ is to be identified with the set of all points in the vector
space. Then considering {\bf C} to be a vector space of {\bf R}, the
($K$,$V$)-carriers of {\bf C} are ({\bf R}, {\bf C}).
\end{DoyeExample}

So we see that the carriers of a particular type are a list of types
which ``have something to do with'' the type in question. The sort
list is a list of the same length where the $i$th element is the
symbol corresponding to a particular abstract datatype to which the
$i$th carrier belongs\footnote{Notice that the sort list is defined
first and that the carriers depend on the sort list. One does not
define a list of carriers and then fix a list of abstract datatypes
{\sl post facto}.}.

\begin{DoyeDefinition}
\label{Doye4.2.4}
Given a sort-list $S$ and 
$n\in \mathbb{N} \cup \{0\}$, an $S$-arity of rank $n$ is an ordered
$n$-tuple of elements of $S$.
\end{DoyeDefinition}

An arity is merely a list of elements of the sort list. This will be
useful when we wish to type polymorphic (or abstract) functions.

\begin{DoyeDefinition}
\label{Doye4.2.5}
Given a sort-list $S$,
$n\in \mathbb{N} \cup \{0\}$, $q=(q_1,\ldots,q_n)$ an $S$-arity of
rank $n$, and a set $A$ of $S$-carriers, we define
\[A^q := \prod_{i\in\{1,\ldots,n\}} A_{q_i}\]
\end{DoyeDefinition}

This is the map of an arity to the list of carriers.

\begin{DoyeDefinition}
\label{Doye4.2.6}
An $S$-operator of arity $q$ is a function from
$A^q$ to one of the elements of $A$.
\end{DoyeDefinition}

\begin{DoyeDefinition}
\label{Doye4.2.7}
An $S$-operator set or $S$-sorted signature is a
set $\Sigma$ of sets $\Sigma_{n,q,s}$ indexed by 
$n\in\mathbb{N}\cup\{0\}$, $q$ an $S$-arity of rank $n$, and $s\in S$,
such that $\cup_{n,q,s}\Sigma_{n,q,s}$ is a subset of some
alphabet. An element of some $\Sigma_{n,q,s}$ is called an operator
symbol.

The usual notation for such a signature is ($\Sigma$,$S$).
\end{DoyeDefinition}

So this defines a set of sets of polymorphic
functions\footnote{Functions without methods.} for a particular
sort-list. A signature is like a category in that it is an abstract
datatype.

A signature collects together all types which share a similar family
of operators.

\begin{DoyeExample}
\label{Doye4.2.8}
Monoids: $\mathbb{N}\cup\{0\}$ is an additive
monoid, whereas $\mathbb{N}$ is a multiplicative monoid.

The signature for monoids could be viewed as being
\[\langle\Upsilon,U\rangle =
\langle(M,B),((c),(),(id),(ep),(),(),(b),()\ldots)\rangle\]
where $M$ is the monoid sort, $B$ is the sort of boolean logic
types. The operator symbol $e$ is a member of $\Upsilon_{(0,(),M)}$
and corresponds to the function which always returns the identity
constant\footnote{Constants are often represented by (if not compiled
in the same way as) functions in programming languages, such as
Axiom's Spad language. This gives a homogeneous interface for
providing constants.} of the monoid.

$id$ is the operator symbol in $\Upsilon_{(1,(M),M)}$ which
corresponds to the identity function.

$ep$ is the operator symbol in $\Upsilon_{(1,(M),B)}$ which
corresponds to the ``is this the identity element?'' function. 

Last (in our example) but by no means least, $b$ is the monoid's
binary operator from $\Upsilon_{(1,(M,M),M)}$.

So in $\mathbb{N}\cup\{0\}$, $e$, $id$, $ep$, $b$ correspond to
0, $id$, 0?, $+$, respectively. Whereas in $\mathbb{N}$, $e$, $id$,
$ep$, $b$, correspond to 1, $id$, 1?, $\times$ respectively.
\end{DoyeExample}

\begin{DoyeDefinition}
\label{Doye4.2.9}
A (multi-sorted, total) $\Sigma$-algebra is an
ordered pair $\langle A,\alpha\rangle$ where $A$ is an $S$-carrier set
and 
\[\alpha=\{\alpha_{n,q,s}\vert n\in\mathbb{N}\cup\{0\},
q {\rm\ an\ arity\ of\ rank\ }n,s\in S\}\]
\[\alpha_{n,q,s}=\{\alpha_{n,q,s,\sigma}:A^q\rightarrow 
A_s\}_{\sigma \in \Sigma_{n,q,s}}\]

So if $\langle\Upsilon,U\rangle$ is the monoidal signature, the
\[\langle({\bf N},{\bf
Boolean}),((1),(),(id),(1?),(),(),(\times),\ldots)\rangle\]
is an $\Upsilon$-algebra.
\end{DoyeDefinition}

\begin{DoyeNotation}
\label{Doye4.2.10}
Let $\langle A,\alpha\rangle$ be a
$\Sigma$-algebra. For an operator symbol $\sigma_{n,q,s}$ of the
signature $\langle\Sigma,S\rangle$ the function associated with this
symbol in $\langle A,\alpha\rangle$ is represented by either
$\alpha_{n,q,s,\sigma}$ or $\alpha_{\sigma_{n,q,s}}$.
\end{DoyeNotation}

This second form of notation is useful for when we refer to operator
symbols by names other than those in the form $\sigma_{n,q,s}$. For
example, if $\tau = \sigma_{n,q,s}$ then
\[\alpha_{n,q,s,\sigma} = \alpha_{\sigma_{n,q,s}} = \alpha_\tau\]

\begin{DoyeDefinition}
\label{Doye4.2.11}
Given $\langle A,\alpha\rangle$,
$\langle B,\beta\rangle$ both $\Sigma$-algebras, then a
$\Sigma$-homomorphism\\
$\phi:\langle A,\alpha\rangle\rightarrow\langle B,\beta\rangle$
is an $S$-indexed family of functions
$\phi_s:A_s\rightarrow B_s$ (for each $s$ in $S$) such that
\[(\forall n\in\mathbb{N}\cup\{0\})
(\forall q{\rm\ arities\ of\ rank\ }n)
(\forall (a_1,\ldots,a_n)\in A^q)
(\forall s\in S)
(\forall \sigma \in \Sigma_{n,q,s})\]
we have
\[\phi_s(\alpha_{n,q,s,\sigma}(a_1,\ldots,a_n))=
\beta_{n,q,s,\sigma}(\phi_{q_1}(a_1),\ldots,\phi_{q_n}(a_n))\]
\end{DoyeDefinition}

As an example of a homomorphism, let us consider our monoidal case
once more. The map $\psi$ from $\mathbb{N}\cup\{0\}$ which
maps\footnote{We haven't provided an exponentiation function in our
monoidal algebra, but adding an extra sort and a function it is
possible. Otherwise think of the map as taking 0 to 1; 1 to 2; 2 to
2$\times$2; 3 to 2$\times$2$\times$2; and so on.} $n\mapsto 2^n$ is 
a homomorphism.

\subsection{Term Algebras}
\label{DoyeSec4.3}

Term algebras provide us with at least one example of an algebra for
each signature. In some sense, it is the ``free-est'' algebra of the
signature and all other algebras are isomorphic to quotients of the
term algebra.

Notice that given $\langle\Upsilon,U\rangle$ the definition of the
monoidal signature from Example \ref{Doye4.2.8}, the term algebra
is not the free monoid, since we have not added in any ``laws'' or
``equations'' to the algebra. Thus we do not have associativity in the
term algebra, whereas we do in the free monoid.

Thus we see that, as yet, universal algebra does not model real
mathematics perfectly. However this situation will be remedied
somewhat in section \ref{DoyeSec4.6}.

\begin{DoyeNotation}
\label{Doye4.3.1}
We will define the set $\Delta$ to be the set
containing three special symbols
\[\Delta := \{(\}\cup\{)\}\cup\{,\}\]
\end{DoyeNotation}

$\Delta$ is just a piece of notation that will make the following
definition less verbose.

\begin{DoyeDefinition}
\label{Doye4.3.2}
Let $X$ be an $S$-indexed family of sets
disjoint from each other, from the set $\Delta$ and from
$\bigcup_{n,q,s}\Sigma_{n,q,s}$. We define $T_\Sigma(X)$ to be the
$S$-indexed family of sets of strings of symbols from
$\bigcup_{s\in S}X_x\cup\Delta\cup\bigcup_{n,q,s}\Sigma_{n,q,s}$
each set as small as possible satisfying these conditions:
\begin{enumerate}
\item $(\forall s \in S)(\Sigma_{0,(),s} \subseteq T_\Sigma(X)_s)$
\item $(\forall s \in S)(X_s \subseteq T_\Sigma(X)_s)$
\item $(\forall \sigma \in \Sigma_{n,q,s})
(\forall i \in \{1,\ldots,n\})
(\forall t_i \in T_\Sigma(X)_{q_i})
(\sigma(t_1,\ldots,t_n)\in T_\Sigma(X)_s)$
\end{enumerate}

We make $T_\Sigma(X)$ into a $\Sigma$-algebra by defining
operators $\sigma_T$ on $T_\Sigma(X)$, for each
$\sigma \in \Sigma_{n,q,s}$ via:
\begin{itemize}
\item If $n=0$ then $\sigma_T:=\sigma$ (Guaranteed to be in
$T_\Sigma(X)$ by (1)).
\item Else, define $\sigma_T(t_1,\ldots,t_n)$ to be the string
$\sigma(t_1,\ldots,t_n)$
\end{itemize}

$T_\Sigma(X)$ is called the {\rm term algebra}, and an element
of $T_\Sigma(X)$ is called a {\rm term}.
\end{DoyeDefinition}

\subsection{Order-sorted algebras}
\label{DoyeSec4.4}

Order sorted algebras extend the concept of universal algebras by
imposing an order on the elements of the sort-list. This can be useful
if we know that all algebras of our signature
$\langle\Sigma, S\rangle$ have the carrier of $S_2$ as a subset of
$S_1$, say.

The ordering of sorts imposes a subtype lattice on the sorts (and
hence their carriers). This can then be used in any algebra of the
signature to either restrict a function already defined on one type
to a subtype of that type, or extend a function on a type to a partial
functon of the supertype. Partial functions and order sorted algebra
are discussed in section \ref{DoyeSec5.2}.

Firstly, we had better define what we mean by an ``order''.

\begin{DoyeDefinition}
\label{Doye4.4.1}
A strict partial order on a set $S$ is a
relation $\prec$ on $S$ which is transitive, antisymmetric, and
irreflexive. 
\end{DoyeDefinition}

A weak partial order on a set $S$ is a relation $\preceq$ such
that,
$a\preceq b \Leftrightarrow (a \prec b) \lor (a = b)$.
Such a relation is transitive.

\begin{DoyeNotation}
\label{Doye4.4.2}
Let $S$ be a set of sort symbols, such that there
is a partial order $\prec$ defined on $S$, and a ``top'' element $u$
of $S$ such that $s \prec u$ for all $s$ in $S$.
\end{DoyeNotation}

$u$ provides us with a universe in which to work. Equivalently, we
could set $u$ to be any class$_w$ which is ``big enough'' (The term
class$_w$ is defined later.)

\begin{DoyeDefinition}
\label{Doye4.4.3}
Extend $\preceq$ from $S$ to the $S$-arities of
rank $n$ by defining\\
$(s_1,\ldots,s_n)\preceq(t_1,\ldots,t_n)$ iff
$(\forall i \in \{1,\ldots,n\})(s_i\preceq t_i)$.
\end{DoyeDefinition}

\begin{DoyeDefinition}
\label{Doye4.4.4}
An order-sorted, total $\Sigma$-algebra is an
ordered triple\\
$\langle A,\{A_s : s \in S\},\alpha\rangle$, where $A$
is a class known as the {\rm universe}, $\{A_s,s \in S\}$ is an
$S$-indexed family of subsets of $A$, known as the carriers of the
algebra, and $\alpha$ is a set of sets of functions
$\alpha_{n,q,s}=\bigcup_{\sigma\in\Sigma_{n,q,s}}
\{\alpha_{n,q,s,\sigma} : A^q\rightarrow A_s\}$, such that:
\begin{enumerate}
\item $A_u=A$
\item If $s\preceq s^\prime$ in $S$, then $A_s \subseteq A_{s^\prime}$
\item If $\sigma\in\Sigma_{n,q,s}\cap\Sigma_{n,q^\prime,s^\prime}$,
with $s\preceq s^\prime$ and $q^\prime \preceq q$, then
$\alpha_{n,q,s,\sigma}\vert_{A^{q^\prime}}=
\alpha_{n,q^\prime,s^\prime,\sigma}$
\end{enumerate}

\end{DoyeDefinition}

Strictly speaking, that last condition should be
\[\iota_{A_s\rightarrow A_{s^\prime}}\circ
\alpha_{n,q,s,\sigma}\vert_{A^{q^\prime}} =
\alpha_{n,q^\prime,s^\prime,\sigma}\]

(where $\iota_{A_s\rightarrow A_{s^\prime}}$ is the inclusion
operator $A_s\rightarrow A^\prime_s$) since otherwise the target
of the left hand side would be $A^s$ and of the right hand side would
be $A^{s^\prime}$.

This is typical of the sort of ``abuse of notation'' that computer
systems often have to implement.

The definition of order sortedness ensures some confluence amongst
operators on subtypes. However, it does not provide enough for most
sensible applications. The following definition ensures a more
confluent system.

\begin{DoyeDefinition}
\label{Doye4.4.5}
The order-sorted signature $\Sigma$ is regular
if whenever $\tilde{q}$ is an arity and $\sigma\in\Sigma_{n,q,s}$ with
$\tilde{q}\preceq q$, there is a least pair $q^\prime$,$s^\prime$ such
that $\sigma\in\Sigma_{n,q^\prime,s^\prime}$ and
$\tilde{q}\preceq q^\prime$ and $s^\prime\preceq s$.
\end{DoyeDefinition}

Now, we will extend the definition of term algebras to the order
sorted case.

\begin{DoyeDefinition}
\label{Doye4.4.6}
Let $X$ be an $S$-indexed family of sets
disjoint from each other, from the set $\Delta$ and from
$\bigcup_{n,q,s}\Sigma_{n,q,s}$. We define $T_\Sigma(X)$ to be the
$S$-indexed family of sets of strings of symbols from
$\bigcup_{n,q,s}X_s\cup\Delta\cup
\bigcup_{n,q,s}\Sigma_{n,q,s}$, each set of small as possible
satisfying these conditions:

\begin{enumerate}
\item $(\forall s\in S)(\Sigma_{0,(),s}\subseteq T_\Sigma(X)_s)$
\item $(\forall s\in S)(X_s \subseteq T_\Sigma(X)_s)$
\item $(\forall s,s^\prime\in S)
((s\preceq s^\prime) \Rightarrow (T_\Sigma(x)_s\subseteq 
T_\Sigma(X)_{s^\prime}))$
\item $(\forall \sigma \in \Sigma_{n,q,s})
(\forall i \in \{1,\ldots,n\})
(\forall t_i\in T_\Sigma(X)_{q_i})
(\sigma(t_1,\ldots,t_n)\in T_\Sigma(X)_s)$
\end{enumerate}

We make $T_\Sigma(X)$ into a $\Sigma$-algebra by letting the
first component be $T_\Sigma(X)_u$ and defining operators $\sigma_T$
on $T_\Sigma(X)$, for each $\sigma\in\Sigma_{n,q,s}$ via:
\begin{itemize}
\item If $n=0$ then $\sigma_T:=\sigma$. (Guaranteed to be in
$T_\Sigma(X)_s$ by (1))
\item Else, define $\sigma_T(t_1,\ldots,t_n)$ to be the string
$\sigma(t_1,\ldots,t_n)$
\end{itemize}

$T_\Sigma(X)$ is called the {\rm algebra}, and an element of
$T_Sigma(X)$ is called a {\rm term}.
\end{DoyeDefinition}

The following theorem proves the ``freeness'' of term algebras. That
is to say all algebras are isomorphic to a quotient of the term
algebra. In this way we see that all algebras, once represented as a
term algebra and a set of rewrite rules are easily implementable in a
rewrite system.

It also says something about the constructibility of types. That is,
if $\bigcup_{n,q,s}\Sigma_{n,q,s}$ is finite then clearly only a
finite number of functions in each and every $\Sigma$-algebra
construct the whole algebra.

Moreover, suppose in every real algebra which we wish to study, a
certain set of equations hold (see section \ref{DoyeSec4.6}). 
Then all our algebras are isomorphic to
factors of the term algebra factored out by that set of equations.

Then if every element of this free-est factor algebra is equal to one
constructed by a (potentially very small, finite) subset of a
(potentially infinite) $\bigcup_{n,q,s}\Sigma_{n,q,s}$, we may utilise
this to construct elements of our algebra.

In the automated coercion algorithm (section \ref{DoyeSec7.3})
we utilise a small (but not
necessarily minimal) set to construct all (or some) of the elements of
one of the sorts of an algebra.

\begin{DoyeTheorem}
\label{Doye4.4.7}
{\bf (First universality theorem)} Let
$\langle A,\alpha\rangle$ be any $\Sigma$-algebra, $\theta$ any map
($S$-indexed family of maps) from $X$ into $A$. Then there exists a
unique $\Sigma$-homomorphism $\theta^*$ from $T_\Sigma(S)$ to $A$ such
that $(\forall s \in S)(\forall x \in X_s)$
\[\theta_s^*(\iota(x))=\theta_s(x)\]
\end{DoyeTheorem}

The proof may be found in \cite{Dave93a}

\subsection{Extension of signatures}
\label{DoyeSec4.5}

In this section we will formalise what we mean for one algebra to be
an extension of another, or more importantly, from our point of view,
for one algebra to be a portion of another.

More formally we are saying how the abstract datatypes (or categories
or signatures) may inherit from each other. This sometimes corresponds
to algebras depending on each other.

First, a piece of notation.

\begin{DoyeNotation}
\label{Doye4.5.1}
If $S$ and $T$ are two sets of sets both indexed
by the same set, $I$, we say that
$S \overset{\rightarrow}{\subseteq} T$ iff
$(\forall i \in I)(S_i \subseteq T_i)$
\end{DoyeNotation}

This extends the definition of subsets to $n$-tuples of sets.

\begin{DoyeDefinition}
\label{Doye4.5.2}
Let $S$, $S^\prime$ be two sort-lists, such
that, as sets of symbols $S\subseteq S^\prime$, and that
$(\forall s,t\in S)((s\preceq_S t) \Leftrightarrow
(s \preceq_{S^\prime} t))$. Let $\Sigma$ be an $S$-sorted signature
and $T$ be a $S^\prime$-sorted signature. If
$\Sigma\overset{\rightarrow}{\subseteq} T$, we say that $\Sigma$ is a
{\rm sub-signature} of $T$ and that $T$ is a super-signature of
$\Sigma$.
\end{DoyeDefinition}

Thus we have the obvious definition of sub-signature. As an example,
the monoidal signature is a sub-signature of the signature for
groups. The notion of sub-signature corresponds directly with that of
sub-category.

\begin{DoyeDefinition}
\label{Doye4.5.3}
With $S$, $S^\prime$, $\Sigma$, $T$ as in the
previous definition, let $\langle A,\alpha\rangle$
be an $S^\prime$-sorted $T$ algebra. Define
$\langle A,\alpha\rangle\vert_\Sigma$,
called $\langle A,\alpha\rangle$ restricted to $\Sigma$, to be the
$S$-sorted $\Sigma$-algebra with carriers, those carriers of
$\langle A,\alpha\rangle$ which are indexed by sort symbols from $S$,
and operators, $n$-ary operators $\alpha_\tau$ of arity $q$, and
result sort $s$, for every $\tau\in\Sigma_{n,q,s}$.
\end{DoyeDefinition}

This is applying the forgetful functor (from the category
(corresponding to) $T$ to that (corresponding to) $\Sigma$) to
$\langle A,\alpha\rangle$.

\subsection{The equational calculus}
\label{DoyeSec4.6}

Again, we borrow heavily from Davenport's lecture
notes\cite{Dave93a}. Throughout this section, we assume that
$S=\{s_1,\ldots,s_n\}$ is our indexing family of sort symbols, and
$\Sigma = \{\Sigma_{n,q,s}\}$ is an $S$-sorted signature.

The equational calculus presented here applies to multi-sorted
algebra. The reader will see that it clearly may be extended to the
order-sorted case.

The equational calculus allows us to add ``equations'' to our
signatures (and hence, algebras). This will allow us to assert facts
about all the algebra in a signature. For example, we may wish to note
that one of the binary operators is always associative. Other more
complicated expressions are available also.

The equational calculus does not allow us to define everything that we
need: only those things that are easily definable as equations.

\begin{DoyeDefinition}
\label{Doye4.6.1}
An $S$-indexed family of relations
$R = \{R_{s_1},\ldots,R_{s_n}\}$ on a $\Sigma$-algebra,
$\langle A,\alpha\rangle$ is called a $\Sigma$-congruence if it
satisfies the following four families of conditions:
\[\begin{array}{rclr}
a\in A_{s_i} & \Rightarrow & aR_{s_i}a & (R)\\
aR_{s_i}b & \Rightarrow & bR_{s_i}a & (S)\\
aR_{s_i}b {\rm\ and\ }bR_{s_i}c & \Rightarrow & aR_{s_i}c & (T)\\
(\forall \sigma\in\Sigma_{n,q,s})
(a_1 R_{q_1}b_1,\ldots,a_n R_{q_n}b_n) & \Rightarrow &
\sigma(a_1,\ldots,a_n)R_s\sigma(b_1,\ldots,b_n) & (C_\sigma)
\end{array}\]
\end{DoyeDefinition}

The first three conditions imply that $R_{s_i}$ is an
{\sl equivalence relation} on the set $A_{s_i}$, while the conditions
($C$) explain how $R$ relates to the various operators of
$\Sigma$. The operators of $\Sigma_{n,q,s}$ are well-defined on the
equivalence classes.

\begin{DoyeDefinition}
\label{Doye4.6.2}
Let $\langle A,\alpha\rangle$ be an
$\Sigma$-algebra, and $\equiv$ be a $\Sigma$-congruence on
$\langle A,\alpha\rangle$. Define $\langle A,\alpha\rangle/\equiv$ to
be the $\Sigma$-algebra $\langle B,\beta\rangle$, where the carrier
set $B_{s_i}$ of $B$ is the set of equivalence classes of $A_{s_i}$
under the equivalence relation $\equiv_{s_i}$, and $\beta_\sigma$ is
the operator defined by
\[\beta_\sigma([a_1],[a_2],\ldots,[a_n])=
[\alpha_\sigma(a_1,a_2,\ldots,a_n)]\]
for every operator symbol $\sigma$ in every $\Sigma_{n,q,s}$.
\end{DoyeDefinition}

This is merely the quotient algebra, and to be sure, the following is
a theorem.

\begin{DoyeTheorem}
\label{Doye4.6.3}
The operators $\beta_\sigma$ in the above definition are well-defined.
\end{DoyeTheorem}

The following definition actually turns out to be very important.

\begin{DoyeDefinition}
\label{Doye4.6.4}
We say that a sort $s$ is {\rm void} in the
signature $\Sigma$ if $T_\Sigma(\emptyset)_s = \emptyset$.
\end{DoyeDefinition}

Basically, having a void sort $\tilde{s}$ in the signature means that
there are no constants of that type
$(\Sigma_{0,(),\tilde{s}}=\emptyset$) and that one of the following is
true.
\begin{itemize}
\item no operators have $\tilde{s}$ as a return type:
$((\forall q,n)(\Sigma_{n,q,s}=\emptyset)$ where $q$ is an arity of
rank $n$)
\item every operator with $\tilde{s}$ as a return type has an argument
whose sort is either void or $\tilde{s}$:
$((\forall q,n)(\Sigma_{n,q,s}\ne\emptyset)\Rightarrow
(\exists i \in \{1,\ldots,n\})
(q_i {\rm\ is\ a\ void\ sort\ or\ }\tilde{s}))$ where $q$ is an arity
of rank $n$)
\end{itemize}

\begin{DoyeLemma}
\label{Doye4.6.5}
If $s$ is not void in the signature $\Sigma$, then in
every $\Sigma$-algebra $\langle A,\alpha\rangle$, $A_s\ne\emptyset$.
\end{DoyeLemma}

Goguen and Meseguer suggest the following rules of deduction for a
sound multi-sorted equational calculus.

\begin{DoyeNotation}
\label{Doye4.6.6}
Let $X$ be an $S$-indexed family of sets of
variable symbols, such that each $X_{s_i}$ is disjoint from all the
others, from the operator symbols of $\Sigma$, and from any symbols in
any particular algebras we may be reasoning over.

We will be reasoning frequently with $S$-indexed families of
sets, and wishing to perform operations on them. Let
$X=\langle X_1,\ldots,X_n\rangle$ and
$Y=\langle Y_1,\ldots,Y_n\rangle$ be two $S$-indexed families of sets,
and define $X\overset{\rightarrow}{\cup}Y$ to be the $S$-indexed family
$\langle X_1\cup Y_1,\ldots,X_n\cup Y_n\rangle$. Similarly, we will
write $X\overset{\rightarrow}{\subseteq}Y$ to indicate that each
component of $X$ is a subset of the corresponding element of $Y$.

A final piece of notation is that the indexed family of empty
sets will be donoted by $\overset{\rightarrow}{\emptyset}$.
\end{DoyeNotation}

\begin{DoyeDefinition}
\label{Doye4.6.7}
A {\rm multi-sorted equation} for the signature
$\Sigma$ consists of a triple $\langle Y,T_1,t_2\rangle$ where
$Y\overset{\rightarrow}{\subseteq}X$, $t_1$ and $t_2$ are terms from
the same carrier set of $T_\Sigma(X)$ (or 
$T_\Sigma(X\overset{\rightarrow}{\cup}A)$ if we are dealing with
equations in the particular algebra $\langle A,\alpha\rangle$), and
every variable occurring in $t_1$ and $t_2$ occurs in the appropriate
memeber of $Y : t_1,t_2\in T_\Sigma(Y)$. Such equations are written
$\forall Y t_1=t_2$.

If $t_1$ and $t_2$ come from the same carrier set of
$T_\Sigma(X)$ we say that the equation is $\Sigma$-generic.
\end{DoyeDefinition}

One should read the symbol $\forall Y$ as meaning ``for all values of
all the variables of $Y$ in the appropriate sorts (and there had
better be some values in those sorts)''. It is this interpretation
that will solve the paradox mentioned earlier.

\begin{DoyeDefinition}
\label{Doye4.6.8}
An {\rm equational system} for the signature
$\Sigma$ is a set of equations for $T_\Sigma(X)$ (or 
$T_\Sigma(X\overset{\rightarrow}{\cup}A$) if we are dealing with
equations in a particular algebra $\langle A,\alpha\rangle$).
\end{DoyeDefinition}

We will tend to write $e = f$ for an equation from an equational
system, meaning $\forall Y e=f$ where $Y$ is the $S$-indexed family of
sets of variables consisting {\sl precisely} of those variables
occurring in $e$ and $f$.

\begin{DoyeNotation}
\label{Doye4.6.9}
$x_i/t_i$ means substitute the variable $x_i$ with the
term $t_i$. We call this a {\rm substitution instance.}
\end{DoyeNotation}

\begin{DoyeDefinition}
\label{Doye4.6.10}
A {\rm proof in the multi-sorted equational calculus}
of the equation $\forall Y e = f$ from the equational system
$\mathcal{E}$ is a finite set of equations $\forall Y_i e_i=f_i$ such
that each equation is justified by one of the seven following rules of
inference.
\[\begin{array}{cr}
\overline\forall Y\quad e[x_1/t_1,\ldots,x_n/t_n] =
f[x_1/t_1,\ldots,x_n/t_n] & (E)
\end{array}\]
where $\forall X e=f$ is an equation of $\mathcal{E}$, the $t_i$
are terms of the appropriate sort of $T_\Sigma(X)$ and $Y$ is the
$S$-indexed family of sets whose $s$-th component is the set of all
variables of sort $s$ in all the $t_i$ and those variables of $X_s$
which have not been substituted for, i.e. which are not one of the
$x_i$.
\[\begin{array}{cr}
\overline\forall Y\quad e = e & (R)\\
\end{array}\]
where $Y$ is the $S$-indexed family of sets whose $s$-th
component is the set of all variables of sort $s$ in $e$
\[\begin{array}{cr}
\forall Ye = f \quad \overline\forall Y f = e & (S)\\
\forall Y e = f \quad \forall Y^\prime f = g \quad
\overline\forall Y \overset{\rightarrow}{\cup}Y^\prime~ e = g &
(T)\\
\left.
\begin{array}{lr}
\forall Y_1~ e_1=f_1 \ldots \forall Y_n~ e_n=f_n\\
\overline{\forall} Y_1\overset{\rightarrow}{\cup}\ldots
\overset{\rightarrow}{\cup}Y_n~
\sigma(e_1,\ldots,e_n)=\sigma(f_1,\ldots,f_n)\\
\end{array}
\right\} & (C_\sigma)
\end{array}\]
where $\sigma$ is any symbol of $\Sigma_{n,q,s}$, and each $e_i$
is a term of sort $q_i$
\[\begin{array}{lr}
\forall Y~e = f\quad\overline{\forall}Y^\prime~e=f & (A)
\end{array}\]
where $Y\overset{\rightarrow}{\subseteq}Y^\prime$
\[\begin{array}{cr}
\forall Y~e=f\quad\overline{\forall}\langle Y_1,\ldots,Y_{i-1}
\backslash \{y\},Y_{i+1},\ldots,Y_n\rangle\quad e=f & (Q)
\end{array}\]
where $y$ does not occur in $e$ or $f$, and the sort $i$ is non-void
for $\Sigma$.
\end{DoyeDefinition}

We use the notation $\vdash\forall Y~e=f$ (or
$\vdash_\mathcal{E}\forall Y~e=f$ if we wish to make clear which
equational system is being considered) to mean that $e=f$ is provable
in the equational system using the above rules of inference.

\begin{DoyeDefinition}
\label{Doye4.6.11}
If $\forall Y~e=f$ is an $S$-sorted 
$\Sigma$-equation (call it E), and $\langle A,\alpha\rangle$ is a
$\Sigma$-algebra, then we say that $\langle A,\alpha\rangle$ satisfies
E, or that $\langle A,\alpha\rangle$ is a model for E, if for all
$S$-sorted maps $\theta$ from $Y$ to $A$, $\theta^*(e)=_A\theta^*(f)$,
where $\theta^*$ is the map from
$T_\Sigma(X\overset{\rightarrow}{\cup}A)$ to
$\langle A,\alpha\rangle$, whose existence is guaranteed by the
theorem \ref{Doye4.4.7}. We extend the notation to sets of equations
$\mathcal{E}$ by insisting that $\langle A,\alpha\rangle$ be a model
for each equation in $\mathcal{E}$.
\end{DoyeDefinition}

\begin{DoyeTheorem}
\label{Doye4.6.12}
{\bf (The Soundness Theorem)} If
$\langle A,\alpha\rangle$ is a model for $\mathcal{E}$, and
$\vdash_\mathcal{E} \forall Y~e=f$, then $\langle A,\alpha\rangle$
is a model for $\mathcal{E}\cup\{\forall Y~e=f\}$.
\end{DoyeTheorem}

The proof of the soundness theorem may be found in \cite{Gogu82}.

\begin{DoyeDefinition}
\label{Doye4.6.13}
Let $\langle A,\alpha\rangle$ be a
$\Sigma$-algebra, and let $\mathcal{E}$ be an equational system for
$\langle A,\alpha\rangle$. The congruence induced by $\mathcal{E}$,
denoted by $\equiv_\mathcal{E}$ on $\langle A,\alpha\rangle$ is
defined by $A\equiv_\mathcal{E} B$ if, and only if,
$\vdash_\mathcal{E} \forall Y~a=b$, where $Y$ is the $S$-sorted family
of empty sets.
\end{DoyeDefinition}

The following two definitions are very important. A theory specifies a
type, and we define a variety to be the collection of all types which
model that theory.

\begin{DoyeDefinition}
\label{Doye4.6.14}
We define a theory to be the ordered pair
$\langle\langle\Sigma, S\rangle,S\rangle$ where $\Sigma$ is an $S$-sorted
signature and $S$ is a set of $S$-sorted $\Sigma$-equations. We say
that a $\Sigma$-algebra {\rm models} or {\rm satisfies} the theory iff
it is a model for $S$.
\end{DoyeDefinition}

\begin{DoyeDefinition}
\label{Doye4.6.15}
The collection of all models of a particular theory
is called a variety
\end{DoyeDefinition}

Clearly, since a signature $\langle\Sigma,S\rangle$ may be viewed as a
theory $(\langle\Sigma,S\rangle,\emptyset)$, the collection of all
$\Sigma$-algebras forms a variety.

When referring to the variety of all models of a particular
signature, (for example, $\langle\langle\Sigma,S\rangle,S\rangle$) we
usually say the ``variety defined by (or specified by) the signature
$\langle\langle\Sigma,S\rangle,S\rangle$.''

\begin{DoyeTheorem}
\label{Doye4.6.16}
{\bf (Second universality theorem)} Let $X$ be an
$S$-sorted set of variables, $\Sigma$ an $S$-sorted operator set,
$\mathcal{E}$ a set of equations for $\Sigma$,
$\langle A,\alpha\rangle$ a $\Sigma$-algebra which is a model for
$\mathcal{E}$, and $\theta$ an $S$-sorted mapping from $X$ to $A$. Then
there is a unique $\Sigma$-homomorphism $\theta^{**}$ from
$T_\Sigma(X)/\equiv_\mathcal{E}$ to $A$ such that
\[\begin{array}{cr}
\theta^{**}(\iota^*(x))=\theta(x) & (**)
\end{array}\]
for all $x\in X$, where $\iota^*$ is the map from $X$ into
$T_\Sigma(X)/\equiv_\mathcal{E}$ defined by $x\mapsto[x]$.
\end{DoyeTheorem}

\begin{DoyeTheorem}
\label{Doye4.6.17}
{\bf (The Completeness Theorem}) If every
$\Sigma$-algebra which satisfies the equation $\mathcal{E}$ also
satisfies the equation $\forall Y~e=f$, then 
$\vdash_\mathcal{E}\forall Y~e=f$.
\end{DoyeTheorem}

Again, the proof of this may be found in \cite{Gogu82}

Finally, we state the definition of an extension and a protecting
extension. Extensions are self-explanatory.

\begin{DoyeDefinition}
\label{Doye4.6.18}
Suppose that $\Sigma$ and
$\Sigma\overset{\rightarrow}{\cup}T$ are signatures where $\Sigma$ is
$S$-sorted and $\Sigma\overset{\rightarrow}{\cup}T$ is
$S^\prime$-sorted where $S\subseteq S^\prime$. Then
$\Sigma\overset{\rightarrow}{\cup}T$ is said to be an extension of
$\Sigma$.
\end{DoyeDefinition}

A protecting extension is an extension which preserves the equational
system for the theory. Combining the second universality theorem
in section \ref{Doye4.6.16} with
the definition of protecting extension allows us to view a model of a
theory as a model of a protecting extension of that theory.

More importantly, if we have an algebra which is a model for a
protecting extension of a theory, then we may view the algebra as a
model of that theory (unextended).

For example, if the Ring theory is defined to be a protecting
extension of the Group theory, then we may view any ring
(Ring-algebra) as a group (Group-algebra).

\begin{DoyeDefinition}
\label{Doye4.6.19}
With $\Sigma$, $T$, $S$, and $S^\prime$ as in
\ref{Doye4.6.18}, Let $\mathcal{E}$ be an equational system on
$\Sigma$. Also let $\mathcal{E}\cup\mathcal{F}$ be an equational
system on $\Sigma\overset{\rightarrow}{\cup}T$. Such an extension is
called a protecting extension if
\[T_{\Sigma\overset{\rightarrow}{\cup}T}(X)/
\equiv_{\mathcal{E}\cup\mathcal{F}}\vert_\Sigma\]
is isomorphic to $T_\Sigma(X)/\equiv_\mathcal{E}$ (isomorphism meaning
``isomorphism as $\Sigma$-algebras'').
\end{DoyeDefinition}

Notice that the definition of protecting extension is equivalent to
the notion of {\sl enrichment} given in \cite{Pada80} (although, this
does not allow for the existence of $S^\prime$). Thus a theory
$\Omega_1$ is a protecting extension of another $\Omega_0$ iff
$\Omega_1$ is complete and consistent with respect to $\Omega_0$.

\subsection{Signatures, theories, varieties, and Axiom}
\label{DoyeSec4.7}

Axiom's type system uses the terminology from category theory, yet its
design is based on order sorted signatures.

A {\bf Category} definition in Axiom (i.e. the source code that
defines the {\bf Category}) is equivalent to a signature or theory,
being a specification of a type or types.

The {\bf Category} viewed as a collection of objects ({\bf Domains})
is the variety specified by the theory which defined the 
{\bf Category} in the {\bf Category} definition.

The sorts are {\bf \%} for the (principal) sort (see definition
\ref{Doye7.2.1}), and the argument and return types of all the operator
symbols of the {\bf Category}\footnote{Unless they are concrete types
-- {\bf Domains}. See section \ref{DoyeSec9.6}}.

For example, {\bf PolynomialCategory(R,E,V)} is a {\bf Category}.
{\bf \%} is the (principal) sort. Other sorts include {\bf R}, 
{\bf E}, {\bf V} a sort each for the {\bf Boolean} and
{\bf PositiveInteger} types.

The equations of a theory in an Axiom {\bf Category} definition are
either defined in the comments or a certain ``attributes'' of
operators. For example {\bf commutative(*)} means that {\bf *} is
commutative and the traditional commutativity equation (for the
operator {\bf *}) is an equation of the theory. (There is currently no
method for enforcing the equations to hold in any model.)

An algebra (or model for a theory) in Axiom is a 
{\bf Domain}. Declaring a {\bf Domain} to be in a {\bf Category} is
equivalent to saying that it is an algebra of that signature (or model
of that theory). Or in other words, a member of the variety defined
by the theory which specifies the {\bf Category}.

For example, {\bf Polynomial(Integer)} is a model for
{\bf PolynomialCategory(R,E,V)}.

An operator symbol in Axiom is a function declaration in a
{\bf Category}. An operator name in Axiom always corresponds to the
operator symbol.

For example, $+$ is an operator symbol in {\bf Ring}. In
{\bf Integer}, a model for {\bf Ring}, the operator name of 
$+$ is (and has to be) $+$.

A carrier is the concrete type substituted for a parameter in any
instantiation of a {\bf Domain}.

In {\bf Polynomial(Integer)}, {\bf Polynomial(Integer)} is the carrier
{\bf \%}. Whereas {\bf Integer}, {\bf NonNegativeInteger} and
{\bf Symbol} are the carriers of {\bf R}, {\bf E}, and {\bf V},
respectively. (No sort is given for the {\bf Boolean} and
{\bf PositiveInteger} types.)

All of Axiom's extensions are protecting extensions. That is, if a
{\bf Category} is declared to extend another then it is always a
protecting extension of that {\bf Category}.

In section \ref{DoyeChap9} we will discuss how Axiom differs from order
sorted algebra and how we may remedy this situation.

\subsection{Conclusion}
\label{DoyeSec4.8}

In this section we have introduced all the basics of order sorted
algebra and the equational calculus. We have also shown how these
notions are represented in Axiom.

We have demonstrated that order sorted algebra combined with the
equational calculus provides a sound basis for a computer algebraic
type system.

A multi-sorted signature is a specification for a type and hence is an
abstract datatype.

Adding an order on the sorts to obtain an order sorted signature
enforces relations between the sorts. It also guarantees sensible
interaction between these carriers of these sorts and operators thereon
in any algebra of the signature.

Combining signatures with the equational calculus yields
theories. Theories further enhance the usefulness of signatures by
ensuring certain equations will hold in any model (algebra) of the
theory (signature).

\section{Extending order sorted algebra}
\label{DoyeChap5}

We will discuss how partial functions and conditional signatures may
``tie-in'' with traditional order sorted algebra. This will mean that
any facts that we may prove for or use from traditional order sorted
algebra will also apply when partial functions and/or conditional
signatures are considered, provided certain extra facts hold.

Next, we will look at the similarities between order sorted algebra
and category theory. This will demonstrate why computer algebra
systems such as Axiom use category theory terminology, whereas most of
this work uses order sorted algebra as its framework.

Last, we define what we mean by a coercion. This definition is
fundamental to the rest of this work.

\subsection{Partial Functions}
\label{DoyeSec5.2}

This subsection is based on \cite{Broy88} after a suggestion by
Richardson and Martin.

We ask the question, ``How do partial functions interact with
classical universal algebra?''. We need this in case some of the
functions used to create our coercions later are only partial. This
can happen (see definition \ref{Doye7.3.3}). For example, the division
operator in any quotient field is partial, but could be viewed as a
constructor function.

In our previous section on universal algebra, we defined an
$S$-operator of arity $q$, to be a (total) function from some $A^q$ to
some element of $A$. (Where $A$ is a set of $S$-carriers.) Is it
possible to redefine this so that the $S$-operators are partial?
Indeed, is this necessary? One of the original reasons for the
``invention'' of order-sorted algebra (definition \ref{Doye4.4.4}) (rather
than multi-sorted algebra (see definition \ref{Doye4.2.9})) was so that all
functions could be considered total. See, for example, \cite{Gogu92}.

For example, if $\alpha_{\sigma_{1,(P),T}} : A_P\rightarrow A_T$ were
a partial function in a $\Sigma$-algebra, $\langle A,\alpha\rangle$,
then we could insert a new sort $N\prec P$, such that
$\alpha_{\sigma_{1,(N),T}} : A_N\rightarrow A_T$ were a total
function.

In the example of a quotient field, one would introduce a subsort of
the integral domain which would represent all the non-zero elements of
the domain.

We may proceed by either attempting to redefine all of universal
algebra using partial functions, or through some different
mechanism. The following mechanism which uses ``virtual sorts'' turns
out to be flawed. Virtual sorts are an on-the-fly way of generating
sorts to represent things like the non-zero elements of an integral
domain or non-empty stacks.

\begin{DoyeDefinition}
\label{Doye5.2.1}
Let $\langle\Sigma, S\rangle$ define an
order-sorted signature. If we define $\langle\Lambda,L\rangle$ to be
the signature where
\begin{enumerate}
\item $L=S\cup\{u_1,\ldots,u_m\}$ (a set of symbols distinct from all
those in $S$)
\item $(\forall i \in \{1,\ldots,m\})(\exists s_{u_i}\in S)
(u_i\prec_L s_{u_i})$
\item $(\forall n\in \mathbb{N}\cup\{0\})(\forall s \in S)
(\forall q\in S^n)(\Lambda_{n,q,s}=\Sigma_{n,q,s})$
\item $(\exists n\in\mathbb{N})(\exists q\in L^n \backslash S^n)
(\exists s\in S)(\Lambda_{n,q,s}\ne\emptyset)$ (Note that there may be
more than one such triple $\langle n,q,s\rangle$)
\end{enumerate}
then we say that the $u_i$ are {\rm virtual sorts} of
$\langle\Sigma, S\rangle$ and\\
$\lambda_{n,q,s}\in\Lambda_{n,q,s}(n\in\mathbb{N})
(q\in L^n \backslash S^n)(s\in S)$ a virtual operator symbol of
$\langle\Sigma,S\rangle$.
\end{DoyeDefinition}

Unfortunately, virtual sorts, virtual operators, and homomorphism do
not interact in a satisfactory manner. The introductions of a virtual
sort in the source of a homomorphism may be meaningless in the target
or vice versa.

For traditional examples, like the non-empty stack, they are fine since
this has some meaning in all stack-algebras. But for types like ``all
the elements which do not map to 5 under a particular coercion,'' then
it may be meaningless to create a virtual sort which attempts to
represent this.

Broy\cite{Broy88} defined $\Sigma$-algebras as having either partial or
total operators. A (partial) $\Sigma$-homomorphism is then defined as
follows:

\begin{DoyeDefinition}
\label{Doye5.2.2}
{\bf ((Partial) homomorphism)} If
$\langle A,\alpha\rangle$ and $\langle B,\beta\rangle$ are
$\langle \Sigma,S\rangle$ algebras\footnote{with partial operators, in
our terminology} and $\psi$ is a family of partial maps
$\psi_s : A_s\rightarrow B_s$, the $\psi$ is a (partial)
$\Sigma$-homomorpsism 
$\langle A,\alpha\rangle\rightarrow\langle B,\beta\rangle$ iff the
following two conditions are fulfilled:

Firstly,
\[(\forall n\in\mathbb{N}\cup\{0\})(\forall q{\rm\ arities\ of\ rank\ }n)
(\forall s\in S)(\forall\sigma\in\Sigma_{n,q,s})\]
\[(\forall(a_1,\ldots,a_n)\in A^q)\]
{\rm if both}
$\psi_s(\alpha_{\sigma_{n,q,s}}(a_1,\ldots,a_n))$ and
$\beta_{\sigma_{n,q,s}}(\psi_{q_1}(a_1),\ldots,\psi_{q_n}(a_n))$
{\rm are defined, then}
\[\psi_s(\alpha_{\sigma_{n,q,s}}(a_1,\ldots,a_n)) =
\beta_{\sigma_{n,q,s}}(\psi_{q_1}(a_1),\ldots,\psi_{q_n}(a_n))\]

Secondly.
\[(\forall n\in\mathbb{N}\cup\{0\})(\forall q{\rm\ arities\ of\ rank\
}n)(\forall s\in S)(\forall\sigma\in\Sigma_{n,q,s})\]
\[((\forall a_1,a^\prime_1\in A_{q_1}),\ldots,
(\forall a_n,a^\prime_n\in A_{q_n})\]
\[(\bigwedge_{i\in\{1,\ldots,n\}}(\phi_q(a_i)=_{\rm strong}
\phi_{q_i}(a^\prime_i))))\]
\[\Rightarrow\phi_s(\alpha_{\sigma_{n,q,s}}(a_1,\ldots,a_n))
=_{\rm strong}\phi_s(\alpha_{\sigma_{n,q,s}}
(a^\prime_1,\ldots,a^\prime_n))\]

Where ``$=_{\rm strong}$'' is the strong equality defined via
\[(a=_{\rm strong})\Leftrightarrow((a{\rm\ defined}\Leftrightarrow
b{\rm\ defined})\land(a{\rm\ defined}\Rightarrow a=b))\]
\end{DoyeDefinition}

Broy's definition of homomorphism (definition \ref{Doye5.2.2}) is (as he
states) rather liberal. A special factor which he notes is that the
composition of partial homomorphisms need not be a homomorphism again.

The following two definitions turn out to be useful in distinguishing
between types of partial homomorphism.

\begin{DoyeDefinition}
\label{Doye5.2.3}
Let $\langle \Sigma^\prime,S^\prime\rangle$ be a sub-signature of
$\langle \Sigma,S\rangle$. A $\Sigma^\prime$-algebra
$\langle A,\alpha\rangle$ is said to be a $\Sigma^\prime$-subalgebra
of the $\Sigma$-algebra $\langle B,\beta\rangle$ iff
\begin{enumerate}
\item $(\forall s\in S^\prime)(A_s=B_s)$
\item $(\forall n\in\mathbb{N}\cup\{0\})
(\forall q{\rm\ arities\ of\ rank\ }n)(\forall s\in S)
(\forall\sigma\in\Sigma_{n,q,s})
(\alpha_\sigma=\beta_\sigma\vert_{A^q})$
\end{enumerate}
\end{DoyeDefinition}

\begin{DoyeDefinition}
\label{Doye5.2.4}
Let $\langle \Sigma^\prime,S^\prime\rangle$ be a sub-signature of
$\langle \Sigma,S\rangle$. A $\Sigma^\prime$-algebra
$\langle A,\alpha\rangle$ is said to be a weak
$\Sigma^\prime$-subalgebra of the $\Sigma$-algebra
$\langle B,\beta\rangle$ iff
\begin{enumerate}
\item $(\forall s\in S^\prime)(A_s=B_s)$
\item $(\forall n\in\mathbb{N}\cup\{0\})
(\forall q{\rm\ arities\ of\ rank\ }n)(\forall s\in S)
(\forall\sigma\in\Sigma_{n,q,s})(\alpha_\sigma 
\le_\bot \beta_\sigma\vert_{A^q})$
\end{enumerate}
where ``$\le_\bot$'' is defined via
\[f\le_\bot g\Leftrightarrow(\forall x)((f(x){\rm\ is\ not\ defined\ }
\lor (f(x)=g(x)))\]
\end{DoyeDefinition}

Broy notes that any partial homomorphism may be decomposed using the
following methodology. A partial homomorphism $\phi : A\rightarrow B$
defines a weak subalgebra $A^\prime$ of $A$ defined via
\[(\forall s\in S)(A^\prime_s=\{a\in A_s : \psi(a){\rm\ defined}\})\]
with the functions taking the meanings
\[\alpha^\prime_{\sigma_{n,q,s}}(a_1,\ldots,a_n)=
\alpha_{\sigma_{n,q,s}}(a_1,\ldots,a_n)\]
if $(\forall i\in\{1,\ldots,n\})$($\phi(a_i)$ are defined) and
$\phi(\alpha^\prime_{\sigma_{n,q,s}}(a_1,\ldots,a_n))$ is defined.
Otherwise $\alpha^\prime_{\sigma_{n,q,s}}(a_1,\ldots,a_n)$ is not
defined.

By $\phi^\prime$ we denote the weak partial identity function
$A\rightarrow A^\prime$. Then we define
$\tilde{\phi} : A^\prime\rightarrow B$ to be the total homomorphism
which is the restriction of $\phi$ to $A^\prime$.

Being total, $\tilde{\phi}$ induces a weak subalgebra $B^\prime$ of
$B$ defined via:
\[(\forall s \in S)(B^\prime_s = 
\{b\in B_s : (\exists a \in A_s)(\tilde{\psi}(a)=b)\})\]
with the functions defined via
\[\beta^\prime_{\sigma_{n,q,s}}(a_1,\ldots,a_n)=
\tilde{\phi}(\alpha_{\sigma_{n,q,s}}(a_1,\ldots,a_n))\]
where $b_i=\tilde{\phi}(a_i)$ and
$\beta_{\sigma_{n,q,s}}(a_1,\ldots,a_n)$ is defined.

This then induces a total surjective homomorphism
$\phi^n : A^\prime\rightarrow B^\prime$. Also, by construction
$B^\prime$ is a weak subalgebra of $B$. Therefore there also exists a
total injective homomorphism $\phi^m : B^\prime\rightarrow B$ which is
the natural inclusion operator.

As examples consider the following coercions. (We are only looking at
the carrier of the principal sort (definition \ref{Doye7.2.1}) in
these examples.

\begin{DoyeExample}
\label{Doye5.2.5}
$\mathbb{Q}\rightarrow\mathbb{Z}_5$

{\bf Fraction Integer $\rightarrow$ PrimeField 5}
\[A^\prime = \mathbb{Q} \backslash \{q\frac{1}{5}~\vert~
q\in\mathbb{Q} \backslash \{0\}\}\]
\[B^\prime = B\]
\end{DoyeExample}

Notice that in this example, we are considering a
field homomorphism, and that $A^\prime$ is a weak subfield of $A$.
(For example $/$ is undefined on the pair (1,5) in $A^\prime$.)
Clearly, $B^\prime$ is a subfield of $B$.

\begin{DoyeExample}
\label{Doye5.2.6}
$\mathbb{Q}[x]\rightarrow\mathbb{Z}(x)=\mathbb{Q}(x)$

{\bf Polynomial Fraction Integer $\rightarrow$
Fraction Polynomial Integer}
\[A^\prime = A\]
\[B^\prime=mathbb{Q}[x]=\{p~\vert~p\in\mathbb{Q}(x)\land
(\exists q\in\mathbb{Q}(x))(p=q\land{\rm\ denom}(q)\in\mathbb{Q})\}\]
\end{DoyeExample}

In this example, it is an integral domain homomorphism that concerns
us. So although $B^\prime$ is only a weak subfield of $B$ this does
not matter, as it is a sub-integral domain of $B$.

\begin{DoyeExample}
\label{Doye5.2.7}

{\bf Polynomial Fraction Integer $\rightarrow$
Fraction Polynomial PrimeField 5}

\[A^\prime = \mathbb{Q}[x] \backslash
\{\Sigma_{i\in\mathbb{N}\cup\{0\}}(q_ix^i)\in\mathbb{Q}[x]~\vert~
(\exists i\in\mathbb{N}\cup\{0\})
(s\in\mathbb{Q} \backslash \{0\})
(q_i=s\frac{1}{5})\}\]
\[B^\prime=\mathbb{Z}_5[x]=
\{p~\vert~p\in\mathbb{Z}_5(x)\land(\exists q\in\mathbb{Z}_5(x))
(p=q \land{\rm\ denom}(q)\in\mathbb{Z}_5)\}\]
\end{DoyeExample}

In this example we are again considering an integral domain
homomorphism, and both $A^\prime$ and $B^\prime$ are sub-integral
domains of $A$ and $B$, respectively.

These show that real examples of coercions may indeed cause $A$ and
$A^\prime$ to differ as well as $B$ and $B^\prime$.

As Broy notes, this definition of partial homomorphism allows the
everywhere undefined function to be a partial homomorphism. Thus this
definition is too weak for our purposes.

\begin{DoyeDefinition}
\label{Doye5.2.8}
A partial homomorphism, $\phi$ is called strict iff
\[(\forall n\in\mathbb{N}\cup\{0\})
(\forall q{\rm\ arities\ of\ rank\ }n)(\forall s \in S)
(\forall \sigma\in\Sigma_{n,q,s})
(\forall(a_1,\ldots,a_n)\in A^q)\]
\[\phi(\alpha_\sigma(a_1,\ldots,a_n))=_{\rm strong}
\beta_\sigma(\phi(a_1),\ldots,\phi(a_n))\]
\end{DoyeDefinition}

Broy notes that a strict homomorphism ensures that $B^\prime$ is a
$\Sigma$-subalgebra of $B$ (and not just a weak $\Sigma$-subalgebra).
$\phi^n$ is always strict, and if $\phi$ is strict, then $B^\prime$ is
indeed a $\Sigma$-subalgebra of $B$.

Our requirements for homomorphism are that it act strictly on a
certain set of partial functions (the constructors, definition
\ref{Doye7.3.3}). In other words, we will require that our coercion be
a strict partial $\Sigma^C$-homomorphism (again, see definition
\ref{Doye7.3.3})

For the rest, we will not in general concern ourselves with the
partiality of homomorphisms, operators, or strictness. Since, provided
our homomorphisms act strictly on the constructors of the type (and
any type recursively required for construction (definition
\ref{Doye7.6.5})) then the strong equality in the definition of
strictness puts us in good shape.

\subsection{Conditional varieties}
\label{DoyeSec5.3}

Axiom contains the notion of ``conditional {\bf Categories}'' or in
the language of universal algebra, ``conditional varieties''. In this
section we define and reconcile the notions of conditional and
non-conditional\footnote{Unconditional may have been a better choice
of word from an English language point of view. However, we are not
saying that an algebra is unconditionally an algebra of a variety; we
are saying that it is an algebra of a variety which has no conditions}
varieties. This means that in future sections we will be able to
ignore the existence of conditional varieties in Axiom.

Notice that our choice of the word conditional here is similar to
that used in the phrase ``conditional algebraic specifications''. The
difference being that our conditions are predicates evaluated over
arities not terms. Ours is a higher order notion and should not be
confused with the ordinary lower order work.

We extend the definition of conditional signature, via the following 
definition:

\begin{DoyeDefinition}
\label{Doye5.3.1}
A $S$-sorted {\rm conditional signature} is a set $C\Sigma$ of
sets $C\Sigma_{n,q,s}$ indexed by $n\in\mathbb{N}\cup\{0\}$, $q$ an
$S$-arity of rank $n$, and $s\in S$. Each element of $C\Sigma$ is of
the form
\[{\rm if\ }P(w){\rm\ then\ }\sigma_{n,q,s}\]
{where $P$ is a well-formed proposition in some language, and $w$ is an
arity of finite rank (if it is a tautology, it is always represented
by $T$). The $\sigma_{n,q,s}$ are the} conditional operator symbols.
\end{DoyeDefinition}

Firstly, we need to define a conditional term algebra.

\begin{DoyeDefinition}
\label{Doye5.3.2}
Let $S$ be an $S$-indexed family of sets disjoint from each
other, from the set $\Delta$, the set \{if,P,then\} and from\\
\[\{\sigma~\vert~(\exists {\rm\ arity\ }w)
(``if\ P(w)\ then\ \sigma''\in\bigcup_{n,q,s}\Sigma_{n,q,s})\}\]
We define $T_{C\Sigma}(X)$ to be the $S$-indexed family of sets of
strings of symbols from 
$\{if,P,then\}\cup\bigcup_{s\in S}X_s\cup\Delta\cup\bigcup_{n,q,s}
\Sigma_{n,q,s}$ each set as small as possible satisfying these
conditions:
\begin{enumerate}
\item $(\forall s\in S)(\Sigma_{0,(),s}\subseteq T_{C\Sigma}(X)_s)$
\item $(\forall s\in S)(X_s\subseteq T_{C\Sigma}(X)_s)$
\item $(\forall ``if\ P(w)\ then\ \sigma''\in C\Sigma_{n,q,s})
(\forall i\in\{1,\ldots,n\})
(\forall t_i\in T_{C\Sigma}(X)_{q_i})\\
(``if\ P(w)\ then\ \sigma'' (t_1,\ldots,t_n)\in T_{C\Sigma}(X)_s)$
\end{enumerate}

We make $T_{C\Sigma}(X)$ into a conditional $C\Sigma$-algebra by
defining conditional operators ``$if\ P(w)\ then\ \sigma_T$'' on
$T_{C\Sigma}(X)$, for each ``$if\ P(w)\ then\ \sigma''\in 
C\Sigma_{n,q,s}$ via:
\begin{itemize}
\item if $n=0$ then ``$~if\ P(w)\ then\ \sigma_T$'':=``$~if\ P(w)\ then\
\sigma$''. (Guaranteed to be in $T_{C\Sigma}(X)_s$ by (1)).
\item Else, define ``$~if\ P(w)\ then\ \sigma_T(t_1,\ldots,t_n)$''
to be the string\\
``$~if\ P(w)\ then\ \sigma(t_1,\ldots,t_n)$''
\end{itemize}
$T_{C\Sigma}(X)$ is called the {\rm conditional term algebra}, and an
element of $T_{C\Sigma}(X)$ is called a {\rm conditional term}.
\end{DoyeDefinition}

Associated with the notion of conditional signature are the notions of
conditional theory and conditional variety. However, we require a new
definition for equations and equational systems.

\begin{DoyeDefinition}
\label{Doye5.3.3}
A {\rm conditional (multi-sorted) equation} for the conditional
signature $C\Sigma$ consists of a quadruple $(P(w),Y,t_1,t_2)$, where
$P$ is a well-formed proposition in some language, and $w$ is an arity
of finite rank, (if it is a tautology, it is always represented by
$T$) where $Y\overset{\rightarrow}{\subseteq}X$, $t_1$ and $t_2$ are
terms from the same carrier set of $T_{C\Sigma}(X)$ (or
$T_{C\Sigma}(X\overset{\rightarrow}{\cup}A)$ if we are dealing with
equations in a particular algebra $\langle A,\alpha\rangle$), and
every variable occurring in $t_1$ and $t_2$ occurs in the appropriate
member of $~Y:t_1,t_2\in T_{C\Sigma}(Y)$. Such equations are more
usually written $({\rm if\ }P(w))(\forall Y~t_1=t_2)$. If $t_1$ and
$t_2$ come from the same carrier set of $T_{C\Sigma}(X)$ we say that
the equation is $C\Sigma$-generic.
\end{DoyeDefinition}

Note that this definition of conditional equation should not be
confused with that found in work such as \cite{Koun90}. In that context,
a conditional equation is a clause such as
$({\rm if\ }P(t))(\forall Y~t1=t2)$ where $t$ is a subterm of $t_1$,
for example.

\begin{DoyeDefinition}
\label{Doye5.3.4}
A {\rm conditional equation system} for the signature $\Sigma$ is
a set of conditional equations for $T_{C\Sigma}(X)$
(or $T_{C\Sigma}(X)\overset{\rightarrow}{\cup}A$ if we are dealing with
equations in a particular algebra $\langle A,\alpha\rangle$).
\end{DoyeDefinition}

Associated with definition \ref{Doye5.3.1} are two special signatures
$\Sigma$ and $\tilde{\Sigma}$. The former is the extension of all the
$C\Sigma$s; the latter is extended by all $C\Sigma$s. More formally,

\begin{DoyeDefinition}
\label{Doye5.3.5}
Using the definitions of definition \ref{5.3.1}, we define
$\Sigma$ to be the $S$-sorted signature where $\forall n,q,s
(n\in\mathbb{N}\cup\{0\}$, $q$ an $S$-arity of rank $n$, and $s\in S$)
\[\Sigma_{n,q,s}=\{\sigma_{n,q,s}~\vert~``~if\ P(w)\ then\
\sigma_{n,q,s}\ '' \in C\Sigma_{n,q,s}\}\]
Now similarly, we define $\tilde{\Sigma}$ to be the
$\tilde{S}$-sorted signature where\\
$\forall n,q,s (n\in\mathbb{N}\cup\{0\}$, 
$q$ an $\tilde{S}$-arity rank $n$,
and $s\in\tilde{S}$)
\[\tilde{\Sigma}_{n,q,s}=\{\sigma_{n,q,s}~\vert~
``~if\ T\ then\ \sigma_{n,q,s}\ ''\in C\Sigma_{n,q,s}\}\]
where $\tilde{S}$ is the largest subset of $S$ such that
$\tilde{\Sigma}$ has no void sorts
\end{DoyeDefinition}

The algebras of conditional signatures are the same as ordinary
algebras, except that the proposition $P$ is evaluated over $A^w$, and
iff this is true, there is an operator $\alpha_{n,q,s,\sigma}$ in that
algebra. More formally,

\begin{DoyeDefinition}
\label{Doye5.3.6}
An {\rm order-sorted, total, conditional $C\Sigma$-algebra} is an
ordered triple $\langle A,\{A_s:s\in S\},\alpha\rangle$, where $A$ is
a class known as the universe, $\{A_s:s\in S\}$ is an $S$-indexed
family of subsets of $A$, known as the {\rm carriers} of the algebra,
and $\alpha$ is a set of sets of functions
$\alpha_{n,q,s}=\bigcup_{\sigma\in\Sigma_{n,q,s}}
\{\alpha_{n,q,s,\sigma}:A^q\rightarrow A_s~\vert~P(A^w)\}$, such that
\begin{enumerate}
\item $A_u=A$
\item If $s\preceq s^\prime\in S$, then $A_s\subseteq A_{s^\prime}$
\item If ``$~if\ P(w)\ then\ \sigma''\in C\Sigma_{n,q,s}$\\
and ``$~if\ P^\prime(w^\prime)\ then\ \sigma'' \in
C\Sigma{n,q^\prime,s^\prime}$, with $s\preceq s^\prime$\\
and 
$q^\prime\preceq q$, and also $P(A^w)$ and $P^\prime(A^{w^\prime})$\\
then
$\alpha_{n,q,s,\sigma}~\vert_{A^{q^\prime}}=
\alpha_{n,q^\prime,s^\prime,\sigma}$
\end{enumerate}
\end{DoyeDefinition}

Clearly, if all the $P$ are tautologies then a conditional signature
$C\Sigma$ is identical to its non-conditional partner,
$\Sigma$. Therefore all conditional $C\Sigma$-algebras are
(non-conditional) $\Sigma$ algebras in this case.

In any case a $C\Sigma$-algebra is a $\tilde{\Sigma}$-algebra. Thus we
can see that $\tilde{\Sigma}$ is a minimal\footnote{Minimal in the
sense that it has fewest operator symbols and sorts. The variety it
specifies is maximal, if you consider the number of algebras that
model it.} signature for $C\Sigma$.

\begin{DoyeDefinition}
\label{Doye5.3.7}
We define a {\rm conditional theory} to be the ordered pair\\
$\langle\langle C\Sigma,S\rangle,S\rangle$ where $C\Sigma$ is an
$S$-sorted conditional signature and $S$ is a set of $S$-sorted
conditional $\Sigma$-equations. We say that a $C\Sigma$-algebra
$\langle A,\alpha\rangle$ {\rm models} or {\rm satisfies} the
conditional theory iff it is a model for 
$S~\vert_{\langle A,\alpha\rangle}$, where
$S~\vert_{\langle A,\alpha\rangle}$ is defined as follows,
\[S~\vert_{\langle A,\alpha\rangle}:=
\{(Y,t_1,t_2)~\vert~((P(w),Y,t_1,t_2)\in S)\land P(A^w)\}\]
\end{DoyeDefinition}

A {\rm theoretical interpretation} of a conditional theory
$\langle\langle C\Sigma,S\rangle,S\rangle$ is a theory
$\langle\langle\Sigma_\tau,S\rangle,S_\tau\rangle$ equivalent to
$\langle\langle C\Sigma,S\rangle,S\rangle$ with all the propositions
$P(w)$ evaluated in some consistent manner.

Finally we demand that for all possible pairs of theoretical
interpretations
$\langle\langle\Sigma_0,S\rangle,S_0\rangle$,
$\langle\langle\Sigma_1,S\rangle,S_1\rangle$ where
$\Sigma_0\overset{\rightarrow}{\subseteq}\Sigma_1$ and
$S_0\subseteq S_1$ that $\langle\langle\Sigma_1,S\rangle,S_1\rangle$
is a protecting extension of
$\langle\langle\Sigma_0,S\rangle,S_0\rangle$.

\begin{DoyeDefinition}
\label{Doye5.3.8}
The collection of all models of a particular conditional theory
is called a {\rm conditional variety}.
\end{DoyeDefinition}

In fact the conditional variety defined by 
$\langle\langle C\Sigma,S\rangle,S\rangle$ is equivalent to the
variety defined by 
$\langle\langle \tilde{\Sigma},\tilde{S}\rangle,S\rangle$.

Also, the variety specified by any theoretical interpretation of the
conditional theory $\langle\langle C\Sigma,S\rangle,S\rangle$ forms a
subclass of the conditional variety. Moreover, for a pair of
theoretical interpretations
$\langle\langle\Sigma_0,S\rangle,S_0\rangle$,
$\langle\langle\Sigma_1,S\rangle,S_1\rangle$ where
$\Sigma_0\overset{\rightarrow}{\subseteq}\Sigma_1$ and
$S_0\subseteq S_1$, the variety specified by
$\langle\langle\Sigma_1,S\rangle,S_1\rangle$ forms a subclass of the
variety specified by
$\langle\langle\Sigma_0,S\rangle,S_0\rangle$.

If we consider two conditional algebras from the same conditional
variety, then we wish to know whether there is a homomorphism from one
to the other. For this we need to define ``conditional
homomorphism''.

A technical definition which will allow us to define homomorphisms
more easily.

\begin{DoyeDefinition}
\label{Doye5.3.9}
Let $\langle A,\alpha\rangle$, and $\langle B,\beta\rangle$ be
$C\Sigma$-algebras. For all $n,q,s$, we define
\[\mu_{\alpha,n,q,s}:\alpha_{n,q,s}\rightarrow\Sigma_{n,q,s}\]
to be the map,
\[\alpha_{n,q,s,\sigma}\mapsto\sigma_{n,q,s}\]
Similarly, define $\mu_{\beta,n,q,s}$ for
$\langle B,\beta\rangle$.
\end{DoyeDefinition}

The family of maps, $\mu_\alpha$ (as we show in notation
\ref{Doye5.3.11} map an algebra, $\langle A,\alpha\rangle$ to a
non-conditional signature which it models.

\begin{DoyeDefinition}
\label{Doye5.3.10}
$\phi:\langle A,\alpha\rangle\rightarrow\langle B,\beta\rangle$
is a {\rm conditional $C\Sigma$-homomorphism} iff it is a
non-conditional $\Sigma^{AB}$-homomorphism, where
\[\left(\begin{array}{rcl}
\Sigma^{AB} & = &\{
\ \{\mu_{\alpha,n,q,s}(\alpha_{n,q,s,\sigma})~\vert~
\alpha_{n,q,s,\sigma}\in\alpha_{n,q,s}\}\cap\\
&&\quad\{\mu_{\beta,n,q,s}(\beta_{n,q,s,\sigma})~\vert~
\beta_{n,q,s,\sigma}\in\beta_{n,q,s}\}~\vert~\\
&&n\in\mathbb{N}\cup\{0\},q{\rm\ \tilde{S}-arity\ of\ rank\ }n,
s\in\tilde{S}\}\end{array}\right)\]
$\Sigma^{AB}$ is a $\tilde{S}$-sorted 
signature where $\tilde{S}$ is
the largest subset of $S$ such that $\Sigma^{AB}$ has no void sorts.
\end{DoyeDefinition}

In fact, if we use the following piece of notation.

\begin{DoyeNotation}
\label{Doye5.3.11}
If $\langle A,\alpha\rangle$ is a conditional $C\Sigma$-algebra,
we define $\Sigma^{NC(A)}$ to be the non-conditional signature
\[\begin{array}{l}
\{\{\mu_{\alpha,n,q,s}(\alpha_{n,q,s,\sigma})~\vert~
\sigma_{n,q,s,\sigma}\in\alpha_{n,q,s}\}~\vert~\\
\quad n\in\mathbb{N}\cup\{0\},q{\rm\ an\ \hat{S}-arity\ of\ rank\ }n,
s\in\hat{S}\}\end{array}\]

$\Sigma^{NC(A)}$ is a $\hat{S}$-sorted signature where $\hat{S}$
is the largest subset of $S$ such that $\Sigma^{NC(A)}$ has no void
sorts.
\end{DoyeNotation}

As an aside, notice that for each $C\Sigma$-algebra 
$\langle A,\alpha\rangle$, there is an associated non-conditional term
algebra $T_{\Sigma^{NC(A)}}(X)$. This term algebra is the equivalent of
evaluating each $P(w)$ over $A$ in $T_{C\Sigma}(X)$.

Then $\langle A,\alpha\rangle$ is a non-conditional
$\Sigma^{NC(A)}$-algebra, and
\[\Sigma^{AB}=\{\Sigma^{NC(A)}_{n,q,s}\cap\Sigma^{NC(B)}_{n,q,s}
~\vert~n\in\mathbb{N}\cup\{0\},q{\rm\ an\ \breve{S}-arity\ of\ rank\ }n,
s\in\breve{S}\}\]

So, for any two conditional algebras of the same conditional
signature, there exists a fixed (maximal) non-conditional signature of
which they are both non-conditional algebras. (We may have to forget
some operators and even some sorts, which may only exist in
conditions).

Our definition of conditional homomorphism is that of the
non-conditional homomorphism from this fixed non-conditional
signature.

Hence, the concept of conditional signatures can always be reduced to
one for non-conditional signatures and therefore any results we prove
or use need only be for non-conditional signatures.

As we have seen in definition \ref{Doye5.3.7} this concept may also be
extended to theories. In addition to adding new operators when certain
propositions hold, we also allow for new equations to be added
(conditionally). However, we demand that any such extension is a
protecting extension of the minimal non-conditional theory (the theory
over the signature $\langle \overline{\Sigma},\overline{S}\rangle$).

More explicitly, let us define $\hat{S}$ to be all those equations
from $S$ which only involve arity from $\Sigma^{NC(A)}$ (as well as
all equations not involving arities)\footnote{Alternatively, define
$\hat{S}$ to be those equations from $S$ for which
$\langle A,\alpha\rangle$ is a model, since it may not model all the
equations in the definition of $\hat{S}$ in the main text.} Similarly,
define $\overline{S}$ to be all those equations from $S$ which only
involve arities from $\overline{\Sigma}$ (as well as all equations not
involving arities).

If $\langle A,\alpha\rangle$ is a conditional $C\Sigma$-model then
$\langle\langle\Sigma^{NC(A)},\hat{S}\rangle\hat{S}\rangle$ must be a
protecting extension of 
$\langle\langle\overline{\Sigma},\overline{S}\rangle,\overline{S}\rangle$.

\subsection{A Category theory approach}
\label{DoyeSec5.4}

This section is an aside from the rest of the section. We are not
extending the theory of order sorted algebra nor category theory,
merely explaining why we cover both.

There is a great deal of correspondence between category theory
(specifically, categorical type theory) and universal algebra. There
is not enough room here to cover this huge topic, but the reader is
pointed to \cite{Crol93}, which covers categoriecal type theory in
great depth.

As usual, Mac Lane \cite{Macl91} also contains a great deal of
information on the correspondence between universal algebra and
adjoint functors in the section on ``Monads and Algebras''.

All the ideas from multi-sorted algebra theory may be represented in
the categorical type theory framework, although an equivalent for
order-sorted algebra appears not to be covered. However, the author
believes that the notion could be introduced.

One area which we will discuss here is the concept of algebraic
homomorphism and the arrows (morphisms) of a given category.

The correspondence between a categorical type theory $C$ and its
associated algebraic type theory
$\langle\langle\Sigma,S\rangle,S\rangle$ is the following
\[{\rm Obj}(C) = {\rm\ all\ the\ } \Sigma{\rm -algebras\ for\ a
\ given\ \Sigma\ which\ model\ } S.\]
\[{\rm Arr}(C)={\rm\ all\ (or\ a\ fixed\ sub-collection\ of)\ the
\ \Sigma-homomorphisms}.\]

The category of all algebras which model a given theory 
$\langle\langle\Sigma,S\rangle,S\rangle$ has as an initial object the
free algebra $T_\Sigma(X)/\equiv_\mathcal{E}$. (This is why the second
universality theorem \ref{Doye4.6.16} holds).

So we see that both categories and theories collect together similar
types. This abstraction of information forms the basis of abstract
datatyping.

\subsection{Coercion}
\label{DoyeSec5.5}

In this work we are interested in coercions which so far have been
explained as being natural, type-changing maps. We may define them in
a far more strict fashion.

The following would seem to be a good definition for
coercion. However, in practice this definition is not well-defined
enough since it does not state which theory a coercion should come
from.

\begin{DoyeDefinition}\label{Doye5.5.1}
Let $\langle A,\alpha\rangle$ and $\langle B,\beta\rangle$ be
algebras from the variety specified by some theory
$\langle\langle \Sigma,S\rangle,S\rangle$. The map
$\phi:\langle A,\alpha\rangle\rightarrow\langle B,\beta\rangle$ is a
coercion if it is a homomorphism of that theory.
\end{DoyeDefinition}

Using the category theory correspondence above, we see that a function
between two types of the category of all algebras which model a
particular theory is a coercion iff it is an arrow of that category.

The only problem with this definition of coercion is that it does not
specify from which theory (or equivalently, category)  we
should demand the homomorphism be taken. In general, we would like
this to be the most specific, or smallest category to which both
algebras belong.

If we just used the definition of coercion above (definition
\ref{Doye5.5.1}) then any total map between types would be a coercion!
Any small category may be forgotten back to {\bf Set} via the
forgetful functor and any total function is an arrow of 
{\bf Set}. Most types in a computer algebra system are objects of 
{\bf Set} (which is equivalent to {\bf SetCategory} in Axiom - the
second most basic {\bf Category} in Axiom).

Richardson notes that we also require a fixed framework in
which we define our coercions. If we were to attempt to define a
coercion $\langle A,\alpha\rangle$ to $\langle B,\beta\rangle$ to be
``any map which is a homomorphism for all theories which both algebras
model'' then the definition would not be well-defined. We need to
state a context.

We are attempting to reflect the situation that appears in a system
like Axiom. Thus we must state that we have been given
{\sl a priori} a fixed collection of theories, and that given any
type, we know precisely which theories it models.

This precludes a user from adding another theory which the algebras
model which could redefine what ``coercion'' means for those algebras.

In the category theory correspondence, we say that Axiom's
{\bf Category} is a fixed collection of categories, and given any type
we know of which categories it is an object.

\begin{DoyeDefinition}{\bf (Coercion)}
\label{Doye5.5.2}
Let $\Upsilon$ be a fixed collection of theories.

Let $\langle A,\alpha \rangle$ be a model for theories $\Theta_i$ (for
$i$ in some indexing set $I$) where $(\forall i\in I)$($\Theta_i$ is a
theory from $\Upsilon$).

Similarly, let $\langle B,\beta\rangle$ be a model for theories
$\Omega_j$ (for $j$ in some indexing set $J$) where
$(\forall j \in J)$($\Omega_j$ is a theory from $\Upsilon$).

Then we call a map $\langle A,\alpha\rangle$ to 
$\langle B,\beta\rangle$ a {\rm coercion} iff it is a homomorphism for
all the theories in
\[\{\Theta_i~\vert~i\in I\}\cap\{\Omega_j~\vert~j\in J\}\]
\end{DoyeDefinition}

In Axiom, {\bf Domains} are only members of a fixed set of
{\bf Categories}.\footnote{A user could re-implement one of these
{\bf Categories} and ruin everything.}

In fact, in Axiom we are in a slightly better position. If a
{\bf Domain} is an object of two {\bf Categories} then there must
exist a {\bf Category} which extends (potentially trivially) both of
these {\bf Categories} of which the {\bf Domain} is an object.

Thus, in Axiom, definition \ref{Doye5.5.2} reduces to,
\begin{quote}
``A coercion in Axiom from a type {\bf A} to a
 type {\bf B} is a homomorphism of the most restrictive
 {\bf Category} to which both {\bf A} and {\bf B} belong''
\end{quote}

It would be useful to have a name for maps which are ``coercions'' in
the sense of definition \ref{Doye5.5.1}. These maps are in a sense
natural, and may be the next-most natural map between two
types. However, as we have shown in the example using {\bf Set} the
map need not be particularly ``natural'' from a realistic point of view.

As it stands, with our current terminology maps which satisfy
definition \ref{Doye5.5.1} which are not coercions are merely
``homomorphisms which are not coercions''.

If no coercion existed between two types, but a ``homomorphism which
is not a coercion'' existed and a user required that homomorphism then
either the user only required a conversion or the theory lattice for
the algebra system has not been designed correctly.

\subsection{Conclusion}
\label{DoySec5.6}

In this section we have shown how classical order sorted algebra may
be extended to encompass the notion of partial functions and
conditional signatures. We have also stated how these notions interact
with the equational calculus. These are important extensions due to
the fact that these notions are used extensively in Axiom.

We have also mentioned some of the correlation between category theory
and universal algebra. Finally we have made the important definition
of a coercion and demonstrated why the definition is necessarily strict.

\section{Coherence}
\label{DoyeChap6}

We will look at a conjecture by Weber which is important to our
work. We will state his assumptions and proof (which is incorrect).

We will then add in some extra assumptions and truly prove the
theorem. Finally we will relax one of Weber's assumptions and prove
that the theorem still holds.

The assumptions made by Weber provide a strict formal setting for
types in an algebra system. The theorem in itself proves confluence
for coercions in this setting.

All the work in this section (except the explanation of ``$n$-ary type
constructor'') and the next are from Weber 
\cite{Webe93b} and \cite{Webe95}. All the rest is original work.

\subsection{Weber's work I: definitions}
\label{DoyeSec6.2}

This section contains the definitions from Weber's thesis
\cite{Webe93b} and \cite{Webe95} required for the statement and
Weber's ``proof' of Weber's coercion conjecture \ref{Doye6.3.7}.
His assumptions are in the next section.

These statements will also be used when we correctly prove the
coherence theorem \ref{Doye6.4.7} in section \ref{DoyeSec6.4}

It should be noted that Weber's coercion conjecture only makes up part
of one section of his thesis \cite{Webe93b} which also covers various
areas of type classes, type inference, and coercion in great depth and
detail.

Weber uses the phrase ``type class'' where we would use the terms
``category'' or ``variety''.

\begin{DoyeDefinition}
\label{Doye621}
A {\rm base type} is any type which is not a parametrically
defined type. (i.e. a 0-ary operator in the term algebra of type
classes) 
\end{DoyeDefinition}

So for example, the {\bf Integer} and {\bf Boolean} types are base
types.

\begin{DoyeDefinition}
\label{Doye6.2.2}
A {\rm ground type} is any type within the system which is either
a base type or a parametrically defined type with all the paramters
present. Any non-ground type is called a {\rm polymorphic type}.
\end{DoyeDefinition}

As examples, we have {\bf Integer} and 
{\bf Polynomial(Fraction(Integer))}. As a non-example, we have
{\bf Polynomial(R:Ring)}.

\begin{DoyeDefinition}
\label{Doye6.2.3}
If there exists a coercion from $t$ to $t^\prime$ we say that
$t\trianglelefteq t^\prime$.
\end{DoyeDefinition}

This definition places an order on the ground types.

Weber uses the phrase ``n-ary type constructor'' to mean a functor
from the product of $n$ categories to a specific category.

Equivalently, it is a function from the product of $n$ varieties
(specified respectively by the theories $\Omega_1,\ldots,\Omega_n$) to
a variety (specified by the theory
$\langle\langle\Sigma,S \rangle,S \rangle$. This is a function which,
for all $i$ maps the carrier of the principle sort (and potentially
the carriers of some of the non-principal sorts) of a model of
$\Omega_i$ to one (for each sort mapped) of the non-principal sorts of
a model of $\langle\langle\Sigma,S \rangle,S\rangle$.

If the model returned as $\langle A,\alpha\rangle$, this function must
map one and only one sort-carrier to each and every member of
$A \backslash \{A_{S_1}\}$ where $S_1$ is the principal sort.

\begin{DoyeDefinition}
\label{Doye6.2.4}
For a ground type $t$ we define {\rm com}($t$) to be 1, if $t$ is a
base type or if $t=f(t_1,\ldots,t_n)$ (an $n$-ary type constructor)
then {\rm com}($t$) is defined to be
$1+{\rm max}(\{{\rm com}(t_i)~\vert~\{1,\ldots,n\}\})$.
\end{DoyeDefinition}

This defined the ``complexity'' of a ground type to be how far up the
type lattice it is.

\begin{DoyeDefinition} {\bf (Coherence)}
\label{Doye6.2.5}
A type system is {\rm coherent} if the following condition is
satisfied.
\[(\forall ground~types~t_1,t_2)
((\phi,\psi:t_1\rightarrow t_2~coercions)\Rightarrow(\phi=\psi))\]
\end{DoyeDefinition}

This guarantees that there only exist one coercion from one ground
type to another. This is a highly desirable feature of any type
system. The main results of this section (the coherence theorem
\ref{Doye6.4.7} and the extended coherence theorem \ref{Doye6.5.4})
are that we may be able to guarantee that our type system is coherent
providing some sensible assumptions hold true.

In the following definitions, all the $\sigma$ and $\sigma^\prime$
are type classes.

\begin{DoyeNotation}
\label{Doye6.2.6}
$t:\sigma$ means that the type $t$ is an object of the type class
$\sigma$.
\end{DoyeNotation}

\begin{DoyeDefinition}
\label{Doye6.2.7}
The $n$-ary type constructor $f$ $(n\in\mathbb{N})$ induces a
{\rm structural coercion} if there are sets
$\mathcal{A}_f\subseteq\{1,\ldots,n\}$ and
$\mathcal{M}_f\subseteq\{1,\ldots,n\}$ such that the following
condition is satisfied:
\begin{quote}
If $f:(\sigma_1,\ldots,\sigma_n)\rightarrow\sigma$ and
$f:(\sigma^\prime_1,\ldots,\sigma^\prime_n)\rightarrow\sigma^\prime$,
and $(\forall i\in\{1,\ldots,n\})(\forall~ground~types~
t_i:\sigma_i$ and $t^\prime_i:\sigma^\prime_i)
(i\notin\mathcal{A}_f\cup\mathcal{M}_f\Rightarrow t_i=t^\prime_i)$ and
there exist coercions:
\[\begin{array}{rcl}
\phi_i:t_i\rightarrow t^\prime_i & {\rm if} & i\in\mathcal{M}_f\\
\phi_i:t^\prime_i\rightarrow t_i & {\rm if} & i\in\mathcal{A}_f\\
\phi_i={\rm id}_{t_i}={\rm id}_{t^\prime_i}  & {\rm if} &
i \neq \mathcal{A}_f\cup\mathcal{M}_f
\end{array}\]
then there exists a {\bf uniquely defined} coercion
\[\mathcal{F}_f(t_1,\ldots,t_n,t^\prime_1,\ldots,t^\prime_n,
\phi_1,\ldots,\phi_n):f(t_1,\ldots,t_n)\rightarrow
f(t^\prime_1,\ldots,t^\prime_n)\]
\end{quote}

The type constructor $f$ is {\rm covariant} (or {\rm monotonic})
in its $i$-th argument if\\
$i\in\mathcal{M}_f$. $f$ is
{\rm contravariant} or ({\rm antimonotonic}) in its $i$-th argument
if $i\in\mathcal{A}_f$
\end{DoyeDefinition}

Note that if $i\in\mathcal{A}_f\cap\mathcal{M}_f$ then
$t_i\cong t^\prime_i$.

As an example of covariance, the list constructor in Axiom,
{\bf List} (a functor ${\bf Set}\rightarrow{\bf ListAggregate()})$
takes one argument in which it is covarient. Given types {\bf A} and
{\bf B}, such that there exists a coercion 
$\phi_1:{\bf A}\rightarrow{\bf B}$ then
\[\mathcal{F}_{\rm List}({\bf A},{\bf B},\phi_1):
{\bf List(A)}\rightarrow{\bf List(B)}\]

Since Axiom's type constructors are functors, then category theory
states this more simply as
\[\mathcal{F}_{\rm List}({\bf A},{\bf B},\phi_1)={\bf List}(\phi_1)\]

Contravariance is a rarer case. However, Axiom's {\bf Mapping} functor
is contravariant in its first argument and covariant in its second.
{\bf Mapping} takes two types {\bf A} and {\bf B} and returns the type
of all mappings from {\bf A} to {\bf B}.

As a concrete example for {\bf Mapping}, suppose we wish to find the
uniquely defined coercion.
\[\begin{array}{c}
{\bf Mapping(Fraction(Integer),Fraction(Integer))}\rightarrow\\
{\bf Mapping(Integer,Fraction(Integer))}
\end{array}\]

There exists a coercion
\[\iota:{\bf Integer}\rightarrow{\bf Fraction(Integer)}\]

the inclusion operator. There also exists a coercion
\[{\rm id}:{\bf Fraction(Integer)}\rightarrow{\bf Fraction(Integer)}\]
which is the identity operation. The uniquely defined coercion is as
follows:

\[\begin{array}{c}
\mathcal{F}_{\rm Mapping}({\bf Fraction(Integer)}),
{\bf Fraction(Integer)},{\bf Integer},{\bf Fraction(Integer)},
\iota,{\rm id}):\\
{\bf Mapping(Fraction(Integer),Fraction(Integer))}\rightarrow\\
{\bf Mapping(Integer,Fraction(Integer))}
\end{array}\]
which sends $f\mapsto{\rm id}\circ f\circ\iota$.

The following definitions shows that there is a homomorphism image of
a parameter in the created type. For example there is a homomorphic
image of the underlying ring in any polynomial ring.

\begin{DoyeDefinition}
\label{Doye6.2.8}
Let $f:(\sigma_1,\ldots,\sigma_n)\rightarrow\sigma$ be an $n$-ary
type constructor.\\
If $(\forall i \in\{1,\ldots,n\})(for~some~ground
~types~t_i:\sigma)$ such that there exists a coercion
\[\Phi^i_{f,t_1,\ldots,t_n}:t_i\rightarrow f(t_1,\ldots,t_n)\]
then we say that $f$ has a {\rm direct embedding} at its $i$-th
position.

Moreover, let
\[\mathcal{D}_f=\{i~\vert~f~has~a~direct~embedding~at~its~i-th~
position\}\]
be the {\rm set of direct embedding positions} of $f$
\end{DoyeDefinition}

The definition of $\mathcal{D}_f$ is a technical definition of
Weber's, needed for one of his assumptions (\ref{Doye6.3.5}).

\subsection{Weber's work II: Assumptions and a conjecture}
\label{DoyeSec6.3}

This section provides all the assumptions and results which Weber used
in his ``proof'' of Weber's coherence conjecture \ref{Doye6.3.7} which
will will state and at the end of this section. The assumptions and
results are also require in our proof of the coherence theorem
\ref{Doye6.4.7}.

\begin{DoyeAssumption}
\label{Doye6.3.1}
For any ground type $t$, the identity on $t$ will be a
coercion. The (well-defined) composition of two coercions is also a
coercion
\end{DoyeAssumption}

This is clearly a sensible (if not-often implemented) statement. Since
our coercions are always arrows of a category the above
assumption must hold.

\begin{DoyeLemma}
\label{Doye6.3.2}
If assumption \ref{Doye6.3.1} holds, then the set of ground types as
objects together with their coercions as arrows form a category.
\end{DoyeLemma}

{\bf Proof}. Immediate.

The following assumption will provide us with the basis for a coherent
type system. Our coherence is built by ensuring confluence amongst
different paths leading to the same coercion. If we do not have
coherence at the base types then we will not have coherence amonst the
general types.

\begin{DoyeAssumption}
\label{Doye6.3.3}
The subcategory of base types and coercions between base types
forms a preorder, i.e. if $t_1$, $t_2$ are base types and
$\phi,\psi:t_1\rightarrow t_2$ are coercions, then $\phi=\psi$.
\end{DoyeAssumption}

The following condition states that $\mathcal{F}_f$ is a functor over
the category of all $f(\cdot,\ldots,\cdot)$s.

\begin{DoyeAssumption}
\label{Doye6.3.4}
Let $f$ be an $n$-ary type constructor which induces a structural
coercion and let $f(t_1,\ldots,t_n)$,
$f(t^\prime_1,\ldots,t^\prime_n)$,
$f(t^{\prime\prime}_1,\ldots,t^{\prime\prime}_n)$ be ground types.
Assume that the following are coercions.

\[\begin{array}{c}
\iota\in\mathcal{M}_f\Rightarrow\phi_i:t_i\rightarrow t^\prime_i,
\phi^\prime_i: t^\prime_i\rightarrow t^{\prime\prime}_i\\
\iota\in\mathcal{A}_f\Rightarrow\phi^\prime_i:t^{\prime\prime}_i
\rightarrow t^\prime_i,\phi_i: t^\prime_i\rightarrow t_i\\
\iota\notin \mathcal{A}_f\cup\mathcal{M}_f\Rightarrow
t_i=t^\prime_i=t^{\prime\prime}_i ~and~
\phi_i=\phi^\prime_i={\rm id}_{t_i}
\end{array}\]

Then the following conditions are satisfied:
\begin{enumerate}
\item $\mathcal{F}_f(t_1,\ldots,t_n,t_1,\ldots,t_n,
{\rm id}_{t_1},\dots,{\rm id}_{t_n})={\rm id}_{f(t_1,\ldots,t_n)}$
\item $\mathcal{F}_f(t_1,\ldots,t_n,
t^{\prime\prime}_1,\ldots,t^{\prime\prime}_n,
\phi_1\circ \phi^\prime_1,\ldots,\phi_n\circ \phi^\prime_n)=$\\
$\mathcal{F}_f(t_1,\ldots,t_n,t^\prime_1,\ldots,t^\prime_n,
\phi_1,\ldots,\phi_n)\circ
\mathcal{F}_f(t^\prime_1,\ldots,t^\prime_n,
t^{\prime\prime}_1,\ldots,t^{\prime\prime}_n,
\phi^\prime_1,\ldots,\phi^\prime_n)$
\end{enumerate}
\end{DoyeAssumption}

This is a condition which stops direct embeddings ``becoming
confused''. Firstly, Weber declares that any type constructor can only
have one direct embedding. (We will show how to relax this condition
in a later section (\ref{DoyeSec6.5}).) Secondly, he states that
direct embeddings, where they exist, are unique.

\begin{DoyeAssumption}
\label{Doye6.3.5}
Let $f$ be an $n$-ary type constructor. Then the following
conditions hold:
\begin{enumerate}
\item $\vert\mathcal{D}_f\vert$ = 1
\item Direct embedding coercions are unique, i.e. if~
$\Phi^i_{f,t_1,\ldots,t_n}:t_i\rightarrow f(t_1,\ldots,t_n)$ and
$\Psi^i_{f,t_1,\ldots,t_n} : t_i\rightarrow f(t_1,\ldots,t_n)$ 
then
\[\Phi^i_{f,t_1,\ldots,t_n}=\Psi^i_{f,t_1,\ldots,t_n}\]
\end{enumerate}
\end{DoyeAssumption}

The following assumption is highly technical and shows how direct
embeddings interact with structural coercions. Basically, they
commute.

\begin{DoyeAssumption}
\label{Doye6.3.6}
Let $f$ be an $n$-ary type constructor which induces a structural
coercion and has a direct embedding at its $r$-th position. Assume that\\
$f:(\sigma_1,\ldots,\sigma_n)\rightarrow\sigma$ and
$f:(\sigma^\prime_1,\ldots,\sigma^\prime_n)\rightarrow\sigma$, and\\
$(\forall i\in\{1,\ldots,n\})
(\exists t_i:\sigma_i~and~t^\prime_i:\sigma^\prime_i)$.
If there are coercions $\psi_r:t_r\rightarrow t^\prime_r$, if the
coercions $\Phi^r_{f,t_1,\ldots,t_n}$ and
$\Phi^r_{f,t^\prime_1,\ldots,t^\prime_n}$ are defined, and if $f$ is
covariant in its $r$-th argument, then the following holds:
\[\Phi^r_{f,t^\prime_1,\ldots,t^\prime_n}\circ\psi_r=
\mathcal{F}_f(t_1,\ldots,t_n,t^\prime_1,\ldots,t^\prime_n,
\psi_1,\ldots,\psi_n)\circ\Phi^r_{f,t_1,\ldots,t_n}\]

\[\begin{array}{c@{\hspace{7cm}}c}
\Rnode{N1}{t_r} & \Rnode{N2}{t^\prime_r}\\
&\\
&\\
&\\
&\\
&\\
\Rnode{N3}{f(t_1,\ldots,t_n)} & 
\Rnode{N4}{f(t^\prime_1,\ldots,t^\prime_n)}
\end{array}
\psset{nodesep=0.3cm}
\everypsbox{\scriptstyle}
\ncLine{->}{N1}{N2}
\Aput{\displaystyle\psi_r}
\ncLine{->}{N1}{N3}
\Aput{\displaystyle\Phi^r_{f,t_1,\ldots,t_n}}
\ncLine{->}{N2}{N4}
\Bput{\displaystyle\Phi^r_{f,t^\prime_1,\ldots,t^\prime_n}}
\ncLine{->}{N3}{N4}
\Aput{\displaystyle\mathcal{F}_f(t_1,\dots,t_n,t^\prime_1,\ldots,t^\prime_n,
\psi_1,\ldots,\psi_n)}
\]

However, if $f$ is contravariant in the $r$-th argument then:
\[\mathcal{F}_f(t_1,\ldots,t_n,t^\prime_t,\ldots,t^\prime_n,
\psi_1,\ldots,\psi_n)\circ
\Phi^r_{f,t^\prime_1,\ldots,t^\prime_n}\circ\psi_r=
\Phi^r_{f,t)_1,\ldots,t*n}\]

or equivalently, the following diagram commutes:

\[\begin{array}{c@{\hspace{7cm}}c}
\Rnode{N1}{t_r} & \Rnode{N2}{t^\prime_r}\\
&\\
&\\
&\\
&\\
&\\
\Rnode{N3}{f(t_1,\ldots,t_n)} & 
\Rnode{N4}{f(t^\prime_1,\ldots,t^\prime_n)}
\end{array}
\psset{nodesep=0.3cm}
\everypsbox{\scriptstyle}
\ncLine{->}{N1}{N2}
\Aput{\displaystyle\psi_r}
\ncLine{->}{N1}{N3}
\Aput{\displaystyle\Phi^r_{f,t_1,\ldots,t_n}}
\ncLine{->}{N2}{N4}
\Bput{\displaystyle\Phi^r_{f,t^\prime_1,\ldots,t^\prime_n}}
\ncLine{->}{N4}{N3}
\Bput{\displaystyle\mathcal{F}_f(t_1,\dots,t_n,t^\prime_1,\ldots,t^\prime_n,
\psi_1,\ldots,\psi_n)}
\]
\end{DoyeAssumption}

We are now in a position to state Weber's coherence conjecture and 
his ``proof''. This attempts to show that when the aforementioned
assumptions hold true, then we have a coherent type system. We will
give more assumptions and a proper proof in section
\ref{DoyeSec6.4}.

\begin{DoyeConjecture} {\bf (Weber's coherence conjecture)}
\label{Doye6.3.7}
Assume that all coercions between ground types are only built by
one of the following mechanisms:
\begin{enumerate}
\item coercions between base types
\item coercions induced by structural coercions
\item direct embeddings in a type constructor
\item composition of coercions
\item identity funciton on ground types as coercions
\end{enumerate}

If assumptions 6.3.1, 6.3.3, 6.3.4, 6.3.5, and 6.3.6 are
satisfied, then the set of ground types as objects, and the coercions
between them as arrows form a category which is a preorder.
\end{DoyeConjecture}

This is the ``proof'' of this conjecture given in
\cite{Webe93b} and \cite{Webe95}.

Weber's ``Proof''. By assumption 6.3.1 and lemma 6.3.2, the set of
ground types as objects and the coercions between them form a
category.

For any two ground types $t$ and $t^\prime$ we will prove by induction
on max(com($t$),com($t^\prime$)) that if 
$\phi,\psi:t\rightarrow t^\prime$ are coercions then
$\phi=\psi$.

If max(com($t$),com($t^\prime$))~$= 1$ then the claim follows by
assumption 6.3.3. Now assume that the inductive hypothesis holds for
$k$, and let max(com($t$),com($t^\prime$))$=k+1$. Assume
w.l.o.g. that $t\trianglelefteq t^\prime$ and that
$\phi,\psi:t\rightarrow t^\prime$ are coercions.

Now $t\trianglelefteq t^\prime\Rightarrow{\rm com}(t)\le
{\rm com}(t^\prime)$. So we may assume that
$t^\prime=f(u_1,\ldots,u_n)$ for some $n$-ary type constructor $f$.

By assumption 6.3.4 and the induction hypothesis, we can assume that
there are ground types $s_1,s_2$ and unique coercions
$\psi_1:t\rightarrow s_1$ and $\psi_2:t\rightarrow s_2$ such that
either
\[\phi=\mathcal{F}_f(\ldots,t,\ldots,s_1,\ldots,\psi_1,\ldots)\]
or
\[\phi=\psi_1\circ\Psi^i_{f,\ldots,s_1,\ldots}\]
Similarly either,
\[\psi=\mathcal{F}_f(\ldots,t,\ldots,s_2,\ldots,\psi_2,\ldots)\]
or
\[\psi=\psi_2\circ\Psi^j_{f,\ldots,s_2,\ldots}\]

If $\phi$ is of form 6.1 and $\psi$ is of the form 6.3 then
$\phi=\psi$ by assumption 6.3.4 and the uniqueness of $\mathcal{F}_f$.

If $\phi$ is of form 6.2 and $\psi$ is of form 6.3 then
$\phi=\psi$ by assumption 6.3.6.

Analogously if $\phi$ is of form 6.1 and $\psi$ is of form 6.4.

If $\phi$ is of form 6.2 and $\psi$ is of form 6.4 then assumption
6.3.5 implies that $i=j$ and $s_1=s_2$. Because of the induction
hypothesis we have $\psi_1=\psi_2$ and hence $\phi=\psi$ again by
assumption 6.3.5.\QED

\subsection{The coherence theorem}
\label{DoyeSec6.4}

In the above proof, there seem to be some irregularities.

\subsubsection{On coercibility and complexity}

Weber states in the proof that
\[t\trianglelefteq t^\prime\Rightarrow{\rm com}(t)\le
{\rm com}(t^\prime)\]

Weber does not prove this, and indeed it is not true.

Suppose that $f$ is an $n$-ary type constructor and that it is
contravarient in its $i$-th position. If
\[\mathcal{F}_f(s_1,\ldots,s_n,t_1,\ldots,t_n,
\phi_1,\ldots,\phi_n):f(s_1,\ldots,s_n)\rightarrow
f(t_1,\ldots,t_n)\]

and $\phi_i:t_i\rightarrow s_i$ with
\[{\rm com}(s_i)>{\rm max}({\rm com}(s_1),\ldots,
{\rm com}(s_{i-1}),{\rm com}(s_{i+1}),\ldots,{\rm com}(s_n))\]

In other words, com($s_i$) is the {\sl unique}, maximum member of the
set
\[\{{\rm com}(s_j)~\vert~j\in\{1,\ldots,n\}\}\]

Then if com($t_i$)$<$com($s_i$) and for all $j$ in
$\{1,\ldots,n\}\backslash\{i\}$ we have that $s_j=t_j$, we have a
counterexample to Weber's claim, that
\[{\rm com}(f(s_1,\ldots,s_n))>{\rm com}(f(t_1,\ldots,t_n))\]

Thus Weber's assertion in invalid.

\subsubsection{Structural coercions (in the ``proof'')}

In equation 6.2 (and similarly equation 6.4) $\phi$ is given as a
function
\[f(\ldots,t,\ldots)\rightarrow f(\ldots,s_1,\ldots)\]
however, $\phi$ is supposed to be a function of $t$.

\subsubsection{Structural coercions (syntax)}

Weber calls the structural coercion function from
$f(s_1,\ldots,s_n)$ to $f(t_1,\ldots,t_n)$
\[\mathcal{F}_f(s_1,\ldots,s_n,t_1,\ldots,t_n,
\phi_1,\ldots,\phi_n)\]

where $\phi_i$ is from $s_i$ to $t_i$ or $t_i$ to $s_i$ depending on
whether $f$ is covariant or contravariant in the $i$th argument,
respectively.

However, this is merely the functorial action of $f$ on the maps
$\phi_1,\ldots,\phi_n$ and could be represented more compactly as
\[f(\phi_1,\ldots,\phi_n)\]

The source and target of each $\phi_i$ and knowledge of the sets
$\mathcal{A}_f$ and $\mathcal{M}_f$ uniquely determine the source and
target of $f(\phi_1,\ldots,\phi_n)$. This also demonstrates the
uniqueness of $f(\phi_1,\ldots,\phi_n)$ and guarantees that assumption
6.3.4 holds.

\subsubsection{Identity coercions}

Weber states in assumption 6.3.1 that the identity function is a
coercion. However, he never proves this to be unique. Indeed,
automorphisms are perfectly natural maps $t\rightarrow t$.

In a computer algebra system like Axiom, many automorphisms are not
automorphisms of the smallest category to which a type belongs.

For example, the ring-automorphism 
$\mathbb{Z}[X,Y]\rightarrow\mathbb{Z}[Y,X]$ is not a
{\bf PolynomialCategory} homomorphism since, for instance, the
{\bf leadingMonomial} function is not preserved under the map.

For some categories, like the category of groups, this may not be so
easy to implement.

We add the following sensible assumption.

\begin{DoyeAssumption}
\label{Doye6.4.1}
The only coercion from a type to itself is the identity function.
\end{DoyeAssumption}

We will of course allow any number of functions from a type to itself,
including conversions. It is merely the number of coercions which we
are restricting.

\subsubsection{Composition of coercions}

It is possibly symptomatic of the previous errors that Weber has
neglected to cover all the possible case of $\phi$ and $\psi$.

All our coercions are compositions of coercions (or just a single
coercion) from the list
\begin{enumerate}
\item coercions between base types
\item coercions induced by structural coercions
\item direct embeddings in a type constructor
\item identiy function on ground types as coercions
\end{enumerate}

Suppose that $\phi=\tau_1\circ\phi^\prime$ and that
$\psi=\tau_2\circ\psi^\prime$ where $\tau_1$ and $\tau_2$ are single
coercions, and $\phi^\prime$ and $\psi^\prime$ are also (compositions
of) coercions. $\phi^\prime$ may be the identity coercion, as may be
$\psi^\prime$.

For a proof by induction on com, we need to cover all the cases of
($\tau_1,\tau_2$) pairs. Thus the list we need to consider is
\begin{enumerate}
\item $\tau_1$ is a coercion between base types.
$\tau_2$ is a coercion between base types.
\item $\tau_1$ is a coercion between base types. 
$\tau_2$ is a structual coercion
\item $\tau_1$ is a coercion between base types. 
$\tau_2$ is a direct embedding.
\item $\tau_1$ is a coercion between base types. 
$\tau_2$ is an identity function.
\item $\tau_1$ is a structural coercion. 
$\tau_2$ is a coercion between base types.
\item $\tau_1$ is a structural coercion. 
$\tau_2$ is a structural coercion.
\item $\tau_1$ is a structural coercion. 
$\tau_2$ is a direct embedding.
\item $\tau_1$ is a structural coercion.
$\tau_2$ is the identity function.
\item $\tau_1$ is a direct embedding.
$\tau_2$ is a coercion between base types.
\item $\tau_1$ is a direct embedding.
$\tau_2$ is a structural coercion.
\item $\tau_1$ is a direct embedding.
$\tau_2$ is a direct embedding.
\item $\tau_1$ is a direct embedding.
$\tau_2$ is an identity function.
\item $\tau_1$ is an identity function.
$\tau_2$ is a coercion between base types.
\item $\tau_1$ is an identity function.
$\tau_2$ is a structural coercion.
\item $\tau_1$ is an identity function.
$\tau_2$ is a direct embedding.
\item $\tau_1$ is an identity function.
$\tau_2$ is a identity function.
\end{enumerate}

Firstly notice we are only interested in the pairs as unordered
entities. Thus some of these cases are duplicates.

Indeed: case 5 is case 2; case 9 is case 3; case 10 is case 7; case 13
is case 4; case 14 is case 8; case 15 is case 12. So we may discard
cases 5, 9, 10, 13, 14, and 15.

Now notice that a base type can not have a direct embedding, neither
can it induce a structural coercion. If either $\tau_1$ or $\tau_2$ is
coercion between base types then the target of $\phi$ and $\psi$ is a
base type. Hence the other $\tau_i$ can not be a direct embedding nor
can it be a structural coercion.

All the cases of this form are 2 (equivalently 5) and 3 (equivalently
9). Thus we may ignore these too.

Our remaining cases are 1, 4, 6, 7, 8, 11, 12, and 16.

To really prove the coherence theorem we are going to require some
more assumptions. Only one of them (6.4.5) is difficult to justify.

We will replace this first assumption which uses Weber's definition
6.2.8 of a direct embedding presently with our own definition
6.4.3. In our definition assumption 6.4.2 will always hold
(trivially).

\begin{DoyeAssumption}
\label{Doye6.4.2}
If a type constructor $f$ has a direct embedding at its $i$-th
position and $f(t_1,\ldots,t_n)$ exists then
$\Phi^i_{f,t_1,\ldots,t_n}:t_i\rightarrow f(t_1,\ldots,t_n)$
\end{DoyeAssumption}

Weber defines an $f$ to have a direct embedding (definition 6.2.8) at a
particular position if there exists some $n$-tuple of types
$(t_1,\ldots,t_n)$ for which $t_i$ directly embeds in
$f(t_1,\ldots,t_n)$.

This seems sensible if we consider a type constructor to be a function
which returns the carrier of the principal sort of the algebra. We are
saying that direct embedding of a type in a constructor is equivalent
to saying that a sort $\preceq$ is the principal sort.

It is true that an Axiom type constructor returns the carrier of the
principal sort of the algebra but Axiom is more specific than
that. The type constructor furnished with full complement of arguments 
{\sl is} the principal sort of the algebra.

In fact it would be better to replace definition \ref{Doye6.2.8} and
assumption \ref{Doye6.4.2} with the following definition.

\begin{DoyeDefinition}
\label{Doye6.4.3}
Let $f:(\sigma_1,\ldots,\sigma_n)\rightarrow\sigma$ be an $n$-ary
type constructor. If\\
$(\forall i \in \{1,\ldots,n\})
(\forall~ground~types~t_i:\sigma_i)$ there exists a coercion
\[\Phi^i_{f,t_1,\ldots,t_n}:t_i\rightarrow f(t_1,\ldots,t_n)\]
then we say that $f$ has a {\rm a direct embedding at its $i$-th position}.

Moreover, let
\[\mathcal{D}_f=\{i~\vert~f~has~a~direct~embedding~at~its~i-th~
position\}\]
be the {\rm set of direct embedding positions of $f$}.
\end{DoyeDefinition}

The next assumption is undesirable due to its difficulty to guarantee
in any implementation. However, it is provable in the more common
covariant case (and we will prove it in the proof of the coherence
theorem \ref{Doye6.4.7}).

We are assuming that structural coercions behave confluently. The
assumption below (\ref{Doye6.4.4}) is stated ``too strongly''. We will
state the assumption we really need (\ref{Doye6.4.5} -- which is more
complicated) immediately after. Assumption \ref{Doye6.4.4} gives the
idea of what we require, whereas assumption \ref{Doye6.4.5} gives us
what is necessary. It is an assumption about type constructors.

\begin{DoyeAssumption}
\label{Doye6.4.4}
Let the type constructor $f$ induce a structural coercion. If\\
$\phi:t\rightarrow f(s_1,\ldots,s_n),\psi:t\rightarrow
f(u_1,\ldots,u_n)$ are coercions and
$t^\prime=f(t^\prime_1,\ldots,t^\prime_n)$ then if there exists
\[\mathcal{F}_f(s_1,\ldots,s_n,t^\prime_1,\ldots,t^\prime_n,
\nu_1,\ldots,\nu_n):f(s_1,\ldots,s_n)\rightarrow t^\prime\]
\[\mathcal{F}_f(u_1,\ldots,u_n,t^\prime_1,\ldots,t^\prime_n,
\eta_1,\ldots,\eta_n):f(u_1,\ldots,u_n)\rightarrow t^\prime\]
the following holds
\[\mathcal{F}_f(s_1,\ldots,s_n,t^\prime_1,\ldots,t^\prime_n,
\eta_1,\ldots,\eta_n)\circ\phi=
\mathcal{F}_f(u_1,\ldots,u_n,t^\prime_1,\ldots,t^\prime_n,
\zeta_1,\ldots,\zeta_n)\circ\psi\]

\[\begin{array}{c@{\hspace{7cm}}c}
\Rnode{N1}{t} & \Rnode{N2}{f(s_1,\ldots,s_n)}\\
&\\
&\\
&\\
&\\
&\\
\Rnode{N3}{f(u_1,\ldots,u_n)} & 
\Rnode{N4}{f(t^\prime_1,\ldots,t^\prime_n)}
\end{array}
\psset{nodesep=0.3cm}
\everypsbox{\scriptstyle}
\ncLine{->}{N1}{N2}
\Aput{\displaystyle\phi}
\ncLine{->}{N1}{N3}
\Bput{\displaystyle\psi}
\ncLine{->}{N2}{N4}
\Bput{\displaystyle\mathcal{F}_f(s_1,\dots,s_n,t^\prime_1,\ldots,t^\prime_n,
\eta_1,\ldots,\eta_n)}
\ncLine{->}{N3}{N4}
\Aput{\displaystyle\mathcal{F}_f(u_1,\dots,u_n,t^\prime_1,\ldots,t^\prime_n,
\zeta_1,\ldots,\zeta_n)}
\]
\end{DoyeAssumption}

Now, what we actually require. Suppose that a type $t$ is not
constructed by the $n$-ary type constructor $f$ which is contravariant
in its $i$-th position at which it also has a direct embedding.

Suppose also that $t$ is coercible to two types which may be directly
embedding in $f$ (at the $i$-th position). Let us directly embed them
to gain two new types (constructed by $f$). If these new types
(constructed by $f$) can both be structurally coerced to the same type
$t^\prime$ (also constructed by $f$) then the two compositions of
coercions $t\rightarrow t^\prime$ are the same.

Succinctly, A structural coercion of a direct embedding of any other
coercion is unique.

\begin{DoyeAssumption}
\label{Doye6.4.5}
Let $f$ be a type constructor contravariant at its $i$-th
position at which it also has a direct embedding, and let $t$ be a
type not contructed by $f$.

If $\phi^\prime_a:t\rightarrow a_i$ and
$\psi^\prime_b:t\rightarrow b_i$ are coercions as
$\phi_a:f(a_1,\ldots,a_n)\rightarrow f(t^\prime_1,\ldots,t^\prime_n)$
and
$\psi_a:f(b_1,\ldots,b_n)\rightarrow f(t^\prime_1,\ldots,t^\prime_n)$
are structural coercions then
\[\phi_a\circ\Phi^i_{f,a_1,\ldots,a_n}\circ\phi^\prime_a=
\psi_b\circ\psi^i_{f,b_1,\ldots,b_n}\circ\psi^\prime_b\]
or equivalently, the following diagram commutes

\[\begin{array}{c@{\hspace{1.5cm}}c@{\hspace{1.5cm}}c}
&\Rnode{N1}{t} & \\
&&\\
&&\\
&&\\
&&\\
\Rnode{N2}{a_i}&&\Rnode{N3}{b_i}\\
&&\\
&&\\
&&\\
&&\\
&&\\
\Rnode{N4}{f(a_1,\ldots,a_n)}&&\Rnode{N5}{f(b_1,\ldots,b_n)}\\
&&\\
&&\\
&&\\
&&\\
&\Rnode{N6}{f(t^\prime_1,\ldots,t^\prime_n)} & \\
\end{array}
\psset{nodesep=0.3cm}
\everypsbox{\scriptstyle}
\ncLine{->}{N1}{N2}
\Bput{\displaystyle\phi^\prime_a}
\ncLine{->}{N1}{N3}
\Aput{\displaystyle\psi^\prime_b}
\ncLine{->}{N2}{N4}
\Bput{\displaystyle\Phi^i_{f,a_1,\ldots,a_n}}
\ncLine{->}{N3}{N5}
\Aput{\displaystyle\Phi^i_{f,b_1,\ldots,b_n}}
\ncLine{->}{N4}{N6}
\Bput{\displaystyle\phi_a}
\ncLine{->}{N5}{N6}
\Aput{\displaystyle\psi_b}
\]
\end{DoyeAssumption}

Notice that assumption \ref{Doye6.4.5} is provable in the case that
$f$ is covariant at $i$ (and we will prove this in the proof of the
coherence theorem \ref{Doye6.4.7}). In the contravariant case, there is
no link between $t$ and $t^\prime_i$ and it is this which make it an
assumption.

The following assumption is more easily ensurable. We merely require
that for two types constructed by the same type constructor, if there
exists a coercion between them then it equals the structural coercions
between them which we now assume to exist. This is also an assumption
on type constructors.

\begin{DoyeAssumption}
\label{Doye6.4.6}
If $f(s_1,\ldots,s_n)\trianglelefteq f(t_1,\ldots,t_n)$ then there exists
a structured coercion $f(s_1,\ldots,s_n)$ to $f(t_1,\ldots,t_n)$ and
it is the unique coercion $f(s_1,\ldots,s_n)$ to $f(t_1,\ldots,t_n)$
\end{DoyeAssumption}

We are now finally in a position to state and prove the coercion
theorem.

\begin{DoyeTheorem} {\bf (Coherence theorem)}
\label{Doye6.4.7}
Assume that all coercions between ground types are only built by
one of the following mechanisms:
\begin{enumerate}
\item coercions between base types
\item coercions induced by structural coercions
\item direct embeddings (definition \ref{Doye6.4.3}) in a type
constructor
\item composition of coercions
\item identity function on ground types as coercions
\end{enumerate}
If assumptions \ref{Doye6.3.1}, \ref{Doye6.3.3}, \ref{Doye6.3,4},
\ref{Doye6.3.5}, \ref{Doye6.3.6}, \ref{Doye6.4.1}, \ref{Doye6.4.5},
\ref{Doye6.4.6} are satisfied, then the set of ground types as
objects, and the coercions between them as arrows form a category
which is a preorder.
\end{DoyeTheorem}

PROOF: By assumption \ref{Doye6.3.1} and lemma \ref{Doye6.3.2}, the
set of ground types as objects and coercions between them form a
category.

All our coercions are either an individual coercion or a composition
of finitely many coercions from the list: coercions between base
types; structural coercions; direct embeddings (\ref{Doye6.4.2}) in a
type constructor; and identity functions.

Thus we may decompose any coercion into such a finite composition. We
define len($\phi$) (the {\sl length} of $\phi$) to be the minimum
number of coercions in any decomposition of $\phi$, Clearly, there are
infinitely many values of len($\phi$) but there is a minimum value.

For any two ground types $t$ and $t^\prime$ we will prove by induction
on the length of the coercions between them that any coercions between
them are unique.

Let $\phi,\psi:t\rightarrow t^\prime$ be coercions.

If the length of $\phi$ and $\psi$ is 1 then (replacing the unordered
pair ($\phi,\psi$) with ($\tau_1,\tau_2$)) we need to look at our eight
cases 1, 4, 6, 7, 8, 11, 12 and 16 defined above.

{\bf Cases 1 and 4:} If $\phi$ is a base type coercion, then $t$ and
$t^\prime$ are base types and coercions between base types are unique
by assumption \ref{Doye6.3.3}.

{\bf Case 8, 12, and 16:} If $\phi$ is an identity function on a
ground type then $t=t^\prime$ then by assumption \ref{Doye6.4.1}
$\phi=\psi$.

{\bf Case 6:} If $\phi$ and $\psi$ are structural coercions then
$\phi$ and $\psi$ are equal by assumption \ref{Doye6.3.4}.

{\bf Case 7:} If $\phi$ is a direct embedding and $\psi$ is a
structural coercion then there are two cases.

{\bf Case 7a:} If $\psi$ is covariant. Then by assumption
\ref{Doye6.4.6} $\psi=\phi$.

{\bf Case 7b:} $\psi$ is contravariant. Thus $t=f(t_1,\ldots,t_n)$ and
there exists $i$ and a coercion $t\rightarrow t_i$. This can not
happen since no composition of our four basic coercions can create
such a coercion.

{\bf Case11:} If $\phi$ and $\psi$ are direct embedings then by
assumption \ref{Doye6.3.5} $\psi=\phi$.

We now may assume that any coercions of length less than or equal to
$k$ are unique.

Suppose that $\phi,\psi:t\rightarrow t^\prime$ are coercions and
max(len($\phi$),len($\psi$))$=k+1$. Also suppose that
$\phi=\tau_1\circ\phi^\prime$ and $\psi=\tau_2\circ\psi^\prime$ where
for $i$ in \{1,2\} we have $\tau_i:s_i\rightarrow t^\prime$ are single
atomic coercions. Also $\phi^\prime:t\rightarrow s_1$ and
$\psi^\prime:t\rightarrow s_2$ are unique coercions since their
lengths are less than or equal to $k$.

If the length of $\phi$ or $\psi$ is 1 then we define $\phi^\prime$ or
$\psi^\prime$ respectively to be the identity function on $t$.

As stated before, there are eight cases 1, 4, 6, 7, 8, 11, 12, and
16. (We may transpose $\tau_1$ and $\tau_2$ if we wish since their
order is unimportant.)

{\bf Case 1:} $\tau_1$ is a coercion between base types. $\tau_2$ is a
coercion between base types. Then $t^\prime$, $s_1$, and $s_2$ are all
base types. Base types may only be the targets of base type coercions
hence $t$ is a base type. Thus $\phi$ and $\psi$ are coercions between
base types and are thus equal by assumption \ref{Doye6.3.3}.

{\bf Case 4:} $\tau_1$ is a coercion between base types. $\tau_2$ is
an identity function. Again $t$ and $t^\prime$ must be base types and
$\phi$ and $\psi$ are equal by assumption \ref{Doye6.3.3}.

{\bf Case 6:} $\tau_1$ is a structural coercion. $\tau_2$ is a
structural coercion. Let\\
$t^\prime=f(t^\prime_1,\ldots,t^\prime_n)$

If $t$ is equal to some $f(t_1,\ldots,t_n)$ then $\phi$ and $\psi$ are
structural coercions $f(t_1,\ldots,t_n)$ to
$t^\prime=f(t^\prime_1,\ldots,t^\prime_n)$ and hence are equal by
assumption \ref{Doye6.3.4}.

Else, we may assume that $t\ne f(t_1,\ldots,t_n)$. Since all our
coercions are built from compositions of the four base types of
coercion we may consider $\phi$ and $\psi$ to be chains of
compositions. Without loss of generality, assume that the length of
$\phi$ is $k+1$ and the length of $\psi$ is $l$ where $l\le k+1$.
\[t=h_0\overset{\phi_0}{\rightarrow}h_1\cdots
h_k\overset{\phi_k}{\rightarrow} h_{k+1}=t^\prime\]
and
\[t=s_0\overset{\psi_0}{\rightarrow}s_1\cdots
s_{l-1}\overset{\psi_{l-1}}{\rightarrow} s_l=t^\prime\]

In both the composition chains of coercions from $t$ to $t^\prime$
there must exist a minimal element of the chain which is constructed
by $f$ and this (by assumption) can not be $s_0$ or $h_0$. Also
because $\tau_1$ and $\tau_2$ are structural coercions, these minimal
elements are not $h_{k+1}$ or $s_l$.

In the $\phi$-chain we will assume that this element is $h_a$. In the
$\psi$-chain we will assume that this element is $s_b$.

Since $h_a$ is constructed by $f$ and $h_{a-1}$ is not and
$\phi_{a-1}$ is one of the four basic coercions, it must be a direct
embedding. (It can not be a structural coercion since $h_{a-1}$ is not
constructed by $f$. It is not a coercion between base types because
$h_a$ is constructed by $f$. It is not an identity coercion since
$h_{a-1}$ is not constructed by $f$ whereas $h_a$ is.)

Similarly $\psi_{b-1}$ is also a direct embedding.

Now by assumption \ref{Doye6.3.5} the direct embedding must be at the
same position. We will assume that this position is the $i$-th.

Let $h_a=f(a_1,\ldots,a_n)$ and $s_b=f(b_1,\ldots,b_n)$. Call the
unique structural coercions \\
$\phi_a:h_a\rightarrow t^\prime$ and
$\psi_b:s_b\rightarrow t^\prime$. (Notice that $h_{a-1}=a_i$ and
$s_{b-1}=b_i$.)

Call the maps $\phi^\prime_a:t\rightarrow h_{a-1}$ and
$\psi^\prime_b:t\rightarrow s_{b-1}$ where
\[\phi^\prime_a=\phi_0\circ\cdots\circ\phi_{a-2}\]
and
\[\psi^\prime_a=\psi_0\circ\cdots\circ\psi_{b-2}\]
(If $a$ or $b$ equal 1 then we may insert an initial identity coercion
and lengthen the chain by one. This does not affect our induction on
length since we are always in good shape with identity
coercions. Neither $a$ nor $b$ may be 0 by our assumption on $t$.)

Thus we have the following diagram, and we wish to prove that it
commutes.

\[\begin{array}{c@{\hspace{1.5cm}}c@{\hspace{1.5cm}}c}
&\Rnode{N1}{t} & \\
&&\\
&&\\
&&\\
&&\\
\Rnode{N2}{a_i}&&\Rnode{N3}{b_i}\\
&&\\
&&\\
&&\\
&&\\
&&\\
\Rnode{N4}{h_a=f(a_1,\ldots,a_n)}&&\Rnode{N5}{s_b=f(b_1,\ldots,b_n)}\\
&&\\
&&\\
&&\\
&&\\
&\Rnode{N6}{f(t^\prime_1,\ldots,t^\prime_n)} & \\
\end{array}
\psset{nodesep=0.3cm}
\everypsbox{\scriptstyle}
\ncLine{->}{N1}{N2}
\Bput{\displaystyle\phi^\prime_a}
\ncLine{->}{N1}{N3}
\Aput{\displaystyle\psi^\prime_b}
\ncLine{->}{N2}{N4}
\Bput{\displaystyle\Phi^i_{f,a_1,\ldots,a_n}}
\ncLine{->}{N3}{N5}
\Aput{\displaystyle\Phi^i_{f,b_1,\ldots,b_n}}
\ncLine{->}{N4}{N6}
\Bput{\displaystyle\phi_a}
\ncLine{->}{N5}{N6}
\Aput{\displaystyle\psi_b}
\]

{\bf Case 6a:} $f$ is covariant in the $i$-th position.

Consider the coercions $\alpha:a_i\rightarrow t^\prime_i$ and
$\beta:b_i\rightarrow t^\prime_i$. (Recall that
$t^\prime=f(t^\prime_1,\ldots,t^\prime_n))$. These must exist because
$\phi_a$ and $\psi_b$ lift them covariently respectively.

The definition of direct embeddings \ref{Doye6.4.3} guarantees the
existence of $\Phi^i_{f,t^\prime_1,\ldots,t^\prime_n}$.

Now by the definitions above
\[\phi=\phi_a\circ\Phi^i_{f,a_1,\ldots,a_n}\circ\phi^\prime_a\]
By assumption \ref{Doye6.3.6}
\[\phi_a\circ\Phi^i_{f,a_1,\ldots,a_n}=
\Phi^i_{f,t^\prime_1,\ldots,t^\prime_n}\circ\alpha\]
Thus
\[\phi=\Phi^i_{f,t^\prime_1,\ldots,t^\prime_n}\circ\alpha\circ
\phi^\prime_a\]
By induction on length we know that the coercion $t$ to $t^\prime_i$
is unique. Hence
\[\alpha\circ\phi^\prime_a=\beta\circ\psi^\prime_b\]
So
\[\phi=\Phi^i_{f,t^\prime_1,\ldots,t^\prime_n}\circ\beta\circ
\psi^\prime_b\]
Assumption \ref{Doye6.3.6} gives us
\[\Phi^i_{f,t^\prime_1,\ldots,t^\prime_n}\circ\beta=
\psi_b\circ\Phi^i_{f,b_1,\ldots,b_n}\]
Thus
\[\phi=\psi_b\circ\Phi^i_{f,b_1,\ldots,b_n}\circ\psi^\prime_b\]
which by definition means $\phi=\psi$.

Graphically, the following is a commutative diagram.

\[\begin{array}{c@{\hspace{2cm}}c@{\hspace{2cm}}c}
&\Rnode{N1}{t} & \\
&&\\
&&\\
&&\\
&&\\
\Rnode{N2}{h_{a-1}=a_i}&&\Rnode{N3}{s_{b-1}=b_i}\\
&&\\
&&\\
&&\\
&&\\
&\Rnode{N4}{t^\prime_i}&\\
&&\\
&&\\
&&\\
\Rnode{N5}{h_a=f(a_1,\ldots,a_n)}&&\Rnode{N6}{s_b=f(b_1,\ldots,b_n)}\\
&&\\
&&\\
&&\\
&&\\
&\Rnode{N7}{t^\prime=f(t^\prime_1,\ldots,t^\prime_n)} & \\
\end{array}
\psset{nodesep=0.3cm}
\everypsbox{\scriptstyle}
\ncLine{->}{N1}{N2}
\Bput{\displaystyle\phi^\prime_a}
\ncLine{->}{N1}{N3}
\Aput{\displaystyle\psi^\prime_b}
\ncLine{->}{N2}{N4}
\Aput{\displaystyle \alpha}
\ncLine{->}{N3}{N4}
\Bput{\displaystyle \beta}
\ncLine{->}{N2}{N5}
\Bput{\displaystyle\Phi^i_{f,a_1,\ldots,a_n}}
\ncLine{->}{N3}{N6}
\Aput{\displaystyle\Phi^i_{f,b_1,\ldots,b_n}}
\ncLine{->}{N5}{N7}
\Bput{\displaystyle\phi_a}
\ncLine{->}{N4}{N7}
\Aput{\displaystyle \Phi^i_{f,t^\prime_1,\ldots,t^\prime_n}}
\ncLine{->}{N6}{N7}
\Aput{\displaystyle\psi_b}
\]

{\bf Case 6b:} $f$ is contravariant in its $i$-th position.

There is nothing to link $t$ and $^\prime_i$ in this case and we must
resort to assumption \ref{Doye6.4.5}.

\[\begin{array}{c@{\hspace{0.5cm}}c@{\hspace{0.5cm}}c}
&\Rnode{N1}{t} & \\
&&\\
&&\\
&&\\
&&\\
&\Rnode{N4}{t^\prime_i}&\\
&&\\
&&\\
&&\\
\Rnode{N2}{h_{a-1}=a_i}&&\Rnode{N3}{s_{b-1}=b_i}\\
&&\\
&&\\
&&\\
&&\\
&&\\
\Rnode{N5}{h_a=f(a_1,\ldots,a_n)}&&\Rnode{N6}{s_b=f(b_1,\ldots,b_n)}\\
&&\\
&&\\
&&\\
&&\\
&\Rnode{N7}{t^\prime=f(t^\prime_1,\ldots,t^\prime_n)} & \\
\end{array}
\psset{nodesep=0.3cm}
\everypsbox{\scriptstyle}
\ncLine{->}{N1}{N2}
\Bput{\displaystyle\phi^\prime_a}
\ncLine{->}{N1}{N3}
\Aput{\displaystyle\psi^\prime_b}
\ncLine{->}{N4}{N2}
\ncLine{->}{N4}{N3}
\ncLine{->}{N2}{N5}
\Bput{\displaystyle\Phi^i_{f,a_1,\ldots,a_n}}
\ncLine{->}{N3}{N6}
\Aput{\displaystyle\Phi^i_{f,b_1,\ldots,b_n}}
\ncLine{->}{N5}{N7}
\Bput{\displaystyle\phi_a}
\ncLine{->}{N6}{N7}
\Aput{\displaystyle\psi_b}
\]

{\bf Case 7:} $\tau_1$ is a direct embedding. $\tau_2$ is a structural
coercion.

{\bf Case 7a:} $\tau_2$ is covariant. Then $\phi=\psi$ by case 6a
above.

{\bf Case 7b:} $\tau_2$ is contravarient. Let
$s_2=f(s_{2_1},\ldots,s_{2_n})$ and
$t^\prime=f(t^\prime_1,\ldots,t^\prime_n)$. If
$\kappa:t^\prime_i\rightarrow s_{2_i}$, is the coercion lifted
(contravariantly) by $\tau_2$ where
\[\tau_2=\mathcal{F}_f(\ldots,s_2,\ldots,t^\prime_i,\ldots,\kappa,\ldots)\]
Also
\[\tau_1=\Phi^i_{f,t^\prime_1,\ldots,\tau^\prime_n}\]
The uniqueness of $\tau^\prime$ implies that
\[\phi^\prime=\Phi^i_{f,s_{2_1},\ldots,s_{2_n}}\circ\kappa\circ\phi^\prime\]
By assumption \ref{Doye6.3.6} we have
\[\mathcal{F}_f(\ldots,s_2,\ldots,t^\prime_i,\ldots,\kappa,\ldots)\circ
\Phi^i_{f,s_{2_1},\ldots,s_{2_n}}\circ\kappa=
\Phi^i_{f,t^\prime_1,\ldots,s^\prime_n}\]
Thus
\[\mathcal{F}_f(\ldots,s_2,\ldots,t^\prime_i,\ldots,\kappa,\ldots)\circ
\Phi^i_{f,s_{2_1},\ldots,s_{2_n}}\circ\kappa=
\Phi^i_{f,t^\prime_1,\ldots,s^\prime_n}\circ\psi^\prime\]
and therefore
\[\mathcal{F}_f(\ldots,s_2,\ldots,t^\prime_i,\ldots,\kappa,\ldots)\circ
\phi^\prime =
\Phi^i_{f,t^\prime_1,\ldots,s^\prime_n}\circ\psi^\prime\]
Hence, $\phi=\psi$.

{\bf Case 8:} $\tau_1$ is a structural coercion. $\tau_2$ is an
identity function. Let
$t^\prime=f(t^\prime_1,\ldots,t^\prime_n)$.
By viewing $\tau_2$ as the identity function
\[\mathcal{F}_f(t^\prime_1,\ldots,t^\prime_n,
t^\prime_1,\ldots,t^\prime_n,{\rm id}_{t^\prime_1},\ldots,
{\rm id}_{t^\prime_n})\]
Thus $\phi=\psi$ by case 6.

{\bf Case 11:} $\tau_1$ is a direct embedding. $\tau_2$ is a direct
embedding. Assumption \ref{Doye6.3.5} implies that the direct
embeddings must be at the same position, $i$. Therefore $s_1=s_2$. So
by the inductive hypothesis $\phi^\prime=\psi^\prime$. By assumption
\ref{Doye6.3.5} $\tau_1=\tau_2$. Hence $\phi=\psi$.

{\bf Case 12:} $\tau_1$ is a direct embedding. $\tau_2$ is an identity
function. Let\\
$t^\prime=f(t^\prime_1,\ldots,t^\prime_n)$. By viewing $\tau_2$ as the
identity structural coercion
\[\mathcal{F}_f(t^\prime_1,\ldots,t^\prime_n,
t^\prime_1,\ldots,t^\prime_n,
{\rm id}_{t^\prime_1},\ldots,{\rm id}_{t^\prime_n})\]
Thus $\phi=\psi$ by case 6, above.

{\bf Case 16:} $\tau_1$ is an identity function. $\tau_2$ is an
identity function. Thus $\phi=\phi^\prime$ and
$\psi=\psi^\prime$. However, since $s_1=s_2$,
$\phi^\prime=\psi^\prime$ by the induction hypothesis. Hence
$\phi=\psi$.

Thus we have proved $\phi=\psi$ for all coercions of length less than
or equal to $k+1$. Hence the result holds for all coercions, by
induction. \QED

In case 10b of the case when the length is 1 we claimed that there can
not be built a coercion from a type constructor to one of its
arguments.

From our description so far of Axiom, one might think that Axiom would
permit the construction of the following type
\[{\bf Fraction(Fraction(Integer))}\]
which is the quotient field of the quotient field of 
integers\footnote{The quotient field of the integers ($\mathbb{Z}$) is
more commonly known as the rationals ($\mathbb{Q}$).} However, the
quotient field of a quotient field is itself and hence,
\[{\bf Fraction(Fraction(Integer))\cong Fraction(Integer)}\]

For this reason, Axiom contains special code in the interpreter to
stop such pathological types being instantiated. A function
({\bf isValidType}) checks to see if one is trying to create a type
like the one above, or
\[{\bf Polynomial(Polynomial(Integer))}\]

But this is hand-crafted, special code covering a few cases and
mentions the types by name. It is conceivable that a user could create
a new type called {\bf MyFraction} which is identical to 
{\bf Fraction}. This would not be picked up by {\bf isValidType} and thus
\[{\bf MyFraction(MyFraction(Integer))}\]
could be instantiated. Since the type is then isomorphic to one of its
arguments, it is feasible that a coercion between the two could be
defined, contradicting our claim in case 10b of the case when the
length is 1.

This coercion can still not be built from our four basic types, thus
defining such a coercion contradicts our assumptions for the coherence
theorem.

\subsection{Extending the coherence theorem}
\label{DoyeSec6.5}

First, Weber's ``proof'' of his conjecture \ref{Doye6.3.7} relies on
induction on com($t$). This assumes that there are no types of
infinite complexity in our system. This is not the case in Axiom,
since one could define the following types (in one file):

{\bf R(r : Ring) : Ring == r}\\
{\bf D1 : Ring == R(D2)}\\
{\bf D2 : Ring == R(D1)}

(though calling {\bf 1()\$D1} would be disastrous\footnote{This would
try to create the constant 1 from the ring {\bf D1} which can not be
evaluated.})! So we add the following extra assumption.

\begin{DoyeAssumption}
\label{Doye6.5.1}
There do not exist any types of non-finite complexity
\end{DoyeAssumption}

The proof of the coherence theorem \ref{Doye6.4.7} does not rely on
type having finite complexity. However it is still a sensible
assumption to make and can be easily guaranteed in a real
implementation. It is also imperative that this assumption holds if
the automated coercion algorithm \ref{Doye7.4.1} is to terminate.

Assumption \ref{Doye6.3.5} states that we may have only one direct
embedding into an algebra. However, in {\bf Polynomial(Integer)} one
would wish to perform the direct embeddings of both of {\bf Integer}
and {\bf Symbol}, yet this violates assumption \ref{Doye6.3.5} and
thus we would not be able to prove that our algebra system is
coherent.

The reason for Weber's assumption is to stop coercions like the
following occurring. If $A$ is a group, then being able to coerce
$A\rightarrow A\times A$ whilst potentially useful, is ambiguous. As
he points out in \cite{Webe93b} and \cite{Webe95}. $A$ can be
coerced into $A\times A$ via the isomorphisms
$A\cong A\times I$ or $A\cong I\times A$ (where $I$ is the trivial
subgroup of $A$).

In this example the inclusions are ambiguous, since using either
coercion, the target of the inclusion is a group. However, in many
cases the types are ``incomparable'', (e.g. {\bf Integer} and
{\bf Symbol}) and thus the assumption seems to be too strong. The
question is what do we mean by ``incomparable''?

Certainly, there is no coercion function 
${\bf Integer}\rightarrow{\bf Symbol}$ or
${\bf Symbol}\rightarrow{\bf Integer}$. But, for two distinct
non-trivial, proper normal subgroups $G$, $H$ of $A$ such that
$G\cap H=I$, then there is no coercion function
$A/G\rightarrow A/H$, and coherency is lost. ($A/G$ is the quotient
group ``$A$ factored out by $G$'' and is the set
$\{aG~\vert~a\in A\}$ where $aG=a^\prime G$ iff
$a^{-1}a^\prime\in G$). Thus, the condition of there not existing a
coercion function between our two types is not sufficient for our
definition of ``incomparability''.

However, we notice that there exists no type which can be coerced to
both {\bf Symbol} and {\bf Integer}, but there does exist
homomorphisms $A\rightarrow A/G$ and $A\rightarrow A/H$. So if we
choose the statement ``Types $A$ and $B$ are incomparable if there
does not exist a type $S$ which can be coerced to both $A$ and $B$''
as a definition of incomparability then we are in good shape.

We state this in the language of Weber as follows. To replace the
assumption we first need to alter definition \ref{Doye6.4.3} (our
previous replacement of definition \ref{Doye6.2.8} to the following.

\begin{DoyeDefinition}
\label{Doye6.5.2}
Let $f:(\sigma_1,\ldots,\sigma_n)\rightarrow\sigma$ be an $n$-ary
type constructor. If\\
$(\forall i\in\{1,\ldots,n\})(\forall~ground~types~t_i:\sigma_i)$
there exists a coercion
\[\Phi^i_{f,t_1,\ldots,t_n}:t_i\rightarrow f(t_1,\ldots,t_n)\]
then we say that $f$ has a {\rm direct embedding at its $i$-th
position}.

Moreover, let $s$ be a ground type and $i\in\{1,\ldots,n\}$ and
define,
\[\mathcal{P}(i,s):=((\exists \psi:s\rightarrow t_i)\land
(\exists \Phi^i_{f,t_1,\ldots,t_n}:t_i\rightarrow
f(t_1,\ldots,t_n)))\]
where $\psi$ is a coercion, and also define,
\[\overline{\mathcal{D}}_f:=\{(i,s)~\vert~\mathcal{P}(i,s)\}\]
\end{DoyeDefinition}

Note then that 
$\mathcal{D}_f=\{i~\vert~(\exists s)(\mathcal{P}(i,s))\}$.

We now alter assumption \ref{Doye6.3.5} to the following

\begin{DoyeAssumption}
\label{Doye6.5.3}
Let $f$ be an $n$-ary type constructor. Then the following
conditions hold:
\begin{enumerate}
\item $(i,s),(j,s)\in\overline{\mathcal{D}}_f\rightarrow i=j$
\item Direct embedding coercions are unique. i.e. if
$\Phi^i_{f,t_1,\ldots,t_n}:t_i\rightarrow f(t_1,\ldots,t_n)$ and
$\Psi^i_{f,t_1,\ldots,t_n}:t_i\rightarrow f(t_1,\ldots,t_n)$ then
\[\Phi^i_{f,t_1,\ldots,t_n}=\Psi^i_{f,t_1,\ldots,t_n}\]
\end{enumerate}
\end{DoyeAssumption}

So we may now extend the coherence theorem (theorem \ref{Doye6.4.7})
by altering the assumption list to our new relaxed set of
assumptions. It is not necessary to reprove the entire theorem, but
merely the cases which involved assumption \ref{Doye6.3.5}.

\begin{DoyeTheorem} {\bf (Extended coherence theorem)}
\label{Doye6.5.4}
Assume that all coercions between ground types are only built by
one of the following mechanisms:
\begin{enumerate}
\item coercions between base types
\item coercions induced by structural coercions
\item direct embeddings (definition \ref{Doye6.5.2}) in a type
constructor
\item composition of coercions
\item identity function on ground types as coercions
\end{enumerate}

If assumptions \ref{Doye6.3.1}, \ref{Doye6.3.3}, \ref{Doye6.3.4},
\ref{Doye6.3.6}, \ref{Doye6.5.1}, \ref{Doye6.4.6}, \ref{Doye6.4.1},
\ref{Doye6.4.5}, and \ref{Doye6.5.3} are satisfied, then the set of
ground types as objects, and the coercions between them as arrows form
a category which is a preorder.
\end{DoyeTheorem}

PROOF: The entire proof of theorem \ref{Doye6.4.7} is valid except where
assumption \ref{Doye6.3.5} was used.

First we deal with the length of $\phi$ and $\psi$ being 1. The only
case which relied on assumption \ref{Doye6.3.5} was case 11.

{\bf Case 11:} If $\phi$ and $\psi$ are direct embeddings then by
assumption \ref{Doye6.5.3} $\psi=\phi$.

Now, the induction case. The only cases which relied on assumption
\ref{Doye6.3.5} were cases 6 and 11.

{\bf Case 6:} In the proof of theorem \ref{Doye6.4.7} in the case we
relied on assumption \ref{Doye6.3.5} to show that the two direct
embeddings $h_{a-1}\rightarrow h_a$ and $s_{b-1}\rightarrow s_b$ were
at the same position.

Now since $t\trianglelefteq h_{a-1}$ and $t\trianglelefteq s_{b-1}$ by
assumption \ref{Doye6.5.3} the direct embeddings must be at the same
position.

Thus the rest of the proof of this case holds.

{\bf Case 11:} $\tau_1$ is a direct embedding. $\tau_2$ is a direct
embedding. Since $t\trianglelefteq s_1$ and
$t\trianglelefteq s_2$ assumption \ref{Doye6.5.3} implies that the
direct embeddings must be at the same position, $i$. Therefore,
$s_1=s_2$. So by the inductive hypothesis
$\phi^\prime=\psi^\prime$. By assumption \ref{Doye6.5.3}
$\tau_1=\tau_2$. Hence, $\phi=\psi$.

Inserting the rest of the proof of theorem \ref{Doye6.4.7} completes
the proof.\QED

Thus we have relaxed an important one of the conditions of the
coherence theorem \ref{Doye6.4.7} and proved that the theorem still holds.

\subsection{Conclusion}
\label{DoyeSec6.6}

In this section we stated all the mathematics needed to state Weber's
coherence conjecture and give his proof.

We have also stated extra mathematics to correct the statement of the
conjecture and then prove it; hence promoting it to a theorem. We have
then relaxed one of the conditions and shown that the theorem still
holds.

This theorem is the cornerstone for ensuring correct coercions.

By having a coherent type system, then provided we are careful in how
we implement our type constructors (they must satisfy the assumptions)
and how we create our coercions (compositions of the four basic types)
all our coercions are then unique.

\section{The automated coercion algorithm}
\label{DoyeChap7}

We will introduce some extra mathematics which will allow us to state
the automated coercion algorithm. Furthermore, this sound foundation
will allow us to demonstrate that all coercions generated by the
algorithm are, in fact, homomorphisms. Indeed, we may even guarantee
that these are coercions in the sense of definition
\ref{Doye5.5.2}.

\subsection{Finitely generated algebras}
\label{DoyeSec7.2}

In this section we will define what it means to say that an algebra is
finitely generated, and also what it means to say that a finitely
generated algebra is decomposable.

\begin{DoyeDefinition}
\label{Doye7.2.1}
Let $\Sigma$ be an order-sorted $S$-signature. We say that $S$
has a {\rm principal sort} if $\langle\Sigma,S\rangle$ defines an
algebra which has one sort (which without loss of generality, we will
always assume that to be $S_1$) which is the ``most interesting'' of
the algebra.
\end{DoyeDefinition}

By ``most interesting'', we mean that a more na\"ive algebraist would
consider this sort to define the entire algebra.

For example, if we define $\Gamma$ to be the Group algebra with sort,
\[(``the~group'',``a~boolean~sort'',``an~integer~number~system'')\]

then the ``most interesting'' sort is the one for ``{\sl the group}''
which would be carried by a set of all the group elements.

All Axiom algebras must have a principal sort, referred to as \%, in
the source code. One can think of $S_1=\%$ as the sort that the Axiom
code ``constructs''. In the terminology of section \ref{DoyeChap6} it
is the type constructed by the type constructor. (Axiom views base
types as type constructors with no arguments.)

\begin{DoyeDefinition}
\label{Doye7.2.2}
We say that a theory $\langle\langle\Sigma,S\rangle,S\rangle$, is
finitely generated iff $S$ has a principal sort, $S_1$, and the
following set is finite,
\[\overset{..}{\sum}:=\bigcup_{n,q}{\Sigma_{n,q,S_1}}\]
\end{DoyeDefinition}

This states that given a finitely generated $\Sigma$-algebra,
$\langle A,\alpha\rangle$ (which is a model for $S$) then there are a
finite number of operators which can return an element of the carrier
of the principal sort.

This means that only finitely many functions (directly) construct (the
carrier of) the principal sort.

The next definition allows us to decompose any element of such a
$\Sigma$-algebra into at least two pieces. Moreover, there exists such
a decomposition for each one of the (finitely many) constructors. 
Furthermore, decomposing such an element and recombining using the 
corresponding constructor is equivalent to the identity function.

\begin{DoyeDefinition}
A finitely generated theory 
$\langle\langle\Sigma,S\rangle,S\rangle$ is decomposable iff
\[\begin{array}{l}
(\forall \sigma\in\overset{..}{\Sigma})
((\sigma\in\Sigma_{n,q,S_1})\Rightarrow\\
\quad(\forall i\in\{1,\ldots,n\})
(\exists \pi_{\sigma,i})
(\pi_{\sigma,i}\in\Sigma_{1,S_1,q_i})\land\\
\quad\quad((\sigma(\pi_{\sigma,1}(\cdot),\ldots,\pi_{\sigma,n}(\cdot))
={\rm id}_{S_1})\in S))
\end{array}\]
\end{DoyeDefinition}

For example, in an algebra of lists, {\bf cons} is a constructor.
The corresponding functions which decompose an element are {\bf car}
and {\bf cdr}. The following equation holds.
\[{\bf cons(car(x),cdr(x))=x}\]

Notice that this equation is only well formed when {\bf x} is not the
empty list. For the empty list, which is constructed by the 0-ary
function {\bf nil()}, there are no decomposers, and the equation is as
follows.
\[{\bf nil() = x}\]

It is important to note that the part functions $\pi_{\sigma,i}$ are
usually partial functions. For example, {\bf car} is not defined on
the empty list.

Notice that we have not yet defined a way of differentiating between
which constructors construct which element. We will do this now.

\subsection{Constructibility}
\label{DoyeSec7.3}

We now come to the most important concept of this thesis. The aim of
this thesis is to provde a method for creating coercions
constructively. We now supply the means to do so.

As usual, we need to make some definitions first. We will state what
we mean by a(n) (algorithmically) (re)constructible algebra. What we
really mean is that an algebra is (algorithmically) (re)constructible
if some subset of the elements of the most interesting sort-carrier of
that algebra can be ``built up'' from a finite family of operators.

This finite family consists of ``constructors'' used to create
increasingly ``large'' elements of the principal sort.

We will also have equations in the theory linking the constructors to
``part functions'' which we will use to split large elements into an
$n$-tuple of smaller elements. The constructor applied to its
associated part functions acting on an element will be equivalent to
the identity function.

We will be able to tell how an element is constructed by using a
``query function''. That is to say we must be able to know which
constructor(s) may construct any particular element of the principal
sort.

We require these definitions so that if an algebra is a model of some
algorithmically reconstructible theory, and another algebra contains
some of the constructors of the first algebra, then some subset of the
elements of the most interesting sort-carrier can be ``coerced'' from
the first type to the second. We will show this to be a
$\Xi$-homomorphism, where $\Xi$ is a particular signature to which
both algebras belong.

We demand our extensions to be protecting extensions so that
performing operations on an algebra (or looking at equations of
elements of that algebra) when viewed as a model of the original
theory or the extension of the theory yield ``the same result''.

\begin{DoyeDefinition}
\label{Doye7.3.1}
We call a theory $\langle\langle\Sigma,S\rangle,S\rangle$
constructible iff $\Sigma$ is a protecting extension of a finitely
generated theory. We then call $\Sigma$ a {\rm constructing}
signature, and any model for the theory, a {\rm constructible algebra}.
\end{DoyeDefinition}

Similarly,

\begin{DoyeDefinition}
\label{Doye7.3.3}
If $\langle\langle\Xi,X\rangle,\mathcal{X}\rangle$ is a decomposable
theory extended by the reconstructible
$\langle\langle\Sigma,S\rangle,S\rangle$, we define the
{\rm constructors} of 
$\langle\langle\Sigma,S\rangle,S\rangle$ to be
\[\Sigma^C:=\overset{..}{\Xi}\]
We define the {\rm constant constructors} to be the constructors of
arity 0, denoted by $\Sigma^0$ and the {\rm non-constant constructors}
to be the constructors of all other arities. We denote this set by
$\Sigma^{C-0}:=\Sigma^C\backslash\Sigma^0$.

We also define the {\rm part functions} of 
$\langle\langle\Sigma,S\rangle,S\rangle$ associated with
$\sigma\in\Sigma^{C-0}$ to be the $\pi_{\sigma,i}$. (Constant
constructors have no associated part functions.)

Finally we define the {\rm constructor equations} to be the following
set and demand it to be a subset of $S$ (and hence $\mathcal{X}$) given by
\[\{(\sigma(\pi_{\sigma,1}(\cdot),\ldots,\pi_{\sigma,n}(\cdot)=
{\rm id}_{S_1})~\vert~\sigma\in\Sigma^{C-0}\}\]
\end{DoyeDefinition}

We also need to ensure that certain other relationships between
constructors which hold in our source algebra hold in our target
algebra.

If an element of the source algebra may be constructed in more than
one way, we require that reconstructing that element in the target
algebra using any of those methods yields the same result.

Notice that since our theories are protecting extensions we are in
good shape, since any equation that holds in the protecting extension
which concerns only the sorts from the original (unextended) signature
must hold in the original signature. Thus these equations will hold in
the target algebra.

We call these equations that link different constructors together the
secondary constructor equations.

\begin{DoyeDefinition}
\label{Doye7.3.4}
We call a reconstructible theory
$\langle\langle\Sigma,S\rangle,S\rangle$ algorithmically
reconstructible iff : $S$ contains a boolean sort, $B$;
$\Sigma$ contains a set of symbols, called the query functions,
denoted and defined by
\[\Sigma^Q:=\{\mathcal{X}_\sigma\in\Sigma_{1,S_1,B}~\vert~
\sigma\in\Sigma^C\}\]
and $S$ contains the following equations, called the {\rm query equations}
\[(\forall \sigma_{n,q,S_1}\in\Sigma^C)(\mathcal{X}_{\sigma_{n,q,S_1}}
(\sigma_{n,q,S_1}(a_1,\ldots,a_n))=T)\]
We also demand that for each $\sigma_{n,q,S_1}$ in $\Sigma^C$
\[((\nexists~a_1,\ldots,a_n)(\omega=\sigma_{n,q,S_1}(a_1,\ldots,a_n)))
\Rightarrow(\mathcal{X}_{\sigma_{n,q,S_1}}(\omega)=F)\]
Finally we demand that all secondary constructor equations are defined
in the theory.

A {\rm secondary constructor equation} is any equation which has
a constructor as the final symbol on both right and left sides.
\end{DoyeDefinition}

We say ``final symbol'' to mean this is the operator applied last. In
our notation (functions written on the left) a final symbol is written
to the left of all the other elements of a formula.

For example, $\sigma()$ and $\sigma(e_1,\sigma^\prime(e_2,e_3))$ both
have $\sigma$ as a final symbol.

In the algorithm we will demand that the decomposable theory which
both types model is the smallest theory which they both model. This is
to ensure that the algorithm not only creates a homomorphism, but that
it is a coercion (definition \ref{Doye5.5.2}). The proof of this is
given in corollary \ref{Doye7.6.8}.

\begin{DoyeExample}
\label{Doye7.3.5}
We will take the variety of {\sl Lists} as our example, and we will
assume that {\bf List} is a member of this variety. Then the
constructors for the model are {\bf nil} and {\bf cons}, where 
{\bf nil} is a constant constructor whereas {\bf cons} is a
non-constant constructor. {\bf cons}' associated part-functions are
{\bf car} and {\bf cdr}. Thus we have the constructor equation
\[{\bf cons(car(x),cdr(x))=id(x)}\]

The query functions are {\bf null} and {\bf consp}, and our query
equations are
\[{\bf null(nil)=}{\sl T}\]
and for any list $l$ and any element of the underlying type, $i$,
\[{\bf consp(cons(}i,l{\bf))=}{\sl T}\]

Notice also that ${\bf consp(nil)=}{\sl F}$ and also, for the same $l$ and
$i$, ${\bf null(cons(}i,l{\bf))=}{\sl F}$
\end{DoyeExample}

It is now worth discussing some of the finer points of homomorphism.

For $\phi$ to be a homomorphism we require that the constructor
functions are preserved by homomorphism. We do not require that the
part or query functions are homomorphically preserved; indeed we do
not even require that they are in the signature of the algebra of the
target of $\phi$.

Thus we do not require that both the source and target of $\phi$ are
models of the same algorithmically reconstructible theory, but that
the source is a model of an algorithmically reconstructible theory
$\mathcal{T}$, and that all the constructors of $\mathcal{T}$ are
functions of the target of $\phi$, inherited from the {\sl same}
signature (theory).

This is an important point. As an example, we will consider polynomial
rings. A polynomial is a function such as
\[5x^2y^3+9xy^{45}-34x+7y^2-12\]

That is a sum of products of an element of the underlying ring (which
in the above example might be the integers) and variables raised to
non-negative powers. A monomial is a polynomial which is a product of
a non-zero element of the underlying ring and variables raised to
non-negative powers.

A polynomial ring is ordered by an extension of the order on the
variables. In particular, in $\mathbb{Z}[x,y]$ if $x>y$ then a
monomial $m_1$ is greater than another $m_2$ if the exponent of $x$ is
greater in $m_1$ than in $m_2$. Should the exponent of $x$ be equal in
both, then the exponent of $y$ is compared in the same manner.

The leading monomial of a polynomial is the largest monomial of a
polynomial. If $x>y$ in the above polynomial then $5x^2y^3$ is the
leading monomial. Should $y>x$ then the leading monomial would be
$9xy^{45}$.

The reductum of a polynomial is the polynomial less its leading
monomial.

Coercions between two polynomial rings need only use
{\bf leadingMonomial} as a part function (which is not preserved via
homomorphism, since it depends on the ordering given to the variables)
instead of some (unnatural) fixed (for all polynomial rings with
variables from a fixed domain) ``most-important monomial'' function
which would choose the same monomial, regardless of variable ordering.

Also, this allows us to form the natural homomorphism from
$\mathbb{Q}[x]$ to $\mathbb{Z}(x)$, which in Axiom is
\[{\bf Polynomial\ Fraction\ Integer}\ to\ 
{\bf Fraction\ Polynomial\ Integer}\]
which are, in Axiom's view (without clever hackery in the interpreter)
two unrelated {\bf Rings}. This may be constructed {\sl without}
having to force $+$ to be a constructor of
{\bf Fraction Polynomial Integer}, or indeed {\bf leadingMonomial},
etc. to be available in {\bf Fraction Polynomial Integer}.

\subsection{The algorithm}
\label{DoyeSec7.4}

We are now in a position to state the automated coercion algorithm.

The algorithm to create the coercion will be stated in English. It is
too implementation dependent to state any finer.

The actual algorithm to coerce will be stated as Lisp pseudo-code. In
Lisp, 
{\bf (a b c)} means apply the function {\bf a} to the arguments
{\bf (b,c)}. {\bf cond} is like the switch statement in C or Java.

{\bf cond} is equivalent to ``if-then-else'' statements -- if a
condition is true, then we evaluate and return the following statement
(and leave the {\bf cond} block), else go to the next condition and
repeat.

For example, the line
\[((\alpha_{\chi_{\sigma_{0,\emptyset,S_1}}} x)~
(\beta_{\sigma_{0,\emptyset,S_1}}))\]
means
\[{\bf if\ }\alpha_{\chi_{\sigma_{0,\emptyset,S_1}}}(x)
{\bf \ then\ }\beta_{\sigma_{0,\emptyset,S_1}}()\]

The final statement {\bf t}, is the default statement since {\bf t} is
always true. This line will only be reached when all the other
conditions have not been satisfied, and shows that we have failed to
build a total automated coercion function.

This could happen if one fails to list all the constructors (and their
queries) for a type.

The lines of the form
\[((\alpha_{\chi_{\sigma_{0,\emptyset,S_1}}} x)~
(\beta_{\sigma_{0,\emptyset,S_1}}))\]
check for the constants of the type. For example, the constant
polynomial 0 can not be constructed using a non-zero number of parts
(using any normal construction methodology) thus coercing 0 from
$\mathbb{Q}[x]$ to $\mathbb{Z}(x)$ we might say
\[\begin{array}{c}
{\bf if~zero?(x)\$Polynomial~Fraction~Integer}\\
\quad{\bf then~zero()\$Fraction~Polynomial~Integer}
\end{array}\]
returning the appropriate 0 in $\mathbb{Z}(x)$.

The lines of the form
\[((\alpha_{\chi_{\sigma_{n,q,S_1}}} x)
(\beta_{\sigma_{n,q,S_1}} 
(\psi_{q_1} (\alpha_{\pi_{\sigma_{n,q,S_1},1}} x)\ldots
(\psi_{q_n} (\alpha_{\pi_{\sigma_{n,q,S_1},n}} x))))\]
are the constructing lines. For example, in a polynomial ring we might
write
\[\begin{array}{c}
{\bf if~x~}{\rm\ is~the~sum~of~a~monomial~and~a~polynomial}\\
{\bf then~coerce(leadingMonomial(x))+coerce(reductum(x))}
\end{array}\]
where the addition function is that taken from the target domain.

In this case {\bf leadingMonomial(x)} and {\bf reductum(x)} are both
polynomials. In general, the parts of the element need not be of the
same type as the original element.

For example, in List algebras, coercing from {\bf List(A)} to
{\bf List(B)} where there exists (or we can build) a coercion from
{\bf A} to {\bf B} then we have (back in Lisp terminology)
\[{\bf ((consp~x)~(cons~(coerce~(car~x))~(coerce~(cdr~x))))}\]
{\bf (car x)} is the first element of the list and is of type
{\bf A} rather than {\bf List(A)} -- the type of {\bf x}. So
{\bf (coerce (car x))} is of type {\bf B} and thus may be
{\bf consed} on to the front of {\bf (coerce (cdr x))} which is
of type {\bf List(B)}.

\begin{DoyeAlgorithm}{\bf (The Automated Coercion Algorithm)}
\label{Doye7.4.1}
Let $\langle A,\alpha\rangle$ be a model for the algorithmically
reconstructible theory
$\langle\langle\Sigma,S\rangle,S\rangle$

Let $\langle B,\beta\rangle$ be a model of
$\langle\langle\Delta,\mathcal{D}\rangle,\mathcal{D}\rangle$ where
$\langle\langle\Sigma,S\rangle,S\rangle$ is a protecting extension of
$\langle\langle\Delta,\mathcal{D}\rangle,\mathcal{D}\rangle$ and some
(or all) of the constructors of
$\langle\langle\Sigma,S\rangle,S\rangle$ are in fact
$\langle\langle\Delta,\mathcal{D}\rangle,\mathcal{D}\rangle$.

Also we demand that there does not exist an extension
$\langle\langle\Delta,\mathcal{D}\rangle,\mathcal{D}\rangle$ which both
$\langle A,\alpha\rangle$ and $\langle B,\beta\rangle$ model.

Then the following is an algorithm to coerce from the
$\langle A,\alpha\rangle$ to the $\langle B,\beta\rangle$ model.

The $\psi_i$ for $i\ne 1$ are the (potentially automated) coercions
from $A_i$ to $B_i$ from the abstract type of $S_i$.

The entire morphism created thus is called $\psi$ and not only is it a
homomorphism, it is a coercion (definition \ref{Doye5.5.2}).
\end{DoyeAlgorithm}

$\psi_1(x) := (cond$\\
\vbox{\vskip 0.4cm}
\hbox{\hskip 1cm}$((\alpha_{\chi_{\sigma_{0,\emptyset,S_1}}} x)~
(\beta_{\sigma_{0,\emptyset,S_1}}))$\\
\vbox{\vskip 0.4cm}
\hbox{\hskip 1cm}$\vdots~Repeat~for~each~\sigma_{0,\emptyset,S_1}~in~
\Sigma^0$\\
\vbox{\vskip 0.4cm}
\hbox{\hskip 1cm}$((\alpha_{\chi_{\sigma_{n,q,S_1}}} x)~
(\beta_{\sigma_{n,q,S_1}}
(\psi_{q_1} (\alpha_{\pi_{\sigma_{n,q,S_1},1}} x))\ldots
(\psi_{q_n} (\alpha_{\pi_{\sigma_{n,q,S_1},n}} x))))$\\
\vbox{\vskip 0.4cm}
\hbox{\hskip 1cm}$\vdots~Repeat~for~each~\sigma_{n,q,S_1}~in~
\Sigma^{C-0}$\\
\vbox{\vskip 0.4cm}
\hbox{\hskip 1cm}$(t~(error)))$

{\sl The algorithm to create the coercion $\psi$ is}

$createCoerce(\langle A,\alpha\rangle,\langle B,\beta\rangle):=$\\
\vbox{\vskip 0.5cm}
\hbox{\hskip 1cm}$determine~
\langle\langle\Delta,\mathcal{D}\rangle,\mathcal{D}\rangle
~(error~if~doesn't~exist)$\\
\vbox{\vskip 0.5cm}
\hbox{\hskip 1cm}$determine~
\langle\langle\Sigma,\mathcal{S}\rangle,\mathcal{S}\rangle
~(error~if~doesn't~exist)$\\
\vbox{\vskip 0.5cm}
\hbox{\hskip 1cm}$determine~\Sigma^0$\\
\vbox{\vskip 0.5cm}
\hbox{\hskip 1cm}$for~\sigma\in\Sigma^0$\\
\vbox{\vskip 0.5cm}
\hbox{\hskip 1.5cm}$determine~\alpha_{\chi_\sigma}$\\
\vbox{\vskip 0.5cm}
\hbox{\hskip 1.5cm}$determine~\beta_\sigma$\\
\vbox{\vskip 0.5cm}
\hbox{\hskip 1cm}$determine~\Sigma^{C-0}$\\
\vbox{\vskip 0.5cm}
\hbox{\hskip 1cm}$for~\sigma\in\Sigma^{C-0}$\\
\vbox{\vskip 0.5cm}
\hbox{\hskip 1.5cm}$determine~\alpha_{\chi_\sigma}$\\
\vbox{\vskip 0.5cm}
\hbox{\hskip 1.5cm}$determine~\beta_\sigma$\\
\vbox{\vskip 0.5cm}
\hbox{\hskip 1.5cm}$determine~\alpha_{\pi_{\sigma,i}}~
(for~all~relevant~i)$\\
\vbox{\vskip 0.5cm}
\hbox{\hskip 1cm}$construct~and~return~\psi_1~as~defined~above$

so the algorithm presented above allows us to algorithmically
reconstruct elements of one type as elements of another.

\subsection{Existence of the coercion}
\label{DoyeSec7.5}

Before we prove that the automated coercion algorithm constructs a
coercion when one exists we must prove that it does not create some
unnatural function when no coercion exists.

We will use the terminology of algorithm \ref{Doye7.4.1} in this
section.

In the simplest case, some of the functions will not exist in the
target type and the algorithm will error at an early stage. This will
be because there is no
$\langle\langle\Delta,\mathcal{D}\rangle,\mathcal{D}\rangle$ which
both $\langle A,\alpha\rangle$ and $\langle B,\beta\rangle$ model.

If a homomorphism exists but not a coercion then this means that there
exists an extension of 
$\langle\langle\Delta,\mathcal{D}\rangle,\mathcal{D}\rangle$ which
both $\langle A,\alpha\rangle$ and $\langle B,\beta\rangle$ model. In
this case the algorithm will error at an early stage.

If all the constructors of $\langle A,\alpha\rangle$ are available in
$\langle B,\beta\rangle$ but no homomorphism (coercion) exists then
this could be because of (at least one of) the following causes.

\subsubsection{One of the coercions to be used does not exist}

At least one of the types which is required for construction or
recursively required for construction (and which is not the carrier of
the principal sort) may not be coercible to its counterpart in the
target. Provided the algorithm checks at construction time that every
coercion used directly or indirectly by $\psi_i$ exists (or is
constructible) then we may report an error at an early stage.

\subsubsection{Non-homomorphic constructors}

The fact that $\langle\langle\Sigma,S\rangle,S\rangle$ is a protecting
extension of
$\langle\langle\Delta,\mathcal{D}\rangle,\mathcal{D}\rangle$ and that
all secondary constructor equations hold in 
$\langle\langle\Sigma,\mathcal{S}\rangle,\mathcal{S}\rangle$
guarantees that these equations hold in
$\langle\langle\Delta,\mathcal{D}\rangle,\mathcal{D}\rangle$
and hence in $\langle B,\beta\rangle$.

This not only ensures that the order of the lines in $\phi_1$ is
unimportant but also that if a certain relationship holds in
$\langle A,\alpha\rangle$ it {\sl must} hold in
$\langle B,\beta\rangle$.

For example, if someone defines that all finite field algebras are
constructed by {\bf 0} and {\bf succ} (the successor function) the
automated coercion algorithm will not attempt to create the
``coercion'' $\mathbb{Z}_5\rightarrow\mathbb{Z}_3$. This is because
there exists a secondary constructor equation in the theory of finite
fields of size $n$.

\[{\bf succ^n(0)=0}\]

So in this example
$\langle A,\alpha\rangle=\mathbb{Z}_5$ and
$\langle B,\beta\rangle=\mathbb{Z}_3$. The equation
${\rm succ}^5(0)=0$ holds in the theory of finite fields of size 5.
$\mathbb{Z}_5$ is obviously a model for this theory.

This equation is not true in any theory which the above theory extends
and of which $\mathbb{Z}_3$ is a model. Otherwise the equation would
hold in $\mathbb{Z}_3$ and that patently is not true.

Thus the automated coercion algorithm will error at any early stage
from not being able to find
$\langle\langle\Delta,\mathcal{D}\rangle,\mathcal{D}\rangle$.

\subsection{Proving homomorphicity and coerciveness}
\label{DoyeSec7.6}
We now make some notational definitions so that we may prove the final
result of the section. That is, we will show that algorithm
\ref{Doye7.4.1} constructs a homomorphism (theorem \ref{Doye7.6.7})
which is a coercion (corollary \ref{Doye7.6.8}).

Recall notation \ref{Doye4.2.10}. Thus for any constructor symbol
$\sigma_{n,q,S_1}$ the associated constructor function in
$\langle A,\alpha\rangle$ is $\alpha_{\sigma_{n,q,S_1}}$ the query
function is $\alpha_{\chi_{\sigma_{n,q,S_1}}}$ and for
$i\in\{1,\ldots,n\}$ the part function
$\alpha_{\pi_{\sigma_{n,q,S_1},i}}$

\begin{DoyeDefinition}
\label{Doye7.6.1}
For a term $t$ in a term algebra $T_\Sigma(X)$ if
\begin{enumerate}
\item $t\in X_s~then~{\rm length}(t)=1$\\
\item $t\in\Sigma_{0,(),s}~then~{\rm length}(t)=1$\\
\item $t=\sigma(t_1,\ldots,t_n)~then~
{\rm length}(t)=1+{\rm length}(t_1)+\cdots+{\rm length}(t_n)$
\end{enumerate}
\end{DoyeDefinition}

The definition of length of an element will form the basis of our
inductive proof of the automated coercion algorithm. However, in
general, we will not be dealing with term algebras but their
homomorphic images.

\begin{DoyeDefinition}
\label{Doye7.6.2}
Let $x$ an element of a $\Sigma$-algebra, we define
\[{\rm length}(x):={\rm min}\{t\in T_\Sigma(X)~\vert~
\theta^*(t)=x\}\]
where $\theta^*$ is the unique homomorphism given in the first
universality theorem \ref{Doye4.4.7}.
\end{DoyeDefinition}

\begin{DoyeAssumption}
\label{Doye7.6.3}
If one of the part functions
$\alpha_{\pi_{\sigma_{n,q,S_1},i}}$ corresponds to
$\alpha_{\sigma_1,(S_1),S_1}$ (thus $q_i=S_1$) then we demand that
\[(\forall t\in T_\Xi(X)_1)({\rm length}(
\alpha_{\pi_{\sigma_{n,q,S_1},i}}(\theta^*(t)))<
{\rm length}(\theta^*(t)))\]
where $\theta^*$ is the unique homomorphism given in the first
universality theorem \ref{Doye4.4.7}
\end{DoyeAssumption}

A couple to technical definitions make the next assumption easier
to understand.

\begin{DoyeDefinition}
\label{Doye7.6.4}
Suppose $\langle A,\alpha\rangle$ is a constructed algebra,
constructed by the $S$-sorted signature $\Sigma$. If for any
$\i\in\{2,\ldots,\vert S\vert\}$ we have that $S_i$ appears in the
arity of any of the constructors of $\Sigma$, we say that $A_i$ is
required for construction by $A_1$.
\end{DoyeDefinition}

So far example, in {\bf List(Integer)} we have that {\bf Integer} is
required for construction since it is an argument of {\bf
cons}. Notice that {\bf List(Integer)} is not required for consruction
itself since it is the carrier of $S_1$.

\begin{DoyeDefinition}
\label{Doye7.6.5}
Suppose $\langle A,\alpha\rangle$ is a constructed algebra,
constructed by the $S$-sorted signautre $\Sigma$. Let $A_i$ be
required for construction by $A_1$. Now consider $A_i$ as the carrier
of the principal sort of $\langle B,\beta\rangle$, a constructed
algebra.

If $B_j$ is required for construction by $B_1=A_i$ (hence $j\ne 1$)
then we say that $B_j$ is {\rm recursively required for construction} by
$A_1$.

Now suppose that the carrier $D_1$ of the principal sort of a
constructed algebra $\langle D,\delta\rangle$ is recursively required
for construction by $A_1$. If $D_k$ is required for construction by
$D_1$, then we also say that $D_k$ is {\rm recursively required for
construction} by $A_1$.
\end{DoyeDefinition}

So for example in {\bf List(List(Integer))}, the only type which is
required for construction is {\bf List(Integer)} since this is the
only argument of {\bf cons} which is not the carrier of the principal
sort, {\bf List(List(Integer))}.

Thus the only types which are recursively required for construction
are {\bf List(Integer)} and the types which are recursively required
for construction by {\bf List(Integer)}.

Since {\bf Integer} is the only type required for construction by
{\bf List(Integer)} it is the only type recurively required for
construction by {\bf List(Integer)}.

Thus the types which are recurively required for construction by
{\bf List(List(Integer))} are {\bf List(Integer)} and {\bf Integer}.

The following assumption is required so that we may prove that the
automated coercion algorithm terminates.

\begin{DoyeAssumption}
\label{Doye7.6.6}
We demand that $A_1$ is not recursively required for construction.
\end{DoyeAssumption}

\begin{DoyeTheorem}
\label{Doye7.6.7}
Let $\langle\langle\Sigma,S\rangle,S\rangle$ be an algorithmically
reconstructible theory which is a protecting extension of the theory
$\langle\langle\Xi,X\rangle,X\rangle$ such that
$\Sigma^C\subseteq\overset{..}{\Xi}$.

Let $\langle B,\beta\rangle$ be a model for
$\langle\langle\Xi,X\rangle,X\rangle$ and $\langle A,\alpha\rangle$ be
a model for $\langle\langle\Sigma,S\rangle,S\rangle$.

Then the function $\psi$ given in algorithm \ref{Doye7.4.1} is a
$\Xi$-homomorphism
$\langle A,\alpha\rangle\rightarrow\langle B,\beta\rangle$
\end{DoyeTheorem}

PROOF:

Let $\phi$ be the correct homomorphism which $\psi$ is attempting to
emulate.

Since $\psi_1$ contains finitely many cases, and covers all cases of
construcctors for $\Sigma$, we need only consider one line from
$\psi_1$. Also by induction on ${\rm com}(t)$ we can assume that\\
$(\forall i \in \{2,\ldots,\vert S\vert\})(\psi_i=\phi_i)$

Since $\psi_1$ covers all cases of constructor for $\Sigma$ we do not
need to consider the error line since it will never be reached.

For $\psi_1$ we may induct on length, and we are in good shape by
assumption \ref{Doye7.6.3} (on length) and \ref{Doye7.6.6} (on the
interaction between length and com).

Here is one line from $\psi_1$
\[(if~(\alpha_{\sigma_q} ~a)~(\beta_{\sigma_c}
~(\psi_{c_1} ~(\alpha_{p_1} ~a)) \ldots
(\psi_{c_n} ~(\alpha_{p_n} ~a))))\]
Now, assuming $(\alpha_{\sigma_q} ~a)$, we know that,
\[(\phi_1 ~a)=(\phi_1 ~(\alpha_{\sigma_c}
~(\alpha_{p_1} ~a)\ldots(\alpha_{p_n} ~a)))\]
then since $\phi$ is a $\Sigma$-homomorphism
\[(\phi_1 ~(\alpha_{\sigma_c} ~(\alpha_{p_1} ~a)\ldots 
(\alpha_{p_n} ~a)))=
(\beta_{\sigma_c} ~(\phi_{c_1} ~(\alpha_{p_1} ~a)\ldots 
(\phi_{c_n} ~(\alpha_{p_n} ~a)))\]
Now we know that, by our induction argument
\[(\beta_{\sigma_c} ~(\phi_{c_1} ~(\alpha_{p_1} ~a)) \ldots
(\phi_{c_n} ~(\alpha_{p_n} ~a))) =
(\beta_{\sigma_c} ~(\psi_{c_1} ~(\alpha_{p_1} ~a))\ldots
(\psi_{c_n} ~(\alpha_{p_n} ~a)))\]
whence for this line, and therefore every line and the entire function\\
$(\psi_1 ~a)=(\phi ~a)$. So by induction, $\psi=\phi$ and thus $\psi$ is
a homomorphism.\QED

Now by adding one extra condition, we may prove that the automated
coercion algorithm generates coercions in the sense of definition
\ref{Doye5.5.2}.

\begin{DoyeCorollary}
\label{Doye7.6.8}
Let $\langle\langle\Sigma,S\rangle,S\rangle$ be an algorithmically
reconstructible theory which is a protecting extension of the theory
$\langle\langle\Xi,X\rangle,X\rangle$ such that
$\Sigma^C\subseteq\overset{..}{\Xi}$

Let $\langle B,\beta\rangle$ be a model for
$\langle\langle\Xi,X\rangle,X\rangle$ and $\langle A,\alpha\rangle$
be a model for $\langle\langle\Sigma,S\rangle,S\rangle$ such that there
does not exist any extension of
$\langle\langle\Xi,X\rangle,X\rangle$ which both
$\langle A,\alpha\rangle$ and $\langle B,\beta\rangle$ model.

Then the function $\psi$ given in algorithm \ref{Doye7.4.1} is a
coercion $\langle A,\alpha\rangle\rightarrow
\langle B,\beta\rangle$
\end{DoyeCorollary}

PROOF: No other theories extend
$\langle\langle\Xi,X\rangle,X\rangle$ which both
$\langle A,\alpha\rangle$ and $\langle B,\beta\rangle$ model so the
only thing which could stop $\psi$ being a coercion would be for there
to exist a theory which both $\langle A,\alpha\rangle$ and
$\langle B,\beta\rangle$ were to model which was not extended by
$\langle\langle\Xi,X\rangle,X\rangle$.

So, suppose (for a contradiction) $\langle A,\alpha\rangle$ and
$\langle B,\beta\rangle$ were to model a theory $\Omega$ which
$\langle\langle\Xi,X\rangle,X\rangle$ does not extend, then we may
manufacture a new theory containing all the sorts, operator symbols,
and equations of both theories.

We may have some duplication of sorts and operators, so this
manufacturing process would need to be performed
intelligently. Explicitly, $\Omega$ and
$\langle\langle\Xi,X\rangle,X\rangle$ may both be extensions of some
theory $\Theta$. We might (but not always\footnote{The theory of rings
inherits from the theory of monoids twice}) only wish our manufactured
theory to contain the sorts, operator symbols, and equations 
$\Theta$ once, not twice.

This new theory would clearly extend
$\langle\langle\Xi,X\rangle,X\rangle$ and $\langle A,\alpha\rangle$
and $\langle B,\beta\rangle$ would both model this new theory. Hence
we have a contradiction and such an $\Omega$ can not exist.

So $\langle\langle\Xi,X\rangle,X\rangle$ must specify the unique
``smallest'' variety to which both $\langle A,\alpha\rangle$ and
$\langle B,\beta\rangle$ belong. We already know by theorem
\ref{Doye7.6.7} that $\psi$ is a $\Xi$-homomorphism; by definition
\ref{Doye5.5.2} it must be a coercion.

This is why algorithm \ref{Doye7.4.1} looks for
$\langle\langle\Delta,\mathcal{D}\rangle,\mathcal{D}\rangle$, since it
must specify the smallest variety to which both
$\langle A,\alpha\rangle$ and $\langle B,\beta\rangle$ belong.

\subsection{Conclusion}
\label{DoyeSec7.7}

In conclusion, we have shown that it is possible to create a
homomorphism between two types from an important subset of types.

Moreover, this homomorphism may be constructed algorithmically and
hence the general construction of homomorphisms is implementable on a
computer.

Furthermore, if the type system adheres to all the assumptions made,
and the homomorphism is from the theory which specifies the smallest
variety to which both types belong, then it is the unique
{\sl coercion} between the two types.

\section{Implementation Details}
\label{DoyeChap8}

The basic design was as follows. To perform a coercion from a
{\bf Domain A} to a {\bf Domain B}, one needs to be able to ascertain
which constants, queries, constructors, and part functions are
available to both {\bf A} and {\bf B}. These functions should all come
from the same algorithmically reconstructible theory 
({\bf Category}).

The automated coercion function should be created and then compiled
and stored away ready for fast access in case of future use. Such
items are called ``cached lambdas'' or ``clams'', for short, and are
already part of the Axiom system. Thus this was implemented.

The automated coercion function should be integrated with the current
coercion mechanism in Axiom. This was done by inserting the relevant
line at the correct point of the function {\bf coerceInteractive}.

\subsection{Labelling operstors}
\label{DoyeSec8.4}

We had to decide on a mechanism to mark which operators were
constructors, part functions, etc.

One method would have been to create sub-types of the Axiom
{\bf Domain}, {\bf Mapping}, which would have been infeasible,
requiring much rewriting of Spad compilers. This is also true of the
method given in a later section.

A Spad {\bf Domain} contains the following parts:
\begin{enumerate}
\item the name of the type constructor
\item a (variable name, {\bf Category}) pair\footnote{Occasionally a
(variable name, {\bf Domain}) pair. For example,
{\bf IntegerMod(p:PositiveInteger)}} for each\footnote{This is not the
same as all the sorts of the signature. One may declare extra unused
sorts. Also we may declare a function where the source of the target
may contain a ground type. For example, functions which return a
boolean-like type are usually declared to return {\bf Boolean} rather
than a pair where the {\bf Category} is the {\bf Category} of
boolean-like types.} of the parameters of the type constructor
\item a {\bf Category} or {\bf Join} (intersection) of
{\bf Categories} to which the {\bf Domain} belongs
\item a list of additional operator symbols (functions) for the type
\item a list of methods for (some of or all) the operator symbols of
the type
\end{enumerate}

Every operator symbol definition in a {\bf Domain} or {\bf Category}
(and hence available in a {\bf Domain} of that {\bf Category}) has a
certain number of special comments called $++$ comments.

These $++$ comments (as opposed to ordinary comments in most languages
and Axiom's other style of comments, $--$ comments) are parsed by the
compiler to produce the documentation for Axiom's sophisticated
on-line help system, {\bf HyperDoc}. ({\bf HyperDoc} was one of the
first hypertext systems.)

This parsing of the $++$ comments allowed us to enter special keywords
for each special function which were read and stored at compile time
for each {\bf Domain}. We could then read them at run-time using the
Axiom interpreter's built in $++$ comments reader.

\subsection{Getting information from domains}
\label{DoyeSec8.5}
The $++$ comments were gathered for a {\bf Domain} by recursion up the
{\bf Category} inheritance lattice, using the {\bf GETDATABASE}
function. We asked the database for all the documentation for each
{\bf Category}, and then asking which {\bf Categories} it
extended. The documentation includes all the functions and their
respective $++$ comments. This allowed the automated searching for
keywords, and hence, the special functions of a {\bf Domain}.

Note that we only needed the comments for functions (operator symbols)
declared in a {\bf Category}. Functions declared in {\bf Domains} are
of no interest to the automated coercion algorithm. This is because
these do not correspond to any of the special functions (constructors,
part functions, or queries) of a signature or theory.

The keywords were checked quite simply, being members of the
following list:
\[{\bf list('"constant",'"constructor",'"part",'"query")}\]
Part number checking was only slightly more difficult with each number
having to be converted from a (Lisp) string to a (Lisp) integer. A
restriction was placed on queries that they must be called the same as
their associated constant (or constructor), but with a "?"
appended. This restriction was not necessary and could have been
worked around easily.

The special function lists from both {\bf Domain}s were compared and
the required functions were extracted from both.

\subsection{Checking information from domains}
\label{DoyeSec8.6}

For each special function, a check was performed to see whether that
function was really available in the {\bf Domain}.

The envisaged problem was that Axiom's {\bf Categories} can be
conditional: that is to say, some function definitions only exist if
certain relationships hold for {\bf Category}'s parameters.

{\bf GETDATABASE} ignores the parameters to a {\bf Category} and
returns all the functions which may be exported by a {\bf Domain} of
that {\bf Category}. Hence, the extra check was necessary to ensure
that the automated coercion function did not try to include these
unavailable functions.

Thus the homomorphism created by the automated coercion algorithm was
always equivalent to definition of a conditional homomorphism given in
section \ref{DoyeSec5.3}.

\subsection{Flaws in the implementation}
\label{DoyeSec8.7}

The implementation was originally envisaged as a fully working piece
of code of production standard and hence shippable with a commercial
release of Axiom. For various reasons outlined below, only a working
prototype was implemented.

Deliberate restrictions placed by the author on the deisgn were:
\begin{enumerate}
\item Query function names were restricted to be the name of their
associated constant function with a "?" appended.
\item The number of (non 0-ary) constructors was limited to one
\item Recursion through {\bf coerceInteractive} was achieved using a
na\"ive means
\item Neither the existence of
$\langle\langle\Sigma,S\rangle,S\rangle$ nor
$\langle\langle\Delta,\mathcal{D}\rangle,\mathcal{D}\rangle$ were
checked. (See algorithm \ref{Doye7.4.1} for the meaning of these).
\end{enumerate}

Both the first two items could have easily been worked around but
would have required a more complex $++$ comment reader. This was
considered to be a minor detail, which would have required too much
implementation time.

The third item was more problematic. Attempts were made to remedy this
situation. However, the depth of knowledge needed to implement this
correctly would have required too much time to learn. This was the
main factor in the downgrading of the implementation to mere prototype
status.

The method to work around the complexities of Axiom's evaluation loop
was to add a (Spad) {\bf Package} which exported a function which then
called the Lisp function. The code for this
{\bf Package} (called {\bf NCoerce}) follows.

$--$\%~{\bf Coercion~Package}\\
{\bf )abbrev package NCOERCE NCoerce}

{\bf NCoerce(Source:Type,Target:Type):Exports $==$
Implementation where}

{\bf\quad Exports $==>$ with}

{\bf\quad\quad nCoerce: Source $->$ Target}

{\bf\quad Implementation $==>$ add}

{\bf\quad\quad nCoerce(x:Source):Target $==$}\\
\vskip -0.6cm
{\bf\quad\quad\quad nCreateCoerce(x,Source,Target)\$Lisp}

To call a function from a {\bf Package} is relatively easy.

The function ({\bf nCoerce} above) then uses the Axiom
{\bf \$Lisp} syntax for calling a function.

One item is a bit more difficult to solve. The existence of
$\langle\langle\Sigma,S\rangle,S\rangle$ is implicit if a
{\bf Domain} has special functions. It would also be possible to check
if both source and target are members of a common {\bf Category}
(using {\bf GETDATABASE}). It would even be possible to check that all
the constructors of the source are from this {\bf Category}. Finally,
it would be possible to ensure that this is the smallest such
{\bf Category}.

However, it would be most problematical to determine whether
$\langle\langle\Sigma,S\rangle,S\rangle$ is a protecting extension of
$\langle\langle\Delta,\mathcal{D}\rangle,\mathcal{D}\rangle$ or not.

This is a more general problem than one which just applies to the
automated coercion algorithm. This is due to the fact that Axiom
believes everything that you say. One may produce a {\bf Category} and
claim that it extends another. Yet if Axiom's {\bf Categories} were
really varieties, specified by theories there would need to be some
way of checking whether the equations in the original {\bf Category}
still held in the new one. Similarly, {\bf Domains} need not model the
theory specifing the variety ({\bf Category}) to which they have been
declared to belong.

The current situation in automated theorem proving means that Axiom
will believe any false assertions like those mentioned in the previous
paragraph. This is not a design flaw of either Axiom or the automated
coercion algorithm; neither Axiom's designers nor I had any choice --
automated theorem proving has not yet advanced sufficiently for us to
take advantage of it. Similarly (and moreover) Axiom will believe that
any function (which one may implement) called {\bf coerce} is a valid
coercion.

This means that in this implementation of the automated coercion
algorithm in the current release of Axiom, it would be possible to
create new ``coercions'' between (potentially) unrelated types or
non-homomorphically between similar types. Using the example from
section \ref{DoyeSec7.5} we could create a ``coercion''
$\mathbb{Z}_5\rightarrow\mathbb{Z}_3$

The present state of automated theorem proving technology with special
reference to Axiom is detailed in \cite{Mart97}. This paper details
recent research and near-future directions for the subject.

Other flaws which will be dealt with next including:
\begin{enumerate}
\item Some Axiom {\bf Categories} do not agree with our notion of
varieties (specified by signatures or theories) (Sections
\ref{DoyeSec9.2}, \ref{DoyeSec9.4}, \ref{DoyeSec9.5})
\item Axiom {\bf Domains} have too much in common with their
{\bf Categories}. There is a lack of distinction between operator
symbols and operators. Similarly between sorts and carriers
(Sections \ref{DoyeSec9.3}, \ref{DoyeSec9.6})
\end{enumerate}

\subsection{Conclusion}
\label{DoyeSec8.8}

We conclude that the automated coercion algorithm is probably
implementable in the Axiom interpreter. It is certainly possible to
create a version which works. However, it would take someone with
deeper knowledge of the code to force a complete integration of the
current implementation with the Axiom interpretation.

\section{Making Axiom algebraically correct}
\label{DoyeChap9}

We will discuss various details which need to be changed to make a
computer algebra system more ``algebraically correct''. By this we
mean that Axiom's {\bf Categories} behave more like our concept of
varieties (specified by order-sorted theories) and that the
{\bf Domains} act more like our notion of order-sorted algebras more
correctly.

\subsection{Explicitly defined theories}
\label{DoyeSec9.2}

In Axiom, the {\bf Categories} were originally intended to be akin to
varieties specified by signatures. The {\bf Domains} were then meant
to be like algebras of those signatures.

The {\bf Categories} do behave and look to the casual observer like
they were specified by multi-sorted algebraic type theories, but there
are some clear differences.

\subsection{Operator symbols and names}
\label{DoyeSec9.3}

In Axiom, if a category defines an operator symbol
$\sigma_{n,q,s}$ then for all domains in that category, that operator
name $\alpha_{n,q,s}$ of $\sigma_{n,q,s}$ will be $\sigma_{n,q,s}$.

It may appear to be not that much of a disadvantage to have such a
restriction on operator names, but an important case in mind is that
of the monoid. A ring, has two binary operators over which it forms a
monoid. The only way in Axiom that we can ensure this is to have two
different monoidal categories. A distinct disadvantage.

If we allowed operator symbols to differ from operator names then
another {\bf Category} could extend {\bf Monoid} in two different
ways.

Syntactically this would be most difficult. We would need a way of
declaring the two operator symbols in {\bf Ring} (traditionally $+$
and $*$) as being from different operator symbols in {\bf Group}
(which extends {\bf Monoid}) and {\bf Monoid}.

For example we could write something like the following

{\bf Ring:Category $==$ with (M:Monoid, G:Group)}

{\bf\quad $--$ Operators from Monoid}\\
\vskip -0.5cm
{\bf\quad $*$: ($*$:(\%,\%) $->$ \%)\$M}\\
\vskip -0.5cm
{\bf\quad 1: (1:() $->$ \%)\$M}\\

{\bf\quad $--$ Operators from Group}\\
\vskip -0.5cm
{\bf\quad $+$: ($*$:(\%,\%) $->$ \%)\$G}\\
\vskip -0.5cm
{\bf\quad 0: (1:() $->$ \%)\$G}\\
\vskip -0.5cm
{\bf\quad inv: (inv:\% $->$ \%)\$G}\\
\vskip -0.5cm
$\ldots$

This would identify $*$ and 1 with the binary operator and identity
element, respectively, from {\bf Monoid}. Whereas $+$, 0, and 
{\bf inv} would be identified with the binary operator, identity
element, and inverse function from {\bf Group}.

Notice that both the operator symbol (e.g. $*$) and both the source
arity (e.g. (\%,\%)) and target (e.g. \%) sort are needed to uniquely
identify the operator. These correspond to $\sigma$, $q$ and $s$
respectively. (we do not need to know $n$ since this is deducible from
$1$).

A function in Axiom (in either a {\bf Domain} or {\bf Category}) is
currently represented by ({\bf name} ({\bf target} {\bf source})
{\bf comments}). Function declarations in {\bf Categories} are
normally of the form\footnote{$s$ and $q$ are reversed in Axiom's
internal representation of a function.}
\[(\sigma~(s~q)~ {\bf comments})\]
and once exported by a ground type (i.e. genuine Axiom
{\bf Domain} with all parameters to the {\bf Domain} fixed) it becomes
\[(\alpha_{n,q,s,\sigma}~(A^s~A^q)~{\bf comments})\]
where $\alpha_{n,q,s,\sigma}=\sigma$.

For the coercion algorithm, we inserted special words in the comments
field to try and mimic an operator symbol.

Another approach might have been to declare other types e.g.
{\bf Constructor(a,b)} which are sub-types of {\bf Mapping(a,b)}.

A far better approach is to extend the information contained within
the function construct. For initial definitions in {\bf Categories} --
that is new operators which have come from this {\bf Category}, not
ones which this {\bf Category} may extend -- then the current
declaration method suffices.

However, as in the above {\bf Ring} example for {\bf Categories} which
extend others then a better method may be
\[(\sigma^\prime~(\sigma~(s~q)~\Sigma)~{\bf comments}\]
where $\sigma^\prime$ is the new operator symbol corresponding to
$\sigma$ a member of $\Sigma_{n,q,s}$ where this signature extends
$\Sigma$.

We have used variable names to avoid confusion. One may wish to define
a {\bf Category} called {\bf DoubleMonoid} where {\bf G} is a
{\bf Monoid} instead of a {\bf Group}. (Obviously no {\bf inv}
operator would be available to such a {\bf Category}.)

The same terminology could also be used to define specific algebras
({\bf Domains}). For example, for all $n$ in $\mathbb{N}$,
$n\mathbb{Z}$ (whose elements is the set
$\{nz~\vert~z\in\mathbb{Z}\})$ is an additive group, whereas
$S(n)$ (the symmetric group on $n$ elements) is usually considered to
be a multiplicative group.

So, for $n\mathbb{Z}$ we might write

{\bf IntegersTimes(n:PositiveInteger):Group $==$}\\

{\bf $+$ : ($*$ : (\%,\%) $->$ \%)}\\
\vskip -0.5cm
{\bf 0 : (1 : () $->$ \%)}\\
\vskip -0.5cm
{\bf - : (inv : \% $->$ \%)}\\
\vskip -0.5cm
\ldots

However for $S(n)$ we might write:

{\bf Symmetricgroup(n:PositiveInteger):Group $==$}\\

{\bf $*$ : ($*$ : (\%,\%) $->$ \%)}\\
\vskip -0.5cm
{\bf () : (1 : () $->$ \%)}\\
\vskip -0.5cm
{\bf inv : (inv : \% $->$ \%)}\\
\vskip -0.5cm
\ldots

In this example we can see the following proposed methodology for
declaring operator names which correspond to operator symbols.
\[(\alpha_{n,q,s,\sigma}~(\sigma~(s~q))~{\bf comments})\]

The signature $\Sigma$ does not need to be mentioned in each function
definition since the algebra is only declared to be a model of one
signature.

This does create more ``unnecessary'' confusion for
{\bf Domain/Category} writers, but correctness should overrule
ease-of-use. It is also conceivable that this will have a negative
effect on the amount of time it takes to compile a {\bf
Domain}. However, this is true of all type-checking compilers.

More inportantly, it should not have any effect on run-time speed of
execution. This should definitely be the case if the internal
representation is moved to something approximating Axiom's current
order.
\[(\alpha_{n,q,s,\sigma}~(s~q)~\sigma~{\bf comments})\]

This puts the target and source closer (and less operations away) from
being discovered.

Clearly, in this proposed methodology the carriers of the same source
and target are not mentioned explicitly in the function
representation.

To speed up function-type look-ups (modemap selection) each 
{\bf Domain} could provide a hash of sorts and carriers. Indeed, then
the internal representation could be
\[(\alpha_{n,q,s,\sigma}~(A^s~A^q)~\sigma~{\bf comments})\]
and then modemap selection would be as fast as in the current version
of Axiom, but the hash could be used (going in the other direction) to
determine $s$ and $q$ for the automated coercion algorithm.

\subsection{Moving certain operators}
\label{DoyeSec9.4}

Another factor which stops Axiom acting totally homomorphically, is
that certain operators appear in {\bf Categories} too far ``up'' the
inheritance lattice. This is best illustrated by an example.

In Axiom, one often would like to be able to convert {\bf Lists} to
{\bf Sets}. (The {\bf Domain} whose items are finite sets (or
actually, sometimes classes), over some particular type is called
{\bf Set} in Axiom.)

{\bf Sets} and {\bf Lists} in Axiom are both certainly finite
collections which can be built by adding in another element at a
time. In {\bf List} this may be achieved using either {\bf cons} or
{\bf append} (though obviously, {\bf cons} is far more efficient)
whereas {\bf Sets} can be built using the (sometimes non-effective)
command {\bf insert}\footnote{This operation inserts a (potentially)
new element into a set. ${\bf insert}(x,s)=s\cup\{x\}=s;x$}.

Axiom knows that {\bf Lists} are sorted whereas {\bf Sets} are
not. The problem, which is immediately obvious, is that adding in a
new element to a {\bf List} will always increase the length of the
{\bf List}. This is not true of (Axiom's finite) {\bf Sets}. They both
however get the same element-counting-operator from the same
{\bf Category}. This function, $\#$ comes from the {\bf Category},
{\bf Aggregate}, which is the most general type of ``collection'' in
Axiom.

{\bf List(S)} (for some fixed {\bf S}) is a model of 
{\bf ListAggregate} and {\bf Set(S)} is a model of
{\bf FiniteSetAggregate}. Both of these theories are extensions of the
theory {\bf Aggregate}.

Therefore if we had a homomorphism, $\phi$ from {\bf Lists} to
{\bf Sets}, then the following equation should hold, yet it clearly
does not,
\[\phi(\#([1,1]))=\#(\phi([1,1]))\]
since the left hand side is,
\[\phi(2)=2\]
and the right hand side is,
\[\#(\{1\})=1\]
and unless the carrier sort of collection length is not
{\bf NonNegativeInteger} but a different type, one in which all
elements are equal, we will not be able to create any homomorphisms
from {\bf List} to {\bf Set}.

In section \ref{DoyeSec7.3} we demanded that if the automated coercion
algorithm was to be applicable (in this case) from {\bf List(S)} to
{\bf Set(S)} then {\bf ListAggregate} would need to be a protecting
extension of {\bf Aggregate}\footnote{Or an extension of
{\bf Aggregate} which both {\bf ListAggregate} and
{\bf FiniteSetAggregate} extends.}.

It is clear that in {\bf ListAggregate} the following equation holds
\[\#({\bf cons}(a,b))=1+\#(b)\]

If this were a protecting extension then this equation would hold in
{\bf Aggregate}. However, as we have already observed.
\[\#({\bf insert}(a,b)\ne 1+\#(b)\]
when $a\in b$.

Thus {\bf ListAggregate} is not a protecting extension of
{\bf Aggregate} and the automated coercion algorithm can not be
applied.

There are three obvious solutions:
\begin{enumerate}
\item Disallow coercions from {\bf List} to {\bf Set}. Although we
would still allow a non-homomorphic {\bf convert} operator
\item Move the element-counting-operation further down the
{\bf Category} inheritance lattice until it does not appear in any
{\bf Categories} to which {\sl both} {\bf List} and {\bf Set}
belong. Or if they do both belong to this {\bf Category}, ensure that
none of the part functions, constants, queries, or constructors are
from this {\bf Category} or any of its ancestors.
\item Move the adding-new-element-operation further down the lattice,
in a similar manner. Thus {\bf cons} and {\bf insert} would not know
anything about each other. In this case the automated coercion
algorithm would still not be applicable
\end{enumerate}

This is merely one example of many such operators which could require
moving.

\subsection{Retyping certain sorts}
\label{DoyeSec9.5}

As in section \ref{DoyeSec9.4}, this is best illustrated by an
example.

In Axiom, the {\bf Category}, {\bf Ring} exports a function,
\[{\bf characteristic:~()~->~NonNegativeInteger}\]
(remember that Axiom does not differentiate between sorts and
carriers). So imagine the natural ring-epimorphism
$\phi:\mathbb{Z}_6\rightarrow\mathbb{Z}_2$. Then the following
equation should hold,
\[{\bf characteristic}(\phi())=\phi({\bf characteristic}())\]
yet clearly this is not the case, since $\phi$ must send the
{\bf Void} sort to {\bf Void}, and thus the left hand side must equal
the characteristic of $\mathbb{Z}_2$, which is 2. Whereas, for the
right hand side, we have $\phi(6)$. This $\phi$ is the natural map
from $\mathbb{N}\cup\{0\}$ to itself. This is, of course, the identity
map, and hence the right hand side has the value 6.

There is clearly something very wrong here. There are two solutions,
the first of which is highly unsatisfactory:
\begin{enumerate}
\item Stop {\bf characteristic} from being a function. One could
persuade it to be an attribute of the {\bf Ring}.
\item Alter the carrier of the return type of {\bf characteristic}
from {\bf NonNegativeInteger} to being a type with the same elements,
but a different idea of equality.
\end{enumerate}

This is one of many such cases in Axiom.

\subsection{Sorts and their order}
\label{DoyeSec9.6}

We have been discussing Axiom's {\bf Category} inheritance system as
if it were a true attempt at modelling order-sorted algebra. There
are, however, two areas which are distinctly missing from the Axiom
model; these are, the sorts and their order.

There is a distinct confusion in Axiom between sorts and carriers. The
author believes that all {\bf Category} definitions (the signatures or
theories which specify varieties) should not use genuine types at any
point in the signatures, or any other point. These should only occur
in the {\bf Domains} of that {\bf Category}.

For example, many {\bf Categories} assume that the only boolean-like
type is {\bf Boolean}. Similarly the types {\bf NonNegativeInteger},
{\bf PositiveInteger}, and {\bf Integer} are often ``assumed'' and are
not parameters of a type.

For the types {\bf Boolean}, {\bf NonNegativeInteger}, and
{\bf PositiveInteger} this is not normally a complete
disadvantage. There are not likely to be other algebras in our system
which behave similarly enough to replace these types.

However, there are coercions between {\bf NonNegativeInteger},
{\bf PositiveInteger}, and {\bf Integer} (in the obvious
directions). This then imposes some order on the sorts of the type
which is not explicitly mentioned.

Some of the sorts of an algebra are defined. \% is always the
principal sort, and any parameters of the {\bf Category} are there,
too. Others, however, are merely mentioned in function prototypes
(signatures). All should be listed in a sort-list (and/or
sort-lattice, see below).

Neglected sorts normally include those which are carried by the four
types mentioned above. However, other types are often neglected
too. These include {\bf List} which is often present so that elements
of the type may be constructed. (The underlying representation of most
types in a Lisp-based system are often lists). More seriously,
particular polynomial representations are sometimes present.

There is no real mechanism built into Axiom to order the sorts of a
signature ({\bf Category}). The order is implicit in
{\bf Categorical} inheritances, such as {\bf CoercibleTo} and
{\bf RetractibleTo}, which give information on how the principal sort
relates to some other sorts.

I believe that a more sensible way would be to declare a lattice like
arrangement with the declaration of a {\bf Category}. Indeed, this
would then make the sort-list clear, too, since this is never
explicitly defined either.

For example, we could add the syntax, {\bf SortOrder} to be used as
follows,

{\bf ACategory(a:A,b:B,$\ldots$):Category $==$}

{\bf\quad with~SortOrder\{}\\
\vskip -0.5cm
{\bf\quad\quad a $<$ \%;}\\
\vskip -0.5cm
{\bf\quad\quad \% $<$ b;}\\
\vskip -0.5cm
{\bf\quad\quad $\ldots$}\\
\vskip -0.5cm
{\bf\quad \}}

{\bf\quad exportedFunction : List A $->$ \%;}

$\ldots$

or (better), {\bf SortOrder} could be a {\bf Category} which could be
defined as (assuming this is compilable\footnote{According to
Peter Broadberry ``One problem is that it may not be possible for the
compiler to figure out exactly what this construct will export'' but
it might still be implementable.}),

{\bf SortOrder(ls:List Pair Type):Category $==$ \{}

{\bf\quad for pr in ls repeat \{}\\
\vskip -0.5cm
{\bf\quad\quad coerce:first pr $->$ second pr;}
\vskip -0.2cm
{\bf\quad \}}
\vskip -0.2cm
{\bf \}}

This {\bf for} loop would not build the entire sort lattice for any
particular category. This would be better done by the compiler at
compile time, on a per-{\bf Category} or per-{\bf Type} basis.

Thus, any {\bf Category} which does not extend {\bf SortOrder} would
be a non-order-sorted signature.

\subsection{Altering Axiom's databases}
\label{DoyeSec9.7}

Axiom has many built in databases for looking up comments, functions,
attributes, etc. from each {\bf Category} or {\bf Domain}. These are
usually text files, with character number keys for faster
cross-referencing.

Axiom could have some extra
databases to aid the automated coercion algorithm. Specifically, there
could be a database containing for each {\bf Category}, a list (of
lists) of special functions exported by that algebra, but not its
ancestors. This could just be also be a compiled fact about each
algebra, but the look-up overhead would be greater.

The restriction on only having functions from that {\bf Category} and
not its ancestors' special functions allows for the dynamic nature of
Axiom's {\bf Category} system. Without this restriction, altering a
{\bf Category} further up the lattice could cause the database
information to become outdated for all of its descendants.

\subsection{Conclusion}
\label{DoyeSec9.8}

Axiom behaves very similarly to a language based on order sorted
theories and the algebras that model them. However, there are some key
areas in which Axiom differs from the mathematical notions.

We summarise these areas in the following table.

\begin{tabular}{| p{6cm} | p{6cm} |}
\hline
{\bf Mathematical Notion} & {\bf Axiom}\\
\hline
\vskip 0.05cm
Operator names need not be the same as operator symbols 
\vskip 0.05cm
&
\vskip 0.05cm
Operator names are always the same as operator symbols
\vskip 0.05cm\\
\hline
\vskip 0.05cm
Coercions are homomorphisms and hence act homomorphically on all operators
\vskip 0.05cm
&
\vskip 0.05cm
Coercions need not act like homomorphisms at all. (If it were
possible to implement a universal equation checker then Axiom
would be alright)
\vskip 0.05cm\\
\hline
\vskip 0.05cm
A signature depends on its sort-list, and hence a sort-list is part of
its definition
\vskip 0.05cm
&
\vskip 0.05cm
Category definitions do not explicitly list their sorts
\vskip 0.05cm\\
\hline
\vskip 0.05cm
No algebra are mentioned in a signature definition, only sorts
\vskip 0.05cm
&
\vskip 0.05cm
Category definitions do depend on specific Domains
\vskip 0.05cm\\
\hline
\vskip 0.05cm
The order on the sorts is part of the definition of a signature
\vskip 0.05cm
&
\vskip 0.05cm
Category definitions do not explictly order their sorts
\vskip 0.05cm\\
\hline
\end{tabular}

We have presented methods for addressing all of these differences.

\section{Conclusions}
\label{DoyeChap10}

\subsection{Summary}
\label{DoyeSec10.2}

\subsubsection{Category theory and order sorted algebras as the bases for
sound strongly typed computer algebra systems}
\label{Doye10.2.1}

We have shown why both category theory and order sorted algebra both
provide solid models for the type systems found in computer algebra.

In section \ref{DoyeSec3.3} we have demonstrated
the correlation between a computer algebra type system and category
theory.

In section \ref{DoyeSec4.7} and \ref{DoyeSec5.2} we have shown how a
computer algebra type system correlates with order sorted algebra. In
section \ref{DoyeSec5.3} we extended the notion of order sorted
signatures to cover conditional signatures which occur in Axiom.

We have mentioned some of the correspondence between category theory
and order sorted algebra (section \ref{DoyeSec5.4}). We have usually
used order sorted algebra as a model. This is because order sorted
algebra more readily corresponds to the algebraic inheritance
mechanism. This is mainly due to the order on the sorts which is not
available in category theory. Also category theory has more difficulty
expressing higher order polymorphism.

\subsubsection{Representation and syntax issues of an order sorted
algebra based type system}
\label{Doye10.2.2}
In section \ref{DoyeChap9} we demonstrated various methodologies for
extending Axiom's current type system so that it may more closely
model order sorted signatures and algebra.

In section \ref{DoyeSec9.3} we showed how Axiom's syntax may be
extended so that a signature may extend another more than
once\footnote{Thus thsi solves an interesting class of multiple
inheritance problem.}. We also showed that this syntax could be used
to uniquely identify operator names with operator symbols. This then
allows operator symbols to differ from operator names.

In section \ref{DoyeSec9.6} we discussed how both the sort list and
the order on the sorts could be introduced to Axiom's signatures.

Section \ref{DoyeSec9.4} commented on how the
Axiom signature tree may be altered to allow coercions to act more
homomorphically.

Finally, we discussed how Axiom signatures could be compiled to
contain extra information which would enhance the speed of the
automated coercion algorithm (section \ref{DoyeSec9.7}).

\subsubsection{On coherence}
\label{Doye10.2.3}

In section \ref{DoyeChap6} we first stated all the mathematics used by
Weber to state and ``prove'' Weber's coherence conjuecture
\ref{Doye6.3.7}.

In section \ref{DoyeSec6.4} we then showed that this ``proof'' is not
correct. We showed that altering a definition and adding a couple new
assumptions that let us prove the coercion theorem \ref{Doye6.4.7}.

In section \ref{DoyeSec6.5} we then showed that it was possible to
relax one of the assumptions which Weber made, and still have a
coherent type system. (The extended coherence theorem \ref{Doye6.5.4})

\subsubsection{The automated coercion algorithm}
\label{Doye10.2.4}
In section \ref{DoyeSec7.2}
we provided enough mathematics to show that the automated coercion
algorithm \ref{Doye7.4.1} (section \ref{DoyeSec7.4}) is an algorithm
which returns a function which is not only a homomorphism, but a
coercion which we defined in definition \ref{Doye5.5.2}.

This homomorphism is unique in a coherent type system, which we can
guarantee providing we can satisfy all the assumptions of the extended
coercion theorem \ref{Doye6.5.4}. The homomorphism created may be
built from the four basic types of coercion given in the statement of
that theorem.

In section \ref{DoyeChap8} we showed that a demonstration
implementation of this algorith is possible in Axiom.

\subsection{Future work and extensions}
\label{DoyeSec10.3}
We have presented in this thesis the basis for a mathematically sound
computer algebra system. We have also shown that it would be possible
to implement the automated coercion algorithm in such a system.

At present Axiom comes close but is not fully conforming.

We have shown how Axiom's syntax could be extended to allow it to
model an order sorted algebra system.

Axiom's categories are not always abstract -- that is they may contain
implementations of functions\footnote{The ``not equals'' fuction is
defined to be ``not(equals())''}. This is a good thing as it enforces
certain truths about a particular function.

Forcing any important facts by making a class which implements a
particular inferface would then ruin any chance of re-attaining
multiple inheritance.

\chapter[Symmetries of Partial Differential Equations]
{Symmetries of Partial Differential Equations by Fritz Schwarz}

This chapter is based on Schwarz \cite{Schw87} ``Programming with
Abstract Data Types: The Symmetry Package SPDE in Scratchpad''.

\section{Symmetries of Differential Equations and the Scratchpad Package SPDE}

Symmetry analysis is the only systematic way to obtain solutions of
differential equations. Yet it is rarely applied for that purpose and
most textbooks do not even mention it. The reason is probably the
enormous amount of calculations which is usually involved in obtaining
the symmetry group of a given differential equation. Therefore the
Scratchpad package SPDE which stands for Symmetries of Partial
Differential Equations has been developed which returns the complete
symmetry group for a wide class of differential equations
automatically. As far as the mathematics is
concerned, only those formulas are given which are a prerequisite for
the main topic mentioned. The details and many examples may be found
in the recent review article on that subject by the author [14]. 

We consider the most general case of a system of differential
equations for an arbitrary number $m$ of the unknown functions
$u^\alpha$ which may depend on $n$ arguments $x_i$. These variables
are collectively denoted by $u=(u^1,\ldots,u^m)$ and
$x=(x_1,\ldots,x_n)$ respectively. We write the system of $N$
differential equations in the form
\[w_\nu(x_i,u^\alpha,u_i^\alpha,u_{ij}^\alpha,\ldots,
u_{i_1\ldots i_k}^\alpha)=0\eqno{(1)}\]
for $\nu=1\ldots N$ where the notation
\[u_{i_1\ldots i_n}^\alpha=\frac{\partial^{i_1+\ldots i_n}u^\alpha}
{\partial x_1^{i_1}\ldots \partial x_n^{i_n}}\]
for derivatives has been used. Furthermore it is assumed that the
equations (1) are polynomial in all arguments. For $m=n=N=1$ a single
ordinary differential equation is obtained.

To formulate the condition for the invariance of (1), the
infinitesimal generator $U$ is defined by
\[U=\xi_i\frac{\partial}{\partial x_i}
+\eta^\alpha\frac{\partial}{\partial u^\alpha}\eqno{(2)}\]
where $\xi_i$ and $\eta^\alpha$ may depend on all dependent and
independent variables. Summation over twice occuring indices is always
assumed. Greek indices run from 1 to $m$ and latin indices from 1 to
$n$. The $k$-th prolongation of $U$ is defined as
\[U^{(k)}=U+\zeta_i^\alpha\frac{\partial}{\partial u_i^\alpha}+
\ldots+\zeta_{i_1\ldots i_k}^\alpha
\frac{\partial}{\partial u_{i_1\ldots i_k}^\alpha}\eqno{(3)}\]
where the functions $\zeta_{i_1,\ldots i_k}^\alpha$ describe the
transformation of partial derivatives of order $k$. The $\zeta$'s
satisfy the recursion relations
\[\zeta_i^\alpha=D_i(\eta^\alpha)-u_s^\alpha D_i(\xi_s)\eqno{(4)}\]
and
\[\zeta_{i_1,\ldots i_k}^\alpha=D_{i_k}(\zeta_{i_1\ldots i_{k-1}}^\alpha)
-u_{i_1,\ldots i_{k-1},s}^\alpha D_{i_k}(\xi_s)\eqno{(5)}\]
\[D_i=\frac{\partial}{\partial x_i} 
+ u_i^\alpha\frac{\partial}{\partial u^\alpha}
+ u_{ki}^\alpha\frac{\partial}{\partial u_k^\alpha}
+ u_{kli}^\alpha\frac{\partial}{\partial
u_{kl}^\alpha}\ldots\eqno{(6)}\]
is the operator of total derivation with respect to $x_i$. The system
of differential equations (1) is invariant under the transformations
of a one-parameter group with the infinitesimal generator (2) if the
$\xi$'s and $\eta$'s are determined from the conditions
\[U^{(k)}\omega_\nu=0\quad{\rm when\ all\ }\omega_\nu=0.\eqno{(7)}\]

Under the constraints for the $\omega_\nu$ which have been mentioned
above, the left hand side of (7) is a polynomial in all its
variables. Because the derivatives of the $u$'s do not occur as
arguments of the $\xi$'s and the $\eta$'s, it has to be decomposed with
respect to these derivatives and the coefficients are equated to
zero. The resulting set of equations is the determining system the
general solution of which determines the full symmetry group of
(1). Starting from a certainset of simplification rules, a solution
algorithm has been designed which is described in detail in a separate
article \cite{Schw85}. The implementation of this algorithm forms the main
part of the package SPDE wich comprises about 1500 lines of Scratchpad
code.

Due to its size, the crucial part of the implementation is to identify
a set of datatypes such that a modularization is obtained. This is not
a single step process but involves a lot of trial and error and also
some backtracking. The basic building block for these new datatypes in
the Scratchpad domain SMP(R,VarSet) which abbreviates Sparse
Multivariate Polynomial in the VarSet over a ring R. The latter may be
e.g. the integers, the rational numbers or another polynomial ring
over some set of variables. There are three basic variables
distinguished which occur in equations (1) to (7). These are the $x_i$
and $u^\alpha$, the derivatives $u_{i_1,\ldots i_k}^\alpha$ and the
differential operaotrs ${\partial}/{\partial x_i}$ and
${\partial}/{\partial u^\alpha}$. They are represented in
Scratchpad Symbols of the type DEVAR, DER, and DO
respectively. Furthermore there are the $\xi$'s and the $\eta$'s
together with the $c_k$'s which are introduced by the solution
algorithm. These variables of the type LDFV are also Scratchpad
Symbols. However they are special in the sense that they carry
dependencies with them which may change while the solution algorithm
proceeds. The bookkeeping for these dependencies is organized in terms
of a Scratchpad association list. For reasons that will become clear
soon it is advantageous to introduce still another kind of variables
of type DK which represent the derivatives of the previously introduced
variables LDFV. They do not correspond straightforwardly to a
Scratchpad system type.

Out of these variables all quantities which occur may be built up in
terms of SMP's as follows. The differential equations themselves are
considered as polynomials in the derivatives of
$u_{i_1,\ldots i_k}^\alpha$ with coefficients which are polynomials in
the $x_i$ and $u^\alpha$ over the rationals, i.e. their type is
SMP(SMP(RN,DEV),DER). The $\zeta$'s are linear polynomials in the
$\xi$'s, the $\eta$'s and derivatives thereof with coefficients
which are polynomials in the derivates $u_{i_1,\ldots i_k}^\alpha$
over the integers, i.e. the appropriate type is
SMP(SMP(I,DER),DK). The equations of the determining system are
obtained by decomposing the left hand side of (7) with respect to the
derivatives $u_{i_1,\ldots i_k}^\alpha$. The resulting equations of
the determining system are linear polynomials in the DK's with
coefficients which are polynomials in the variables $x_i$ and
$u^\alpha$ over the rational numbers. They are denoted by the new type
LDF. The symmetry generators which are obtained from the solution of
the determining system are linear polynomials in the differential
operators $\partial/\partial x_i$ and $\partial/\partial
u^\alpha$. Depending on whether or not there is a functional
dependency involved in the final solution their coefficients are LDF's
or polynomials in the DEV's over the rational numbers respectively. So
the two kinds of generators are SMP(LDF,DO)'s or SMP(SMP(RN,DEV),DO)'s
for which the two type CSG and DSG respectively are introduced. The
complete set of domains of the symmetry package SPDE is listed below
where the full names are also given.

\begin{tabular}{|c|c|c|}
\hline
Abbreviation & Full Name & Scratchpad Datatype\\
\hline
SPDE & SymmetricPartialDifferentialEquation & Package\\
\hline
CSG & ContinuousSymmetryGenerator & SMP(LDF,DO)\\
\hline
DSG & DiscreteSymmetryGenerator & SMP(SMP(RN,DEV),DO)\\
\hline
DS & DeterminingSystem & List List LDF\\
\hline
LDF & LinearDifferentialForm & SMP(SMP(RN,DEV),DK)\\
\hline
DK & DifferentialKernel & New Domain\\
\hline
LDFV & LDFVariable & Symbol\\
\hline
DE & DifferentialEquation & SMP(SMP(RN,DEV),DER)\\
\hline
DER & Derivative & Symbol\\
\hline
DO & DifferentialOperator & Symbol\\
\hline
DEV & DEVariable & Symbol\\
\hline
\end{tabular}

An abstract data type is realized in Scratchpad in terms of a domain
constructor. As an example in the following figure the specification
of the domain DifferentialKernel is shown. According to the principles
of Scratchpad, there is a public- or category part Cat and a
private part Tar. The category part Cat defines the outside view. It
consists of the syntax specification for the exported functions in
terms of its modemaps and the semantic part in which the meaning of
these functions is specified. A modemap for a function is a statement
which determines the number and the types of its arguments and the
type of the object it returns. Instead of a so called axiomatic or
algebraic specification a concise and precise description of the
action of each function in plain English is preferred. It is included
as a comment in the domain constructor. Analogously the private part
Tar specifies the syntax and the semantics of the internal
functions. The difference between the public- and the private part
should be noted. In the former there is no mention whatsoever of the
internal representation of these objects in terms of certain records.
The semantic specification is mostly given in mathematical terms. On
the contrary, in the private part the internal representation of these
quantities is established. The terms which are used in its
specification are typical for the Scratchpad system.

The function randDK is a random generator for DK's. Its two arguments
specify the values of rn and n. It works according to the following
algorithm. At first a variable of the type LDFV is created by calling
the random generator from the corresponding domain LDFV. Then a random
integer between 0 and 5 is generated which specifies an upper bound
for the total order of the kernel to be returned. Finally in a loop
the derivatives with respect to the various arguments are determined
by generating random integers between 0 and 5. The loop terminates if
the total order is exceeded. In this way DK's are obtained which cover
fairly uniformly the parameter space which is expected to be relevant
for applications of the full package, including special cases like
e.g. 0-th order derivatives. This random generator for DK's is called
by the test program testDK and by test programs for other domains like
e.g. LDF. The details of this testing process will be discussed later
in this Section. The domain constructors for the other datatypes are
similarly organized. The reason for choosing DK as an example has been
that it is short enough to be reproduced on a single page but still
contains all the relevant details.

\begin{verbatim}
)abbreviate domain DK DifferentialKernel
DifferenetialKernel: Cat == Tar where
  I ==> Integer
  DEV ==> DEVariable
  LDFV ==> LDFVariable
  VAR ==> Record(Var: DEV, Ord: Integer)
  Cat == OrderedSet with
    funDK: $ -> LDFV            -- function argument
    varDK: $ -> List DEV        -- derivative variables
    zeroDK: $ -> Boolean        -- true if derivative is 0
    newKD: LDFV -> $            -- creates DK of 0th order from argument
    difDK: ($,DEV) -> $         -- derivative w.r.t 2nd argument
    intDK: ($,DEV) -> $         -- integration w.r.t 2nd argument
    ordDK: $ -> I               -- total order of derivative
    ordDK: ($,DEV) -> I         -- order w.r.t 2nd argument
    oneDK: List $ -> LIst $     -- list elements occuring once
    randDK: (I,I) -> $          -- generates random DK
    testDK: (I,I,I,I) -> Void   -- test program
    coerce: $ -> E              -- print function
  Tar == add
    Rep := Record(fn: LDFV, args: List VAR)
    mkDfL: (List VAR,DEV) -> List VAR
        -- creates record VAR for derivative
    mkIntL: (List VAR,DEV) -> List Var
        -- creates record VAR for integral
    creDK: (LDFV,List VAR) -> $
        -- creates DK from LDFV and record VAR
    VarDK: $ -> List VAR
        -- returns record VAR of argument
\end{verbatim}

After the various modules which build up the full package SPDE have
been established~ their mutual relations have to be investigated. All
dependencies between the modules are most clearly seen from the
structure chart which is shown in Figure 4. It makes obvious the
hierarchical order between the various modules which is based on the
datatypes. The tree-like appearance reflects the most valuable feature
of the design, i.e. the partial independence among the modules. For
example, those at the bottom which belong to the level of symbols and
kernels are almost completely independent from each other. The same is
true at the next level of the SMP~s. Only at the uppermost level a
strong interconneetion is established among the modules of the full
package due to the operations of the module SPDE. This is not
surprising however since it is the task of that latter module to
organize the cooperation within the package. This becomes clear
already from the fact that SPDE is a Scratchpad package constructor
whereas all other modules are domain constructors. To emphasize this
significant difference~ the interconnections between modules have been
marked with heavy ~nes whereas for all dependencies on the package
constructor SPDE thin lines are applied.

\chapter{Primality Testing Revisited by James Davenport}

This is from Davenport \cite{Dave92}

It is customary in computer algebra to use the algorithm presented 
by Rabin \cite{Rabi80} to determine if numbers are prime (and primes
are needed throughout algebraic algorithms). As is well known, a
single iteration of Rabin's algorithm, applied to the number $N$, has
probability at most of 0.25 of reporting ``$N$ is probably prime'',
when in fact $N$ is composite. For most $N$, the probability is much
less than 0.25. Here, ``probability'' refers to the fact hat Rabin's
algorithm begins with the choice of a ``random'' seed $x$, not
congruent to 0 modulo $N$. In practice, however, true randomness is
hard to achieve, and computer algebra systems often use a fixed set of
$x$ -- for example, Axiom release 1 uses the set
\[\{3,4,7,11,13,17,19,23,29,31\}\eqno{(1)}\]
As Pomerance {\sl et al.} \cite{Pome80} point out, there is some
sense in using primes as the $x$-values: for example the value $x=4$
gives no more information than the value $x=2$, and the value $x=6$
can only give more information than 2 or 3 under rare circumstances
(in particular, we need the 2-part of the orders of 2 and 3 to differ,
but be adjacent). By Rabin's theorem, a group-theoretic proof of which
is given in Davenport and Smith \cite{Dave87}, 10 elements in the set
gives a probability less than 1 in $10^6$ of giving the wrong
answer. In fact, it is possible to do rather better than this: for
example Damgard and Landrock \cite{Damg91} show that, for 256-bit
integers, six tests give a probability of less than $2^{-51}$ of
giving the wrong answer.

Nevertheless, given any such fixed set of $x$ values, there are
probably some composite $N$ for which all the $x$ in the set report
``$N$ is probably prime''. In particluar Jaeschke \cite{Jaes91}
reports that the 29-digit number
\[56897193526942024370326972321=\\
137716125329053\cdot 413148375987157\]
has this property for the set (1). For brevity, let us call this
number $J$ -- ``the Jaeschke number'', and its factors $J_1$ and $J_2$
respectively. Now 
\[J=1+2^5\cdot 1778037297716938261572717885\]
so Rabin's algorithm will begin by raising each element of (1) to the
power 1778037297716938261572717885 (modulo $J$), thus getting
\[\begin{array}{rcccr}
3 & \rightarrow & 1 
& \stackrel{\text{squaring}}{\rightarrow} & 1\\
5 & \rightarrow & 4199061068131012714084074012
& \stackrel{\text{squaring}}{\rightarrow} & J-1\\
7 & \rightarrow & 40249683417692701270027867121
& \stackrel{\text{squaring}}{\rightarrow} & J-1\\
11 & \rightarrow & 40249683417692701270027867121
& \stackrel{\text{squaring}}{\rightarrow} & J-1\\
13 & \rightarrow & 52698132458811011656242898309
& \stackrel{\text{squaring}}{\rightarrow} & J-1\\
17 & \rightarrow & 4199061068131012714084074012
& \stackrel{\text{squaring}}{\rightarrow} & J-1\\
19 & \rightarrow & 40249683417692701270027867121
& \stackrel{\text{squaring}}{\rightarrow} & J-1\\
23 & \rightarrow & 16647510109249323100299105200
& \stackrel{\text{squaring}}{\rightarrow} & J-1\\
29 & \rightarrow & 40249683417692701270027867121
& \stackrel{\text{squaring}}{\rightarrow} & J-1\\
31 & \rightarrow & 1 
& \stackrel{\text{squaring}}{\rightarrow} & 1\\
\end{array}\]
Hence, for all these $x$-values, Rabin's algorithm will say ``$J$ is
probably prime'', since we arrive at a value of 1 in our repeated
squaring, either directly ($x=3$ and $x=31$) or via $J-1$. However,
this table indicates (to the suspicious human eye) two things.

(A) The first is that $-1$ appears to have four square roots modulo
$J$, viz.
\[4199061068131012714084074012\]
\[40249683417692701270027867121\]
\[16647510109249323100299105200\]
\[52698132458811011656242898309\]
This contradicts Lagrange's theorem, so $J$ cannot be prime.

(B) The second is that, if $J$ were prime, we would expect about half
of the elements of (1) to be quadratic non-residues, and hence to need
five squarings to reach 1 (4 to reach $J-1$), about a quarter to be
quadratic residues, but quartic non-residues, hence needing three
squarings to reach $J-1$, and only an eighth to be octic residues or
better, and to reach $J-1$ in at most one squaring. Hence, if $J$ were
prime, we have observed an event of probability $(1/8)^{10}$ -- less
than 1 in $10^9$.

Much of thie paper is taken up with a detailed exploration of these
observations and their generalisations. We observe that, at least in
principle, we are only concerned with the problem of testing
relatively large numbers: numbers less than $25\cdot 10^9$ are covered
by Pomerance {\sl et al.} \cite{Pome80}.

\section{Rabin revisited}

Throughout this paper, we assume that all integers to be tested for
primality are positive and odd. We use the standard notation
\[\phi(n)=\vert\{x:0<x\le n; \text{gcd}(x,n)=1\}\vert\]
from which the Chinese Remainder Theorem gives (here and always, we
assume in such formulae that the $p_i$ are distinct primes)
\[\phi(\prod p_i^{\alpha_i})=\prod p_i^{\alpha_{i-1}}(p_i-1)\]

In addition, we introduce
\[\hat{\phi}(\prod p_i^{\alpha_i})=\text{lcm}(p_i^{\alpha_{i-1}}(p_i-1))\]
Clearly $\hat{\phi}(n)~|~\phi(n)$

The Fermat-Euler Theorem states that, if $\text{gcd}(x,n)=1$, then
$x^{\phi(n)}\equiv 1~(\text{mod}~n)$. This leads to what might be
called the Fermat primality test: pick some 
$x\not\equiv 0~(\text{mod}~n)$ and compute 
$x^{n-1}~(\text{mod}~n)$. If this is not 1, then $n-1\ne \phi(n)$, so
$n$ cannot be prime. If 
$x^{n-1}\equiv 1~(\text{mod}~n)$, but $n$ is not prime, we say that
$n$ is a pseudoprime($x$). All composite numbers are pseudoprime(1).

However, the Chinese Remainder Theorem implies a stronger result than
the Fermat-Euler Theorem, viz. the following.

{\bf Lemma 1.} {\sl if $gcd(x,n)=1$, then 
$x^{\phi(n)}\equiv 1~(\text{mod}~n)$. Furthermore, $\hat{\phi}(n)$ is
minimal with this property.}

A non-prime number $N$ for which $\hat{\phi}(N)~|~N-1$ is called a
Carmichael number. Any Carmichael number has to have at least three
prime factors. (If $pq$ were a Carmichael number, then
$pq\equiv 1~(\text{mod}~p-1)$, so 
$q\equiv 1~(\text{mod}~p-1)$ and $q\ge p$. Similarly, 
$p\equiv 1~(\text{mod}~q-1)$ and $p\ge q$. So $p=q$, but
$\hat{\phi}(p^2)=p(p-1)$, which can never divide $p^2-1$.) These
numbers, of which we now know that there are infinitely many 
[Alford {\sl et al.} \cite{Alfo92}], are the bane of the Fermat
primality test, since, unless we hit on an $x$ with
$\text{gcd}(x,n)\not\equiv 1$, we will always have 
$x^{N-1}\equiv 1~(\text{mod}~N)$.

Hence we need a stronger test: Rabin's test, which is finer than the
Fermat test since, instead of computing $x^{N-1}$, it writes
$N-1=2^k-l$ with $l$ odd, and then considers each of 
$x^1, x^{2l}, \ldots, x^{2^kl}$, (each obtained by squaring the
previous one, and all computed modulo $N$). If the last is not 1, we
have a non-prime by the Fermat test. If the first is 1 or $N-1$, we
know nothing and say ``N is probably prime''. If, however, the first 1
is preceded by a number other than $N-1$, we can assert that $N$ is
definitely composite, since we have found a square root of unity other
than 1 and $N-1$.

Another way of seeing the difference between Rabin's test and the
Fermat test is to say that we are analysing the 2-part of the order of
$x$ modulo $N$ more carefully. We reply ``$N$ is definitely not
prime'' if the order of $x$ has different 2-parts modulo different
factors of $N$.

Our starting code for Axiom's implementation (slightly modified from
that distributed with Axiom release 1, in particular to split out the
auxiliary function {\tt rabinProvesComposite}, but using the same
algorithm) is 
\begin{verbatim}
                         Original Code

[ 1] prime? n ==
[ 2]    n < two => false
[ 3]    n < nextSmallPrime => member?(n, smallPrimes)
[ 4]    not one? gcd(n, productSmallPrimes) => false
[ 5]    n < nextSmallPrimeSquared => true
[ 6]    nm1:=n-1
[ 7]    q := (nm1) quo two
[ 8]    for k in 1.. while not odd? q repeat q := q quo two
[ 9]    -- q = (n-1) quo 2**k for largest possible k
[10]    mn := minIndex smallPrimes
[11]    for i in mn+1..mn+10 repeat
[12]      rabinProvesComposit(smallPrimes i,n,nm1,q,k) => return false
[13]    true
[14]
[15] rabinProvesComposite(p,n,mn1,q,k) ==
[16]    -- nm1 = n-1 = q*2**k; q odd
[17]    -- probability false for n composite is < 1/4
[18]    -- for most n this probability is much less than 1/4
[19]    t := powmod(p, q, n)
[20]    -- neither of these cases tells us anything
[21]    if not (one? t or t = nm1) then
[22]       for j in 1..k-1 repeat
[23]           t := mulmod(t, t, n)
[24]           one? t => return true
[25]           -- we have squared somethiong not -1 and got 1
[26]           t = mn1 =>
[27]               leave
[28]       not (t = nm1) => return true
[29]    false
\end{verbatim}

where we have numbered the lines for ease of refernce. {\bf I} is the
datatype of $n$, and can be thought of as being the integers. 
{\tt smallPrimes} is a list of primes up to 313, and 
{\tt nextSmallPrime} is therefore 317.

\section{Non-square-free numbers}

If Rabin's algorithm is handed a number $N$ with a repeated prime
factor $p^k$, then the factor of $p^{k-1}$ in $\hat{\phi}(N)$ will
certainly be coprime to $N-1$. This means that we will return ``$N$ is
definitely not prime'' unless we use an $x$-value which is actually a
perfect $p^{k-1}$-st power -- an event with probability
$1/p^{k-1}$. This probability is less than 0.25 except in the case
$p=3$, $k=2$, when we can calculate explicitly that the probability of
incorrectly saying ``$N$ is probably prime'' is exactly 0.25 in the
case $N=9$.

In the implementation given above, then test at line [ 4] ensures that
$N$ has no factors less than 317, and, {\sl a fortiori}, no such
repeated factors. Hence the probability that an $x$-value would be a
perfect $p$-th power is at most 1/317. This compares favourably with
some of the probabilities that will be analysed later, and shows the
practical utility of this preliminary test.

\section{Jaeschke analysed}

Let us analyse the number $J$ more closely. To begin with, both $J_1$
and $J_2$ are prime. These numbers can be written as
\[J_1=1+2^2\cdot 3^2\cdot 829\cdot 4614533083\]
\[J_2=1+2^2\cdot 3^3\cdot 829\cdot 4614533083\]
\[J=1+2^5\cdot 3^2\cdot 5\cdot 11\cdot 59\cdot 829\cdot 34849\cdot
456679\cdot 4614533083\]

$J$ is not a Carmichael number, but it is ``fairly close'', since it
is only the factor of $3^3$, rather than $3^2$, in $J_2-1$ which
prevents it from being so. In addition, $J$ is a product of two
primes, of the form $(K+1)\cdot (rK+1)$ (with $r=3$) -- a form
observed by Pomerance {\sl et al.} \cite{Pome80} to account for
nearly all pseudoprimes.

Why does Rabin's test (using the primes (1)) think that $J$ is prime?
To begin with, all the primes in the set (1) are actually perfect
cubes modulo $J_2$, so their orders divide $(J_2-1)/3$, and hence
$J-1$. Put another way, $J$ is a pseudoprime($p$) for all the $p$ in
(1): these 10 primes all cause the Fermat test to be
satisfied. Assuming that $3~|~p-1$, 1/3 of non-zero congruenece
classes are perfect cubes modulo $p$.

For $J$ to pass Rabin's test, we must also ensure that, for every $p$
in (1), the 2-part of the order of $p$ modulo $J_1$ is equal to the
2-part of the order of $p$ modulo $J_2$. 3 and 3l are both quadratic
residues modulo both $J_1$ and $J_2$, whilst the other primes are all
non-residues. For the non-residues, the 2-part is maximal, viz $2^2$
modulo both these factors, so these eight primes all cause $J$ to pass
Rabin's test, as well as Fermat's. 3 and 3l are, in fact, not only
quadratic residues, but also quartic residues for both $J_1$ and
$J_2$, so their orders have 2-part $2^0$, and hence also cause $J$ to
pass Rabin's test.

Since $J_2\equiv J_1\equiv 1~(\text{mod}~4)$, the quadratic character 
$(a~|~J_i)=(J_i~|~a)$, and so depends only on the value of 
$J_i~(\text{mod}~a)$ (in general, one might have to work modulo
$4a$). $J_2=3J_1-2$, so the two are not independent, but we would
expect 1/4 of congruence classes of $J_1~(\text{mod}~a)$ to make $a$ a
non-residue for both $J_1$ and $J_2$. Another 1/4 would have $a$ a
quadratic residue for both, but it would then be necessary to
investigate quartic properties, and so on. For a given $a$,
asymptotically, about 1/3 of the values of $J_1$ will arrange that the
quadratic, quartic, octic etc. characters of $a$ module $J_1$ and
$J_2$ are compatible with passing Rabin's tightening of the Fermat
test. 

What are the implications of this for an $n$-step Rabin algorithm, if
our opponent, the person who is trying to find a composite $N$ such
that our use of Rabin's algorithm says ``$N$ is probably prime'',
chooses $N=M_1\cdot M_2$, with $M_1$, $M_2$ prime and $M_2-1=3(M_1-1)$
(and hence $M_1\equiv 1~(\text{mod}~3)$, otherwise $x=3$ will fail
Rabin's test)? Each prime $p$ we use forces the condition that $p$
should be a perfect cube module $M_2$ -- satisfied about 1/3 of the
time. In addition, the quadratic characters of $p$ module $M_1$ and
$M_2$ must be compatible -- at best, with 
$M_1-1\equiv 2~(\text{mod}~4)$, this happens 1/3 of the time on
average. Hence each $p$ we use imposes constraints satisfied about 1/9
of the time (assuming independence, which seems in practice to be the
case). So we might expect to find a ``rogue'' number with $M_1$ about 
$9^n$, and so $N$ is about $9^{2n}$, which is $10^{19}$ if
$n=10$. However, we also have to insist that $M_1$ and $M_2$ are prime,
which reduces our chance of finding a rogue pair quite considerably --
roughly by 1/22 for each of $M_1$ and $M_2$, which would give us an
estimated ``time to find a rogue value'' of $5\cdot 10^{21}$. We can,
in fact, be surprised that $J$ is as large as it is -- perhaps a
smaller value exists.

\section{Roots of $-1$}

Here we look at observation (A) above -- that a suspicious human being
would observe more than two square roots of $-1$, and hence deduce
that $J$ was not prime, irrespective of the details of Rabin's
algorithm. This is certainly true -- how programmable, and how widely
applicable, is it? A global (to {\tt prime?} and 
{\tt rabinProvesComposite}) variable {\tt rootsMinus1} is added, whose
type is a {\tt Set} of {\tt I}. The code now reads:
\begin{verbatim}
                         "Roots of -1" Modifications

[ 1] prime? n ==
[ 2]    n < two => false
[ 3]    n < nextSmallPrime => member?(n, smallPrimes)
[ 4]    not one? gcd(n, productSmallPrimes) => false
[ 5]    n < nextSmallPrimeSquared => true
[ 6]    nm1:=n-1
[ 7]    q := (nm1) quo two
[ 8]    for k in 1.. while not odd? q repeat q := q quo two
[ 9]    -- q = (n-1) quo 2**k for largest possible k
[10]    mn := minIndex smallPrimes
[10g]   rootsMinus1 := [] -- the empty set
[11]    for i in mn+1..mn+10 repeat
[12]      rabinProvesComposit(smallPrimes i,n,nm1,q,k) => return false
[13]    true
[14]
[15] rabinProvesComposite(p,n,mn1,q,k) ==
[16]    -- nm1 = n-1 = q*2**k; q odd
[17]    -- probability false for n composite is < 1/4
[18]    -- for most n this probability is much less than 1/4
[19]    t := powmod(p, q, n)
[20]    -- neither of these cases tells us anything
[21]    if not (one? t or t = nm1) then
[22]       for j in 1..k-1 repeat
[22a]          oldt := t
[23]           t := mulmod(t, t, n)
[24]           one? t => return true
[25]           -- we have squared somethiong not -1 and got 1
[26]           t = mn1 =>
[26a]              rootsMinus1:=union(rootsMinus1,oldt)
[26b]              # rootsMinus1 > 2 => return true
[27]               leave
[28]       not (t = nm1) => return true
[29]    false
\end{verbatim}

These changes certainly stop the algorithm from returning ``N is
probably prime'' on the Jaeschke number, and do not otherwise alter
the correctness of the algorithm, so might as well be
incorporated. They only take effect when $k>1$, since only then is the
loop at [22] onwards executed.

If $k>1$ then these changes certainly may be executed. But if all the
prime factors $p_i$ of $N$ have small 2-part in $\phi(p_i)$, in
particular if the 2-part of $\hat{\phi}(N)=2^1$, then these changes
will not take effect (but those proposed in the next section will). In
general it is hard to analyse the precise contribution of these
changes, other than to be certain that it is never negative.

\section{The ``maximial 2-part'' test}

Here we attempt to generalise observation (B) above. Let us suppose
that $N$ is still the composite number that we wish to prove is
composite, and that $N=\prod_{i=1}^n p_i$ with the $p_i$
distinct. Write $N=1+2^kl$ with $l$ odd, and $p_i=1+2^{k_i}l_i$, with
$l_i$ odd. Clearly $k \ge \text{min}_i~k_i$, if $N$ were prime, we
would know that half the residue classes modulo $N$ were quadratic
non-residue, and hence we would expect half the $x$-values chosen to
have 2-order $k$. Conversely, if all the $k_i$ were equal to each
other and to $k$, we would expect $X$ to be a quadratic non-residue
about half the time {\sl with respect to each $p_i$}, and so about 1
in $2^n$ of the $x$-values will have maximal 2-rank.

One very simple variant on this test that can be imposed is to insist
that, before deciding that ``$N$ is probably prime'', we actually
observe an element of 2-order $k$. If $N$ actually were prime, we
would have a chance of 1023/1024 of observing this before finishing
the loop starting on line [11], so this test is extremely unlikely to
slow down the performance of the system on primes. On non-primes, it
may slow us down, but increases the change of our giving the
``correct'' answer.

We need a global (to {\tt prime?} and {\tt rabinProvesComposite})
variable {\tt count2Order} whose type is a {\tt Vector} of
{\tt NonNegativeInteger}s. This variable is used to count the number
of elements of each 2-order.
\begin{verbatim}
                         "Maximal 2-part" Modifications

[ 1] prime? n ==
[ 2]    n < two => false
[ 3]    n < nextSmallPrime => member?(n, smallPrimes)
[ 4]    not one? gcd(n, productSmallPrimes) => false
[ 5]    n < nextSmallPrimeSquared => true
[ 6]    nm1:=n-1
[ 7]    q := (nm1) quo two
[ 8]    for k in 1.. while not odd? q repeat q := q quo two
[ 9]    -- q = (n-1) quo 2**k for largest possible k
[10]    mn := minIndex smallPrimes
[10g]   rootsMinus1 := [] -- the empty set
[10h]   count2Order := new(k,0) -- vector of k zeros
[11]    for i in mn+1..mn+10 repeat
[12]      rabinProvesComposit(smallPrimes i,n,nm1,q,k) => return false
[12e]     currPrime:=smallPrimes(mn+10)
[12f]     while count2Order(k) = 0 repeat
[12g]           currPrime := nextPrime currPrime
[12h]           rabinProvesComposite(currPrime,n,nm1,q,k) => false
[13]    true
[14]
[15] rabinProvesComposite(p,n,mn1,q,k) ==
[16]    -- nm1 = n-1 = q*2**k; q odd
[17]    -- probability false for n composite is < 1/4
[18]    -- for most n this probability is much less than 1/4
[19]    t := powmod(p, q, n)
[19a]   if t=mn1 then count2Order(1):=count2Order(1)+1
[20]    -- neither of these cases tells us anything
[21]    if not (one? t or t = nm1) then
[22]       for j in 1..k-1 repeat
[22a]          oldt := t
[23]           t := mulmod(t, t, n)
[24]           one? t => return true
[25]           -- we have squared somethiong not -1 and got 1
[26]           t = mn1 =>
[26a]              rootsMinus1:=union(rootsMinus1,oldt)
[26b]              # rootsMinus1 > 2 => return true
[26c]              count2Order(j+1):=count2Order(j+1)+1
[27]               leave
[28]       not (t = nm1) => return true
[29]    false
\end{verbatim}
We currently collect more information than we use. Again, this 
modification to the Rabin algorithm proves that the Jaeschke number 
is not prime.

\section{How would one defeat these modifications?}

It is all very well to propose new algorithms, an demonstrate that
they are ``better'' than the old ones, but might they really have
loop-holes just as large? The ``maximal 2-part'' requirement defeats a
whole family of pseudoprimes -- all those of the form
$(K+1)\cdot (rK+1)$ with $r$ odd, since then $N-1$ has a higher 2-part
than $\hat{\phi}(N)$. This test is therefore useful in general, and
defeats any straight-forward generalisation of the Jaeschke number to
larger sets of $x$.

There are various possible constructions which these modifications do
not defeat. We could make our pseudoprime $N$ take the form
$(K+1)\cdot(6K+1)$ with $K\equiv 2~(\text{mod}~4)$. Then the 2-part of
$\hat{\phi}(N)$ would be $2^2$, whereas that of $N-1$ would be $2^1$
(and so the ``roots of -1'' enhancement would never operate). A value
$x$ would pass Rabin's test, with the ``maximal 2-part'' enhancement,
if it were
\begin{itemize}
\item[(1)] a cubic residue modulo $6K+1$
\item[(2)] a quadratic residue modulo $6K+1$
\item[(3)] a quartic non-residue modulo $6K+1$
\item[(4)] a quadratic non-residue modulo $K+1$
\end{itemize}

On average, one $K$-value in 24 will have these properties for a fixed
$x$. 

A value $x$ would also pass Rabin's test, but would not contribute to
the ``maximal 2-part'', if it were
\begin{itemize}
\item[(1)] a cubic residue modulo $6K+1$
\item[(2)] a quadratic residue modulo $6K+1$
\item[(3)] a quartic residue modulo $6K+1$
\item[(4)] a quadratic residue modulo $K+1$
\end{itemize}

Again, on average, one $K$-value in 24 will have these properties for
a fixed $x$.

We note, therefore, that we might expect 50\% of $x$-values causing
$N$ to pass Rabin's test to have 2-part $2^1$ and 50\% to have 2-part
$2^0$; the same distribution as for a prime value of $N$ (with
$k=1$). If we use $n$ different $x$-values, we might expect $K$ to
have to be of the order of $12^n$, and $N$ to be of the order of
$144^n$. In addition, both $K+1$ and $6K+1$ have to be prime. For
$n=10$, the probability of this is about 1/25, so we might expect to
find such an $N$ at around $2\cdot 10^{24}$.

\section{Leech's attack}

Leech \cite{Leec92} has suggested an attack of the form 
$N=(K+1)\cdot (2K+1)\cdot (3K+1)$. If the three factors are prime
(which incidentally forces $K=2$, a case we can discard, or 
$K\equiv 0~(\text{mod}~6$)), then these numbers are certainly Carmichael,
and hence a good attack on the original version of Rabin's
algorithm. Indeed, almost 25\% of seed values will yield the result 
``$N$ is probably prime''.

Fortunately, we are saved by the ``maximal 2-part'' variant. Suppose 
$K=3\cdot n\cdot 2^m$ with $n$ odd (and $m$ at least 1). Then the
maximal 2-part we can actually observe is $2^m$, whereas
\[N-1=162 n^3 (2^m)^3 + 90 n^2 (2^m)^2 + 18 n 2^m\]
which is divisible by $2^{m+1}$. Hence we will never observe an element
of the maximal 2-part, and the loop at line [12f] will run until a
counter-example to primality is found.

In fact, if $m=1$, $N-1$ is divisible by 8, and if $m>1$, $N-1$ is
divisible by $2^{m+1}$, which is at least 8. Hence the ``roots of -1''
test also acts, and reduces the probability of passing the modified
Rabin well below 25\%.

Other forms of attack are certainly possible, e.g. taking
$N=(K+1)\cdot (3K+1)\cdot (5K+1)$. Here the ``maximal 2-part'' does
not help us, since the 2-part of $N-1$ is equal to that of 
$\hat{\phi}(N)$. However these numbers are not generally Carmichael,
only ``nearly Carmichael'', since 5 does not divide $N-1$. Hence we
would need to insist that all our seed values were quintic residues
modulo $5K+1$ as well as having the same 2-part modulo all the
factors, and so on. These more complex families seem to create more
problems for the inventor of counter-examples, so we can probably say
that taking one prime for every factor of 100 in $N$ probably makes the
systematic construction of counter-examples by this technique
impossible. 

However, if we also force $K\equiv 12~(\text{mod}~30)$, Leech
\cite{Leec92} has pointed out that $N$ is Charmichael. By
construction, the factors are congruent to each other, and to their
product, modulo 12, so the quadratic characters of 3 modulo the
different factors are compatible. In fact we also need 
$K\equiv 0~(\text{mod}~7)$, since $K\equiv 1,3,5$ gives incompatible
quadratic characters for 7 modulo the different factors, and 
$K\equiv 2,4,6$ gives non-prime factors. However, the three factors
are congruent respectively to 3, 2 and 1 modulo 5, and so 5 will be a
quadratic non-residue modulo the first two factors, but a residue
modulo the last, hence ensuring that Rabin's algorithm with $x=5$
always says ``$N$ is certainly composite''.

\section{The $(K+1)\cdot (2K+1)$ attack}

This attack has been used recently by Arnault \cite{Arna91} to defeat
the set of $x$-values
\[\{2,3,5,7,11,13,17,19,23,29\}\]
The number
\[1195068768795265792518361315725116351898245581=\]
\[48889032896862784894921\cdot 24444516448431392447461\]
passes all these tests. In effect, the requirement is that $x$ be a
quadratic residue modulo $2K+1$, and that the quadratic character of
$x$ modulo $K+1$ should equate to the quartic character of $x$ modulo
$2K+1$. These conditions are satisfied for approximately 25\% of
$K$-values. In addition, of course, $K+1$ and $2K+1$ must be prime. It
would almost certainly be possible to construct a much smaller number
than Arnault's, with the same properties -- he fixed the congruences
classes he was considering; for example he chose one class modulo 116,
rather than examining all 29 satisfactory classes.

This form of attack is particularly worrying, since it is much easier
to use than the other attacks in the previous sections. Indeed, one
should probably consider $\text{log}_4~N$ values $x$ to test a number
$N$ if one is to defend against this attack. Fortunately, we have a
simpler defence; we can check explicitly if the number we are given is
of the form.

It is worth noting that Damgard and Landrock \cite{Damg91} prove the
following.

{\bf Theorem} {\sl If N is an odd composite number, such that N is not
divisible by 3, and more than 1/8 of the x-values yield ``N is
probably prime'' then one of the following holds}
\begin{itemize}
\item {\sl N is a Carmichael number with precisely three prime factors}
\item $3N+1$ {\sl is a perfect square}
\item $8N+1$ {\sl is a perfect square}
\end{itemize}

$8(K+1)\cdot (2K+1)+1=(4K+3)^2$, so this attack is a special case of
the above theorem. There seems no reason not to test both the
exceptional conditions in the Damgard-Landrock Theorem -- such numbers
are always composite, except for trivial cases ruled out by lines
before [ 5]. 
\begin{verbatim}
                         "Damgard-Landrock" Modifications

[ 1] prime? n ==
[ 2]    n < two => false
[ 3]    n < nextSmallPrime => member?(n, smallPrimes)
[ 4]    not one? gcd(n, productSmallPrimes) => false
[ 5]    n < nextSmallPrimeSquared => true
[ 6]    nm1:=n-1
[ 7]    q := (nm1) quo two
[ 8]    for k in 1.. while not odd? q repeat q := q quo two
[ 9]    -- q = (n-1) quo 2**k for largest possible k
[10]    mn := minIndex smallPrimes
[10g]   rootsMinus1 := [] -- the empty set
[10h]   count2Order := new(k,0) -- vector of k zeros
[11]    for i in mn+1..mn+10 repeat
[12]      rabinProvesComposit(smallPrimes i,n,nm1,q,k) => return false
[12a]     import IntegerRoots(I)
[12b]     q > 1 and perfectSquare?(3*n+1) => false
[12c]     ((n9:=n rem (9::I))=1 or n9 = -1) and perfectSquare(8*n+1) => false
[12d]     -- Both previous tests fro Damgard & Landrock
[12e]     currPrime:=smallPrimes(mn+10)
[12f]     while count2Order(k) = 0 repeat
[12g]           currPrime := nextPrime currPrime
[12h]           rabinProvesComposite(currPrime,n,nm1,q,k) => false
[13]    true
[14]
[15] rabinProvesComposite(p,n,mn1,q,k) ==
[16]    -- nm1 = n-1 = q*2**k; q odd
[17]    -- probability false for n composite is < 1/4
[18]    -- for most n this probability is much less than 1/4
[19]    t := powmod(p, q, n)
[19a]   if t=mn1 then count2Order(1):=count2Order(1)+1
[20]    -- neither of these cases tells us anything
[21]    if not (one? t or t = nm1) then
[22]       for j in 1..k-1 repeat
[22a]          oldt := t
[23]           t := mulmod(t, t, n)
[24]           one? t => return true
[25]           -- we have squared somethiong not -1 and got 1
[26]           t = mn1 =>
[26a]              rootsMinus1:=union(rootsMinus1,oldt)
[26b]              # rootsMinus1 > 2 => return true
[26c]              count2Order(j+1):=count2Order(j+1)+1
[27]               leave
[28]       not (t = nm1) => return true
[29]    false
\end{verbatim}

\section{Conclusions}

It is certainly possible to draw more information from a fixed set of
$x$-values than Rabin's original algorithm does, and we have explained
two ways of doing this. While we have not yet constructed a number
that defeats our enhanced version of Rabin's algorithm, it should
certainly be possible to do so, if the set of $x$-values is fixed. In
general, the number of primes used should be proportional to log $N$,
and we have made some suggestions as to what the constant of
proportionality should be. A better constant of proportionality can be
used if we test explicitly for numbers of the form 
$(K+1)\cdot (2K+1)$, probably via the Damgard-Landrock Theorem. This
approach converts Rabin's algorithm form a $O(\text{log}^3~N)$ test to
a $O(\text{log}^4~N)$, but we believe that a general-purpose system
needs the additional security.

It must be emphasised that we have not produced a guaranteed
$O(\text{log}^4~N)$ primality test: merely one that we do not believe
we can break by the technology we know. It would be tempting to
conjecture that, with an appropriate constant of proportionality, this
test is guaranteed never to return ``$N$ is probalby prime'' when in
fact it is composite. The closest result to this we know of is a
statement by Lenstra \cite{Lens81} (see also Koblitz \cite{Kobl87})
that, if suitable assumptions similar to the generalised Riemann
hypothesis are made, the 70 $\text{log}^2~N$ values suffice, which
would be a $O(\text{log}^5~N)$ primality test.
\begin{verbatim}
           The Pomerance et al. Modifications

[ 0a] PomeranceList:=[25326001::I,161304001::I,960946321::I,1157839381::I,
[ 0b]                  -- 3215031751::I, -- has a factor of 151
[ 0c]                 3697278427::I, 5764643587::I, 6770862367::I,
[ 0d]                 14386156093::I, 15579919981::I, 18459366157::I,
[ 0e]                 18459366157::I, 21276028621::I]::(List I)
[ 0f] PomeranceLimit:=(25*10**9)::I
[ 1] prime? n ==
[ 2]    n < two => false
[ 3]    n < nextSmallPrime => member?(n, smallPrimes)
[ 4]    not one? gcd(n, productSmallPrimes) => false
[ 5]    n < nextSmallPrimeSquared => true
[ 6]    nm1:=n-1
[ 7]    q := (nm1) quo two
[ 8]    for k in 1.. while not odd? q repeat q := q quo two
[ 9]    -- q = (n-1) quo 2**k for largest possible k
[10]    mn := minIndex smallPrimes
[10a]   n < PomeranceLimit =>
[10b]     rabinProvesCompositeSmall(2::I,n,nm1,q,k) => return false
[10c]     rabinProvesCompositeSmall(3::I,n,nm1,q,k) => return false
[10d]     rabinProvesCompositeSmall(5::I,n,nm1,q,k) => return false
[10e]     member?(n,PomeranceList) => return false
[10f]     true
[10g]   rootsMinus1 := [] -- the empty set
[10h]   count2Order := new(k,0) -- vector of k zeros
[11]    for i in mn+1..mn+10 repeat
[12]      rabinProvesComposit(smallPrimes i,n,nm1,q,k) => return false
[12a]     import IntegerRoots(I)
[12b]     q > 1 and perfectSquare?(3*n+1) => false
[12c]     ((n9:=n rem (9::I))=1 or n9 = -1) and perfectSquare(8*n+1) => false
[12d]     -- Both previous tests fro Damgard & Landrock
[12e]     currPrime:=smallPrimes(mn+10)
[12f]     while count2Order(k) = 0 repeat
[12g]           currPrime := nextPrime currPrime
[12h]           rabinProvesComposite(currPrime,n,nm1,q,k) => false
[13]    true
[14]
[15] rabinProvesComposite(p,n,mn1,q,k) ==
[16]    -- nm1 = n-1 = q*2**k; q odd
[17]    -- probability false for n composite is < 1/4
[18]    -- for most n this probability is much less than 1/4
[19]    t := powmod(p, q, n)
[19a]   if t=mn1 then count2Order(1):=count2Order(1)+1
[20]    -- neither of these cases tells us anything
[21]    if not (one? t or t = nm1) then
[22]       for j in 1..k-1 repeat
[22a]          oldt := t
[23]           t := mulmod(t, t, n)
[24]           one? t => return true
[25]           -- we have squared somethiong not -1 and got 1
[26]           t = mn1 =>
[26a]              rootsMinus1:=union(rootsMinus1,oldt)
[26b]              # rootsMinus1 > 2 => return true
[26c]              count2Order(j+1):=count2Order(j+1)+1
[27]               leave
[28]       not (t = nm1) => return true
[29]    false
\end{verbatim}

\begin{verbatim}
           O(Log^4 N) Modifications

[ 0a] PomeranceList:=[25326001::I,161304001::I,960946321::I,1157839381::I,
[ 0b]                  -- 3215031751::I, -- has a factor of 151
[ 0c]                 3697278427::I, 5764643587::I, 6770862367::I,
[ 0d]                 14386156093::I, 15579919981::I, 18459366157::I,
[ 0e]                 18459366157::I, 21276028621::I]::(List I)
[ 0f] PomeranceLimit:=(25*10**9)::I
[ 1] prime? n ==
[ 2]    n < two => false
[ 3]    n < nextSmallPrime => member?(n, smallPrimes)
[ 4]    not one? gcd(n, productSmallPrimes) => false
[ 5]    n < nextSmallPrimeSquared => true
[ 6]    nm1:=n-1
[ 7]    q := (nm1) quo two
[ 8]    for k in 1.. while not odd? q repeat q := q quo two
[ 9]    -- q = (n-1) quo 2**k for largest possible k
[10]    mn := minIndex smallPrimes
[10a]   n < PomeranceLimit =>
[10b]     rabinProvesCompositeSmall(2::I,n,nm1,q,k) => return false
[10c]     rabinProvesCompositeSmall(3::I,n,nm1,q,k) => return false
[10d]     rabinProvesCompositeSmall(5::I,n,nm1,q,k) => return false
[10e]     member?(n,PomeranceList) => return false
[10f]     true
[10g]   rootsMinus1 := [] -- the empty set
[10h]   count2Order := new(k,0) -- vector of k zeros
[11]    for i in mn+1..mn+10 repeat
[12]      rabinProvesComposit(smallPrimes i,n,nm1,q,k) => return false
[12a]     import IntegerRoots(I)
[12b]     q > 1 and perfectSquare?(3*n+1) => false
[12c]     ((n9:=n rem (9::I))=1 or n9 = -1) and perfectSquare(8*n+1) => false
[12d]     -- Both previous tests fro Damgard & Landrock
[12e]     currPrime:=smallPrimes(mn+10)
[12f]     probablySav=fe:=tenPowerTwenty
[12g]     while count2Order(k) = 0 or n > probablySaferepeat
[12h]           currPrime := nextPrime currPrime
[12i]           probablySafe:=probablySafe*(100::I)
[12j]           rabinProvesComposite(currPrime,n,nm1,q,k) => false
[13]    true
[14]
[15] rabinProvesComposite(p,n,mn1,q,k) ==
[16]    -- nm1 = n-1 = q*2**k; q odd
[17]    -- probability false for n composite is < 1/4
[18]    -- for most n this probability is much less than 1/4
[19]    t := powmod(p, q, n)
[19a]   if t=mn1 then count2Order(1):=count2Order(1)+1
[20]    -- neither of these cases tells us anything
[21]    if not (one? t or t = nm1) then
[22]       for j in 1..k-1 repeat
[22a]          oldt := t
[23]           t := mulmod(t, t, n)
[24]           one? t => return true
[25]           -- we have squared somethiong not -1 and got 1
[26]           t = mn1 =>
[26a]              rootsMinus1:=union(rootsMinus1,oldt)
[26b]              # rootsMinus1 > 2 => return true
[26c]              count2Order(j+1):=count2Order(j+1)+1
[27]               leave
[28]       not (t = nm1) => return true
[29]    false
\end{verbatim}

\chapter{Finite Fields in Axiom (Grabmeier/Scheerhorn)}

This was written by Johannes Grabmeier and Alfred Scheerhorn.
\href{http://axiom-developer.org/axiom-website/GroupTheoryII/Salomone.html}
{Matthew Salomone's video course}\cite{Salo16}
provides useful background material.

Finite fields play an important role in mathematics and in many
applications as coding theory or factorizing polynomials in computer
algebra systems. They are the finite sets which have a computational
structure as the classical fields of rational or complex numbers,
i.e. addition + and multiplication with inverses and the usual group
axioms as commutativity and associativity laws and their interaction
via the distributivity laws. For further details see any book on
algebra or our preferred reference for finite fields \cite{Lidl83}.

The finite fields are classified: For each prime power $q=p^n$ there
is up to isomorphism exactly one finite field of these size and there
are no more. So far this looks nice, easy and complete. However, there
are different constructions of a finite field of a given size $q$,
each having different advantages and disadvantages.  This paper deals
with such constructions and implementations in the computer algebra
system Axiom, various isomorphims and embeddings. We have three
different kinds of constructions, namely polynomial basis
representation, normal basis representation, and cyclic group
representation.

The various advantages and disadvantages which will be discussed along
with the special implementations of the representations in the
respective sections. All are strongly connected with the construction
of irreducible polynomials which have additional properties. The user
of Axiom may choose the representation which best meets the needs for
his applications. For each type we have provided automatic choices as
well as the liberty to use a favourite polynomial. In addition there
are implementations for mechanisms to convert the data from one
representation to the other.

The paper is organized as follows: For convenience of the readers we
first recall some basic facts from the theory of finite fields. Then
we introduce our category design of the finite field world in
Axiom. Next comes the description of all the functions which are valid
and useful for every finite field. We employ the abstract datatype
concepts of Axiom, which allows to implement such functions in the
default packages of these categories. This makes an implementation in
the different domains superfluous. Using a special kind of
representation the implementation of many functions can be improved in
order to have a more effcient computation. We describe these improved,
additional domain implementations in the sections according to the
special representations. Section \ref{section10} is devoted to the various
constructions of polynomials. In section \ref{section12} 
we finally give results of
time comparison of the various representations.

\section{Basic theory and notations}
\label{section2}

\href{http://axiom-developer.org/axiom-website/GroupTheoryII/Salomone.html}
{Salomone's lectures}
provide background material for this section.

We denote a finite field with $q=p^r$ elements, $p$ a prime and 
$r \in \mathbb{N}$, by $GF(q)$.  The {\sl prime field GF(p)} can be 
constructed as $\mathbb{Z}/p\mathbb{Z} = \{0,1,\ldots,p-1\}$.
The finite field $GF(q)$ is an algebraic extension of the field $GF(p)$ 
and isomorphic to the splitting field of $X^q-X$ over $GF(p)$. 
Let $\alpha,\beta \in GF(q)$ and $c \in GF(p)$, then we have 
$(\alpha+\beta)^p=\alpha^p+\beta^p$ and $c\alpha^p=(c\alpha)^p$.
Therefore powering with $p$ or powers of $p$ is a linear operation 
over $GF(p)$. Let $E=GF(q^n)$ be an extension of $F=GF(q)$ of degree
$n \in \mathbb{N}$. The automorphism group of $E$ over $F$ is cyclic of 
order $n$ and generated by
\[\sigma : \alpha \rightarrow \alpha^q\]
which is called a {\sl Frobenius automorphism}.

Let $f \in F[X]$ be a monic, irreducible polynomial of degree $n$. Then
\[E \simeq F[X]/(f)\]
where $(f)=f\cdot F[X]$ denotes the principal ideal generated by $f$, and
the isomorphism is given by 
$\alpha \mapsto (X {\rm\ mod\ }f)$, where $\alpha$
is a root of $f$ in $E$. $\alpha$ generates a {\sl polynomial basis}
$\{1,\alpha,\ldots,\alpha^{n-1}\}$ of $E$ over $F$. Every element 
$\beta \in E$ can be expressed uniquely in the form
\[\beta=\sum_{i=0}^{n-1}{b_i\alpha^i}\]
This kinds of representing elements of $E$ is called {\sl polynomial basis
representation}.

Let $\alpha \in E$, the monic, irreducible polynomial $f \in F[X]$ with
$f(\alpha)=0$ is called the {\sl minimal polynomial} of $\alpha$ over $F$.
All the roots of $f$ are given by the set of {\sl conjugates}
\[\{\alpha,\alpha^q,\ldots,\alpha^{q^n}\}\]
of $\alpha$ of $F$. Therefore
\[f=\prod_{i=0}^{n-1}{(X-\alpha^{q^i})}\]

The {\sl trace} $T_{E/F}(\alpha)$ and the {\sl norm} $N_{E/F}(\alpha)$ of 
$\alpha$ over $F$ are defined by the sum and the product of the conjugates
of $\alpha$, respectively,
\[T_{E/F}(\alpha)=\sum_{i=0}^{n-1}{\alpha^{q^i}},\quad
N_{E/F}(\alpha)=\prod_{i=0}^{n-1}{\alpha^{q^i}}=
\alpha^{\frac{q^n-1}{q-1}}\eqno{(1)}\]
These values can be read off from the minimal polynomial
\[f=\sum_{i=0}^n{f_iX^i} \in F[X]\] of $\alpha$ over $F$:
\[T_{E/F}(\alpha)=-f_{n-1},\quad N_{E/F}(\alpha)=(-1)^n f_0\]
The {\sl degree} of $\alpha$ over $F$ is the degree of the smallest
subfield of $E$ over $F$, which contains $\alpha$, i.e. the minimal
integer $d>0$ for which
\[\alpha^{q^d}=\alpha\]
If $\alpha$ has degree $d$ over $F$, the minimal polynomial 
$m_\alpha(X)$ of $\alpha$ then has degree $d$, too.

The multiplicative group $E^*$ of $E$ is cyclic of order $q^n-1$. A
generator of this group is called a {\sl primitive element} of $E$
and the minimal polynomial of such an element is called a
{\sl primitive polynomial}. Every nonzero element $\beta \in E$ can be
expressed as a power of $\alpha$:
\[\beta=\alpha^e\]
where $0\le e < q^n-1$ is uniquely determined. This kind of representing
the elements of $E$ is called {\sl cyclic group representation} of $E$.
The exponent $e$ is called the {\sl discrete logarithm} of $\beta$ to
base $\alpha$ denoted by $e=log_\alpha(\beta)$. Note, that exponentiaion
\[\mathbb{Z}\times E \rightarrow E:(e,\alpha)\mapsto \alpha^e\]
defined a $\mathbb{Z}$-module structure on the multiplicative group $E^*$.

Analogically one can define a module structure on the additive group
of $E$ in the following way.

Let $\circ : F[X] \times E \rightarrow E$ be defined by
\[\sum_i{a_iX^i} \circ \alpha := \sum_i{a_i\alpha^{q^i}}\]
Then we get for $\alpha \in E$, 
$g=\sum_i{g_iX^i}$, 
$f=\sum_j{f_jX^j} \in F[X]$,
\[g\circ(f\circ\alpha)=g\circ(\sum_j{f_j\alpha^{q^j}})=
\sum_i{g_i(\sum_j{f_j\alpha^{q^j}})^{q^i}}=
\sum_{i,j}{g_if_j\alpha^{q^{i+j}}}=
(g\cdot f)\circ\alpha\]
This proves that the operation $\circ$ defines an $F[X]$-module
structure on the additive group of $E$.

For $\alpha\in E$ the annihilator ideal 
$Ann_\alpha=\{f\in F[X] : f\circ\alpha=0\}$ of $\alpha$ is
generated by a single polynomial of $F[X]$, since $F[X]$ is a principle
ideal domain. We call the unique, monic generator of $Ann_\alpha$ the
{\sl linear associated order} of $\alpha$ over $F$, denoted by
${\rm Ord}_q(\alpha)$:
\[\{f\in F[X] : f\circ\alpha=0\}={\rm Ord}_q(\alpha)F[X]\]
Since $(X^n-1)\circ\alpha=\alpha^{q^n}-\alpha=0$ for all $\alpha\in E$,
${\rm Ord}_q(\alpha)$ divides $(X^n-1)$.

If ${\rm Ord}_q(\alpha)=(X^n-1)$ then there exists no polynomial of
degree less $n$ in F[X] annihilating $\alpha$, i.e. if $f\in F[X]$ is
of degree ${\rm deg}(f)<n$ and $f\circ\alpha=0$, then $f=0$. That is
\[\sum_{i=0}^{n-1}{f_i\alpha^{q^i}}=0~{\rm implicates}~f_i=0,
0 \le i < n\]
Therefore $\{\alpha,\alpha^q,\ldots,\alpha^{a^{n-1}}\}$ constitutes a
basis of $E$ over $F$, called a {\sl normal basis}.
Therefore, we call an element with linear associated order $(X^n-1)$
{\sl normal (over F)}. In this case every element $\beta\in E$ can be
expressed uniquely in the form
\[\beta=\sum_{i=0}^{n-1}{b_i\alpha^{q^i}}=b(X)\circ\alpha,\quad
b(X)=\sum_{i=0}^{n-1}{b_iX^i}\in F[X]\]
This kind of representing elements of $E$ is called
{\sl normal basis representation} of $E$. The polynomial $b(X)$ of
degree ${\rm deg}(b) < n$ is called the {\sl linear associated logarithm}
of $\beta$ to base $\alpha$ over $F$, denoted by 
$b(X)={\rm Log}_{q,\alpha}(\beta)$. It is uniquely determined modulo
${\rm Ord}_q(\alpha)$.

Again, with $\alpha$ all of its conjugates are normal over $F$, too.
Therefore we call an irreducible polynomial $f\in F[X]$ of degree $n$
normal over $F$, if the roots of $f$ in $GF(q^n)$ are linearly independent.

\section{Categories for finite field domains}
\label{section3}

Each domain in Axiom which represents a finite field is an object
in the category
\begin{verbatim}
   FFIELDC FiniteFieldCategory
\end{verbatim}

This category defines functions which are typical for finite sets as
{\tt order} or {\tt random} and those which are typical for finite
fields and do not depend on the ground field. An example is {\tt
primeFrobenius}, which implements the Frobenius automorphism with
respect to the prime field, or functions concerning the cyclic
multiplicative group structure as {\tt primitiveElement}, 
see section \ref{section4.3}

Functions which need to know the {\sl groundfield} $F$ are defined in
\begin{verbatim}
   FAXF(F)  FiniteAlgebraicExtensionField(F)
\end{verbatim}
These are functions considering the extension field as an algebra of
finite rank over $F$, 
see section \ref{section4.1}, and functions concerning the 
$F[X]$-module structure of the additive group of the extension field,
see section \ref{section4.2}

Every finite field can be considered as an extension field of each of
its subfields, in particular the prime field. In Axiom every finite
field constructor is implemented to belong to the category {\tt
FAXF(F)} for exactly one $F$. Constructors as {\tt FF} are considered
as extensions of its prime field, while others use the explicit given
ground field. Note that even a prime field is an extension of
itself. Mainly for technical reasons, but also for restrictions of the
present compiler, every finite field datatype in Axiom is an extension
of exactly {\sl one} subfield, usually called the {\sl ground
field}. Otherwise functions like {\tt extensionDegree} would depend on
the various ground fields and this would be not very convenient. For
possible enhancements in future releases, 
see section \ref{section11}. NOt that
this of course does not affect the ability to build arbitrary towers
of finite field extensions.

\section{General finite field functions}
\label{section4}

Let $E=GF(q^n)$ and $F=GF(q)$ be represented by {\tt E} of type 
{\tt FAXF(F)} and {\tt F} respectively. The characteristic of $E$ is 
given by {\tt characteristic()\$E}.

\subsection{$E$ as an algebra of rank $n$ over $F$}
\label{section4.1}

The degree $n$ of $E$ over $F$ is returned by {\tt extensionDegree()\$E}

The {\tt definingPolynomial()\$E} yields the polynomial $f\in F[X]$ by 
which the field extension $E$ over $F$ is defined. The element of {\tt E}
representing $(X {\rm\ mod\ } f(X))$, 
i.e. a root of the defining polynomial, is
returned by {\tt generator()\$E}.

A basis of $E$ over $F$ is yielded by calling {\tt basis()\$E}. In the
polynomial basis representation and cyclic group representation this is
the polynomial basis generated by {\tt generator()\$E}. In the normal basis
representation it is the normal basis generated by this element.

Let $\alpha=\sum_{i=0}^{n-1}{a_i\alpha_i}$, where $a_i$ are elements of
{\tt F} and $(\alpha_0,\ldots,\alpha_{n-1})=${\tt basis()\$E}.
The {\tt coordinates}$(\alpha)$ function computes the coordinate vector
$(a_0,\ldots,a_{n-1})$ of type {\tt Vector F} of $\alpha$ over $F$. 
The {\tt represents} function is the inverse of {\tt coordinates} and
yields $\alpha$, if applied to the vector $(a_0,\ldots,a_{n-1})$.

The implementation of these functions is straightforward in the polynomial
and normal basis representation. For the cyclic group representation see
section \ref{section7.3}

The functions {\tt degree}, {\tt trace}, and {\tt norm} applied to 
$\alpha \in E$ compute the degree, trace, and norm of $\alpha$ over $F$,
respectively. For their implementation see the sections according to the
special representations.

There are two ways of computing the minimal polynomial $m_\alpha(X)$ of
$\alpha\in E$ over $F$. The first method is to compute
\[m_\alpha(X)=\prod_{i=0}^{d-1}{(X-\alpha^{q^d})}\]
where $d$ denotes the degree of $\alpha$ over $F$. It unfortunately needs
$d(d-1)/2$ multiplications in $E$ and $(d-1)$ exponentiations by $q$.

The second method is to compute first a matrix $M\in F^{n\times(d+1)}$
whose $i$-th column is the coordinate vector of $\alpha^i$ w.r.t. an
arbitrary base of $E$ over $F$, for $0 \le i \le d$. Then there exists a
vector $b=(b_0,\ldots,b_d)^t$ in the nullspace of $M$, which can be 
computed. Hence, the polynomial
\[m_\alpha=X^d+\frac{1}{b^d}\sum_{i=0}^{d-1}{b_iX^i}\]
has the root $\alpha$ and is of correct degree, hence it is the minimal
polynomial. This method requires only about $(d-1)$ multiplications in $E$
and $O(d^2n)$ operations in $F$. Which approach is more time efficient
depends on the relation between the computation time of operations in $F$
and multiplications in $E$. Since multiplication in $E$ is cheap in a cyclic
group represention of $E$ 
(see section \ref{section7.1}) we use the first method there.
The second method was found to be more time efficient for the other 
representations.

Let $d$ be a divisor of $n$ and $L$ be a subfield of $E$ of degree $d$ over 
$F$. For the functions {\tt trace}, {\tt norm}, {\tt basis}, and 
{\tt minimalPolynomial} we offer a second version, which gets $d$ as an
additional parameter. {\tt basis}$(d)$ returns a basis of $L$ over $F$.
In the polynomial basis representation and cyclic group representation, this
is the polynomial basis generated by {\tt norm(primitiveElement()\$E,d)}.
In the normal basis representation it is the normal basis generated by
{\tt trace(generator()\$E,d)}. For $\alpha\in E$ {\tt trace}$(\alpha,d)$
and {\tt norm}$(\alpha,d)$ compute $T_{E/L}(\alpha)$ and $N_{E/L}(\alpha)$,
respectively, by 
\[T_{E/L}(\alpha)=\sum_{i=0}^{n/d-1}{\alpha^{q^{id}}}\quad{\rm and}\quad
N_{E/L}(\alpha)=\prod_{i=0}^{n/d-1}{\alpha^{q^{id}}}\]
Similarly we get the minimal polynomial of $\alpha$ over $L$ by
\[\prod_{i=0}^{m-1}{(X-\alpha^{q^{id}})}\]
where $m$ denotes the degree of $\alpha$ over $L$.

\subsection{The $F[X]$-module structure of $E$}
\label{section4.2}

In this section we discuss the three operations
{\tt linearAssociatedExp}, {\tt linearAssociatedLog}, and
{\tt linearAssociatedOrder}.

For $f=\sum_{i=0}^{n-1}{f_iX^i}\in F[X]$ and $\alpha\in E$ the function
{\tt linearAssociatedExp}$(\alpha,f)$ computes $f\circ\alpha$ in a 
straightforward way via
\[f\circ\alpha = \sum_{i=0}^{n-1}{f_i\alpha^{q^i}}\]

For $\alpha\in E$ we want to construct the monic polynomial 
{\tt Ord}$_q(\alpha)$ of least degree $d$ such that
\[{\rm Ord}_q(\alpha)\circ \alpha=0\]
This suggests to analyze the linear relations between the $q$-powers of
$\alpha$. Therefore we define $m\in F^{n\times n}$ to be the matrix whose
$i$-th column is the coordinate vector of $\alpha^{q^{i}}$ w.r.t. a basis
$B$ of $E$ over $F$, for $0 \le i \le n$. Its rank equals the degree $d$ of
{\tt Ord}$_q(\alpha)$. If $m$ has rank $n$ then $\alpha$ is normal in $E$
over $F$ and {\tt Ord}$_q(\alpha)=X^n-1$. Otherwise a vector
$b=(b_0,\ldots,b_{n-1})^t$ of the nullspace of $M$ is computed. It is easy
to see that we can choose $b_{d+1},b_{d+2},\ldots,b_{n-1}=0$ and finally
we get
\[{\rm Ord}_q(\alpha)=X^d+\frac{1}{b^d}\sum_{i=0}^{d-1}{b_iX^i}\]
which is computed by {\tt linearAssociatedOrder}$(\alpha)$ in Axiom.

For $\beta$ and $\alpha\in E$ {\tt linearAllocatedLog}$(\alpha,\beta)$
computed {\tt Log}$_{q,\alpha}(\beta)$, if it exists, i.e. the unique
polynomial $f=\sum_i{f_iX^i}\in F[X]$ of degree less then 
deg$({\rm Ord}_q(\alpha))$, for which $f\circ\alpha=\beta$.

As before we solve a system of linear equations
\[M\cdot(f_0,f_1,\ldots,f_{n-1})^t=b^t\]
where $M$ is as above and $b$ denotes the coordinate vector of $\beta$
w.r.t. the basis $B$. If there exists a solution satisfying
$f_d,f_{d+1},\ldots,f_{n-1}=0$ for the degree $d$ of ${\rm Ord}_q(\alpha)$,
then
\[{\rm Log}_{q,\alpha}(\beta)=\sum_{i=0}^{d-1}{f_iX^i}\]
otherwise {\tt linearAssociatedLog}$(\alpha,\beta)$ returned {\tt "failed"}.

The function {\tt linearAssociatedLog}$(\alpha)$ with only one argument
computed the linear associated logarithm of $\alpha$ to the base given by
{\tt normalElement()\$E}.

\subsection{The cyclic group $E^*$}
\label{section4.3}

The inheritance mechanisms of Axiom provide for each multiplicative
structure automatically an implementation of the exponentiation by the
defaulting repeating squaring algorithm. Finite Fields are an example
where it makes sense to overwrite this defaulting function. In all the
implementations we first reduce the exponent modulo $q^n-1$ and then do
repeated squaring or other algorithms.

To compute the multiplicative order $e:={\rm order}(\alpha)$ of
$\alpha\in E$ we proceed as follows: We start with $e:=q^n-1$. For every
prime $p$ of $(q^n-1)$ we divide $e$ by $p$: $e:=e/p$, as long as $e$
is divisible by $p$ and $\alpha^{e/p}=1$.

Note that this implementation requires factorizing the order $(q^n-1)$ of
the multiplicative group. Other functions as {\tt discreteLog} also need
these factors. Since factoring $(q^n-1)$ is time expensive, 
the factorization of $(q^n-1)$ is stored in a global variable
\begin{verbatim}
   facOfGroupSize: List Record(factor:Integer,exponent:Integer)
\end{verbatim}
in the respective domain. To make this information available to the category
for default implementations, we export the function
\begin{verbatim}
   factorsOfCyclicGroupSize()
\end{verbatim}
which returns the factorization. $(q^n-1)$ is factored only the first time
this function is called.

\subsection{Discrete logarithm}
\label{section4.4}

For the computation of discrete logarithms we implemented the
{\sl Silver-Pohlig-Hellman algorithm} combined with the 
{\sl baby steps-giant
steps} technique of Shank. A nice survey of discrete logarithm algorithms
included the algorithms we used is given by Odlyzko in \cite{Odly85}.

The Silver-Pohlig-Hellman algorithm breaks the problem of computing
discrete logarithms in a cyclic group of order $q^n-1$ down to computing
discrete logarithms in cyclic groups of order $p$, where $p$ ranges over
all prime factors of $q^n-1$. For a detailed description 
see \cite{Odly85} or \cite{Pohl78}.

To compute discrete logarithms in cyclic groups of prime order $p$ we
use Shank's algorithm implemented by
\begin{verbatim}
   shanksDiscLogAlgorithm:(M,M,NonNegativeInteger) ->
                           Union(NonNegativeInteger,"failed")
\end{verbatim}
in the package {\tt DiscreteLogarithmPackage(M)} and {\tt M} has to be
of type {\tt Join(Monoid,Finite)}, i.e. a finite multiplicative Monoid.
\begin{verbatim}
   shanksDiscLogAlgorithm(b,a,p)
\end{verbatim}
computes $e$ with $b^e=a$ assuming that $b$ and $a$ are elements of a
finite cyclic group of order $p$. If no such $e$ exists, it returns
{\tt "failed"}.

Here is a brief description of the algorithm: Let $\tilde{p}$ be an
integer close to $\sqrt{p}$. First we create a key-access table with
entries $(b^k,k)$. Then we look whether the table contains an entry
with key $a\cdot b^{-j\tilde{p}}$ and get $k$ with 
$a\cdot b^{-j\tilde{p}}=b^k$ for the smallest $j=0,1,\ldots,
\ceiling{p/\tilde{p}}-1$ or {\tt "failed"}. In the first case the result
is $e=k+j\tilde{p}$.

In the Silver-Pohlig-Hellman algorithm for a given base $\beta\in E$,
the first argument $b$ when calling {\tt shanksDiscLogAlgorithm(b,.,.)}
is for a fixed prime factor $p$ of $(q^n-1)$ always the same. To compute
logarithms to the base given by {\tt primitiveElement()\$E} efficiently,
the tables needed by Shanks algorithm are precomputed and stored in the
global variable
\begin{verbatim}
   discLogTable : Table(PI,Table(PI,NNI))
\end{verbatim}
in {\tt E}. It is initialized at the first call of the function
{\tt discreteLog}. Here {\tt PI} abbreviates {\tt PositiveInteger}
and {\tt NNI} abbreviates {\tt NonNegativeInteger}.

To implement the discrete logarithm function on category level in 
{\tt FiniteFieldCategory}, we have to make this data available to the
category. This is done by exporting the function
\begin{verbatim}
   tableForDiscreteLogarithm: Integer -> Table(PI,NNI)
\end{verbatim}
Called with a prime divisor $p$ of $q^n-1$ as argument, it returns a
table of size roughly $\tilde{p}\approx\sqrt{p}$, whose $k$-th entry for
$0 \le k < \tilde{p}$ is of the form
\begin{verbatim}
   [lookup(a**k),k) : Record(key:PI,entry:PI)
\end{verbatim}
where $a=\alpha^{(q^n-1)/p}$

We implemented two functions for discrete logarithms:
\begin{verbatim}
   discreteLog: E     -> NonNegativeInteger
   discreteLog: (E,E) -> Unioin(NonNegativeInteger,"failed")
\end{verbatim}
The first one computes discrete logarithms to the base
{\tt primitiveElement()\$E} using the precomputed tables. 
{\tt discreteLog(b,a)} computes ${\rm log}_b(a)$ if $a$ belongs
to the cyclic group generated by $b$ and fails otherwise. This function
does not use the table {\tt discLogTable}. No initialization of this
table is performed using the second function.

\subsection{Elements of maximal order}
\label{section4.5}

The functions
\begin{verbatim}
   primitiveElement()$E
   normalElement()$E
\end{verbatim}
yields a primitive element of $E$ and a generator of a normal basis of
$E$ over $F$, respectively. The first having maximal multiplicative order
$(q^n-1)$, the second maximal linear associated order $(X^n-1)$.

To compute elements of maximal order there exist algorithms which construct
elements of high order from elements of low order. For a unifying module
theoretic approach see L\"uneburg\cite{Lune87} chap.IV. For the construction of
primitive elements see \cite{Rybo89} where an algorithm is described which is 
originally given in \cite{Vars81}. 
Algorithms for finding generators of normal basis
are described in \cite{Gath90}, \cite{Lens91}, or \cite{Pinc89}.

Experiments have shown that in practice it is more efficient to simply run
through the field and test until an element of maximal order is found. 
However, not very many theoretical results are known on deterministic
search procedures. In some situations there are results depending on the
Extended Riemann Hypothesis, see \cite{Shou92}.

To aviod searching a second time for the same element, we store the results
after the first computation of such an element using the helper functions
\begin{verbatim}
   createPrimitiveElement()$E
   createNormalElement()$E
\end{verbatim}

The functions
\begin{verbatim}
  primitive?
  normal?
\end{verbatim}
check whether the multiplicative resp. linear associated order of an
element is maximal.

The primitivity of an $\alpha$ in $E$ is tested using the fact that
$\alpha$ is primitive in $E$ if and only if for all prime factors $p$
of $(q^n-1)$ holds:
\[\alpha^{(q^n-1)/p} \ne 1\]
see e.g. Theorem 2 of chap. I.X.1.3 in \cite{Lips81}.

For testing whether a given element $\alpha$ generates a normal basis
of $E$ over $F$ we use Theorem 2.39 of \cite{Lidl83}: $\alpha\in E$ generates a
normal basis of $E$ over $F$ if and only if $(X^n-1)$ and
$\sum_{i=0}^{n-1}{\alpha^{q^i}X^{n-1-i}}$ are relatively prime in $E[X]$

\subsection{Enumeration of elements of $E$}
\label{section4.6}

The number of elements of $E$ can be found out by calling {\tt size()\$E}.
The elements of {\tt E} are enumerated by 
\begin{verbatim}
   index: PositiveInteger -> E
\end{verbatim}
with inverse
\begin{verbatim}
   lookup: E -> PositiveInteger
\end{verbatim}

These functions implement a bijection between {\tt E} and the set of 
positive integers $\{1,2,\ldots,q^n\}$. They allow iterating over all
field elements. {\tt lookup} can be used to store field elements in a
variable of type {\tt PositiveInteger}. This is often less memory
expensive than storing the field element which may be represented in a
complicated way. For time efficiency reasons these functions have different
implementations according to the different representations. ALl of them have
in common that {\tt index}$(q^n)=0\$E$ and {\tt lookup}$(0\$E)=q^n$.

\subsection{Conversion between elements of the field and its groundfield}
\label{section4.7}

To check whether a given element $\alpha$, representation by {\tt a} in
the field {\tt E}, belongs to its groundfield $F$ use 
{\tt inGroundField?(a)}. If $\alpha$ belongs of $F$, datatype conversion
is provided by {\tt retract(a)}.

Embedding from {\tt F} to {\tt E} is done using {\tt coerce} abbreviated
by {\tt ::} in Axiom. If {\tt a} is the representation of $\alpha$ in 
{\tt F}, then {\tt (a::E)} is the element of {\tt E} representing $\alpha$.

All these functions depend on the representation used and are explained in
the sections according to the special representations.

\section{Prime field}
\label{section5}

Let $p$ be a prime number. Since $GF(p)\simeq\mathbb{Z}/p\mathbb{Z}$
in Axiom the internal representation of elements of the domains
\begin{verbatim}
   IPF(p)   InnerPrimeField(p)
   PF(p)    PrimeField(p)
\end{verbatim}
is {\tt IntegerMod(p)}, from which most functions are inherited. The
only difference between {\tt IPF} and {\tt PF} is, that in {\tt PF} it 
is checked whether the parameter $p$ is prime while this is not checked
in {\tt IPF}.

Many functions as {\tt trace} or {\tt inGroundField?} are trivial for
a prime field. For a human being there is no problem to consider a
prime field as an extension of degree 1 of itself. For recursions
depending e.g. on the extension degree and simply for completeness,
we have decided to make {\tt PrimeField} an {\tt FiniteAlgebraicExtensionField}
of itself. The trivial implementations include
\begin{verbatim}
   normalElement()    == 1
   inGroundField?(a)  == true
   generator()        == 1
\end{verbatim}

Since the value returned by {\tt generator()} should be a root of the
defining polynomial of the field extension, we had to code
{\tt definingPolynomial} to be $X-1$ and not e.g. $X$.

\subsection{Extension Constructors of Finite Fields}
\label{section5.1}

There are three choices to make when one wants to construct a
finite field as an extension of a ground field in Axiom.

The first choice is the type of representation. It can be remainder
classes of polynomials, 
see section \ref{section6}, exponents of a primitive element,
see section \ref{section7}, or a normal basis representation, 
see section \ref{section8}. The part of the 
abbreviation of the corresponding domain constructors are
{\tt FF}, {\tt FFCG}, and {\tt FFNB}, respectively.

Secondly, we have to decide which ground field we choose, either the
prime field or any other subfield. In the first case the first parameter
of all domain constructors is just a prime number and one has to use
the names above. In the other case the first parameter is the domain
constructed recursively to represent the chosen subfield.

The second parameter governs the extension. All constructions depend on an
irreducible polynomial, whose degree is the extension degree and has, if
necessary, additional properties. If one doesn't care about this polynomial,
one has to give the degree as the second parameter and the polynomial will
be chosen approximately by Axiom. In the case where we define the prime
field by supplying a prime number, this is the only choice. In the other
case one has to append the letter {\tt X} to the receive the domain 
constructors {\tt FFX}, {\tt FFCGX}, and {\tt FFNBX}, respectively. If
one wants to supply one's favorite polynomial as the second parameter we
have to substitute the letter {\tt X} by {\tt P}.

Here are a few datatype constructions for these nine possibilities:
{\tt FF(2,10)} implements an extension of the prime field {\tt PF 2}
of degree 10. Axiom chooses an irreducible polynomial of degree 10 for
this polynomial basis representation.

{\tt FFNBX(FFCGP(3,f),5)} implements an extension of the field with $3^n$
elements, represented as exponents of a primitive element, where $f$ is a
primitive polynomial of degree $n$. The extension of degree 5 is realized
by Axiom by choosing a normal polynomial of degree 5 with coefficients in
{\tt FFCGP(3,f)}.

As overloading of constructor names is not supported by the current
compiler, we had to create all these different names as explained above.
As soon as the new compiler will support this we may consider to unify
these domain domain names, see section \ref{section11}.

\section{Polynomial basis representation}
\label{section6}

Let $E=GF(q^n)$ be an extension of degree $n$ over $F=GF(q)$. Then
\[E\simeq F[X]/(f)\]
where $f\in F[X]$ is an arbitrary monic irreducible polynomial 
of degree $n$.
If $\alpha$ is a root of $f$ in $E$, then $\{1,\alpha,\ldots,\alpha^{n-1}\}$
constitutes a basis of $E$ over $F$ and we can write all elements
$\beta\in E$ uniquely in the form:
\[\beta=\sum_{i=0}^{n-1}{b_i\alpha^i},\quad b_i\in F\]

This kind of representation is used in the domains
\begin{verbatim}
   FFP   FiniteFieldExtensionByPolynomial
   FFX   FiniteFieldExtension
   IFF   InnerFiniteField
   FF    FiniteField
\end{verbatim}

The only difference between these domains are the different natures of
their parameterization, 
see section \ref{section5.1}. The datatype {\tt InnerFiniteField}
extends the prime field {\tt InnerPrimeField}, 
see section \ref{section5}. Let {\tt F}
be the domain representing $F$ and {\tt E:=FFP(F,f)} the extension of $F$
defined by $f$.

For the internal representation of elements of $E$ we use polynomials
modulo $f$. This structure is in Axiom implemented by the domain
\begin{verbatim}
   SAE(F,SUP(F),f)  SimpleAlgebraicExtension(F,SUP(F),f)
\end{verbatim}
Here {\tt SUP(F)} abbreviates {\tt SparseUnivariatePolynomial(F)} a
domain representing polynomials over $F$. Most arithmetic operations are
inherited from this domain. There are only a few functions which have a
special implementation.

The imbedding of {\tt F} in {\tt E} is obvious:
$\sum_{i=0}^{n-1}{a_i\alpha^i}$ is in $F$ if and only if
$a_1,a_2,\ldots,a_{n-1}=0$. Using this, the functions
\begin{verbatim}
   retract:       E -> F
   coerce:        F -> E
   inGroundfield? E -> Boolean
\end{verbatim}
are implemented in the obvious way.

To check whether $\beta$ is normal in $E$ over $F$ we proceed as follows.
Let $M \in F^{n\times n}$ be the matrix, whose $i$-th column is the 
coordinate vector of $\beta^{q^i}$ with respect to the polynomial basis
for $0 \le i < n$. The element $\beta$ is normal if and only if the rank
of $M$ equals $n$.

The coordinates of $\beta\alpha^i$ collected in a matrix similar as before
give the {\sl regular matrix representation} 
(see e.g. chap.7.3 in \cite{Jaco85}) of
$\beta$. The functions {\tt trace} and {\tt norm} compute the trace and
norm of $\beta$ over $F$ by computing the trace and determinant of this
matrix, respectively.

\section{Cyclic group representation}
\label{section7}

In this section we make use of the fact, that the multiplicative group of 
a finite Field $E$ with $q^n$ Elements is cyclic of order $q^n-1$. Therefore
it is isomorphic to $\mathbb{Z}/(q^n-1)\mathbb{Z}$. Once a primitive
element $\alpha$ of $E$ is fixed (i.e. a generator of the multiplicative
group), ever nonzero element $\beta$ of $E$ is uniquely determined by
its discrete logarithm $e$ to $\alpha$, i.e. the element 
$0 \le e \le q^n-1$ with $\alpha^e=\beta$.

In the three domains
\begin{verbatim}
   FFCGP   FiniteFieldCyclicGroupExtensionByPolynomial
   FFCGX   FiniteFieldCyclicGroupExtension
   FFCG    FiniteFieldCyclicGroup
\end{verbatim}
the nonzero field elements are represented by powers of a fixed primitive
element. Let {\tt F} be a finite field domain representing $F$, $E$ the
extension of $F$ defined by the monic, irreducible polynomial $f(x) \in F[X]$
with root $\alpha \in E$. Let $\alpha$ be primitive in $E$ and
{\tt E:=FFCGP(F,f)} be the domain representing $E$.

As the fixed primitive element used for the representation, we take the root
$\alpha$ of $f$. It is returned by calling {\tt generator()\$E}, which is 
equal to the function {\tt primitiveElement()\$E} in this representation.

The aim of a cyclic group representation of finite fields is to offer
a very fast field arithmetic. All operation concerning the multiplicative
structure of the field are quite easy to compute. To have a quick addition
one has to store a Zech (Jacobi) logarithm table in memory, see setion 7.2.
This table is of size about $q^n/2$. For efficiency reasons we also want
to use {\tt SmallInteger} and not {\tt Integer} as the internal 
representation of the field elements. Therefore we restricted ourself
to a field size of maximal $2^{20}$ elements:
\begin{verbatim}
   if sizeFF > 2**20 then
     error "field too large for cyclic group representation"
\end{verbatim}

A nonzero element $\beta\in E$ is represented by the unique
$n \in \{0,1,\ldots,q^n-2\}$ of type {\tt SmallInteger} with
$\alpha^n=\beta$ and $0\in E$ is represented by the {\tt SmallInteger} $-1$.

\subsection{Operations of multiplicative nature}
\label{section7.1}

The implementation of the operations concerning the multiplicative group
is very easy. Since $\alpha^n\cdot \alpha^m=\alpha^{n+m}$, multiplication
of nonzero elements becomes a {\tt SmallInteger} addition modulo $q^n-1$.
Similarly the exponentiation of field elements and the norm function are
done by a modular {\tt SmallInteger} multiplication. Inversion is nothing
more than changing the sign of the representing {\tt SmallInteger} module
$q^n-1$. Discrete logarithms to base $\alpha$ can be read off directly 
from the representation and for computing the discrete logarithm to a
arbitrary base one has to perform the extended euclidean algorithm in
$\mathbb{Z}$.

If we want to compute the discrete logarithms $d$ of $\gamma=\alpha^m$
to base $\beta=\alpha^b$ we have to solve:
\[(\alpha^b)^d=\alpha^m\]

This is solvable if and only if $m$ is divisible by 
\[g:={\rm gcd}(b,q^n-1)=rb+s(q^n-1)\]. In this case we set
\[d=\frac{rm}{g}{\rm mod}(q^n-1)\]

Computing the multiplicative order of elements is done by
\[{\rm ord}(\alpha^e)=\frac{{\rm ord}(\alpha)}{{\rm gcd}(e,{\rm ord}(\alpha))}
=\frac{q^n-1}{{\rm gcd}(e,q^n-1)}\]

Therefore an element $\beta$ is primitive in $E$ if and only if its
representation is relatively prime to $q^n-1$.

\subsection{Addition and Zech logarithm}
\label{section7.2}

Addition is performed via the Zech (or Jacobi) logarithm $Z(k)$ which is
defined by
\[\alpha^{Z(k)}=\alpha^k+1\]

In the domain {\tt E} the Zech logarithm array is stored in the global
variable
\begin{verbatim}
   zechlog : PrimitiveArray(SmallInteger)
\end{verbatim}
of the datatype of length $(q^n+1)/2$. Its $k$-th entry
corresponds to $Z(k)$. $Z(k)$ is undefined, if $\alpha^k+1=0$. 
This exception appears in characteristic 2 if $k=0$ and in odd
characteristic if $k=(q^n-1)/2$. To indicate this we define
{\tt zechlog.k=-1} in these cases. Now the sum of $\alpha^i$ and
$\alpha^j$ is computed in the following way:

Let $k$ be the smaller one of $\{(i-j){\rm\ mod\ }(q^n-1)$,
$(j-i){\rm\ mod\ }(q^n-1)\}$. Notice that $k\le(q^n-1)/2$. Then
\[\alpha^i+\alpha^j:=
\begin{cases}
0 & {\rm if}~Z(k)~{\rm undefined}\\
\alpha^{i+Z(k)} & {\rm if}~k=(j-i)~{\rm mod}~(q^n-1)\\
\alpha^{j+Z(k)} & {\rm if}~k=(j-i)~{\rm mod}~(q^n-1)
\end{cases}
\]

In addition to some {\tt SmallInteger} operatoins there is only
one access to the Jacobi logarithm array. Therefore addition is
very fast too.

Since $-1=\alpha^{(q^n-1)/2}$ in odd characteristic, $-\alpha^n$
can be computed by
\[-\alpha^n:=
\begin{cases}
\alpha^n & {\rm if\ char}(E)=2\\
\alpha^{n+(q^n-1)/2} & {\rm otherwise}
\end{cases}
\]

The Jacobi logarithm array is initialized at the first time when
it is needed. This procedure may last some time. The initialization
is done by calling the function {\tt createZechTable(f)} parameterized
with the defining polynomial of the field in the package
{\tt FiniteFieldFunctions(F)}. The user gets access to this table
by calling {\tt getZechTable()\$E}.

In K. Huber\cite{Hube90} explains how to reduce the size of the table needed
to compute all Jacobi logarithms. These observations are useful for
high extension degree, but the times for addition are increased,
therefore these ideas were not implemented.

To check if a given element belongs to $F$ is quite easy. It depends
on whether the representation $e$ of $\alpha^e$ is divisible by
$(q^n-1)/(q-1)$ or not. If $e=k(q^n-1)/(q-1)$ we have
\[(\alpha^e)^{q-1}=\alpha^{k(q^n-1)}=1\]
and therefore $\alpha^e$ belongs to $F$. The degree of $\alpha^e$ 
over $F$ is given by the minimal integer $d>0$ for which
$eq^d\equiv e{\rm\ mod\ }(q^n-1)$. The trace and the norm function
are computed directly using (1) on page 6.

\subsection{Time expensive operations}
\label{section7.3}

But there remain some operations, which are quite time expensive.
That are those operations, which change the representation of the
field elements:
\begin{verbatim}
   coerce:      F -> E
   retract:     E -> F
   represents:  Vector F -> E
   coordinates: E -> Vector F
\end{verbatim}

Let $\beta=N_{E/F}(\alpha)$ be the norm of $\alpha$ over $F$. 
Since $\beta$ is primitive in $F$, every nonzero element $\gamma$
of $F$ can be expressed by a power of $\beta$. We get
\[\gamma=\beta^e=\alpha^{e\frac{q^n-1}{q-1}}\]
for a suitable value $0 \le e \le q-1$. This $\beta$ is stored
in the global variable {\tt primEltGF} in {\tt E}. The function
{\tt retract} applied to $(\alpha^e)$ first checks if the element
$\alpha^e$ belongs to $F$. In this case $e$ is divisible by
$(q^n-1)/(q-1)$ and {\tt retract} can raise {\tt primEltGF} in 
{\tt F} to the power $e(q-1)/(q^n-1)$.

{\tt coerce} applied to $\gamma\in F$, $\gamma \ne 0$, computes
the discrete logarithm of $\gamma$ to the base {\tt primEltGF} in
{\tt F} and multiples this value by $(q^n-1)/(q-1)$ to get the
desired representation of $\gamma$ in {\tt E}.

{\tt coordinates} applied to $(\alpha^e)$ raises the residue 
class $(X{\rm\ mod\ }f)$ to the power $e$. This is performed in a
{\tt SimpleAlgebraicExtension} of the {\tt F} by {\tt f}. The
returned vector is the coordinate vector of $\alpha^e$ to the
polynomial basis generated by $\alpha$.

{\tt represents} considers the given vector as coordinate vector of
an element $\beta$ in\\
{\tt FiniteFieldExtensionByPolynomial(F,f)}
and computes its discrete logarithm to base $\alpha$ in that domain.
This logarithm is the representation of $\beta$ in {\tt E}.

\section{Normal basis representation}
\label{section8}

Let $E=GF(q^n)$ be an extension of degree $n$ over the finite
field $F=GF(q)$ and $f\in F[X]$ be the polynomial which defines
the extension. Assume further that the roots 
$\{\alpha,\alpha^q,\ldots,\alpha^{q^{n-1}}\}$ of $f$ in $E$ are
linearly independent over $F$. Then $\alpha$ is normal in $E$
over $F$ and every element of $E$ can be expressed in the form
\[\beta=\sum_{i=0}^{n-1}{b_i\alpha^{q^i}}\] 
with $b_i\in F$. This kind of representation is used in the domains
\begin{verbatim}
   FFNBP   FiniteFieldNormalBasisExtensionByPolynomial
   FFNBX   FiniteFieldNormalBasisExtension
   FFNB    FiniteFieldNormalBasis
\end{verbatim}

Let {\tt F} be a finite field domain representing $F$ and
{\tt E:=FFNBP(F,f)} the normal basis extension of $F$ by $f$.

We get the root $\alpha$ of $f$ in $E$ by calling {\tt generator()\$E}
which in this representation is equal to {\tt normalElement()\$E}.

The internal representation of the elements of {\tt E} is
{\tt Vector F}. The element $\beta=\sum_{i=0}^{n-1}{b_i\alpha^{q^i}}$
is represented by the coordinate vector
$(b_0,b_1,\ldots,b_{n-1})$ of $\beta$ w.r.t. the normal basis
generated by $\alpha$ and computed by {\tt coordinates}$(\beta)$.
The normal basis is returned by {\tt basis()\$E}. In the sequel we
identify coordinate vectors $(b_0,b_1,\ldots,b_{n-1})$
representing $\sum_{i=0}^{n-1}{b_i\alpha^{q^i}}$ with the
corresponding polynomial
$b=\sum_{i=0}^{n-1}{b_iX^i}\in F[X]/(X^n-1)$, since
\[b\circ\alpha=\sum_{i=0}^{n-1}{b_i\alpha^{q^i}}\]

The lengthy code for the arithmetic is shared by the three different
versions of normal basis representations and the package
{\tt FiniteFieldFunctions}. Hence, we decide to have a 
parameterized package
\begin{verbatim}
   INBFF   InnerNormalBasisFieldFunctions(F)
\end{verbatim}
where most of the arithmetic in {\tt E} is performed.

\subsection{Operations of additive nature}
\label{section8.1}

All field functions concerning the cyclic $F[X]$-module structure
of the additive group of $E$ are very easy to implement and to 
compute. Since
\[\beta^q=\sum_{i=0}^{n-1}{b_{(i-1){\rm\ mod\ }n}\alpha^{q^i}}\]
the Frobenius automorphism becomes a simple cyclic shift of the
coordinate vector. The linear associated logarithm of $\beta$ to
base $\alpha$ can be directly read off from the representation of
$\beta$
\[Log_\alpha(\beta)=\sum_{i=0}^{n-1}{b_iX^i=b}\]
To compute the linear associated logarithm $a$ of $\beta$ to
another logarithm base $\gamma=c\circ\alpha$, one has to perform
an extended euclidean algorithm in $F[X]$:
\[a\circ\gamma=\beta \Longleftrightarrow (ac)\circ\alpha=b\circ\alpha\]
This is solvable if and only if $b$ is divisible by 
$g:={\rm gcd}(c,X^n-1)=rc+s(x^n-1)$. In this case we get
\[a=\frac{rb}{g}{\rm\ mod\ }(X^n-1)\]

The operation $\circ$ becomes a modular polynomial multiplication
\[h\circ\beta=(hb)\circ\alpha\]
for $h\in F[X]$. The linear associated order of $\beta$ can be
computed using
\[{\rm Ord}_q(b\circ\alpha)=
\frac{{\rm Ord}_q(\alpha)}{{\rm gcd}(b,{\rm Ord}_q(\alpha))}=
\frac{(X^n-1)}{{\rm gcd}(b,X^n-1)}\]
Therefore $\beta$ is normal in $E$ over $F$ if and only if
gcd$(b,X^n-1)=1$, which is quite easy to check. The degree of
$\beta$ over $F$ is given by the minimal integer $d>0$ which satisfies
\[bX^d\equiv b{\rm\ mod\ }(X^n-1)\]

The embedding of {\tt F} into {\tt E} is determined by the trace of
$\alpha$: Let $a=T_{E/F}(\alpha)$, then 
$1-T_{E/F}(a^{-1}\alpha)=\sum_{i=0}^{n-1}{a^{-1}\alpha^{q^i}}$
and we get for $d\in F$
\[d=\sum_{i=0}^{n-1}{(a^{-1}d)\alpha^{q^i}}\]
which gives as representation of $d$ in {\tt E} the vector 
consisting of equal entries $a^{-1}d$. Since the value
$T_{E/F}(\alpha)$ is needed quite often, it is stored in the global
variable {\tt traceAlpha} in {\tt E}. The trace $T_{E/F}(\beta)$ of
$\beta=\sum_{i=0}^{n-1}{b_i\alpha^{q^i}}$ is simply computed by
\[T_{E/F}(\beta)=\sum_{i,j=1}^{n-1}{b_i\alpha^{q^{i+j}}}=
\sum_{j=0}^{n-1}{(\sum_{i=0}^{n-1}{b_i})\alpha^{q^i}}\]
Traces onto intermediate fields of $F \le E$ are computed in a
similar fashion.

\subsection{Multiplication and normal basis complexity}
\label{section8.2}

In the contrary to the {\sl additive} functions, the operations
concerning the multiplicative structure of the field are more
difficult to compute. Actually the multiplication of field elements
is somewhat complicated and hence slow.

To multiply field elements we use the representing matrix
$M_\alpha=(m_{i,j})\in F^{n\times n}$ of the left multiplication
by $\alpha$ w.r.t. the distinguished normal basis, which is called
{\sl multiplication matrix} in Geiselmann/Gollmann \cite{Geis89}:
\[\alpha\alpha^{q^i}=\sum_{j=0}^{n-1}{m_{i,j}\alpha^{q^j}}\]

Knowing this matrix the product of 
$\beta=\sum_{i=0}^{n-1}{b_i\alpha^{q^i}}$ and
$\gamma=\sum_{j=0}^{n-1}{c_j\alpha^{q^j}}$ is given by
\[\beta\gamma=\sum_{i,j=0}^{n-1}{b_ic_j(\alpha^{q^{i-j}+1})^{q^j}}=
\sum_{i,j,k=0}^{n-1}{b_ic_jm_{i-j,k-j}\alpha^{q^k}}\]
with indices module $n$. This shows immediately that multiplication
in this representation needs $O(n^3)$ $F$-operations.

Recently there has been much interest in so called {\sl low
complexity} and {\sl optimal} normal bases: By choosing the normal
element $\alpha\in E$ carefully one tries to minimize the number of
nonzero entries in $M_\alpha$, which is called the {\sl complexity}
of the normal basis. This obviously reduces the multiplication time. 
In the best case one can reduce the number of entries to $2n-1$.
For special pairs $(q,n)$ a direct construction of a normal base
of low complexity $O(kn)$, $k\ll n$, is possible (see Wassermann \cite{Wass89},
Beth/Geiselmann/Meyer\cite{Beth91}, Mullin/Onyszchuk/Vanstone\cite{Mull88}
or Ash/Blake/Vanstone\cite{Ashx89}). The problem of efficiently computing
the minimal normal basis complexity or even a generator of such
a base for given $(q,n)$ is unsolved.

The algorithm described by A. Wassermann in \cite{Wass89} is implemented
in the function
\begin{verbatim}
   createLowComplexityNormalBasis(n)
\end{verbatim}
in the package {\tt FiniteFieldFunctions(F)}. If for the given
$(q,n)$ a direct construction of a low complexity normal basis
is possible, the algorithm computes the multiplication matrix of
this base and the function returns this matrix in form of a
variable of type 
{\tt Vector List Record(value:F,index:SmallInteger)} (see below).
If such a construction is not possible for $(q,n)$ the function
\begin{verbatim}
   createNormalPoly(n)$FiniteFieldPolynomialPackage(F)
\end{verbatim}
is called to produce a normal polynomial of degree $n$. To have the
nice embedding $d\mapsto(d,d,\ldots,d)$ of {\tt F} in {\tt E},
the computed normal basis has in both cases the property that its
generator has trace 1 over $F$. The constructors {\tt FFNBX} and
{\tt FFNB} makes use of this function and use, if possible, 
automatically a low complexity normal basis.

If we would store the multiplication Matrix $M_\alpha$ in a variable
of type {\tt Matrix F}, everytime we multiply we would have to inspect
all $n^2$ entries of $M_\alpha$. In this case a low complexity basis
would hardly speed up the multiplication time.

This is why we store $M_\alpha$ in the domain $E$ in a global
variable of the form
\begin{verbatim}
   multTable : Vector List Record(value:F,index:SmallInteger)
\end{verbatim}

The entry $m_{i,j}$ of $M_\alpha$ corresponds to the element of
{\tt multTable.i} with {\tt index} $j-1$ and {\tt value} $m_{i,j}$.
Of course only the nonzero entries of $M_\alpha$ are stored in
{\tt multTable}. When multiplying now we are inspecting only the
nonzero entries of $M_\alpha$ and get time advantages using bases
of low complexity.

The first time when the multiplication matrix $M_\alpha$ is needed,
it is initialized by an automatic call of the function
{\tt createMultiplicationTable(f)} in the package 
{\tt FinitFieldFunctions(F)}.

The user has access to the multiplication matrix of the field by
\begin{verbatim}
   getMultiplicationTable: () ->
        Vector List Record(value:F,index:SmallInteger)
     ++ getMultiplicationTable() returns the multiplication
     ++ table for the normal basis of the field
   getMultiplicationMatrix: () -> Matrix F
     ++ getMultiplicationMatrix() returns the multiplication
     ++ table in the form of a matrix
\end{verbatim}

The complexity of the normal basis can be found out by calling
\begin{verbatim}
   sizeMultiplication:() -> NonNegativeInteger
\end{verbatim}

\subsection{Norm and multiplicative inverse}
\label{section8.3}

The functions {\tt norm} and {\tt inv} are the power functions with
exponents $(q^n-1)/(q-1)$ and $(q^n-2)$, respectively. We do not use
the default repeated squaring algorithm, as we can do better: The
algorithm due to Itoh and Tsujii\cite{Itoh88} uses a clever partitioning of
these special exponents and the following help function {\tt expPot}.
It computes
\[expPot(\beta,k,d)=\prod_{i=0}^{k-1}{\beta^{q^{id}}}\]
for $\beta\in E$ and integers $k,d>0$ is computed by the (slightly
simplified) algorithm
\begin{verbatim}
   expPot(beta,k,d) ==
     e:Integer:=0
     gamma:E:=1
     for i in 0..length(k) repeat
       if bit?(k,i) then gamma:=gamma * beta**(q**e); e:=e+d
       beta:=beta * beta**(q**d); d:=2*d
     return(gamma)
\end{verbatim}
where {\tt length(k)} denotes the number of bits of $k$, i.e.
$\ceiling{log_2(k+1)}$, and {\tt bit?(k,i)} tests whether the
$i$-th bit of $k$, binary represented, is set or not. The average
number of $E$-multiplications of this algorithm is about 
$3/2\ceiling{log_2(k)}$

Let $d$ be a divisor of $n$ and $K$ and extension of degree $d$
over $F$. With the above algorithm we can compute the norm of
$\beta\in E$ over $K$ by
\[N_{E/K}(\beta)={\rm expPot}(\beta,n/d,d)\]
using about $3/2\ceiling{log_2(n/d)}$ multiplications in $E$.

For computing the inverse $\beta^{-1}=\beta^{q^n-2}$ of $\beta\in E$
notice that
\[q^n-2=(q-2)(\frac{q^n-1}{q-1})+q(\frac{q^{n-1}-1}{q-1})\]
therefore
\[\beta^{-1}=N_{E/F}(\beta)^{-1}\cdot(\beta^{\frac{q^{m-1}-1}{q-1}})^q\]

Now we get $\beta^{-1}$ by computing first
\[\gamma={\rm expPot}(\beta,n-1,1)^q=(\beta^{\frac{q^{m-1}-1}{q-1}})^q\]
and then $\beta^{-1}=(\beta\gamma)^{-1}\cdot\gamma$. Notice that the
inversion of $\beta\gamma$ is performed in $F$. Altogether we used
$3/2\ceiling{log_2(n)}+2$ multiplications in $E$.

\subsection{Exponentiation}
\label{section8.4}

Next we show how we have implemented the function
\[(\beta,e)\rightarrow \beta^e\]

For a $1 \le k < n$ we can write
\[\beta^e=\prod_i{(\beta^{e_i})^{q^{ki}}}\]
if $e=\sum_i{e_iq^{ik}}$ is the $q^k$-adic expansion of $e$.

An obvious implementation of this formula first of all has to 
initialize the array $[\beta,\beta^2,\ldots,\beta^{q^k-1}]$.
This costs $(q-1)q^{k-1}-1$ (expensive) field multiplications.
Taking $q^{ki}$-powers is cheap while multiplying the results
together costs another $\ceiling{(log_q(e)/k)}-1$ field
multiplications, altogether this algorithm requires
\[M(q,k,e):=(q-1)q^{k-1}+\ceiling{\frac{log_q(e)}{k}}-2\]
multiplications.

Depending on $k$ there is a tradeoff between slow multiplication
and fast powering. Therefore we adaptively choose a good $k$
depending on $e$ and $q$ to minimize the number of multiplications.

The computation of such 
$k\sim log_q log_q(e)-log_q log_q log_q(e)$ is performed using
exclusively {\tt SmallInteger} arithmetic to minimize the decision
time. It is supported by the two global variables
\begin{verbatim}
   logq:List SmallInteger
   expTable:List List SmallInteger
\end{verbatim}
which contain some precomputed auxiliary values.

Then the actual number of multiplications is compared with the
number $3/2\ceiling{log_2(e)}$ of multiplications needed by the
standard repeated squaring algorithm and the better method is
chosen.

The ideas of this divide and conquer algorithm are due to Stinson,
see \cite{Stin90}, for the case $q=2$.

\section{Homomorphisms between finite fields}
\label{section9}

Changing an object from one finite field to another can be necessary
in three cases. The first case is that we have two different defining
polynomials for the same field in the same type of representations. In
order to consider one object in the other data type we have to
implement a field isomorphism. A generalization thereof is when one
field is isomorphic to a subfield of another field in the same type of
representation. The most complicated case is when in addition to the
last situation we also change the representation.

These data type conversions - called {\sl coercions} in Axiom, and
hence are under the control of the interpreter - are realized by the
package 
\begin{verbatim}
   FFHOM FiniteFieldHomomorphisms 
\end{verbatim}

It is parameterized by three parameters: a source field $K_1$
represented by {\tt K1}, a target field $K_2$ represented by {\tt K2}
and a common groundfield $F$ of $K_1$ and $K_2$, represented by {\tt
F}. Note, that due to the symmetry of the provided functions, the
order of the parameters can either be $(K1,F,K2)$ or $(K2,F,K1)$. The
order comes from arranging the situation in a lattice.

However, this package cannot be used for the general situation as this
settings suggests. We had to restrict ourselves to the case where both
{\tt K1} and {\tt K2} are realized as simple extensions of {\tt F},
i.e. of type 
\begin{verbatim} 
   FiniteAlgebraicExtensionField(F)
\end{verbatim}

To implement the general case also, it would be necessary to have a
function 
\begin{verbatim} 
   groundfield: () -> FiniteFieldCategory
\end{verbatim}

Its result could be used for package calling functions of that
subfield. Using such a function it would be possible to build a
recursive coercion algorithm between different towers of finite field
extensions. As the old compiler does not support functions whose
values are domains, this idea will be possible when the new compiler
will be available.

The source field and the destination field may appear in arbitrary
order in the parametrization of {\tt FFHOM}, since {\tt FFHOM}
supports coercions in both directions: 
\begin{verbatim} 
   coerce: K1 -> K2 
   coerce: K2 -> K1 
\end{verbatim}

Restricted to $K_1 \cap K_2$ these two mappings are inverses of each
other, i.e. for $\alpha \in K_1 \cap K_2$ and $a$ being a
representation of $\alpha$ in {\tt K1} or in {\tt K2} holds:
\begin{verbatim} 
   coerce(coerce(a)$FFHOM(K1,F,K2))$FFHOM(K1,F,K2)=a
\end{verbatim}

To be independent of the ordering of the arguments we have ordered the
fields inside the package by comparing the defining polynomials of the
fields lexicographically using the local function {\tt compare}.

To explain the details of the implementation let $\beta\in K_1$ and
{\tt b} be the representation of $\beta$ in {\tt K1}. We have to
distinguish between some cases:

First check whether $\beta$ is in $F$. In this case 
\begin{verbatim}
   retract(b)@F$K1::K2 
\end{verbatim} 
is used.

The next case is that {\tt K1} and {\tt K2} are constructed using the
same defining polynomial $f$. If furthermore {\tt K1} and {\tt K2} are
represented in the same say, the elements of {\tt K1} and {\tt K2} are
represented completely identical. Therefore the coercion may be
performed by 
\begin{verbatim} 
   b pretend K2 
\end{verbatim}

Now assume that one of {\tt K1} and {\tt K2} is represented using
cyclic groups and the other one represented by a polynomial basis.

If {\tt K1} is cyclically represented, we coerce by 
\begin{verbatim}
   represents(coordinates(b)$K1)$K2 
\end{verbatim} 
and vice versa, if {\tt K2} is cyclically represented.

All remaining cases are treated in the same way which we explain
now. Denote by {\tt degree1} and {\tt degree2} the extension degrees
of $K_1$ and $K_2$ over $F$, respectively. The first time a coercion
from {\tt K1} into {\tt K2}, or vice versa, is called, two conversion
matrices stored in global variables 
\begin{verbatim} 
   conMat1to2:Matrix F 
     -- conversion Matrix for the conversion direction K1 -> K2
   conMat2to1:Matrix F 
     -- conversion Matrix for the conversion direction K2 -> K1 
\end{verbatim} 
in the package are initialized. Once these
matrices are initialized, the coercion is performed by
\begin{verbatim} 
   represents(conMat1to2 * coordinates(b)$K1)$K2
\end{verbatim}

Notice, that we do not have to care about the coercion between cyclic
representation and polynomial representation as above, since this step
is implicitly performed by the function calls to {\tt represents} and
{\tt coordinates} (see section \ref{section7.3}).

The rest of this section describes the initialization of the conversion
matrices.

\subsection{Basis change between normal and polynomial basis
representation}
\label{section9.1}

We first consider the case of equal defining polynomials. We can
assume without loss of
generality that $E=K_1=K_2$ and the root $\alpha\in E$ of this
polynomial both generates a polynomial and a normal basis. To
convert $\beta=\sum_{i=0}^{n-1}{b_i\alpha^i}$ into
$\beta=\sum_{j=0}^{n-1}{c_j\alpha^{q^j}}$ we have to set up the
basis change matrix $M=(m_{i,j})$, defined by
\[\alpha^{q^j}=\sum_{i=0}^{n-1}{m_{i,j}\alpha^i,\quad 0\le j<n}\]
Then we have
\[\beta=\sum_{j=0}^{n-1}{c_j\alpha^{q^j}}=
\sum_{i,j=0}^{n-1}{c_jm_{i,j}\alpha^i}\]
consequently
\[b_i=\sum_{j=0}^{n-1}{c_jm_{i,j}}\]

Therefore $M$ can be used to change between polynomial representation
and normal representation by
\[M(c_0,c_1,\ldots,c_{n-1})^t=(b_0,b_1,\ldots,b_{n-1})^t\]
and
\[M^{-1}(b_0,b_1,\ldots,b_{n-1})^t=(c_0,c_1,\ldots,c_{n-1})^t\]
where the $t$ denotes the transposition of the vector. The matrix
$M$ is computed efficiently by using the function
{\tt reducedQPowers} in the package\\
{\tt FiniteFieldPolynomialPackage(F)}.

\subsection{Conversion between different extensions}
\label{section9.2}

Now suppose that {\tt K1} and {\tt K2} are built using different
defining polynomials {\tt defpol1} and {\tt defpol2}, respectively.
As noticed earlier we have to order these polynomials to get the
same homomorphisms when calling {\tt FFHOM} with parameters
{\tt K1} and {\tt K2} swapped.

Without loss of generality 
let {\tt defpol1} be the lexicographical smaller polynomial
of {\tt defpol1} and {\tt defpol2}. In the other case '1' and '2' are
swapped in what follows. Let $n_1$ and $n_2$ be the extension 
degrees of $K_1$ and $K_2$ over $F$, respectively. Note that
$n_2 \ge n_1$.

We first compute a root 
$\alpha_1=\sum_{i=0}^{n_2-1}{a_i\alpha_2^i}$ of {\tt defpol1} in a
polynomial basis representation {\tt FFP(F,defpol2)} of $K_2$ using
\begin{verbatim}
   rootOfIrreduciblePoly(defpol1)$FFPOLY2(FFP(F,defpol2),F)
\end{verbatim}

By powering this root we compute the Matrix 
$R=(r_{i,j}) \in F^{n_2\times n_1}$ defined by
\[\alpha_1^j=\sum_{i=0}^{n_2-1}{r_{i,j}\alpha_2^i}\quad 0\le j<n_1\]

If $\beta=\sum_{j=0}^{n_1-1}{b_j\alpha_1^j\in K_1}$ we get
\[\beta=\sum_{j=0}^{n_1-1}\sum_{i=0}^{n_2-1}{b_jr_{i,j}\alpha_2^i}=
\sum_{i=0}^{n_2-1}{c_i\alpha_2^i}\]
where $c_i=\sum_{j=0}^{n_1-1}{b_jr_{i,j}}$. In the situation where
both representations are polynomial it is enough to use $R$ for
converting elements from $K_1$ into $K_2$ by
\[(c_0,c_1,\ldots,c_{n_2-1})^t=R(b_0,b_1,\ldots,b_{n_1-1})^t\]

To construct a concrete matrix representation $S$ of a left inverse
linear map of the $F$-Monomorphism represented by $R$ we proceed as
follows. We construct a square matrix $P$ by taking the first $n_1$
linearly independent rows of $R$, invert $P$ and put zero columns
at the proper places of $P$ to get $S\in F^{n_1\times n_2}$.

If one or both representations are normal, we use the basis change
matrices from section \ref{section9.1} 
to reduce this case to the polynomial case.
More precisely, if $K_2$ is normal basis represented we construct a
basis change matrix $M_2\in F^{n_2\times n_2}$ and compute
\[R:=M_2^{-1}R,\quad S:=SM_2\]

If $K_1$ is normal basis represented we construct a basis change
matrix $M_1\in F^{n_1\times n_1}$ and compute
\[R:=RM_1,\quad S:=M_1^{-1}S\].

After this computation $R$ and $S$ are the desired conversion matrices
\begin{verbatim}
   conMat1to2:=R
   conMat2to1:=S
\end{verbatim}

Now we can coerce from {\tt K1} into {\tt K2} by
\begin{verbatim}
   represents(conMat1to2 * coordinates(b)$K1)$K2
\end{verbatim}
and in the other direction by
\begin{verbatim}
   represents(conMat2to1 * coordinates(b)$K2)$K1
\end{verbatim}

\section{Polynomials over finite fields}
\label{section10}

Let $F=GF(q)$ and $E=GF(q^n)$ be represented by the domain
{\tt F} and {\tt E}, respectively. There are two packages with
functions concerning polynomials over finite fields.
\begin{verbatim}
   FFPOLY(F)      FiniteFieldPolynomialPackage(F)
   FFPOLY2(E,F)   FiniteFieldPolynomialPackage2(E,F)
\end{verbatim}

\subsection{Root finding}
\label{section10.1}

Although Axiom has a good factorizer for polynomials over fields,
which can be used to find roots in extensions, we have implemented
a function
\begin{verbatim}
   rootOfIrreduciblePoly(f)
\end{verbatim}
in the package {\tt FFPOLY2(E,F)}. This function computes only one
root in $E$ of a monic, irreducible polynomial $f\in F[X]$. This is
useful as in section \ref{section9} 
we have described how the knowledge of one
root is enough to find homomorphisms between finite fields by the
package {\tt FFHOM}. Furthermore, this implementation is in general
faster than the whole factorization. It uses Berlekamp's algorithm
as described in chap.3.4 of \cite{Lidl83}.

\subsection{Polynomials with certain properties}
\label{section10.2}

In {\tt FFPOLY(F)} there are functions concerning the properties
{\sl irreducibility}, {\sl primitivity}, {\sl normality} of monic
polynomials. Furthermore, Lenstra and Schoof proved in \cite{Lens87} the
existence of polynomials of arbitrary degree, which are both
primitive and normal for arbitrary finite of finite fields $F$.

Hence, we have four kinds of properties. We have implemented 
functions to check, whether a given polynomial is of a certain kind,
create a polynomial of a given degree of a certain kind, search the
next polynomial of a certain kind w.r.t. an ordering that will be
described below, and compute the number of polynomials of a given
degree of a certain kind.

The functions for normal polynomials are for example
\begin{verbatim}
   normal?(f)
   createNormalPoly(n)
   nextNormalPoly(f)
   numberOfNormalPoly(n)
\end{verbatim}

Unfortunately, there is no formula known to compute the number of
primitive and normal polynomials of a given degree. Up to this
exception similar operations are available for all four kinds of
properties. To get these functions one has to substitute {\tt normal}
by {\tt irreducible}, {\tt primitive}, or {\tt normalPrimitive}
(or equivently {\tt primitiveNormal}), respectively.

\subsection{Testing whether a polynomial is of a given kind}
\label{section10.3}

The function {\tt irreducible?} is in the package
{\tt DistinctDegreeFactorize}, where it had been implemented for
polynomial factorization purposes. It uses the fact that a
polynomial $f\in F[X]$ of degree $n$ is irreducible, if and only 
if $f$ is relatively prime to $X^{q^i}-X$ for all 
$i=1,2,\ldots,\floor{n/2}$. This and two other methods for
testing irreducibility are described in a nice way in A.K. Lenstra's
paper\cite{Lens82}.

To check whether a polynomial $f\in F[X]$ of degree $n$ is primitive
we use the algorithm described on page 87 in \cite{Lidl83} together with
theorem 3.16. It is the same algorithm as used for testing elements
of finite field domains on primitivity:

$f$ is primitive if and only if $X^{q^n-1}{\rm\ mod\ }f=1$ and if
for all prime factors $p$ of $q^n-1$ holds:
\[X^{(q^n-1)/p}{\rm\ mod\ }f \ne 1\]

To check whether a polynomial $f\in F[X]$ of degree $n\ge 1$ is
normal, we first check its irreducibility and compute then the
residues
\[X^{q^i}{\rm\ mod\ f},\quad {\rm for\ }0\le i < n\]
using the function {\tt reducedQPowers(f)}. Now $f$ is normal if
and only if this $n$ residues are linearly independent over $F$.

To check whether a polynomial is primitive and normal we have to
combine both tests.

\subsection{Searching the next polynomial of a given kind}
\label{section10.4}

In this section we describe various total orderings on the set of
monic polynomials $f=X^n+\sum_{i=0}^{n-1}{f_iX^i}\in F[X]$ of degree
$n$ with constant nonzero term $f_0$. These orderings are chosen
carefully to reflect special requirements on embedding and
constructing properties and sparsity for the various kinds.

Let $\#f$ be the number of nonzero coefficients of $f$ and 
terms$(f)$ denote the vector $(i:f_i\ne 0)$ of exponents appearing
in $f$ in ascending order. The core of all our orderings is the
following total order. If $g=X^n+\sum_{i=0}^{n-1}{g_iX^i}$ is
another element in $F[X]$, then $f<g$, if $f$ is more sparse than
$g$, i.e. $\#f < \#g$. This is very useful, as dealing with sparse
polynomials in polynomially represented finite field arithmetic 
is less time consuming, see section \ref{section12}. 
The ordering then is refined
by the lexicographical ordering of the exponent vectors terms$(f)$
and terms$(g)$, and, if they are equal, too, by the lexicographical
ordering of the coefficient vectors, induced by the comparison of
elements in $F$ via {\tt lookup}, i.e. for elements {\tt fi},
{\tt gi} of {\tt F}, {\tt fi $<$ gi} if and only if
{\tt lookup(fi) $<$ lookup(gi)}.

For the irreducibility we directly use this ordering. 
{\tt nextIrreduciblePoly(f)} returns the next irreducible polynomial
of degree $n$ which is greater than $f$.

The ordering for {\tt nextPrimitivePoly(f)} first compares the
constant terms. If $f_0=g_0$, then we use the ordering defined
above. This is for efficiency reasons. Changing $f_0$ requires the
computation of the next (w.r.t {\tt lookup}) primitive element of
$F$. This procedure takes more time than changing other coefficients
and is therefore done last.

Similarly, for normal polynomials we first pick the trace coefficients
$f_{n-1}$ and $g_{n-1}$ and compare these, if they are equal we
compare as described above. So we have tied together all those
normal polynomials, which define the same embedding of the ground
field.

For {\tt nextPrimitiveNormalPoly(f)} we order first according to
the constant terms, then according to the trace terms and finally
as above.

All of the functions yield {\tt "failed"} when called with a 
polynomial of degree $n$, which is greater than the greatest 
polynomial with the required property.

\subsection{Creating polynomials}
\label{section10.5}

To create an irreducible polynomial Axiom proceeds as follows.
Depending on the desired degree $n$, a start polynomial $f$ is
initialized with one value of 
$\{X^n,X^n+1,X^n+X\}$. Then {\tt nextIrreduciblePoly(f)} is called
to produce an irreducible polynomial.

If $f(X)=X^n+\sum_{i=0}^{n-1}{f_iX^i}\in F[X]$ is primitive and
$\alpha$ is a root of $f$ in $E$ then $N_{E/F}(\alpha)=(-1)^nf_0$
is primitive in $F$. Since the norm function is surjective, there
exist primitive polynomials $f$ for all $(-1)^nf_0$ which are
primitive in $F$. Therefore, to get a primitive polynomial $f_0$
is set to $(-1)^n$ times {\tt primitiveElement()\$F}. Then the
polynomials with $(f_1,\ldots,f_{n-1})$ ranging over $\{0,1\}^{n-1}$
are checked on primitivity in increasing order. The used ordering
of $\{0,1\}^{n-1}$ is ordering of the vectors by (hamming) weight,
where the vectors of equal weight are ordered lexicographically.
If no primitive polynomial is found by this procedure
{\tt nextPrimitivePoly}$(X^n+f_0)$ is called to produce one.

If $f(X)=X^n+\sum_{i=0}^{n-1}{f_iX^i}\in F[X]$ is normal over $F$
and $\alpha$ is a root of $f$ in $E$ then 
$T_{E/F}(\alpha)=(-f_{n-1})\ne 0$. Since the trace function is
surjective, there exist normal polynomials $f$ for all choices of
$0 \ne f_{n-1}\in F$. We set $f_{n-1}=-1$, thus $T_{E/F}(\alpha)=1$
and we get a nice embedding of $F$ into $E$ 
(see section \ref{section8.1}). A
normal polynomial is generated by calling 
{\tt nextNormalPoly}($X^n-X^{n-1})$.

Normal polynomials which are primitive, too, are constructed
analogically. We set $f_{n-1}=-1$ and $f_0=(-1)^nc$, where
$c=${\tt primitiveElement()\$F}. Then
{\tt nextNormalPrimitivePoly}$(X^n-X^{n-1}+f_0)$ is called.

\subsection{Number of polynomials of a given kind and degree}
\label{section10.6}

Formulae for the number of polynomials of the above kinds can be
found in many textbooks about finite fields, e.g. \cite{Lidl83}

The number of monic irreducible polynomials in $F[X]$ of degree $n$
is given by 
\[\frac{1}{n}\sum_{d|n}{\mu(n/d)q^d}\]
where $\mu$ denotes the number-theoretical M\"obius function on
$(\mathbb{N},|)$, see Theorem 3.25 in \cite{Lidl83}.

The number of primitive polynomials in $F[X]$ of degree $n$ is 1 if
$q=2$ and $n=1$. Otherwise it is given by
\[\frac{1}{n}\varphi(q^n-1)\]
where $\varphi$ denotes the Euler $\varphi$-function, 
see Theorem 3.5 in \cite{Lidl83}.

The number of normal polynomials in $F[X]$ of degree $n$ is given by
\[\frac{1}{n}q^n\prod_{n_i}{(1-q^{-n_i})}\]
where the product ranges over the degrees $n_i$ of the distinct
monic irreducible polynomials appearing in the canonical 
factorization of $(X^n-1)$ in $F[X]$, see Theorem 3.73 in \cite{Lidl83}.

\subsection{Some other functions concerning polynomials}
\label{section10.7}

There are two functions for generating random polynomials.

{\tt random(n)} yields a monic random polynomial of degree $n$.
For the random selection of the coefficients the function
{\tt random()\$F} from {\tt F} is used.

The function {\tt random(n,m)} yields a random polynomial $f$ of
degree $n \le deg(f) \le m$. After determining a random degree $d$
by using the function {\tt random()\$Integer} the polynomial is
computed by calling {\tt random(d)}.

The least affine multiple of a polynomial $f$ is defined to be
the polynomial $g\in F[X]$ of least degree of the form
\[g=\sum_i{g_iX^{q^i}}+\tilde{g},\quad g_i,\tilde{g}\in F_q\]
which is divisble by $f$ -- remember that $q$ is the order of $F!$.
The function call\\
{\tt leastAffineMultiple(f)} computes the least
affine multiple of $f$ using the algorithm describe on page 112
of \cite{Lidl83}.

Let $f\in F[X]$ be monic irreducible of degree $n$ and $\alpha$ 
be a root of $f$ in $E$. The matrix $Q\in F^{n\times n}$ defined by
\[\alpha^{q^j}=\sum_{i=0}^{n-1}{q_{i,j}\alpha^i},\quad
0 \le j < n\]
plays an important role in many computations concerning finite
fields (see e.g. section \ref{section9.1}). 
Therefore we implemented the function
{\tt reducedQPowers(f)}, which yields the array
\[[X{\rm\ mod\ }f,X^q{\rm\ mod\ }f,\ldots,X^{q^{n-1}}{\rm\ mod\ }f]\]
of $n$ polynomials. This array is computed efficiently using the
fact that powering by $q$ is a $F$-linear operation.

\section{Future directions}

As already mentioned in the text the new compiler will open much
more flexibility with datatypes. This could be used for an 
implementation which realizes each finite field datatype as an
extension of each of its subfields.

The use of domain-valued functions as e.g. {\tt groundField} would
allow more general extension mechanisms which in particular could
be used for recursive {\tt coerce}-functions between different
towers of extensions and representations.

Next we discuss the problem of better embedding between different
extensions for special representations. In the case of cyclic 
group representation J.H. Conway has suggested to use
{\sl norm-compatible} defining polynomials, which were called
{\sl Conway}-polynomials (satisfying an additional condition)
by R.A. Parker (see \cite{Nick88}). 
These polynomials are primitive polynomials,
whose roots are related to each other by the norm functions. Such
classes of polynomials are difficult to compute. The pay-off of this
computations are easy embeddings by integer multiplications.

The second author has developed a theory of {\sl trace-compatible}
defining polynomials for the embedding of normal basis representation
of the extensions. The resulting embedding are polynomials
multiplications in $F[X]$. He has implemementations of both these
methods in Axiom and furthermore he has used these for an
implementation of different representations of the algebraic
closure of a finite field. Contrary to all the other datatypes,
which became part of the Axiom system, these domain constructors
are right now not generally available. We hope that finally there
will be a way of distribution of this code.

A detailed description of both Conway's and the 
trace-compatibility method as well as the further implementations
can be found in the {\sl Doktorarbeit} \cite{Sche93} of the second author,
see also \cite{Lidl83}.

\section{Comparison of computation times between different
representations}
\label{section12}

To demonstrate the effects of different representations on time
efficiency, we added tables with computation times for
representative functions of some finite fields. The tables show
the computation times of these functions in milliseconds. The
times were yield in an Axiom session on an IBM RISC System/6000
model 550 workstation using the Axiom system command
{\tt )set message time on}. Each command was run 50 to 100 times
for random values and the measured time was divided by the
number of runs.

\subsection{The extension fields $GF(5^4)$ over $GF(5)$ and
$GF(2^{10})$ over $GF(2)$}
\label{section12.1}

We first consider two small fields:

\begin{center}
Table 1: Computation times for $GF(2^{10})$ and $GF(5^4)$
\end{center}
\begin{tabular}{|l|r|r|r|r|r|r|}
\hline
 & \multicolumn{3}{|c|}{$GF(2^{10})$}
 & \multicolumn{3}{|c|}{$GF(5^4)$}\\
\hline
operation      & poly & normal & cyclic & poly & normal & cyclic\\
\hline
addition       &    1 &      1 &      1 &    1 &      1 &      1\\
multiplication &    4 &     18 &      1 &    3 &      4 &      1\\
inversion      &   13 &    105 &      1 &   10 &     16 &      1\\
primitive?     &   71 &    158 &      1 &   28 &     41 &      1\\
normal?        &   88 &      4 &     15 &   25 &      4 &      5\\
minimalPolynomial & 130 &  290 &     28 &   33 &     44 &     10\\
degree         &   92 &      3 &      1 &   17 &      2 &      1\\
discreteLog    &  150 &    440 &      1 &  110 &    200 &      1\\
norm           &   10 &    106 &      1 &    6 &     17 &      1\\
associateLog   &   67 &      1 &    120 &   25 &      1 &     50\\
associateOrder &  150 &      5 &    310 &   49 &      4 &    100\\
associateEsp   &   22 &      4 &      2 &   29 &      4 &     15\\
trace          &   16 &      5 &      3 &    5 &      3 &      3\\
exponentiation &   37 &     70 &      1 &   20 &     21 &      1\\
\hline
\end{tabular}

$GF(2^{10})$ was built up as an extension of $GF(2)$ with the
polynomial $X^{10}+X^9+x^4+X+1$ which is primitive and normal
over $GF(2)$ at the same time.

An extension $GF(5^4)$ of $GF(5)$ via the primitive and normal
polynomial $X^4+4X^3+X+2$ over $GF(5)$.

Table 1 shows the computation times for the three different
representations of both fields.

\subsection{Different extensions of $GF(5^{21})$ over $GF(5)$}

For fields with many elements we omit the cyclic group representation.
In extensions of high degree the computation time depends on whether
one takes a 'good' or 'bad' polynomial for the representation. 'Good'
in the normal basis representation means a low complexity for the
according normal basis while 'good' in the polynomial basis 
representation means a small number of nonzero coefficients of the
polynomial.

\begin{center}
Table 2: Computation times for $GF(5^{21})$
\end{center}
\begin{tabular}{|l|r|r|r|r|}
\hline
 & \multicolumn{2}{|c|}{'good'}
 & \multicolumn{2}{|c|}{'bad'}\\
\hline
operation      & polynomial & normal & polynomial & normal\\
\hline
addition       &          1 &      1 &          1 &      1\\
multiplication &         35 &     86 &         34 &    188\\
inversion      &         95 &    710 &         95 &   3040\\
trace over GF(5) &      206 &     14 &        208 &     17\\
norm over GF(5) &        78 &    606 &         78 &   2660\\
minimalPolynomial &    1370 &   2590 &       1360 &   9000\\
associateOrder &       2780 &     35 &       2700 &     34\\
associateLog   &        710 &      1 &       1500 &      1\\
associateEsp   &       1140 &     28 &       1690 &     20\\
\hline
\multicolumn{5}{|l|}{exponentiation with exponents in range}\\
\hline
$<$100         &        210 &    350 &        250 &   1570\\
$<$1000        &        420 &    510 &        420 &   2200\\
$<$10000       &        590 &    600 &        600 &   2700\\
$\sim 5^{21}$   &       2400 &   1600 &       2500 &   7400\\
\hline
\end{tabular}

We examine the field extension $GF(5^{21})$ over $GF(5)$. In
the normal basis representation we chose the 'good' polynomial
\[X^{21}+X^{20}+X^{18}+X^{17}+3X^{16}+4X^{15}+2X^{11}+2X^{10}+
3X^8+3X^7+4X^6+2X^5+X+1\eqno{(2)}\]
which yields a normal basis of low complexity of 61 and the 'bad'
polynomial 
\[X^{21}+4X^{20}+1\]
which yields a normal basis complexity of 323.

For the polynomial basis representation we chose as the 'bad'
polynomial (2) and the 'good' polynomial $X^{21}+4X+1$. The
table2 shows the results.

Although the multiplication time is much higher in the normal
basis representation, the adaptive exponentiation algorithm
yields for high exponents lower exponentiation times.

\section{Dependencies between the constructors}
\label{section13}

The picture on the next page visualises the dependencies between
the different constructors of the finite field world of Axiom.

Here is the list of used abbreviations:

{\bf Categories}
\begin{verbatim}
   CHARNZ  CharacteristicNonZero
   FPC     FieldOfPrimeCharacteristic
   XF      ExtensionField
   FFC     FiniteFieldCategory
   FAXF    FiniteAlgebraicExtensionField
\end{verbatim}

{\bf Domains}
\begin{verbatim}
   SAE     SimpleAlgebraicExtension
   IPF     InnerPrimeField
   PF      PrimeField
   FFP     FiniteFieldExtensionByPolynomial
   FFCGP   FiniteFieldCyclicGroupExtensionByPolynomial
   FFNBP   FiniteFieldNormalBasisExtensionByPolynomial 
   FFX     FiniteFieldExtension
   FFCGX   FiniteFieldCyclicGroupExtension
   FFNBC   FiniteFieldNormalBasisExtension
   IFF     InnerFiniteField
   FF      FiniteField
   FFCG    FiniteFieldCyclicGroup
   FFNB    FiniteFieldNormalBasis
\end{verbatim}

{\bf Packages}
\begin{verbatim}
   DLP     DiscreteLogarithmPackage
   FFF     FiniteFieldFunctions
   INBFF   InnerNormalBasisFieldFunctions
   FFPOLY  FiniteFieldPolynomialPackage
   FFPOLY2 FiniteFieldPolynomialPackage2
   FFHOM   FiniteFieldHomomorphisms
\end{verbatim}

\chapter[Real Quantifier Elimination]
{Real Quantifier Elimination Tutorial by Hoon Hong}

This material is quoted from Hoon Hong's presentation at
Kyushu University in Japan in 2005. The state of the art
has moved a bit but this is an excellent presentation up
to that date.\cite{Hong05}

\section{Overview}

{\bf What is the real quantifier elimination problem?} As a simple example,
consider the following ``toy'' problem: Find a condition on $b$ and $c$ such
that $x^2 + bx + c > 0$ for all $x$. Recalling middle school math, we know
that an answer is $b^2-4c < 0$. A bit more formally, one can write the
above process as

\begin{tabular}{ll}
In: & $(\forall x)\quad x^2 +bx +c > 0$\\
&\\
Out: & $b^2 - 4c < 0$
\end{tabular}

Note that the input formula and the output formula are equivalent but
the output formula does not have the universal quantifier $(\forall x)$. 
We have just carried out the ``real quantifier elimination''. Generally, it
is a problem:

\begin{tabular}{ll}
In: & a formula (made of integral polynomials, $=$, $>$, $\land$, $\lor$,
$\lnot$, $\forall$, $\exists$),\\
& the so-called ``a well-formed formula in the first order theory\\
& of real closed field''.\\
&\\
Out: & an equivalent formula {\sl without} quantifiers.
\end{tabular}

Note that the decision problem (proving) is a special case where the
input formula does not have free variables (since it is trivial to
decide a formula without variables). The research goal, then, is to
devise efficient algorithms/software that carry out the real
quantifier elimination.

{\bf Why work on this problem?} There are two main sources of motivation for
tackling the quantifier elimination problem: the original motivation
from the foundational questions of mathematics, and the more recent
motivation from applications in various areas.

The original motivation was based on the observation that the
``existence'' of a quantifier elimination procedure often implies
various other important properties (such as completeness) about the
theory under investigation and other theories that can be reduced to it.

On the other hand, the more recent motivation is based on the
observation that real quantifier eliminating provides a simple but
expressive abstraction for various important problems arising from
constructive mathematics, and in particular various non-trivial
problems in science and engineering, such as geometric modeling,
geometric theorem proving and discovery, termination proof of term
rewrite systems, stability analysis of control systems or numerical
schemes for partial differential equations, approximation theory, 
optimization, constraint logic programming, fracture mechanics, robot
motion planning, etc. Thus, progress in real quantifier elimination
can have significant impact on algorithmic (computer-assisted)
mathematics, science and engineering.

{\bf What has been done so far?} 
Alfred Tarski\cite{Tars48}, around 1930, gave the first
algorithm for the problem.  This was an epoch making event in the
foundational studies in mathematics and logic because it meant
that not only the elementary real algebra but also various
mathematical theories built on real numbers are also decidable, for
instance, the elementary algebra of complex numbers, that of
quarternions, that of n-dimensional vectors, and elementary
geometry (Euclidean, non-Euclidean, projective).

Since then, various improved, new, and specialized algorithms have
been devised with better computing times for large inputs
(asymptotically) or for small inputs or for special inputs.  Now
computer implementations exist and are applied to various problems
from mathematics, sicence and engineering.

{\bf References} There are several collections dedicated to quantifier
elimination (as of 2005):
\begin{itemize}[noitemsep]
\item {\sl Algorithms in Real Algebraic Geometry.}\cite{Arno88a}
\item {\sl Computational Quantifier Elimination} Hoon Hong, Oxford
University Press (1993)
\item {\sl Quantifier Elimination and Cylindrical Algebraic Decomposition}
\cite{Cavi98}
\item {\sl Quantifier Elimination and Applications} Journal of Symbolic
Computation, Hoon Hong and Richard Liska, Academic Press (1996)
\end{itemize}

There are also a few books that give and exposition on the subject
\begin{itemize}[noitemsep]
\item {\sl Algorithmic Algebra}\cite{Mish93}
\item {\sl Goemetrie algebrique reelle} Jacek, Bochnak, Michel Coste,
Marie-Francoius Roy (1986)
\end{itemize}

\section{General Methods}

We begin by describing several general methods that work for arbitrary
formulas. We will start with the historically first method (by
Tarski\cite{Tars48}), then proceed to the next important method of cylindrical
algebraic decomposition (by Collins\cite{Coll75}), then conclude with recent
methods with better asymptotic computing bounds.

\subsection{The First Method}

Alfred Tarski, the Polish-born U.S. mathematician, logician and
philosopher, discovered the first quantifier elimination algorithm
about 1930, and submitted a monograph for publication in 1939 which
was scheduled to appear in 1939 under the title {\sl The completeness of
elementary algebra and geometry} in the collection {\sl Autualit\'es
scientifiques et industrielles}. However due to the World War, the
publication did not materialize.  In 1948, the monograph was rewritten
by his friend J. C. C. McKinsey and was published under the title {\sl A
Decision Method for Elementary Algebra and Geometry}. In 1967,
Tarksi's original monograph was also published by the Institute Blaise
Pascal in Paris.

The method of Tarski is not efficient. The worst case asymptotic
computing time is {\sl non-elementary} in that it cannot be bounded by an
finite tower of exponential functions in the number of variables. But
it contains several important ideas/techniques that had significant
influence on the later methods.

\subsubsection{Method}

Tarski's method carries out a series of reduction steps until it comes
down to some simple cases which are tackled using 
Sturm's theorem\cite{Stur1829} and its generalization.

\vskip 0.5cm
\noindent
{\bf Reduction 1}: First, the general quantifier elimination problem can be
easily reduced to that for the formulas with {\sl one existential}
quantifier. This is because we can rewrite universal quantifiers in
term of existential quantifiers 
($\forall x \phi$ is equivalent to $\lnot\exists x \lnot \phi$) and then
repeatedly eliminate the innermost existential quantifier one at a time.

\vskip 0.5cm
\noindent
{\bf Reduction 2}: Now, we need to eliminate the quantifier from a
formula of the form $\exists x \phi$. By pushing the negations inward
onto the relational operators, rewriting $\ge$ in terms of disjunction
of $=$ and $>$, putting the resulting formula into disjunctive normal
form, and by using that the fact that disjunction commutes with an
existential quantifier, we can reduce the problem to the following
three types:

\begin{tabular}{ll}
 & \\
(1) & $\exists x [P_1 = 0 \land \cdots \land P_r=0]$\\
(2) & $\exists x [P_1 = 0 \land \cdots \land P_r=0 \land Q_1 > 0 \land
\cdots \land Q_s]$\\
(3) & $\exists x [Q_1 > 0 \land \cdots \land Q_x > 0]$\\
 & \\
\end{tabular}

\vskip 0.5cm
\noindent
{\bf Reduction 3}: Using the fact that
$P_1 = 0 \land \cdots \land P_r = 0$ is equivalent to
$P_1^2 + \ldots + P_r^2$, we can reduce the above (1) and (2) to

\begin{tabular}{ll}
 & \\
($1^\prime$) & $\exists x [P = 0]$\\
($2^\prime$) & $\exists x [P=0 \land Q_1 > 0 \land \cdots \land Q_s]$\\
 & \\
\end{tabular}

\vskip 0.5cm
\noindent
{\bf Reduction 4}: Next we will reduce the case (3) to the case
($2^\prime$). Suppose that all the polynomials $Q_i$'s are positive
on some value $p$ of $x$. Then there are only three possibilities:
\begin{itemize}
\item All the polynomials continued to be positive for $x > p$.
\item All the polynomials continued to be positive for $x < p$
\item There are two values $p_1$ and $p_2$, $p_1 < p < p_2$ such that
one of the polynomials vanish on $p_1$ and one (possibly the same)
of the polynomials vanish on $p_2$. More succinctly put, the product of
all the polynomials vanishes on $p_1$ and $p_2$. 
By Rolle's theorem\cite{Roll1691}, then,
there must exist in ($p_1,p_2$) a real root of the derivative of the
product of the polynomials.
\end{itemize}

Let lc denote leading coefficient. We can write the above case
analysis formally:
\begin{itemize}
\item lc$(Q_1) > 0 \land \cdots \land $ lc$(Q_s) > 0$
\item lc$(Q_1)(-1)^{{\rm deg}(Q_1)} > 0 \land \cdots \land $
lc$(Q_s)(-1)^{{\rm deg}(Q_s)} > 0$
\item $\exists x [(Q_1\cdots Q_s)^\prime = 0 \land Q_1 > 0\cdots Q_s > 0]$
\end{itemize}

Note that the first two are already quantifier-free and the last one
belongs to the case $(2^\prime)$. Thus, the case (3) has been reduced
to the case ($2^\prime$).

Actually the reasoning above is not entirely correct. It can happen
that a (formal) leading coefficient might depend on free variables,
and for some values of the free variables, the leading coefficient
might vanish, making it no longer the leading coefficient. In order to
correct it, we will have do some more easy but tedious case analysis
depending on the vanishing of the coefficients.

\vskip 0.5cm
\noindent
{\bf Reduction 5}: Let $\exists_k x \phi$ means that there exactly $k$
distinct real values of $x$ that satisfy $\phi$. Using this notation,
we can reduce the case ($1^\prime$) and ($2^\prime$) to the following:

\begin{tabular}{ll}
&\\
($1^{\prime\prime}$) & $\exists_0 x [P = 0]$\\
&\\
($2^{\prime\prime}$) & $\exists_0 x [P=0 \land Q_1 > 0 \land \cdots
\land Q_s]$\\
&\\
\end{tabular}

This is because $\exists x \phi$ is equivalent to
$\lnot \exists_0 x \phi$. The reason for such rewriting is to prepare
for a certain induction done in the next reduction.

\vskip 0.5cm
\noindent
{\bf Reduction 6}: We will reduce the case ($2^{\prime\prime}$) to the
following:

\begin{tabular}{ll}
&\\
($2^{\prime\prime\prime}$) & $\exists_k x [P=0 \land Q > 0]$\\
&\\
\end{tabular}

\noindent
We will do this by repeatedly decreasing the number of inequalities one
at a time. For this, we only need to observe the following. Let
\[\begin{array}{rcl}
n_{++} & = &\#\{x|P = 0 \land Q_1 > 0 \land\cdots\land Q_{s-2} > 0\\
&&\hskip 2.5cm \land~ Q_{s-1} > 0 \land Q_s > 0\}\\
&&\\
n_{+-} & = &\#\{x|P = 0 \land Q_1 > 0 \land\cdots\land Q_{s-2} > 0\\
&&\hskip 2.5cm \land~ Q_{s-1} > 0 \land Q_s < 0\}\\
&&\\
n_{-+} & = &\#\{x|P = 0 \land Q_1 > 0 \land\cdots\land Q_{s-2} > 0\\
&&\hskip 2.5cm \land~ Q_{s-1} < 0 \land Q_s > 0\}\\
&&\\
r_1 & = & \#\{x|P=0 \land Q_1 > 0 \land\cdots\land Q_{s-2} > 0\\
&&\hskip 2.5cm \land~ Q_{s-1}Q_s^2 > 0\}\\
&&\\
r_2 & = & \#\{x|P=0 \land Q_1 > 0 \land\cdots\land Q_{s-2} > 0\\
&&\hskip 2.5cm \land~ Q_{s-1}^2Q_s > 0\}\\
&&\\
r_3 & = & \#\{x|P=0 \land Q_1 > 0 \land\cdots\land Q_{s-2} > 0\\
&&\hskip 2.5cm \land~ Q_{s-1}Q_s > 0\}\\
\end{array}\]
Then we have the following
\[\begin{array}{rcl}
r_1 & = & n_{++} + n_{+-}\\
r_2 & = & n_{++} + n_{-+}\\
r_3 & = & n_{+-} + n_{-+}
\end{array}\]
By solving for $n_{++}$, we immediately obtain
\[n_{++} = \frac{r_1+r_2-r_3}{2}\]
From this observation, we see that the formula
\[\exists_k x [P=0 \land Q_1 > 0 \land\cdots\land Q_s]\]
is equivalent to the formula with one less inequalities
\[\begin{array}{rl}
\displaystyle\bigvee_{k=\frac{r_1+r_2-r_3}{2}} &
\left\{\begin{array}{c}
\exists_{r_1} x [P=0 \land Q_1 > 0 \land\cdots\land Q_{s-2} > 0\\
\land~Q_{s-1}^2 Q_s > 0]~ \land\\
\exists_{r_2} x [P=0 \land Q_1 > 0 \land\cdots\land Q_{s-2} > 0\\
\land~Q_{s-1} Q_s^2 > 0]~ \land\\
\exists_{r_3} x [P=0 \land Q_1 > 0 \land\cdots\land Q_{s-2} > 0\\
\land~Q_{s-1} Q_s > 0]
\end{array}\right\}
\end{array}\]
By inductively applying the same trick on the above three subformulas
we can arrive at the case ($2^{\prime\prime\prime}$).

\vskip 0.5cm
\noindent

{\bf Summary of all reductions:} Through all the above reduction
steps, the general quantifier elimination problem is reduced to the
following two special cases:

\begin{tabular}{cl}
($1^{\prime\prime\prime}$) & $\exists_k x [P = 0]$\\
($2^{\prime\prime\prime}$) & $\exists_k x [P = 0 \land Q > 0]$
\end{tabular}

\noindent
The case ($1^{\prime\prime\prime}$) is a bit more general than the case
($1^{\prime\prime}$) that we actually need. But we will keep it that way
since it is symmetric to the case ($2^{\prime\prime\prime}$). From now on,
we will tackle these two special cases.

\vskip 0.5cm
\noindent
{\bf Case ($1^{\prime\prime\prime}$)}
This case can be readily handled by 
{\sl Sturm's theorem}\cite{Stur1829}. Let $P$ be a
univariate polynomial. Let
$P_1=P, P_2=P^\prime, P_3,\ldots,P_n$ be a sequence where $P_{k+1}$ is
the negative of the remainder obtained by dividing $P_{k-1}$ by $P_k$,
and $P_n$ is the last non-zero polynomial in the sequence. Let $\alpha$
be the number of changes of sign in the sequence for $x \rightarrow -\infty$
and let $\beta$ be that for $x \rightarrow \infty$. Then Sturm proved that
the number of distinct real roots of $P$ is exactly $\alpha - \beta$.

When there are no free variables in the formula
($1^{\prime\prime\prime}$), we only need
to apply the Sturm's method to see if the number of distinct real
roots is indeed $k$. When there are free variables, we need to
parametrize the Sturm's method. We will have to do some easy but
tedius case analysis depending the signs of the leading coefficients.

\vskip 0.5cm
\noindent
{\bf Case ($2^{\prime\prime\prime}$)}
This case can be handled again readily by Sylvester's generalization of
Sturm's theorem. Let $P$ and $Q$ be two univariate polynomials and let
$P_1=P, P_2=P^\prime Q, P_3,\ldots,P_n$ be a sequence obtained the same
way as in Sturm's method. Note that the only difference is that
$P^\prime Q$ is used in place of $P^\prime$. Then Sylvester proved that
\[\alpha - \beta = \#\{x|P = 0 \land Q > 0\}-\#\{x|P=0 \land Q < 0\}\]
Let $N(P,Q)$ denote this number. Further let
\[\begin{array}{rcl}
n_+ & = & \#\{x|P = 0 \land Q > 0\}\\
n_- & = & \#\{x|P = 0 \land Q < 0\}\\
n_= & = & \#\{x|P = 0 \land Q = 0\}\\
\end{array}\]
Then we have the following
\[\begin{array}{rcl}
N(P,1) & = & n_+ + n_- + n_=\\
N(P,Q) & = & n_+ - n_-\\
N(P^2+Q^2,1) & = & n_=
\end{array}\]
By solving for $n_+$, we immediately obtain
\[n_+ = \frac{N(P,1)+N(P,Q)-N(P^2+Q^2,1)}{2}\]
Again, when there are no free variables in the formula
($2^{\prime\prime\prime}$), we only need to apply Sylvester's method
to see if $n_+$ is indeed $k$. When there are free variables, we need
to parameterize the method. This concludes the description of Tarski's
method.

\subsection{Cylindrical Algebraic Decomposition Method}

Collins\cite{Coll75}, a former Ph.D. student of Rosser (the logician known
for Church-Rosser property of Lambda calculus) introduced a new method
which has a much better bound than Tarski's (doubly exponential in the
number of variables rather than non-elementary, thus the infinite
tower of exponents is reduced to two floors). 
Specifically, the maximum computing time of the method is
dominated by $(2n)^{2^{2r+8}}m^{2^{r+6}}d^3a$,
where $r$ is the number of variables in $\phi$ , $m$ is the
number of polynomials occurring in $\phi$, $n$ is the maximum degree of any
such polynomial in any variable, $d$ is the maximum length of any
integer coefficient of any such polynomial, and $a$ is the number of
occurrences of atomic formulas.

Since the introduction, this method has gone through various improvments by
Arnon\cite{Arno81}, 
Collins\cite{Coll91}\cite{Coll98},
Hong\cite{Hong90}\cite{Hong98}\cite{Hong90a}, 
McCallum\cite{Mcca93}\cite{Mcca84}
and also now has been implemented twice completely
Arnon\cite{Arno81},Hong\cite{Hong90a}

Hong's implementation
is currently used in several engineering/sicientific applications.
In this section, we describe the method and various improvements.

\subsubsection{Method}

We first begin by explaining the name of the method:
{\sl Cylindrical Algebraic Decomposition}

{\bf Definition 1}
\begin{itemize}[noitemsep]
\item A {\sl decomposition} of $U \subseteq \mathbb{R}^r$ is a finite
collection of disjoint connected subsets of $U$ whose union is $U$.
Each subset is called a {\sl cell}.
\item A cell $c$ is called {\sl algebraic} if it can be described by a
quantifier-free formula. The formula is called a {\sl defining formula}
of the cell, denoted by $d_c$.
\item A decomposition is called {\sl algebraic} if every cell is algebraic
\item a {\sl stack} over $U \subseteq \mathbb{R}^r$ is a decomposition of
$U\times\mathbb{R}$ such that the projection of each cell on $\mathbb{R}^r$
is exactly $U$.
\item A decomposition of $D$ of $\mathbb{R}^r$ is called {\sl cylindrical}
if $r=1$, or $r > 0$ and $D$ can be partitioned into stacks over cells of
a cylindrical decomposition of $D^\prime$ of $\mathbb{R}^{r-1}$. It is easy
to see that $D^\prime$ is unique and we call it the {\sl induced}
decomposition of $D$.
\item Let $A$ be a finite subset of $\mathbb{Z}[x_1,\ldots,x_r]$. 
A decomposition of $\mathbb{R}^\prime$ is 
called {\sl A-sign invariant} if each polynomial
in $A$ has a constant sign throughout each cell. \quad$\blacksquare$
\end{itemize}

Let $F\equiv (Q_{f+1}x_{f+1})\cdots(Q_rx_r)F(x_1,\ldots,x_r)$.
Let $A$ be the set of polynomials occurring in $F$.
Let $D_r$ be a $A$-sign invariant algebraic decomposition of $\mathbb{R}^r$.
Let $D_k$ be the induced cylindrical algebraic decomposition of
$D_{k+1}$ for $k=1,\ldots,r-1$. We can immediately observe the following:

{\bf Proposition 1}
\begin{itemize}[noitemsep]
\item The truth of $(Q_{f+1}x_{f+1})\cdots(Q_rx_r)F(x_1,\ldots,x_r)$
is constant throughout each cell $c$ of $D_k$. Let $v_c$ denote this
truth value.
\item Let $c$ be a cell of $D_r$, and let $s_c$ be a point in $c$.
Then $v_c=F(s_c)$. We will call $s_c$ a {\sl sample} point of $c$.
\item Let $c$ be a cell of $D_k$, $f\le k < r$, and let $c_1,\ldots,c_n$
be the cells in the stack over $c$. Then we have
\[v_c =
\left\{
\begin{array}{rl}
\lor_{i=1}^{n}v_{c_i} & {\rm\ if\ }Q_{k+1}=\exists\\
\land_{i=1}^{n}v_{c_i} & {\rm\ if\ }Q_{k+1}=\forall\\
\end{array}
\right.
\]
\item $F \Longleftrightarrow \bigvee_{c\in D_f, v_c={\rm true}}d_c$
\quad$\blacksquare$
\end{itemize}

Now assuming we have an algorithm (CAD) that constructs a $A$-sign
invariant cylindrical algebraic decomposition, we can immediately
devise a quantifier elimination algorithm.

\vskip 0.5cm
\noindent
{\bf Algorithm 1}
\vskip -0.2cm
\noindent
\hrulefill
\begin{center}
$F^\prime \longleftarrow QE(F)$
\end{center}
\noindent
\begin{tabular}{ll}
{\sl Input}: & $F$ is a formula\\
{\sl Output}: & $F^\prime$ is a quantifier-free formula equivalent to $F$
\end{tabular}

\begin{itemize}[noitemsep]
\item[(1)] $D_r \longleftarrow CAD(A)$
\item[(2)] For every cell $c$ in $D_r$, $v_c \longleftarrow F(s_c)$
\item[(3)] For $k=r-1,\ldots,f$ and for every cell $c$ in $D_k$
\begin{itemize}[noitemsep]
\item[(i)] Let $c_1,\ldots,c_n$ be the cells in the stack over $c$
\item[(ii)] $v_c \longleftarrow
\left\{
\begin{array}{rl}
\lor_{i=1}^n v_{c_i} & {\rm\ if\ }Q_{k+1} = \exists\\
\land_{i=1}^n v_{c_i} & {\rm\ if\ }Q_{k+1} = \forall\\
\end{array}
\right.$
\end{itemize}
\item[(4)] $F^\prime \longleftarrow \bigvee_{c\in D_f,v_c={\rm true}}d_c$
\quad$\blacksquare$
\end{itemize}
\vskip -0.5cm
\noindent
\hrulefill

Now it remains to describe how to construct an $A$-sign invariant
cylindrical algebraic decomposition. This will be done inductively
on the number of variables $r$.

\vskip 0.5cm
\noindent
{\bf CAD for $r=1$}.
Let $\alpha_1 < \cdots < \alpha_\ell$ be the real roots of the polynomials
in $A$. Then the $2\ell + 1$ cells
\[(-\infty,\alpha_1),[\alpha_1,\alpha_1],(\alpha_1,\alpha_2),\ldots,
(\alpha_{\ell-1},\alpha_{\ell}),[\alpha_{\ell},\alpha_{\ell}],
(\alpha_{\ell},\infty)\]
form a $A$-sign invariant cylindrical decomposition $D$ of $\mathbb{R}^1$.
Now for each $c$ in $D$, we need to construct a sample point $s_c$ and
a defining formula $d_c$. A sample point $s_c$ can be chosen to be any
point of $c$.

In order to construct a defining formula $d_c$, let $\sigma_1,\ldots,\sigma_n$
be the signs of the polynomials in $A$ on $c$. Let $\rho_i$ be
$>$, $=$, $<$ respectively if $\sigma_i$ is $+$, $0$, $-$. Then 
the formula
\[F_c\equiv A_1\rho_10 \land\cdots\land A_n\rho_n0\]
captures all points in $c$. If we are lucky, it will capture no other
points, and $F_c$ can be used as a defining formula $d_c$. But it can
happen that another cell shares the same signs, and $F_c$ captures that
cell also.

A general way to overcome this difficulty is to ``separate'' the cells
using derivatives.\footnote{Actually, for the $r=1$ case, one can
separate the cells simply by using the sample points of the sectors.
But this method is not generalizable to $r>1$.}
Specifically, let $A^\prime$ be the set of the
derivatives of the polynomials in $A$. We inductively build a $A^\prime$-sign
invariant cylindrical algebraic decomposition $D^\prime$ of $\mathbb{R}^1$.
Then for each cell $c$ of $D$, we can easily determine, by comparing the 
sample points of $D^\prime$ and $D$, 
the conjective cells $c_\mu^\prime,\ldots,c_\nu^\prime$ of $D^\prime$
whose union contains $c$ but is disjoint from any other cell of $D$ that
has the same signs as $c$. Then, we can set
\[d_c\equiv F_c \land \bigvee_{i=\mu}^\nu d_{c_i^\prime}\]

\vskip 0.5cm
\noindent
{\bf CAD for $r>1$}.
In order to do induction on the number of variables, it will be nice to
have a finite subset $P$ of 
$\mathbb{Z}[x_1,\ldots,x^{r-1}]$ such that we can
``easily build'' an $A$-sign invariant cylindrical algebraic decomposition
$D$ of $\mathbb{R}^r$ from a $P$-sign invariant cylindrical algebraic 
decomposition $E$ of $\mathbb{R}^{r-1}$.

What do we mean by ``easily build''? Let $c$ be a cell of $E$. We want to
build an $A$-sign invariant stack over $c$. This becomes easy if
the zeros of the polynomials in $A$ naturally ``delineate'' the cylinder
over $c$, that is, the zeros of $A$ within the cylinder consist of
disjoint graphs of continous functions from $c$ to $\mathbb{R}$, 
say, $f_1 < \ldots < f_\ell$. If so, the following forms an
$A$-sign invariant stack over $c$:
\[\begin{array}{l}
\{x|x^\prime \in c,x_r < f_1(x^\prime)\},\\
\{x|x^\prime \in c,x_r = f_1(x^\prime)\},\\
\{x|x^\prime \in c,f_1(x^\prime) < x_r < f_2(x^\prime)\},\\
\{x|x^\prime \in c,x_r = f_2(x^\prime)\},\\
\{x|x^\prime \in c,f_2(x^\prime) < x_r < f_3(x^\prime)\},\\
\vdots\\
\{x|x^\prime \in c,x_r=f_\ell(x^\prime)\},\\
\{x|x^\prime \in c,f_\ell(x^\prime) < x_r\}\\
\end{array}\]
where $x$ stands for $(x_1,\ldots,x_r)$ and $x^\prime$ stands for
$(x_1,\ldots,x_{r-1})$. Let's call them $c_1,\ldots,c_{2\ell+1}$.
We call the odd-indexed cells {\sl sectors} and the even-indexed cells
{\sl sections}. Now we need to compute sample points and defining
formulas for these cells. Let $s=(s_1,\ldots,s_{r-1})$ be the sample
point of $c$. Let $A^*$ be the univariate polynomials in $x_r$ obtained
by evaluating the polynomials in $A$ on $s$. Then, we compute an $A^*$-sign
invariant cylindrical algebraic decomposition $D^*$ of $\mathbb{R}^1$ (as
shown before). Let $c_1^*,\ldots,c_{2\ell+1}^*$ be the cells of $D^*$.
Let $s_1^*,\ldots,s_{2\ell+1}^*$ be their sample points. Then, we can
set the following as the sample points of the cells in the stack over
$c$: $(s_1,\ldots,s_{r-1},s_i^*)$ where $1 \le i \le 2\ell+1$.

Let $d_1^*,\ldots,d_{2\ell+1}^*$ be the defining formulas of the cells of
$D^*$. We can obtain a defining formula for the cell $c_i$ by going
through the formula $d_i^*$ and replacing every instance of the
\[\frac{d^kA_\mu^*}{dx_r^k}{\rm\ by\ }
\frac{\partial^kA_\mu^*}{\partial x_r^k}\]

A caution. In order for the above argument to hold, the zeros of all
orders of the partial derivative of $A$ in $x_r$ (not just merely $A$)
must also delineate the cylinder over $c$. This is needed to ensure that
the relative positions of the zeros of the derivatives $A^*$ do not
change as $s$ ranges over $c$. If not, the defining formula for the cell
$c_i^*$ might not provide a structure for the defining formula for the
cell $c_i$.

Finally, it only remains to decide what to put into the set $P$ to
ensure the delineability of the zeros of $A$ and its derivatives.
Intuitively, we need to put sufficiently many polynomials so that their
zeros contain the projection of the ``critical'' points of the zeros of
$A$ and its derivatives. By critical points, we mean ``crossing'',
``vertical tangent'', ``vertical asymptotes'', and ``isolated points''.
Collins proved that the following set $P$ is sufficient.

\vskip 0.5cm
\noindent
{\bf Theorem 1 (Collin's projection)}
\[\begin{array}{rcl}
B &=& \{{\rm red}^k(a) ~|~ a\in A, {\rm deg}({\rm red}^k(a)) \ge 1\}\\
L &=& \{{\rm lc}(b) ~|~ b \in B\}\\
R &=& \{{\rm psc}_k(b_1,b_2)~|~
b_1,b_2\in B, 0\le k < {\rm min}({\rm deg}(b_1),{\rm deg}(b_2))\}\\
D &=& \{{\rm psc}_k(b,b^\prime) ~|~ b \in B, 0 \le k < {\rm deg}(b^\prime)\}\\
C &=& \{{\rm der}^k(b) ~|~ b \in B, 0 \le k < {\rm deg}(b)\}\\
D^\prime &=& \{{\rm psc}_k(c,c^\prime) ~|~ c \in C, 
0 \le k < {\rm deg}(c^\prime)\}\\
P &=& L \cup R \cup D \cup D^\prime
\end{array}\]
where
\begin{itemize}[noitemsep]
\item 'lc' stands for the leading coefficient
\item 'red' for polynomial reductum, that is, red($a$) is the polynomial
obtained from $a$ by removing the leading term.
\item 'der' for derivative
\item 'psc' for principal subresultant coefficient. Let
$a=a_mx^m+\ldots+a_0$ and $b=b_nx^n+\ldots+b_0$. Then
psc$_k(a,b)$ is the determinant of the square (Sylvester) matrix
\[\left[
\begin{array}{ccccccc}
a_m & a_{m-1} & \cdots  &  a_0   &         &        &\\
    &   a_m   & a_{m-1} & \cdots &  a_0    &        &\\
    &         & \cdots  & \cdots & \cdots  & \cdots &\\
    &         &         &  a_m   & a_{m-1} & \cdots & \cdots \\
b_n & b_{n-1} & \cdots  &  b_0   &         &        &\\
    &  b_n    & b_{n-1} & \cdots &  b_0    &        &\\
    &         & \cdots  & \cdots & \cdots  & \cdots &\\
    &         &         &  b_n   & b_{n-1} & \cdots & \cdots
\end{array}
\right]\]
in which there are $n-k$ rows of $a$ coefficients, $m-k$ rows of $b$
coefficients, and all elements not shown are zero.\quad$\blacksquare$
\end{itemize}
Roughly put, $L$ is there for vertical asymptotes, $R$ for crossing,
$D$ for vertical tangents or isolated points, and $D^\prime$ for
vertical tangents/isolated points of derivatives.

Let {\sl PROJ} denote the map that takes $A$ and produces $P$. Summarizing,
we have the following algorithm:

\vskip 0.5cm
\noindent
{\bf Algorithm 2}
\vskip -0.2cm
\noindent
\hrulefill

\begin{center}
$D \longleftarrow CAD(A)$
\end{center}

\noindent
\begin{tabular}{ll}
{\sl Input}: & $A$ is a finite set of polynomials in $r$ variables\\
{\sl Output}: & $D$ is an $A$-sign invariant cylindrical algebraic
decomposition
\end{tabular}

\begin{itemize}[noitemsep]
\item[(1)] if $r=1$ then build a $A$-cylindrical algebraic decomposition
$D$ of $\mathbb{R}^1$ as described above and return
\item[(2)] $P \longleftarrow PROJ(A)$
\item[(3)] $E \longleftarrow CAD(P)$
\item[(4)] For each cell $c$ of $E$, build an $A$-sign invariant stack $S_c$
over $c$ as described above
\item[(5)] $D \longleftarrow \cup_{c\in E}S_c$\quad$\blacksquare$
\end{itemize}
\vskip -0.5cm
\noindent
\hrulefill

\subsubsection{Improvements}

\noindent

Collins' method allowed various improvements: 
clustering\cite{Arno81}\cite{Mcca02},
smaller projections \cite{Mcca84} \cite{Hong90} \cite{Brow01a},
efficient order of projections\cite{Dolz04},
partial cylindrical algebraic decomposition\cite{Hong90a}\cite{Coll91},
solution formula construction\cite{Hong98}\cite{Brow01a},
strict inequalities\cite{Mcca93},
equational constraints\cite{Coll98},
use of interval methods\cite{Coll02},
and so on. We will briefly discuss a few of the above improvements.

\vskip 0.5cm
\noindent
{\bf Clustering} Arnon\cite{Arno81}  made an observation that a 
stack can be constructed
over a union of cells, provided that the union (cluster) is connected
and the projection polynomials are sign-invariant on it. Thus, before
lifting we first combine cells into clusters and choose only one
sample point per each cluster and lift it. This usually significantly
reduces the number of stack constructions which must be performed.

This idea requires a method for computing a cluster, which boils down
to adjacency computation (finding out which cells are topologically
adjacent). In 
\cite{Arno84}\cite{Arno88b}\cite{Mcca02}
methods for 2 and 3 and more variables were developed.

\vskip 0.5cm
\noindent
{\bf Smaller Projection} In \cite{Mcca84}\cite{Mcca88} McCallum proved
that, if the polynomials in $A$ are ``well'' ordered, the sets
$R$, $D$, $C$, $D^\prime$ can be made smaller.

\[\begin{array}{rcl}
R &=& \{{\rm psc}_0(a_1,a_2)~|~a_1,a_2 \in A\}\\
D &=& \{{\rm psc}_0(a,a^\prime)~|~a\in A\}\\
C &=& \{{\rm der}^k(a)~|~ a\in A, 0\le k < {\rm deg}(a)\}\\
D^\prime &=& \{{\rm psc}_0(c,c^\prime)~|~c\in C\}
\end{array}\]

Note that these sets are much smaller than Collins' original ones,
because it does not involve reductums nor higher order principal
subresultant coefficients.

Hong\cite{Hong90} proved that the set R in the original projection
could be restricted to (without any side conditions)
\[R = \{{\rm psc}_k(a,b)~|~a\in A, b\in B, 
0 \le k < {\rm min}({\rm deg}(a),{\rm deg}(b))\}\]

The resulting projection set is smaller than the original one and
larger than McCallum's. But it can be useful since it, unlike
McCallum's, does not impose any condition on the polynomials and
further McCallum's projection requires that the clusters must be order
invariant. These can result in possibly smaller clusters.  
Brown\cite{Brow01a} made a further improvement.

\vskip 0.5cm
\noindent

{\bf Partial CADs} Hong\cite{Hong90a} (see also \cite{Coll91}) showed
that we can {\sl very often} complete quantifier elimination by a
partially built cylindrical algebraic decomposition, if we utilize,
during cylindrical algebraic decomposition, more information contained
in the input formula such as quantifiers, the boolean connectives,
the absence of some variables from some polynomials occurring in the
input formula, etc. These improvements did not change the asymptotic
worst case bound, but gave significant speedups in most problems
known in the literature, that many problems that would require at
least several months could now be solved within a few seconds.

As an example, one can utilize the quantifier information as follows:
Let us consider a sentence in two variables 
$(\exists x)(\exists y)F(x,y)$. 
The original method computes a certain decomposition $D_1$ of
$\mathbb{R}$ and then lifts this to a decomposition $D_2$ of
$\mathbb{R}^2$ by constructing a stack of cells in the cylinder over
each cell of $D_1$. Then the quantifier elimination proceeds by
determining the set of all cells of $D_1$ in which $(\exists y)F(x,y)$
is true.  Finally, it computes the truth value of 
$(\exists x)(\exists y)F(x,y)$ by checking whether the set is empty. In
contrast, one may construct only one stack at a time, aborting the CAD
construction as soon as a cell of $D_1$ is found which satisfies
$(\exists y)F(x,y)$,
if any such cell exists. The method illustrated above for two
variables extends in an obvious way to more variables, with even
greater effectiveness because the CAD construction can be partial in
each dimension.

\vskip 0.5cm
\noindent
{\bf Simple Solution Formula Construction}

Hong\cite{Hong90a}\cite{Hong98} 
devised a new method for constructing solution
formulas which is more efficient and produces much simpler
formulas. The original algorithm obtains a solution formula by forming
a disjunction of defining formulas of solution cells. This method
works for any input formula, but produces very large formulas and
often increases greatly the amount of computation required because of
the augmented projection (projection involving derivatives).

The method of Hong does not use augmented projection, but instead
tries to construct solution formulas using only projection
polynomials. It can fail to produce a solution formula, but the
experiments with many QE problems from diverse application areas
suggest that it will rarely fail. The method also uses a logic
minimization algorithm to simplify solution formulas. It carries out
simplification based not only on the logical connectives but also on
the relational operators. This is done with three-valued logic. It
further reduces the size of the inputs to the multiple-valued logic
minimization algorithm by taking advantage of the structure of the
input formula. As the result, the method produces simpler formulas (a
few lines instead of several pages) faster (a few seconds instead of
hours).

Brown\cite{Brow99} provided a method that generates the augmented
projection polynomials on demand, thus making the method complete
(never fails, unlike Hong's).

\vskip 0.5cm
\noindent
{\bf Strict Equalities}

McCallum\cite{Mcca93} observed that the stack construction phase can be
significantly improved if the input formula is an existentially
quantified sentence of a system of strict inequalities. This kind of
sentence arises naturally from geometric modeling, robot simulation,
non-linear optimization, etc.

The key observation is that the solution set of the quantifier-free
matrix (conjunction/disjunction of strict inequalities) is either
empty or must contain a full-dimension open set. Thus, during the
stack construction, one only needs build stacks over sectors (since
any cells belonging to stacks over sections cannot not be of
full-dimension). If one of the constructed full-dimensional cells
satisfies the matrix, then the sentence is true.  If not, one can stop
and report that the sentence is false.

This gives a huge savings not only because the number of the stack
constructions is reduced, but also the full-dimensional cells are much
cheaper to construct since it does not involve algebraic number
computation.

\vskip 0.5cm
\noindent
{\bf Equational Constraints}

McCallum\cite{Mcca93} observed that the projection phase and the stack
construction phase can be significantly improved when the input
formula has ``equational constraints''. A {\sl equational constraint} of a
prenex formula is a polynomial equation which is implied by its
quantifier-free matrix. For example, if the quantifier-free matrix is
of the form: $A=0 \land F$, then $A = 0$ is an equational constraint.

The key observation is that the matrix is false if $A \ne 0$  regardless
of the truth of $F$. Thus, we only need to ensure that the other
polynomials (occurring in $F$) are sign-invariant on the sections of
$A$. For a simple presentation, let us assume that the inputs
polynomials are well-oriented (and thus McCallum's projection theorem
can be applied). Then, we only need to put the following into the
projection set:
\begin{itemize}[noitemsep]
\item The discriminant and (enough of) the coefficients of $A$
\item The resultant of $A$ and each polynomial occuring in $F$
\end{itemize}

Sometimes we can propagate the property of ``equational constraint''
down to lower dimensional space so that we can apply the smaller
projection operation again. For instance, let $A_1$ and $A_2$ both be
equational constraint polynomials. Let $R$ be their resultant. It is
well-known that 
$A_1=0 \land A_2 = 0 \Longrightarrow R=0$ Thus, $R=0$ is also an
equational constraint, we can use it to reduce the second
projection. This idea obviously generalizes, with greater effect, when
there are more than two equational constraints.

When there are equational constrains obtained by resultant
computations (as shown above), then they can be used for pruning stack
constructions. Whenever a stack has already been constructed in which
there are sections of an equational constraint polynomial, all other
cells in the stack can be marked false and it is unnecessary to
construct stacks over them. When there are many equational constraints
in the input formulas, the accumulated reduction on the number of
stack constructions can be drastic.

\subsection{Quantifier-Block Elimination Methods}

During last decade, there was an intensive research on improving
asymptotic worst case computing bounds 
\cite{Beno86},
\cite{Fitc87},
\cite{Grig88}, 
\cite{Grig88a},
\cite{Cann88},
\cite{Cann93},
\cite{Hein89},
\cite{Rene92},
\cite{Weis98}.
The key idea is that a consecutive block of same quantifiers can
be eliminated by a single projection in an exponential time (in the
number of variables). Thus, the total complexity is doubly exponential
in the number of quantifier alternations, not on the number of
variables. If all quantifiers are the same (1 quantifier-block), then
the total complexity is singly exponential in the number of variables.

Let $n$ be the number of variables in the input sentence, $m$ the number
of polynomials, $d$ the degree, $L$ the bit length bound for the
coefficients, and $n_i$ is the number of quantifiers in the $i$-th
quantifier block. The $\omega$ Best asymptotic bound so far (obtained by
Renegar) is is $L(log~L)(log~log~L)(md)^{\prod_{k=0}^\omega O(n_k)}$.

\subsubsection{Method}

There are several methods with the similar complexity bounds.
\cite{Grig88a}, 
\cite{Cann88},
\cite{Cann93},
\cite{Hein89},
\cite{Rene92}.
Though different in details, they are very similar in the
overall strategy. Thus in this tutorial, we will be satisfied with
sketching the overall strategy/ideas.

\vskip 0.5cm
\noindent
{\bf Overall Ideas}
\begin{enumerate}
\item First note that the arbitrary quantifier elimination problem can
be trivially reduced to the {\sl existential} quantifier elimination problem
(all quantifiers are existential), because if we have an algorithm for
the existential quantifier elimination, we can repeatedly apply it to
eliminate each block of quantifiers one at a time (for a universal
block after double negation).

One important exception. Renegar's method does not apply the
existential quantifier elimination repeatedly. Instead, it follows the
idea of Collins' Cylindrical Algebraic Decomposition: namely the
repeated projection. But its projection removes a block of variables
at once, while Collins' projection removes one variable one at time.

\item The existential quantifier elimination can be straightforwardly
(though very tedious) reduced to the {\sl existential} decision problem (no
free variables), because we can ``unravel'' a decision algorithm to turn
it into a parametric one. More specifically, a decision algorithm can
be viewed as a tree where each internal node corresponds computation
(such addition/multiplication) or comparison (for branching) and where
each leaf is associated with true or false. Given a sentence, we
follow only one path until we reach a leaf. If it is a true node, then
the sentence is true, else false. But when the input is not a sentence
and contains free variables, we do not know which path to follow at a
branching node. Thus, we follow all the paths while accumulating the
branch conditions (on the free variables) associated with each
path. Then, the disjunction of all the accumulated conditions
associated with true leaves will be the quantifier-free formula that
we desire.

Again, Renegar does not carry this unraveling at each quantifier
block. But he does this only once for the ``last'' free variable block.

\item The existential decision problem can be reduced to the {\sl smooth}
existential decision problem where the solution set of the
quantifier-free matrix is closed and smooth. This can be done by some
simple geometric tricks and (careful) infinitesimal smoothing.

\item Next the smooth existential decision problem is reduced to the
{\sl equationally constrained} existential decision problem:
\[\exists(x_1,\ldots,x_n)P_1(x_1,\ldots,x_n) \land\cdots
P_n(x_1,\ldots,x_n) \land F\]
where $F$ is a quantifier-free formula and the system
$P_1 = P_2 = \cdots = P_n = 0$
has only finitely many (complex) solutions. This can be done by
applying the Lagrange multiplier method since a smooth set has a
``optimal/critical'' point for ``suitably'' chosen objective functions
(Morse function). The suitable choice can be done either by (another)
infinitesimal perturbation or some searching (in a finite but very big set).

Note that this idea of using ``optimal/critical'' points is not new, but
was already used by Seidenberg\cite{Seid54} for improving Tarski's method. He
used the ``distance'' from the origin as the objective function. When
this turns out to be not suitable, then he tried linear transformation
of coordinates.

Weispfenning's method is an exception here in that it does not require
that the system $P_1,\ldots,P_n$ has a finitely many complex
solution. It checks if the system indeed has the property, if so
continue, continue, if not, instead of perturbing the system, it view
some of the variables as parameters and checks again whether the
system has finitely many complex solutions (parametrically). This can
be done by using the method of comprehensive Gr\"obner basis method
\cite{Weis92}. Thus in the worst case, the complexity of this method becomes
doubly exponential in the number of variables. But some preliminary
experiments show that this might be a very promising method for small
inputs.

\item The equationally constrained existential decision problem can be
solved in various ways. Usually, the system of equations are ``solved''
by using the $u$-resultants \cite{Cann87}
or the Hermite's quadratic form \cite{Pede93}
The sign computation of the polynomials occurring $F$ on the roots of
the system of equations can be obtained by the method of BKR \cite{Beno86}.

\end{enumerate}
\vskip 0.5cm
\noindent
{\bf Grigorev's Algorithm}:
Now for those who want to get a glance at the details of some
algorithms, we give a detailed description of the algorithm of
Grigor'ev and Vorobjov \cite{Grig88} and Renegar's \cite{Rene92}, but without
explanation. First we give Grigorev's algorithm for existential
decision problem. In the case of a positive answer, the algorithm also
constructs a representative set for the family of components of
connectivity of the set of all real solutions of the system, which is
$\mathcal{T}$ 
in Step (8) of the algorithm described below. We begin by defining
several notations which will be used in the algorithm description:
\begin{itemize}
\item Let $K$ be an arbitrary ordered field. The $\tilde{K}$ denotes
the algebraic closure $K$, and $\tilde{K}$ the real algebraic closure
of $K$.
\item Let $\epsilon_1$ be a positive infinitesimal over $\tilde{\mathbb{Q}}$.
Then $F_1$ denotes the real algebraic closure of 
$\tilde{\mathbb{Q}}(\epsilon_1)$.
\item Let $\epsilon$ be a positive infinitesimal over $F_1$.
Then $F$ denotes the real algebraic closure of $F_1(\epsilon)$.
\item Let $f_1,\ldots,f_m$ be elements of $K[x_1,\ldots,x_n]$.
Then $\{f_1=0,\ldots,f_m=0\}$ denotes the set of all solutions
of the system $f_1=\cdots =f_m = 0$ over the algebraic closure of $K$.
\end{itemize}

\vskip 0.5cm
\noindent
\hrulefill
\begin{center}
$t \longleftarrow {\bf Decide}(P)$
\end{center}

\noindent
\begin{tabular}{ll}
{\sl Input}: & $P$ is a quantifier-free formula of the following kind:\\
\end{tabular}
\[f_1 > 0 \land\cdots\land f_k>0 \land f_{k+1}\ge 0 \land\cdots\land
f_m \ge 0\]
\hskip 1.5cm where $f_i \in \mathbb{Z}[x_1,\ldots,x_n]$.

\noindent
\begin{tabular}{ll}
{\sl Output}: & $t$ is the truth of the sentence
$(\exists x_1 \in \mathbb{R})\cdots(\exists x_n \in \mathbb{R})
P(x_1,\ldots,x_n)$
\end{tabular}

\begin{itemize}[noitemsep]
\item[(1)] Let $f_{m+1} = x_0f_1\cdots f_{k-1}$\\
Let $g_1=(f_1+\epsilon_1)\cdots(f_{m+1}+\epsilon_1)-\epsilon_1^{m+1}
\in \mathbb{Z}[\epsilon_1][x_0,\ldots,x_n]$
\item[(2)] Let $\tilde{L}$ be a bit length bound for the coefficients of
$f_i$, and $\tilde{d}$ a degree bound for $f_i$ for every
$1 \le i \le m+1$.\\
Let $R=3^{(\tilde{L}+log(m+1))p(\tilde{d}^{n+1})}$,
where $p \in \mathbb{Z}[x]$ is a certain polynomial defined in Page 62 of
\cite{Grig88}.\\
Let $g=g_1^2+(x_0^2+\cdots+x_{n+1}^2-(R+1))^2\in 
\mathbb{Z}[\epsilon_1][x_0,\ldots,x_{n+1}]$
\item[(3)] Let $N = (8md)^{n+2}$\\
Let $\Gamma = \{1,\ldots,N\}^{n+1}$\\
Let $\mathcal{I}^\prime = \{\}$
\item[(4)] For each $\gamma = (\gamma_1,\ldots,\gamma_{n+1})\in \Gamma$ do
\begin{itemize}[noitemsep]
\item[(4.1)] Let $\tilde{V}^{(\epsilon)}\subset\tilde{F}^{n+2}$ be the
variety of the system
\[\begin{array}{rcl}
g-\epsilon&=&\displaystyle\left(\frac{\partial g}{\partial x_1}\right)^2
-\frac{\gamma_1}{N(n+2)}\Delta = \cdots\\
&&\\
&=&\displaystyle\left(\frac{\partial g}{\partial x_{n+1}}\right)^2
-\frac{\gamma_{n+1}}{N(n+2)}\Delta = 0
\end{array}\]
where $\Delta=\sum_{j=0}^{n+1}\left(\frac{\partial g}{\partial x_j}\right)^2$\\
Let $\tilde{V}^{(\epsilon)}=\bigcup_j \tilde{V}_j^{(\epsilon)}$,
where each $\tilde{V}_j^{(\epsilon)}$ is a component irreducible over the
field $\mathbb{Q}(\epsilon_1,\epsilon)$.
\item[(4.2)] For each null dimensional $\tilde{V}_j^{(\epsilon)}$ do
\begin{itemize}[noitemsep]
\item[(4.2.1)] Let $\mathcal{R}\subset\tilde{F}_1^n$ be a set containing the
standard part (relative to $\epsilon$) of every point (for which the standard
part is definable) from the component $\tilde{V}_j^{(\epsilon)}$.
\item[(4.2.2)] Let $\mathcal{R}_1^\prime = \mathcal{R}_1 \cap
\{g=0\}\cap F_1^{n+2}$
\item[(4.2.3)] Set $\mathcal{I}^\prime to 
~\mathcal{I}^\prime\cup\mathcal{R}_1^\prime$
\end{itemize}
\end{itemize}
\item[(5)] Let $\mathcal{I}=\pi(\mathcal{I}^\prime) \subset F_1^{n+1}$
where $\pi(x_0,\ldots,x_{n+1})=(x_0,\ldots,x_n)$.
\item[(6)] Let $\mathcal{R}\subset\tilde{\mathbb{Q}}^{n+1}$ be a set
containing the standard part (relative to $\epsilon_1$) of every point
(for which the standard part is definable) from the set $\mathcal{I}$.
\item[(7)] Let $\mathcal{T}^\prime=\mathcal{R}\cap\{f_1 \ge 0,
\ldots,f_{m+1} \ge 0\}\cap\tilde{\mathbb{Q}}^{n+1}$
\item[(8)] Let $\mathcal{T}=\pi_1(\mathcal{T}^\prime)$
where $\pi_1(x_0,\ldots,x_n)=(x_1,\ldots,x_n)$
\item[(9)] Finally let $t$ be true if $\mathcal{T}$ is non-empty,
false otherwise.\quad$\blacksquare$
\end{itemize}
\vskip -0.5cm
\noindent
\hrulefill

\vskip 0.5cm
In \cite{Grig88} it is shown that the worst case time complexity of 
this algorithm is dominated by $L(md)^{n^2}$.

In \cite{Grig88a} Grigor'ev, by generalizing this algorithm, gave a
decision algorithm for the first order theory of real closed fields.

\vskip 0.5cm
\noindent
{\bf Renegar's Method}. Now we give an overview of Renegar's\cite{Rene92}
algorithm.

\vskip 0.5cm
\noindent
\hrulefill
\begin{center}
$t \longleftarrow {\bf MainAlgorithm}(P)$
\end{center}

\noindent
\begin{tabular}{ll}
{\sl Input}: & $P$ is a quantifier-free formula with variables
$x_1,\ldots,x_n$.\\
{\sl Output}: & $t$ is the truth of the sentence
$(\exists x_1 \in \mathbb{R})\cdots(\exists x_n \in \mathbb{R})
P(x_1,\ldots,x_n)$.
\end{tabular}

\begin{itemize}[noitemsep]
\item[(1)] Let $g=\{g_1,\ldots,g_m\}$,$g_i\in\mathbb{Z}[x_1,\ldots,x_n]$
be the polynomials occurring in the formula $P$.\\
Let $h=\{h_1,\ldots,h_{6m+2}\}$ where
\[\begin{array}{lclcr}
h_i        &=& g_i       && i=1,\ldots,m\\
h_{m+i}    &=& x_0g_i-1  && i=1,\ldots,m\\
h_{2m+i}   &=& x_0g_i+1  && i=1,\ldots,m\\
h_{3m+1}   &=& x_0-1     &&\\
h_{3m+1+i} &=& -g_i      && i=1,\ldots,m\\
h_{4m+1+i} &=& -x_0g_i+1 && i=1,\ldots,m\\
h_{5m+1+i} &=& -x_0g_i-1 && i=1,\ldots,m\\
h_{6m+2}   &=& -x_0+1    &&\\
\end{array}\]
\item[(2)] Let $d$ be the maximum of the degrees of the polynomials $g_i$\\
Let $d^\prime$ be the least even integer which is greater than or equal to
$d+1$.\\
Let $\tilde{h}=\{\tilde{h}_1,\ldots,\tilde{h}_{6m+2}\}$ where
\[\tilde{h}_i(\delta;x_0,\ldots,x_n)=(1-\delta)h_i+
\delta(1+\sum_{j=0}^n i^jx_j^{d^\prime})\]
\item[(3)] For each $A\subseteq\{1,\ldots,6m+2\}$ such that
$|A|\le n+1$ do:
\begin{itemize}[noitemsep]
\item[(3.1)] Let $\tilde{h}(x_0,\ldots,x_n)=
\sum_{j=0}^n(6m+3)^jx_j^{d^\prime}$\\
Let $M_A$ be the matrix with the last row $\nabla_{x_0,\ldots,x_n}\hat{h}$
and with earlier rows $\nabla_{x_0,\ldots,x_n}\overline{h}, ~i\in A$,
ordered by increasing indices $i$.
\item[(3.2)] Let $h_A(\delta;x_0,\ldots,x_n)=
{\rm det}(M_AM_A^T)+\sum_{i\in A}\overline{h}_i^2$\\
Let $d_A$ be the degree of $h_A$ with respect to $x_0,\ldots,x_n$
\item[(3.3)] Let $\tilde{h}_A(\epsilon,\delta;x_0,\ldots,x_n)=
(1-\epsilon)h_A-\epsilon\sum_{j=0}^nx_j^{d_A}$\\
Let $\tilde{h}_A^{(i)}=\frac{\partial\tilde{h}_A}{\partial x_i}$ for each $i$
\item[(3.4)] Let $R_A(\epsilon,\delta;u_0,\ldots,u_{n+1})$ be the
$u$-resultant of the polynomials $\tilde{h}_A^{(0)},\ldots,\tilde{h}_A^{(n)}$
(Use the subalgorithm shown below)
\item[(3.5)] Let $R_A$ be expanded as
$\sum_{i,j,k}\epsilon^i\delta^ju_0^kR_A^{<i,j,k>}$
\end{itemize}
\item[(4)] Let $\mathcal{R}$ be the set of all $\mathcal{R}_A^{<i,j,k>}$
computed in Step (3)
\item[(5)] Let $\mathcal{B}_{n+1,D}=
\{(i^{n-1},i^{n-2},\ldots,1,0)|0 \le i \le nD^2\}$\\
For $R\in\mathcal{R}$, let $D_R$ be the degree of $R$\\
Let $\mathcal{Q}=\{\frac{d^j}{dt^j}\nabla_{u_1,\ldots,u_{n+1}}
R(\beta+te_{n+1})|R\in\mathcal{R},\beta\in\mathcal{B}_{n+1,D_R},
j\in[0,D_R]\}$
\item[(6)] Let $G=\{G_1,\ldots,G_m\}$,~$G_i\in\mathbb{Z}[x_1,\ldots,x_{n+1}]$
be such that each $G_i$ is the degree $d$-homogenization of $g_i$, i.e.
the monomials of $G_i$ are obtained from those of $g_i$ by multiplying by
the appropriate powers of $x_{n+1}$ so as to become of degree $d$\\
Let $\mathcal{F}^+=\{(G_1(q),\ldots,G_m(q),q_{n+1}|q\in\mathcal{Q}\}$\\
Let $\mathcal{F}^-=\{(G_1(-q),\ldots,G_m(-q),-q_{n+1}|q\in\mathcal{Q}\}$\\
Let $\mathcal{F}=\mathcal{F}^+\cup\mathcal{F}^-$
\item[(7)] For each $f\in\mathcal{F}$ let $\overline{S}_f$ be the set of
all consistent sign vectors for the polynomials in $f$. (Use the algorithm
of Ben-Or, Kozen, and Reif\cite{Beno86})
\item[(8)] Let $\overline{S}=\bigcup_{f\in\mathcal{F}}\overline{S}_f$\\
Let $S=\{(\sigma_1,\ldots,\sigma_m)|(\sigma_1,\ldots,\sigma_m,1)\in
\overline{S}\}$\\
Finally let $t=\bigvee_{\sigma\in S}P_\sigma$, where the formula
$P_\sigma$ is obtained from the formula $P$ by replacing $g_i$ with
$\sigma_i$
$\blacksquare$
\end{itemize}

\vskip 0.5cm
\noindent
\begin{tabular}{ll}
{\sl Input}: & $f_0,\ldots,f_n\in W[x_0,\ldots,x_n]$, where $W$
is a commutatuve ring with unity.\\
{\sl Output}: & $R\in W[u_0,\ldots,u_{n+1}]$ is the $u$-resultant of
$f_0,\ldots,f_n$.\\
& (See Renegar\cite{Rene92,Rene92a,Rene92b} for the definition)
\end{tabular}

\begin{itemize}[noitemsep]
\item[(1)] Let $\tilde{d}$ be the maximum of the degrees of $f_i$\\
Let $\mathbb{B}=\{x_0^{d_0}\cdots x_{n+1}^{d_{n+1}}|
d_0+\cdots+d_{n+1}=\hat{d}\}$ where $\hat{d}=(n+1)(\tilde{d}-1)+1$\\
Let $\mathbb{H}$ be the vector space generated by the basis $\mathbb{B}$
over the ring $W$
\item[(2)] For each $x_0^{d_0}\cdots x_{n+1}^{d_{n+1}} \in \mathbb{B}$,
let $i$ denote the least index $i\le n$ and $\tilde{d}\le d_i$, if such
an $i$ exists, otherwise let $i=n+1$\\
Let $F_i\in W[x_0,\ldots,x_{n+1}]$ be the degreee $\tilde{d}$-homogenization
of $f_i$ for each $i$\\
Let $\mathcal{T}:\mathbb{H}\longrightarrow\mathbb{H}$ be the linear
transformation defined by the following mapping on the basis $\mathbb{B}$
\[\mathcal{T}(x_0^{d_0}\cdots x_{n+1}^{d_{n+1}})=
\left\{
\begin{array}{ll}
x_0^{d_0}\cdots x_i^{d_i-\tilde{d}}\cdots x_{n+1}^{d_{n+1}}F_i &
{\rm if\ }i\le n\\
x_0^{d_0}\cdots x_n^{d_n}x_{n+1}^{d_{n+1}-1}u\cdot x &
{\rm if\ }i = n+1
\end{array}
\right.\]
where $u=(u_0,\ldots,u_{n+1})$ and $x=(x_0,\ldots,x_{n+1})$\\
Let $M$ be the matrix representing $\mathcal{T}$ with respect to the
basis $\mathbb{B}$
\item[(3)] Finally set $R=D!{\rm det}(M)$ where $m$ is a $D\times D$
matrix.$\blacksquare$
\end{itemize}

In \cite{Rene92} it is shown that the worst case time complexity of this
algorithm is dominated by $L(log~L)(log~log~L)(md)^{O(n)}$,
which is the best compared to the theoretical complexities of all other
algorithms proposed in the literature so far.

Renegar\cite{Rene92}, by generalizing the above algorithm, gave a
quantifier elimination algorithm for the first order theory of the reals.

\section{Special Methods}

During last several years, there have been efforts for developing
methods that work on a restricted class of inputs. Such research is
motivated by the observation that one can identify interesting and
useful sub-class of problems and that one might be able to develop
more efficient (in practice) methods for them.

\subsection{Low Degrees}

Weispfenning\cite{Weis88}\cite{Weis94}\cite{Weis97},
Loos\cite{Loos93}, Anai\cite{Anai00} and others devised methods for
formulas where the bound variables occur in low degrees (1,2, and 3).

\subsubsection{Restricted Problem Class}

A formula with one quantifier is called $degree~n$ if the bound variable
occur in degree at most $n$. We would like to find methods for formulas
with small degrees.

For formulas with many quantifiers, we can repeatedly apply such a
method to eliminate all the quantifiers one at a time, starting from
the innermost ones, provided that the result of each quantifier
elimination stays degree at most $n$ (in the remaining bound variables).

\subsubsection{Method}

In order to get the main intuition, let us consider the case when
there are no free variables. The initial idea is similar to that of
the Cylindrical Algebraic Decomposition. 

Let $\alpha_1 < \cdots < \alpha_\ell$ be the
real roots of the polynomials occurring in the quantifier-free matrix
$F$. Then the sign of each polynomial (and thus the truth of the
sentence) is constant throughout each cell:
\[(-\infty,\alpha_1),[\alpha_1,\alpha_1],(\alpha_1,\alpha_2),\ldots,
(\alpha_{\ell-1}),[\alpha_\ell,\alpha_\ell],(\alpha_\ell,\infty)\]
Thus, we only need to ``sample'' one point from each cell. For instance,
we could choose
\[S=\{\alpha_i|1\le i\le \ell\}\cup\{\alpha_\ell+1,\alpha_1-1\}\cup
\{\frac{1}{2}(\alpha_i+\alpha_{i+1})|1\le i\le \ell\}\]
Then we have
\[\exists x F(x) \Longleftrightarrow \bigvee_{t\in S}F(t)\]
\[\forall x F(x) \Longleftrightarrow \bigwedge_{t\in S}F(t)\]
Thus, the finite set $S$ contains sufficiently many samples from an
infinite set $\mathbb{R}$ for the purpose of deciding the sentences.
Let us call such a set the {\sl sample set}.

This method is generalizable in a obvious way to the case with free
variables, but not {\sl efficiently}, because when there are free
variables, the order of the roots depend on the values of the free
variables and we will have anticipate all the ``potential'' orders,
blowing up the size of the output formula.

A better approach (taken by Weispfenning) is to utilize the symbolic
devices such as infinities and (positive) infinitesimals: $\infty$ and
$\epsilon$. Using these, one can see immediately the following set also 
forms a sample set (though a rigorous proof will require either the 
tedious $\epsilon - \delta$ reasoning or the non-standard analysis):
\[S=\{\alpha_i~|~1\le i \le \ell\}\cup\{-\infty\}\cup
\{\alpha_i+\epsilon~|~1\le i\le \ell\}\]

Note that the resulting formula is quantifier-free but it instead
contains other new symbols such as $\infty$, and the ``roots''
$\alpha_i$. Now, we
need to eliminate them, thus the quantifier elimination problem is
reduced to infinity elimination, infinitesimal elimination, and root
elimination. Clearly it suffices to show how to eliminate those
symbols from each atomic formula.

\vskip 0.5cm
\noindent
{\bf Infinity Elimination}: Let $f_n(x)=a_nx^n+\cdots+a_0$ be a polynomial
in an atomic formula. Then from the intended meaning of $-\infty$, the
correctness of the following rewrite rules is immediate:
\[\begin{array}{rcl}
f_n(-\infty)=0 & \rightarrow & a_n=0 \land \cdots \land a_0=0,\\
f_n(-\infty)<0 & \rightarrow & 
(-1)^na_n < 0 \lor [a_n=0 \land f_{n-1}(-\infty)<0]
\end{array}\]
For the case of $<$, we only need to apply the rule repeatedly until $n$
becomes 1. Atomic formulas with other relational operators can be
rewritten to formulas involving only $=$ and $<$ to which the above rules
can be applied.

\vskip 0.5cm
\noindent
{\bf Infinitesimal Elimination}:
Let $f(x)=a_nx^n+\cdots+a_0$ be a polynomial in an atomic formula.
From the intended meaning of $\epsilon$, we have the followings (assuming
that $f$ is a non-zero polynomial):
\begin{itemize}
\item $f(\alpha + \epsilon) \ne 0$
\item The sign of $f(\alpha + \epsilon)$ is the same as that of the
highest non-vanishing derivative of $f$ an $\alpha$.
\end{itemize}
From these observations, we immediately obtain the following rewrite rules:
\[\begin{array}{rcl}
f(\alpha+\epsilon)=0 & \rightarrow & a_n=0 \land\cdots\land a_0=0\\
f(\alpha+\epsilon)<0 & \rightarrow & f(\alpha)<0 \lor
[f(\alpha)=0 \land f^\prime(\alpha+\epsilon)<0]
\end{array}\]
For the case of $<$, we only need to apply the rule repeatedly until the
degree of $f$ becomes 1.

\vskip 0.5cm
\noindent
{\bf Linear Root Elimination}: Let $f$ be a polynomial in $x$. Let $\alpha$
be a real root of a linear polynomial $a_1x+a_0=0$, thus $\alpha=-a_0/a_1$.
We would like to eliminate the root (or root term) from the atomic formula
$f(-a_0/a_1)~\rho~0$ where $\rho$ is a relational operator. Essentially we
would like to eliminate the division symbol: $/$. One could immediately
think of the following rewrite rule:
\[\begin{array}{rcl}
f(b/a)=0 & \rightarrow & a^nf(b/a)=0\\
f(b/a)<0 & \rightarrow & a^{n+\delta}f(b/a)<0
\end{array}\]
where $n$ is the formal degree of $f$, $\delta$ is its parity, 
and $a^kf(b/a)$
stands for the polynomial obtained by canceling out the denominators
of $f(b/a)$ by multiplying with $a^k$. But this is not correct. The
denominator $a$ might vanish for some values of the free variables, thus
making the root undefined. But it also means that it is not a root for
the specific values of the free variables. Thus, we could simply
ignore it. But it can happen that all roots are such. In that case, we
should not ignore all of them, since we need to have at least one
sample point. One solution is to view such a case (vanishing
denominator) as always supplying the sample point 0.  The following
rewrite rules implement these ideas.\footnote{At the time of writing
this tutorial, I notice that it is not actually necessary to duplicate
0 so many times. We need only one 0 to ensure that there is at least one
sample point. Further $a\ne 0$ can be ``factored out''. So we can go back
to the ``incorrect'' rewrite rules:
\[\begin{array}{rcl}
f(b/a)=0 & \rightarrow & a^nf(b/a)=0\\
f(b/a)<0 & \rightarrow & a^{n+\delta}f(b/a)<0
\end{array}\]
provided that we use
\[\exists x F \Longleftrightarrow F(0) \lor \bigvee_{b/a\in S}
a\ne 0 \land F(b/a)\]}
\[\begin{array}{rcl}
f(b/a)=0 & \rightarrow & [a=0 \land f(0)=0]\lor
[a\ne 0 \land b^nf(b/a)=0]\\
f(b/a)<0 & \rightarrow & [a=0 \land f(0)<0]\lor
[a\ne 0 \land b^{n+\delta}f(b/a)<0]
\end{array}\]

\vskip 0.5cm
\noindent
{\bf Quadratic Root Elimination}: Let $f$ be a polynomial in $x$.
Let $\alpha$ be a real root of a quadratic polynomial
$a_2x^2 + a_1x + a_0=0$, thus $\alpha$ is one of
$\frac{-a_1\pm \sqrt{a_1^2-a_2a_0}}{2a_2}$. We would like to eliminate the
root from the atomic formula
$f(\frac{-a_1\pm \sqrt{a_1^2-a_2a_0}}{2a_2})~\rho~0$ where $\rho$ is a
relational operator. Essentially we would like to eliminate the symbols:
$\sqrt{\phantom{x}}$ and $/$. The following observation helps.
\[\displaystyle
\frac{a+b\sqrt{c}}{d}+\frac{a^\prime+b^\prime\sqrt{c^\prime}}{d^\prime} =
\frac{(ad^\prime+a^\prime d)+(bd^\prime+b^\prime d)\sqrt{c}}{dd^\prime}\]
\[\displaystyle
\frac{a+b\sqrt{c}}{d}\times\frac{a^\prime+b^\prime\sqrt{c^\prime}}{d^\prime} =
\frac{(aa^\prime+bb^\prime c)+(ab^\prime+a^\prime b)\sqrt{c}}{dd^\prime}\]

By applying these equality repeatedly,
$f(\frac{a+b\sqrt{c}}{d})$ results in the same form:
\[\frac{a^*+b^*\sqrt{c^*}}{d^*}\]
Note that $d^*=d^k$ where $k$ is the
formal degree of $f$. Let $\delta$ be the parity of $k$. Then we can rewrite:
\[\begin{array}{rcl}
\displaystyle
f(\frac{a+b\sqrt{c}}{d})=0 &\rightarrow& a^{*^2}-b^{*^2}c=0 \land a^*b^*\le 0\\
&&\\
\displaystyle
f(\frac{a+b\sqrt{c}}{d})<0 & \rightarrow &
[[a^*d^\delta\le 0 \land a^{*^2}-b^{*^2}c\ge 0]\lor b^*d^\delta\le 0]\land\\
&&\\
\displaystyle
&&[a^*d^\delta \le 0 \lor a^{*^2}-b^{*^2}c\le 0]
\end{array}\]

Again we need to take care of the ``bad'' situations such as $d=0$ or
$c < 0$. This can be done by doing some case analysis, and this will
increase the size of the formula a bit.

\vskip 0.5cm
\noindent
{\bf Improvements} In \cite{Loos93}, 
several optimizations are described, such as
reducing the size of the sample set by utilizing the relational operators,
the boolean structure, the quantification structure, etc.

\subsection{Constrained by Quadratic Equation}

Hong\cite{Hong93}\cite{Hong93a}
devised an algorithm that eliminates a quantifier
from a formula which is {\sl constrained by a quadratic equation}. The
output formulas are made of resultants and their variants called {\sl slope}
resultants.  The slope resultants can be, like the resultants,
expressed as determinants of certain matrices.

Weispfenning\cite{Weis94} gave a method (as described in the previous
section) which handles this case (and more). The method there does not
require extended resultant calculus. The outputs are also very similar
except that the method of Weispfenning systematically introduces some
extraneous factors in the output polynomials.  In fact, the initial
motivation for developing the variant resultant calculus was to avoid
the introduction of extraneous factors. Further the variant resultant
calculus has some nice properties and might provide some new way to
higher degrees.

\subsubsection{Restricted Problem Class}

A formula is said to be {\sl constrained by a quadratic equation} if it is
of the following form:
\[(\exists x \in \mathbb{R})[a_2x^2+a_1x+a_0=0 \land F]\]
where
\begin{itemize}
\item $F$ is a quantifier free formula in $x_1,\ldots,x_r,x$
\item $a_2,a_1,a_0$ are polynomials over $x_1,\ldots,x_r$ such that
$a_2,a_1$ and $a_0$ do not have a common real zero in $\mathbb{R}^r$.
\end{itemize}

\subsubsection{Mathematical Tool: Slope Resultants}

Let us first define a variant of resultant, which we call {\sl slope}
resultant. Later we will use this while developing a quantifier
elimination algorithm for the problem stated above.

Let $I$ be an integral domain, and let $\tilde{I}$ be the unique (up to
isomorphism) algebraic closure of the quotient field of $I$.

\vskip 0.5cm
\noindent
{\bf Defintion 2 (Slope)} {\sl Let P be a univariate polynomial over I.
The $k$-th slope of $P$, written as $P^{<k>}$, for $k\ge 0$,
is the $(k+1)$-variate polynomial over $I$ defined recursively by}
\[\begin{array}{rcl}
P^{<0>}(x_1) &=& P(x_1)\\
P^{<k>}(x_1,\ldots,x_{k+1}) &=& \displaystyle
\frac{P^{<k-1>}(x_1,\ldots,x_k)-P^{<k-1>}(x_2,\ldots,x_{k+1})}
{x_1-x_{k+1}}
\end{array}\]
{\sl for\ }$k\ge 1$.$\blacksquare$

Though the definition of slope involves rational functions, the
divisions are always exact, and thus a slope is a polynomial. We keep
the rational function formulation in this definition because it is
more natural and intuitive.

\vskip 0.5cm
\noindent
{\bf Definition 3 (Slope Resultant)} {\sl Let A and B be univariate
polynomials over I.\\
Let $A=a_m\prod_{i=1}^m(x-\alpha_i)$ where
$\alpha_i\in\tilde{I}$. Then the k-th slope resultant of A and B,
written as ${\rm sres}_k(A,B)$, is defined by}
\[{\rm sres}_k(A,B)=a_m^{n-k}
\sum_{1\le i_1 <\cdots < i_{k+1} \le m} B^{<k>}
(\alpha_{i_1},\ldots\alpha_{i_{k+1}})\quad\blacksquare\]

Intuitively this is the average of the $k$-th ``derivative'' of $B$
at the roots of $A$, up to a constant factor.

The slope resultant can be computed in various different ways:
recurrence formula, generating functions, and determinants of certain
matrix \cite{Hong98a}\cite{Hong93a}. 
Here we only give the determinant method.

\vskip 0.5cm
\noindent
{\bf Theorem 2 (Slope Resultant as Determinant)}
{\sl Let $A=\sum_{i=0}^m a_ix^i$ and $B=\sum_{i=0}^n b_ix^i$. Then we have}
\[{\rm sres}_k(A,B)={\rm det}(M)\]
{\sl where $M$ is the $n+1-k$ by $n+1-k$ matrix defined by}
\[M=\left[
\begin{array}{cccccc}
a_m & a_{m-1} & \cdots & \cdots  & \cdots  &\\
    & \cdots  & \cdots & \cdots  & \cdots  & \cdots\\
    &         &  a_m   & a_{m-1} & a_{m-2} & c_3^{k+1}\cdot a_{m-3}\\
    &         &        &  a_m    & a_{m-1} & c_2^{k+1}\cdot a_{m-2}\\
    &         &        &         &  a_m    & c_1^{k+1}\cdot a_{m-1}\\
b_n & b_{n-1} & \cdots & \cdots  & b_{k+1} & c_m^{k+1}\cdot b_k\\
\end{array}
\right]\]
{\sl where $c_i^k={m \choose k}-{{m-i} \choose k}$. Let
$n^\prime = n + 1 - k$. Precisely, the matrix is defined by}
\[M_{i,j}=
\left\{\begin{array}{lr}
a_{m-(j-i)} & {\rm if}~i < n^\prime ~{\rm and}~ j < n^\prime\\
&\\
b_{n-j+1}   & {\rm if}~i = n^\prime ~{\rm and}~ j < n^\prime\\
&\\
c_{j-i}^{k+1}a_{m-(j-i)}  & {\rm if}~i < n^\prime ~{\rm and}~ j = n^\prime\\
&\\
c_m^{k+1}b_k   & {\rm if}~i = n^\prime ~{\rm and}~ j = n^\prime\\
\end{array}\right.\]
{\sl where $a_\mu=0$ if $\mu > m$ or $\mu < 0$ and $b_\nu=0$ if
$\nu > n$ or $\nu < 0$}\quad$\blacksquare$

\subsubsection{Method}

Now we are ready to give a quantifier elimination algorithm that
utilizes the slope resultants. Without losing generality, let us
assume that the quantifier free forula $F$ involves only the two
relation operators: $=$ and $>$. We can decompose the input formula $F^*$
into two sub-problems in an obvious way.
\[F^* \Longleftrightarrow [a_2=0 \land F_1^*] \lor F_2^*\]
where
\[\begin{array}{rcl}
F_1^* & \equiv & a_1\ne 0 \land (\exists x)[a_1x+a_0=0 \land F]\\
F_2^* & \equiv & a_2\ne 0 \land (\exists x)[a_2x^2+a_1x+a_0=0 \land F]
\end{array}\]

Thus the problem is reduced into two quantifier elimination problems:
one for $F_1^*$ and the other for $F_2^*$ .  One way for solving these
problems might be to put the ``parametric'' roots (expressions) into $F$
and eliminate them by rewriting. (as we have seen in the previous
section on Linear Quantifier Elimination). But the following two
theorems show us that we can bypass these two steps by utlizing the
resultants and the slope resultants.

\vskip 0.5cm
\noindent
{\bf Theorem 3 (Linear Case)}
{\sl Let $F^\prime$ be the quantifier free formula in the variables
$x_1,\ldots,x_r$ defined recursively by}
\[F^\prime \equiv
\left\{
\begin{array}{ll}
R=0 & {\rm if\ }F\equiv B=0\\
R>0 & {\rm if\ }F\equiv B > 0, {\rm deg}_x(B){\rm\ is\ even}\\
a_1R>0 & {\rm if\ }F\equiv B>0,{\rm deg}_x(B){\rm\ is\ odd}\\
F_1^\prime \lor F_2^\prime & {\rm if\ }F\equiv F_1 \lor F_2\\
F_1^\prime \land F_2^\prime & {\rm if\ }F\equiv F_1 \land F_2\\
\lnot F_1^\prime & {\rm if\ }F\equiv \lnot F_1
\end{array}\right.\]
{\sl where $R={\rm res}(a_1x+a_0,B)$. Then we have}
\[F_1^* \Longleftrightarrow a_1\ne 0 \land F^\prime\quad\blacksquare\]

The following theorem shows a way to eliminate the existential
quantifier from the formula $F_2^*$.

\vskip 0.5cm
\noindent
{\bf Theorem 4 (Quadratic Case)}
{\sl Let $F^{(1)}$ be the quantifier free formula in the variables
$x_1,\ldots,x_r$ defined recursively by}
\[F^{(1)}\equiv
\left\{\begin{array}{ll}
R=0 \land TS \le 0 & {\rm if\ }F\equiv B=0\\
R>0 \land T > 0 ~\lor &\\
\quad R<0 \land S > 0 ~\lor &\\
\quad T > 0 \land S > 0 & {\rm if\ }F \equiv B > 0,\\
&\quad {\rm deg}_x(B){\rm\ is\ even}\\
a_2 R>0 \land a_2 T > 0 ~\lor&\\
\quad a_2R < 0 \land a_2S > 0 ~\lor&\\
\quad a_2T > 0 \land a_2S > 0 & {\rm if\ }F \equiv B > 0\\
& \quad {\rm deg}_x(B){\rm\ is\ odd}\\
F_1^{(1)} \lor F_2^{(1)} & {\rm if\ }F \equiv F_1 \lor F_2\\
F_1^{(1)} \land F_2^{(1)}  & {\rm if\ }F \equiv F_1 \land F_2\\
\lnot F_1^{(1)} & {\rm if\ }F \equiv \lnot F_1
\end{array}\right.\]
{\sl where}
\[\begin{array}{rcl}
R & = & {\rm res}(a_2x^2 + a_1x + a_0,B)\\
T & = & {\rm sres}_0(a_2x^2 + a_1x + a_0,B)\\
S & = & {\rm sres}_1(a_2x^2 + a_1x + a_0,B)\\
\end{array}\]

{\sl Let $F^{(2)}$ be the quantifier formula in the variables
$x_1,\ldots x_r$ defined in the same way as $F^{(1)}$, except that
$S$, $F_1^{(1)}$, and $F_2^{(1)}$ are replaced by
$-S$, $F_1^{(2)}$, and $F_2^{(2)}$. Then we have}
\[F_2^* \Longleftrightarrow a_2\ne 0 \land a_1^2-4a_2a_0 \ge 0 \land
[F^{(1)} \lor F^{(2)}]\quad\blacksquare\]

We summarize the above results in the following algorithm.

\vskip 0.5cm
\noindent
{\bf Algorithm 3 (Quantifier Elimination)}
\vskip 0.2cm
\noindent
\begin{tabular}{ll}
{\sl Input\ \ }: & A formula $F^*$ of the form
\end{tabular}
\[(\exists x \in \mathbb{R})[a_2x^2 +a_1x +a_0 = 0 \land F]\]
\begin{tabular}{ll}
&where $F$ is a quantifier free formula in $x_1,\ldots,x_r,x$ and\\
&$a_2,a_1,a_0$ are polynomials over $x_1,\ldots,x_r$ such that\\
&$a_2,a_1$ and $a_0$ do not have a common real zero in $\mathbb{R}^r$\\
{\sl Output}: & A quantifier-free formulat $\tilde{F}$ equivalent to $F$
\end{tabular}

\begin{itemize}[noitemsep]
\item[(1)] Apply on $F$ the recursive algorithm in Theorem 3, obtaining
a quantifier free formula $F^\prime$.
\item[(2)] Apply on $F$ the recursive algorithm in Theorem 4, obtaining
two quantifier free formulas $F^{(1)}$ and $F^{(2)}$
\item[(3)] Obtain $\tilde{F}$ by putting together $F^\prime$, $F^{(1)}$,
and $F^{(2)}$ as follows:
\[\begin{array}{rcl}
\tilde{F} & \equiv & a_2=0 \land a_1 \ne 0 \land F^\prime \lor\\
&&a_2 \ne 0 \land a_1^2-4a_2a_0 \ge 0 \land [F^{(1)} \lor F^{(2)}]
\quad\blacksquare
\end{array}\]
\end{itemize}

\vskip 0.5cm
\noindent
{\bf Performance}
If we allow the determinant symbol in the output, the computing time
of the algorithm is {\sl linear} in the length of the input. If not, the
computing time is dominated by
$N(n^{2r+1}\ell+n^{2r}\ell^2)$ where $N$ is the
number of polynomials in the input formula, $r$ is the number of
variables, $n$ is the maximum of the degrees for every variable, and $\ell$
is the maximum of the integer coefficient bit lengths. Experiments
with implementation suggest that the algorithm is sufficiently
efficient to be useful in practice.

\subsection{Single Atomic Formula}

Gonzales-Vega\cite{Gonz98} gave algorithms for formulas with single
atomic formula. The algorithms first compute a finite number of
polynomials and select among all the possible sign conditions over
these polynomials those making the considered formula true. The main
mathematical tool used is the Sturm-Habicht sequence\cite{Gonz89}
which is essentially a careful adaptation of Sturm's sequence to the
subresultant chain.

\subsubsection{Restricted Problem Class}

Let $P_n(\underbar{a},x)$ denote the polynomial
$x^n+a_{n-1}x^{n-1}+\ldots+a_1x+a_0$. Then we are interested in the
formulas of the following kind:
\[(Qx)P_n(\underbar{a},x)~\rho~0\]
where $Q$ is either $\forall$ or $\exists$, and $\rho$ is a relational
operator. One sees that the problem can be trivially reduced to the
following two types:
\[\begin{array}{rcl}
{\bf Type1}: & \exists x & P_n(\underbar{a},x) = 0\\
{\bf Type2}: & \exists x & P_n(\underbar{a},x) < 0\\
\end{array}\]
for even $n$.

\subsubsection{Mathematical Tool: Sturm-Habicht}

Let $\mathbb{D}$ be an integral domain

\vskip 0.2cm
\noindent
{\bf Definition 4}.
{\sl Let $P=\sum_{k=0}^p a_kx^k$ and $Q=\sum_{k=0}^q b_kx^k$ be polynomials
in $\mathbb{D}[x]$ with {\rm deg}$(P)\le p$ and 
{\rm deg}$(Q)\le q$. The $i$-th formal
principal subresultant coefficient.} {\bf sres}$_i(P,p,Q,q)$, {\sl is
the determinant of the following matrix:}

\[\begin{array}{cl}

\overbrace{
\begin{array}{ccccc}
& \quad\quad & \quad\quad & \quad\quad & \quad\quad \\
\end{array}}^{p+q-2i} &\\

\left(\begin{array}{ccccc}
a_p & a_{p-1} & \cdots &        &\\
    & \ddots  &        & \ddots  &\\
    &         &   a_p  & a_{p-1} & \cdots\\
b_1 & b_{q-1} & \cdots &         &\\
    & \ddots  &        & \cdots  &\\
    &         &   b_q  & b_{q-1} & \cdots\\
\end{array}\right) 
&
\begin{array}{l}
\left.
\begin{array}{l}
\\
\\
\\
\end{array}\right\} 
q-i \\
\left.
\begin{array}{l}
\\
\\
\\
\end{array}\right\} 
p-i
\end{array}
\end{array}\]
\noindent\quad$\blacksquare$

This is essentially the same as the usual definition of the principal
subresultant coefficients (used in the projection of the Cylindrical
Algebraic Decomposition method), except that it allows {\sl formal} leading
coefficients, that is, deg$(P)$ and deg$(Q)$ do not have to be exactly $p$
and $q$.

\vskip 0.5cm
\noindent
{\bf Definition 5} {\sl Let $P,Q$ be polynomials in $\mathbb{D}[x]$.
Let $v={\rm deg}(P)+{\rm deg}(Q)-1$. The principal $i$-th Sturm-Habicht
coefficient,} {\bf stha}$_i(P,Q)$, {\sl is defined by}
\[{\rm {\bf stha}}_i(P,Q)=(-1)^{(v-i)(v-i+1)/2}
{\rm {\bf sres}}_i(P,v+1,P^\prime Q,v)\quad\blacksquare\]

The {\bf stha} has a nice {\sl specialization property}. The $\phi$
stand for the specialization of $a_i$ and $b_i$ to some real numbers
such that the degrees stay the same. Then we obviously have the following:
\[\phi({\rm {\bf stha}}_i(P,Q))={\rm {\bf stha}}_i(\phi(P),\phi(Q))\]
that is, the {\bf stha} commutes with specialization.

\vskip 0.2cm
\noindent
{\bf Definition 6} {\sl Let $a_0,a_1,\ldots,a_n$ be a list of non zero
elements in $\mathbb{R}$. We define:
\begin{itemize}[noitemsep]
\item {\bf P}($a_0,a_1,\ldots,a_n$) as the number of sign permanence in
the list $a_0,a_1,\ldots,a_n$
\item {\bf V}($a_0,a_1,\ldots,a_n$) as the number of sign variations in
the list $a_0,a_1,\ldots,a_n$\quad$\blacksquare$
\end{itemize}}

\vskip 0.2cm
\noindent
{\bf Definition 7} {\sl Let $a_0,a_1,\ldots,a_n$ be elements in $\mathbb{R}$
with $a_0\ne 0$. Suppose that it is made of $A_1,Z_1,A_2,Z_2,\ldots,A_t,Z_t$
where each $A_i$ is a sequence of non-zeros and $Z_i$ is a sequence of zeros.
Let $k_i$ be the length of $Z_i$. Let $h_i$ and $t_i$ be respectively, the
sign of the head and the tail element of $A_i$. Then we define}
\[{\rm {\bf C}}(a_0,a_1,\ldots,a_n)=\sum_{i=1}^t{\rm {\bf P}}(A_i)-
\sum_{i=1}^t{\rm {\bf V}}(A_i)+\sum_{i=1}^{t-1}\epsilon_i\]
{\sl where}
\[\epsilon_i=\left\{
\begin{array}{ll}
0 & {\rm if\ }k_i{\rm\ is\ odd}\\
(-1)^{\frac{k_i}{2}}t_ih_{i+1} & {\rm if\ }k_i{\rm\ is\ even}\quad\blacksquare
\end{array}\right.\]

\vskip 0.2cm
\noindent
{\bf Definition 8} {\sl Let $P$ and $Q$ be polynomials in $\mathbb{R}[x]$.
Then let}
\[c_\epsilon(P;Q)={\rm card}(\{\alpha \in \mathbb{R}|P(\alpha)=0,\quad
{\rm sign}(Q(\alpha))=\epsilon\})\]
{\sl where $\epsilon$ is a sign} ($+$,$-$,0).\quad$\blacksquare$

\vskip 0.2cm
\noindent
{\bf Theorem 5} {\sl Let $P$ and $Q$ be polynomials in $\mathbb{R}[x]$ with
{\rm deg}$(P)=p$. Then we have}
\[{\rm {\bf C}}({\rm {\bf stha}}_p(P,Q),\ldots,{\rm {\bf stha}}_0(P,Q))=
c_+(P;Q)-c_-(P;Q)\quad\blacksquare\]

\subsubsection{Method}

Let us tackle now the two types of quantifier elimination problems
mentioned above. From now, let
\[{\rm {\bf C}}(P,Q)={\rm {\bf C}}({\rm {\bf stha}}_p(P,Q),\ldots,
{\rm {\bf stha}}_0(P,Q))\]

\noindent
{\bf Type 1}
From the good specialization property of {\bf stha} and Theorem 5, 
we immediately obtain
\[\exists x P_n(\underbar{a},x)=0 \Longleftrightarrow
{\rm {\bf C}}(P_n,1)>0\]
Now we only need to ``expand'' the right hand side by checking all the 
$3^{n+1}$ possible sign conditions over the polynomials
{\bf stha}$_i(P_n,1)$ and keeping those making $C > 0$.

\vskip 0.5cm
\noindent
{\bf Type 2}
Let $I_n^i$ denote the formula stating that the polynomial $P_n(a,x)$
has a real root with multiplicity $i$.  Obviously we have
\[\exists x P_n(\underbar{a},x)<0 \Longleftrightarrow
\bigvee_{i=1,{\rm odd}}^{n-1} I_n^i(\underbar{a})\]
So we only need to find a quantifier-free formula for $I_n^i$. From
the definition of multiplicity, we immediately have
\[\begin{array}{rcl}
I_n^i(\underbar{a}) & \Longleftrightarrow &
\displaystyle
(\exists x)[P_n^{(0)}(\underbar{a},x)=
\ldots =P_n^{(i-1)}(\underbar{a},x)=0\\
&&\land~ P_n^{(i)}(\underbar{a},x)\ne 0]\\
 & \Longleftrightarrow &
\displaystyle
(\exists x)[\sum_{k=0}^{i-1}(P_n^{(k)}(\underbar{a},x))^2
 =0 \land P_n^{(i)}(\underbar{a},x)\ne 0]\\
 & \Longleftrightarrow &
\displaystyle
(\exists x)[\sum_{k=0}^{i-1}(P_n^{(k)}(\underbar{a},x))^2
 =0 \land (P_n^{(i)}(\underbar{a},x))^2 > 0]
\end{array}\]
Since square of real number is non-negative, 
we immediately see from Theorem 5 that
\[I_n^i(\underbar{a}) \Longleftrightarrow
\displaystyle
{\rm {\bf C}}(\sum_{k=0}^{i-1}(P_n^{(k)}(\underbar{a},x))^2,
(P_n^{(i)})^2)>0\]
Thus, finally we have
\[\exists x P_n(\underbar{a},x)<0 \Longleftrightarrow
\displaystyle
\bigvee_{i=1,{\rm odd}}^{n-1}
\displaystyle
{\rm {\bf C}}(\sum_{k=0}^{i-1}(P_n^{(k)}(\underbar{a},x))^2,
(P_n^{(i)})^2)>0\]
Again we will have to expand the right hand side by checking all the
possible sign conditions over the {\bf stha} polynomials and keeping
those making ${\bf C}>0$.

\section{Approximate Methods}

By now, it is well understood both theoretically and experimentally
that the full exact quantifier elimination is intrinsically difficult,
posing a formidable barrier to efforts for devising general but
efficient methods. One response to this is specialization, as we have
surveyed in the previous section. In this section, we see another
response: {\sl approximation}.

In many applications (in particular in science and engineering),
an approximate answer is acceptable. In fact, sometimes, it is even
meaningless to think of exact answers because the input itself often
contain non-exactness.

This is a good news for algorithm designers, since experience shows
that the enormous complexity of quantifier elimination mainly stems
from the insistence on exactness. If we are willing to compromise
this, then we might be able to devise methods that can handle large
problems arising from real-life applications.

This does not mean that approximation will give a better asymptotic
worst case bound. It might be the case that the approximated methods
have the same bound, or even worse. But there is evidence that the
approximated methods are much more efficient for a large class of
moderate size inputs.

The word ``approximate'' carries different meanings in different
contexts. In this section, we survey two different interpretations and
methods for them: {\sl generic} and {\sl volume-approximate}.

\subsection{Generic Quantifier Elimination}

One often observes that exact algorithms spend most of their computing
time in analyzing and taking care of non-generic cases. But the
end-users of the methods are often interested only in generic
(regular) cases. In such situations, the exact methods wasted
computing resource to produce unnecessary information. It will be
better to have a method that works only on generic cases.

This observation lead Hong to interpret approximatedness as
being {\sl correct except for some non- generic cases}. The non-generic
cases usually form a measure-zero set, and thus generically correct
answers are different from the exact answers only by measure zero
sets. Thus, it can be also viewed as being {\sl correct up to measure zero}.

\subsubsection{Problem}

We begin by giving a precise statement of the generic quantifier
elimination problem. 
Let $F(x_1,\ldots,x_f)$ be a formula. Let $S(F)$
stand for the solution set of $F$, that is, the set\\
$\{p\in\mathbb{R}^f~|~F(p){\rm\ is\ true}\}$.
We will define two notions: {\sl generic quantifiers} and
{\sl generic equivalence}.

\vskip 0.5cm
\noindent
{\bf Definition 9 (Generic Quantifiers)}
{\sl Let $F$ be a formula with one free variable $x$.}
\begin{itemize}[noitemsep]
\item {\sl The {\rm generic universal quantifier}, $\dot{\forall}$,
is defined by: $\dot{\forall} x F(x)$ is true iff $S(\lnot F)$ is of
measure zero}
\item {\sl The {\rm generic existential quantifier}, $\dot{\exists}$,
is defined by: $\dot{\exists} x F(x)$ is true iff $S(F)$ is of
measure zero}\quad$\blacksquare$
\end{itemize}
Intuitively, the generic universal quantifier can be read as ``for
almost all'', and the generic existential quantifier can be be read as
``for sufficiently many''. As expected, the following properties hold
for the generic quantifiers.

\vskip 0.5cm
\noindent
{\bf Proposition 2 (Commutativity)}
\begin{itemize}[noitemsep]
\item $\dot{\forall}x\dot{\forall}y F(x,y) \Longleftrightarrow
\dot{\forall} y\dot{\forall} x F(x,y)$
\item $\dot{\exists}x\dot{\exists}y F(x,y) \Longleftrightarrow
\dot{\exists} y\dot{\exists} x F(x,y)\quad\blacksquare$
\end{itemize}

\vskip 0.5cm
\noindent
{\bf Proposition 3 (Negation)}
\begin{itemize}[noitemsep]
\item $\lnot\dot{\forall} x F(x) \Longleftrightarrow
\dot{\exists} x \lnot F(x)$
\item $\lnot\dot{\exists} x F(x) \Longleftrightarrow
\dot{\forall} x \lnot F(x)\quad\blacksquare$
\end{itemize}
The ``strength'' of the four quantifiers (two exact and two generic)
form the following spectrum.

\vskip 0.5cm
\noindent
{\bf Proposition 4 (Spectrum of quantification strength)}
\[\forall x F(x) \Longrightarrow
\dot{\forall} x F(x) \Longrightarrow
\dot{\exists} x F(x) \Longrightarrow
\exists x F(x)\quad\blacksquare\]


\vskip 0.5cm
\noindent
{\bf Definition 10 (Generic equivalence)}
{\sl Two formulas $F$ and $G$ with free variables
$x_1,\ldots,x_f$ are generically equivalent iff the sentence}
\[\dot{\forall}x_1,\ldots\dot{\forall}x_f
[~F(x_1,\ldots,x_f) \Longleftrightarrow G(x_1,\ldots,x_f)~]\]
{\sl is true}\quad$\blacksquare$

\vskip 0.2cm
\noindent
Intuitively, two formulas are generically equivalent if their solution
sets are ``almost the same''. Finally we are ready to state the generic
quantifier elimination problem.

\vskip 0.5cm
\noindent
{\bf Problem}: Device an algorithm with the specification:
\begin{itemize}[noitemsep]
\item[{\sl In:}] A formula $F(x_1,\ldots,x_f)$ involving only generic
quantifiers
\item[{\sl Out:}] A formula $G(x_1,\ldots,x_f)$ such that it involves
no (generic) quantifiers and $F$ and $G$ are generically equaivalent.
\quad$\blacksquare$
\end{itemize}

\subsubsection{Method}

A natural way is to take an existing exact algorithm and turn it into
a generic one. We will do so with the method of (partial) cylindrical
algebraic decomposition \cite{Coll75}\cite{Hong90a}\cite{Coll91}.

\vskip 0.5cm
\noindent
{\bf Idea 1: No algebraic numbers}.
Since the generic quantifier elimination ignores measure zero sets, we
do not need to construct stacks over even-indexed cells (because they
are of measure zero). This results in a significant reduction of
computing time since we can completely eliminate the heavy real
algebraic number computations (which are needed for working with even
indexed cells).

\vskip 0.5cm
\noindent
{\bf Idea 2: Smaller Projection}.
Further, the projection polynomials are non-zero throughout odd
indexed cells. Thus we can reduce the size of projection by removing
the polynomials that are put in there to ensure delineability over
even indexed cells. Precisely, we have the following:

\vskip 0.5cm
\noindent
{\bf Definition 11 (Generic Projection)}
{\sl Let $A$ be a finite subset of $\mathbb{Z}[x_1,\ldots,x_r]$.
The generic projection of $A$, written as {\rm GProj}$(A)$, is defined as:}
\[\begin{array}{rcl}
{\rm GProj}(A) & = & L \cup R \cup D\\
L & = & \{lc(a)~|~a\in A\}\\
R & = & \{{\rm psc}_0(a_1,a_2)~|~a_1,a_2\in A\}\\
D & = & \{{\rm psc}_0(a,a^\prime)~|~a\in A\}\quad\blacksquare
\end{array}\]

\vskip 0.5cm
\noindent
{\bf Theorem 6 (Generic Projectiong gives Delineability)}
{\sl Let $A$ be a finite subset of $\mathbb{Z}[x_1,\ldots,x_r]$, $r\ge 2$.
Let $c$ be a non-empty connected subset of $\mathbb{R}^{r-1}$ such that
for all $J \in {\rm GProj}(A)$ and for all $p \in c$ we have
$J(p)\ne 0$. Then the zero of $A$ delineates the cylinder over $c$.}
\quad$\blacksquare$

\vskip 0.5cm
\noindent
{\bf Idea 3: No square-free factorization needed}.
Stack construction consists of the following steps: 
\begin{itemize}[noitemsep]
\item[(1)] Evaluate the
projection polynomials on a sample sample, obtaining a set of
univariate polynomials. 
\item[(2)] Factorize the univariate polynomials,
obtaining square-free and relatively prime factors. 
\item[(3)] Isolate the
real roots of the univariate polynomials. 
\end{itemize}
The step (2) is often most
time-consuming. But in generic quantifier elimination, we can drop
this step since:
\begin{itemize}[noitemsep]
\item During the projection phase, the projection polynomials are
already made to be square-free and relatively prime. (If not, their
projection polynomials will identically vanish and provide no information).
\item Since the leading coefficients of the polynomials do not vanish on
the sample point, the evaluated univariate polynomials will also be
square-free and relatively prime.
\end{itemize}

\vskip 0.5cm
\noindent
{\bf Idea 4: Sturm sequence for free}.
Again utilizing the fact that the leading coefficients of the
polynomials do not vanish on the sample point, we can obtain a Sturm
sequences for them almost for free if we have computed the projection
polynomials by subresultant sequence or Sturm-Habicht sequence\cite{Gonz89}.
This is because the sequences have a good specialization property
when the leading coefficients do not vanish. Thus the obtained Sturm
sequence can be used for real root isolation.

Let us make this idea more precise for a subresultant sequence.
We first need some notation and notions.
\begin{itemize}[noitemsep]
\item Let $\mathbb{D}$ denote an integral domain. When necessary, it will
also be assumed that $\mathbb{D}$ is ordered.
\item Let $A_1,A_2\in\mathbb{D}[x]$ be such that deg$(A_1)=n_1$,
deg$(A_2)=n_2$, and $n_1 \ge n_2 \ge 0$.
\item A {\sl polynomial remainder sequence} (prs) of $A_1$ and $A_2$ is a
sequence $A_1,\ldots,A_r$ satisfying
\[e_iA_i = Q_iA_{i+1}+f_iA_{i+2}\]
where $A_i,Q_i\in\mathbb{D}[x]$, $e_i,f_i\in\mathbb{D}$,
${\rm deg}(A_{i+1}) > {\rm deg}(A_{i+2})$,
$A_{r+1}=0$ and $e_if_i\ne 0$.
\item A {\sl negative} prs of $A_1$ and $A_2$ is a prs such that
$e_if_i < 0$ for every $i$
\item A {\sl Sturm sequence} of $A\in\mathbb{D}[x]$ is a negative prs
of $A$ and $A^\prime$.
\item The $k$-th {\sl Sylvester matrix} of $A_1$ and $A_2$, $0 \le k \le n_2$,
is the matrix whose rows are the coefficients of the polynomials:
\[x^{n_2-1-k}A_1,\ldots,A_1,x^{n_1-1-k}A_2,\ldots,A_2\]
written in the basis:
$x^{n_1+n_2-1-k},\ldots,1$
\item The $k$-th {\sl subresultant} of $A_1$ and $A_2$, $0 \le k \le n_2$,
written as $S_k$, is defined by
\[S_k=\sum_{i=0}^k {\rm det}(M_k^{(i)})x^i\]
wher $M_k^{(i)}$ is the submatrix of the $k$-th Sylviester matrix of
$A_1$ and $A_2$ obtained by taking the first
$n_1+n_2-1-2k$ columns and the $(n_1+n_2-k-i)$-th column.
\item The sequence $A_1,A_2,S_{n_2-1},S_{n_3-1},\ldots,S_{n_{r-1}-1}$
is called the {\sl first kind subresultant} prs of $A_1$ and $A_2$
$\blacksquare$
\end{itemize}
Let $\phi$ stand for an evaluation homomorphism that preserves the
degree (that is, the leading coefficients do not vanish under it).
Let $A_1,A_2\in\mathbb{D}[y]$. The following theorem shows how to
obtain the first kind subresultant prs of $\phi(A_1)$ and $\phi(A_2)$
from that of $A_1$ and $A_2$.

\vskip 0.5cm
\noindent
{\bf Theorem 7 (Homomorphism on first kind)}
{\sl Let $A_1,A_2,\ldots,A_r$ be the first kind subresultant {\rm prs}
of $A_1$ and $A_2$. Let $\tilde{A}_1,\ldots,\tilde{A}_{\tilde{r}}$
be the sequence obtained from $\phi(A_1),\ldots,\phi(A_r)$ by deleting
the vanishing ones, and deleting the second one in case there are two of them
with the same degree. (At most two can be of the same degree.) More
specifically if ${\rm deg}(\phi(A_k)) = {\rm deg}(\phi(A_j))$ for
$i < j$, then we delete $\phi(A_j)$. Then the sequence is the first kind
subresultant {\rm prs} of $\phi(A_1)$ and $\phi(A_2)$\quad$\blacksquare$}

\vskip 0.5cm
Let $\tilde{A_1},\tilde{A_2},\ldots,\tilde{A}_r$ be the first kind
subresultant prs of $\tilde{A}$ and $\tilde{A}^\prime$, obtained as
described above. Let $\tilde{n}_i={\rm deg}(\tilde{A}_i)$,
$\delta_i=\tilde{n}_i-\tilde{n}_{i+1}$, and $\tilde{c}_i$ be the
leading coefficient of $\tilde{A}_i$. Then the following theorem shows
how to obtain a Sturm sequence of $\tilde{A}$ from the first kind
resultant prs of $\tilde{A}$ and $\tilde{A}^\prime$.

\vskip 0.5cm
\noindent
{\bf Theorem 8 (Sturm sequence from sres {\sl prs})} {\sl Let}
\[\begin{array}{rcl}
\lambda_i & = & {\rm the\ sign\ of\ }\tilde{c}_i{\rm\ for\ }
2 \le i \le r-1\\
\mu_i & = & \lambda_{i+1}^{\delta_i+1}{\rm\ for\ }
1 \le i \le r-2\\
\rho_1 & = & 1\\
\rho_i & = & \lambda_i^{\delta_{i-1}}\rho_{i-1}^{\delta_{i-1}-1}
{\rm\ for\ }2 \le i \le r-2\\
\nu_1 & = & 1\\
\nu_i & = & -\lambda_i(-\rho_i)^{\delta_i}
{\rm\ for\ }2 \le i \le r-2\\
\sigma_1 & = & 1\\
\sigma_2 & = & 1\\
\sigma_i & = & -\mu_{i-2}\nu_{i-2}\sigma_{i-2}{\rm\ for\ } 3 \le i \le r\\
\tilde{A}_i & = & \sigma_i\tilde{A}_i{\rm\ for\ }1 \le i \le r
\end{array}\]
{\sl Then $\tilde{A}_1,\ldots,\tilde{A}_r$ is a Sturm sequence of}
$\tilde{A}$\quad$\blacksquare$

Now we present an algorithm for generic quantifier elimination, based
on the above ideas. The general structure of the algorithm is based on
that of the partial cylindrical algebraic decomposition
\cite{Hong90a}\cite{Coll91}. We use some standard terminology from there.

\vskip 0.5cm
\noindent
{\bf Algorithm 4 (Generic Quantifier Elimination)}

\vskip 0.5cm
\noindent
\begin{tabular}{ll}
{\sl Input}: & A formula $F^\prime = (Q_{f+1}x_{f+1})\cdots
(Q_rx_r)F(x_1,\ldots,x_r)$\\
& where $Q_i$ is either $\dot{\forall}$
or $\dot{\exists}$ and $F$ is a quantifier free formula.\\
{\sl Output}: & A quantifier-free formula $G$ which is generically
equivalent to $F$.
\end{tabular}

\begin{itemize}[noitemsep]
\item[(1)] [Projection]\\
Factorize the polynomials occurring in the formula $M$.\\
For $k=1,\ldots,r$, let $P_k$ be the set of the $i$-level factors.\\
For $k=r,\ldots,2$ do
\begin{itemize}[noitemsep]
\item[(a)] $J \longleftarrow{\rm GProj}(P_k)$.
{\sl Use the first kind subresultant polynomial remainder sequence to
compute} psc's
\item[(b)] Factorize the polynomials in $J$
\item[(c)] For $i=1,\ldots,k-1$, append the $i$-level factors to $P_i$
if they are not already there.
\end{itemize}
\item[(2)] [Stack Construction]\\
Initialize the tree as a single node whose truth is ? and whose sample
point is ().\\
While there is a candidate node do
\begin{itemize}[noitemsep]
\item[(a)] Choose a candidate node $c$. Let $k$ be the level of the node
and let $s=(s_1,\ldots,s_k)$ be its sample point
\item[(b)] Evaluate the polynomials in $P_{k+1}$ on $s$ obtaining
univariate polynomials
\item[(c)] Obtain Sturm sequences of the polynomials of the univariate
polynomials by specializing the first kind subresultant polynomials
remainder sequences computed during projection.
\item[(d)] Using the Sturm sequences, find rational numbers
$t_1,\ldots,t_\ell$ such that\\
$t_1 < \alpha_1 < t_2 < \alpha_2 < \cdots < \alpha_{\ell-1} < t_\ell$ 
where $\alpha_1,\ldots,\alpha_{\ell-1}$ are
the real roots of the univariate polynomials
\item[(e)] Attach $\ell$ nodes, say, $c_1,\ldots,c_\ell$, to the node $c$
where the sample point of $c_i$ is set to be $(s_1,\ldots,s_k,t_i)$
\item[(f)] For each child $c_i$, determine the signs of the polynomials
in $P_{k+1}$ on it
\item[(g)] For each child $c_i$, try to determine its truth by using the
signs
\item[(h)] Determine the truth values of $c$ if possible, and if so, the
truth values of as many ancestors of $c$ as possible, removing from the
tree the subtree of each cell whose truth value is thus determined
\end{itemize}
\item[(3)] [Solution Formula Construction]\\
By using the method described in \cite{Hong98},
construct a quantifier free formula $G$ such that it is true on all the
true cells and false on all the false cells.\quad$\blacksquare$
\end{itemize}

\subsection{Volume Approximate Quantifier Elimination}

\noindent

Hong introduced the notion of volume approximate quantifier
elimination and a method for it. Let us first clarify what is meant by
``volume approximate'' quantifier elimination. It was further improved
by Andreas Neubacher and completed by Stefan Ratschan\cite{Rats02}.

\subsubsection{Problem}

We define the volume approximate quantifier elimination problem as
follows. Devise an algorithm for

\noindent
\begin{tabular}{ll}
{\bf In:} & $F$, a formula\\
& $\epsilon$, a positive real number\\
{\bf Out:} & $N$, a quantifier-free formula necessary to $F$\\
& $S$, a quantifier-free formula sufficient to $S$ such that
$V(N)-V(S)\le \epsilon$
\end{tabular}

\noindent
where $V(G)$ stands for the volume of the set defined by the formula
$G$. Usually, $N$ and $S$ are required to belong to a small subset of the
full language. For instance, they are required to be a disjunctive
normal form of linear inequalities, that is, a collection of convex
polytopes. Or one can put even stronger restriction on them to be a
disjunctive normal form of ``de-coupled'' linear inequalities, that is,
a collection of boxes aligned with the coordinate axis. Throughout
this section, we will restrict our discussion to the case when $N$ and $S$
are required to be a collection of closed boxes. Further, we will
require that the boxes do not overlap. We will call such a collection
a {\sl box-set}.

Note that a box can viewed as a set of points or a formula that
defines the set. We will use these two views interchangeably. Thus,
sometimes we will speak of ``union'' of boxes and some other times we
will speak of ``disjunction'' of boxes. Obviously these are the same
operations.

The boxes that we will use are closed ones. This causes some
complications in defining the notion of ``disjointness''. We will not go
through the technical details in this tutorial. Intuitively, we will
interpret that two boxes are disjoint if their intersection is either
empty or belongs to the boundaries.

\subsubsection{Method}

\noindent
One natural idea is to approximate each atomic formula by two
box-sets: one as a necessary condition, the other as a sufficient
condition. Then carry out the logical operations (disjunction,
quantification, etc) on the box-sets. These logical operations are
much easier, though not trivial, to do on box-sets than on arbitrary
formulas. Also they can be done {\sl exactly}.

A question naturally arises: How accurately should we approximate each
atomic formula? One can take an ``eager'' approach. That is to carry out
an {\sl error-analysis} to estimate a lower bound on the accuracy that will
ensure that, after the logical operations on the box-sets, the volume
difference is smaller than $\epsilon$. The advantage of this scheme is
that once the error-analysis is done, we need to carry out the
approximation of atomic formulas and the logical operations on them
only once. But there are two serious disadvantages/difficulties.
\begin{itemize}
\item The error-analysis expects and prepares for the ``worst-case''
situation. Thus, in average, it can induce much more work than
actually needed.
\item A reasonable error-analysis is very difficult to do for
quantifications. A natural (geometric) method gives a totally useless
bound (too big). One can give better analysis using a root-separation
theorem, gap theorem, etc. But such analysis is computationally too
costly.
\end{itemize}

\noindent 
A better way is to take the so-called ``lazy''
approach. This means to compute only when necessity arises.  In that
sense it is ``demand-driven''. Thus, at first, we carry out a very
``coarse'' box-approximation of atomic formulas. If, after logical
operations on the boxes, the volume difference is already less than
$\epsilon$, then we can stop. If not, we can refine the initial
box-approximation of atomic formulas and iterate the same process
until the volume difference is smaller than $\epsilon$.


Further, during the execution of the method, we would like to utilize
any intermediate information as soon as they become available, in
particular for pruning/reducing the ``search-space''. As an example,
consider the formula $F\equiv F_1\land F_2$. Now we would like to approximate
it by a necessary box-set $N$ and a sufficient box-set $S$. One elegant
way is to approximate $F_1$ and $F_2$ independently and and carry out the
conjunction on the resulting box-sets $N_1,S_1$ and $N_2,S_2$. But
here, the result of the approximation of $F_1$ is not used during the
approximation of $F_2$. This can cause useless work. For instance, it
might the case that $N_1$ is empty (that is false). Then obviously the
result $N$ is also false, no matter what $N_2$ is. Thus a better way is to
use $N_1$ and $S_1$ during the approximation of $F_2$.

Now we turn the ideas discussed above into algorithms. We will use the
following notations:
\begin{itemize}
\item[$+$] {\sl disjoint disjunction} (or {\sl sum}). $B_1+B_2$ is the
same as $B_1 \lor B_2$ except that it also states that $B_1$ and $B_2$
are disjoint. Theus, this operation can be trivially done by collecting
all the boxes in the two box-sets
\item[$\times$] {\sl independent conjunction} (or {\sl product}).
$B_1\times B_2$ is the same as $B_1 \land B_2$ except that it also states
that $B_1$ and $B_2$ are independent (they do not share variables). Thus,
it is also trivial to carry out.
\end{itemize}

A technical remark. Though in the end, we would like to obtain a
necessary box-set and a sufficient box, it is more
convenient/efficient to keep track of ``yes box-set'', ``unknown box
set'', ``no box-set'' where the ``yes box-set'' is inside the solution set
of $F$, the ``no box-set'' is outside the solution set of $F$, the
``unknown box-set'' is not known yet whether it is inside or
outside. These will be denoted as $B_y$, $B_u$, $B_n$ respectively.

\vskip 0.5cm
\noindent
{\bf Algorithm 5}

\vskip -0.2cm
\noindent
\hrulefill
\begin{center}
$(N,S) \longleftarrow VolumeApproxQE(F)$
\end{center}

\noindent
\begin{tabular}{ll}
{\sl Input}: & $F$ is a formula\\
{\sl Output}: & $N$ is a necessary condition of $F$ and $S$ is a sufficient
condition of $F$\\
& such that $V(N)-V(S)\le \epsilon$.
\end{tabular}

\begin{itemize}
\item[(1)] $B_y \longleftarrow$ the empty set (which represents the
logical false)\\
$B_n \longleftarrow$ the empty set\\
$B_u \longleftarrow$ the whole free variable space
\item[(2)] While $V(B_u) > \epsilon$ repeat\\
$(B_{uy},B_{uu},B_{un}) \longleftarrow ApproxFormula(F,B_u)$\\
$B_y \longleftarrow B_y+B_{uy}$\\
$B_u \longleftarrow B_n+B_{un}$\\
$B_u \longleftarrow B_{uu}$
\item[(3)] $N \longleftarrow B_y+B_u$\\
$S \longleftarrow B_y$ \quad $\blacksquare$
\end{itemize}
\vskip -0.4cm
\noindent
\hrulefill

Now we describe the sub-algorithm {\sl ApproxFormula}. We will use the
following notational convention: $B_{pq}$ where $p$, $q$ can be one of
$y$, $n$, $u$, $*$, which respectively stand for yes, no, unknown, any
of them. The first index $p$ tells us the status of the box with respect
to a formula $F_1$ and the second index $q$ to $F_2$ . For instance $B_{yu}$
means that the box-set is a yes box-set with respect to $F_1$ and a
unknown box-set with respect to $F_2$.

\vskip 0.5cm
\noindent
{\bf Algorithm 6}

\vskip -0.2cm
\noindent
\hrulefill
\begin{center}
$(B_y,B_u,B_n) \longleftarrow ApproxFormula(F,B)$
\end{center}

\noindent
\begin{tabular}{ll}
{\sl Input}: & $F$ is a formula and $B$ is a box-set\\
{\sl Output}: & $B_y$, $B_u$, $B_n$ are box-sets that partition $B$ such
that $B_y \Longrightarrow F$ and $B_n \Longrightarrow \lnot F$
\end{tabular}

\begin{itemize}
\item[(1)] $F$ is an atomic formula\\
$(B_y,B_u,B_n) \longleftarrow ApproxAtomic(F,B)$
\item[(2)] $F \equiv F_1 \land F_2$\\
$(B_{y*},B_{u*},B_{n*}) \longleftarrow ApproxFormula(F_1,B)$\\
$(B_{yy},B_{yu},B_{yn}) \longleftarrow ApproxFormula(F_2,B_{y*})$\\
$(B_{uy},B_{uu},B_{un}) \longleftarrow ApproxFormula(F_2,B_{u*})$\\
$B_y \longleftarrow B_{yy}$\\
$B_u \longleftarrow B_{yu}+B_{uy}+B_{uu}$\\
$B_n \longleftarrow B_{n*}+B_{yn}+B_{un}$\\
\item[(3)] $F \equiv F_1 \lor F_2$\\
$(B_{y*},B_{u*},B_{n*}) \longleftarrow ApproxFormula(F_1,B)$\\
$(B_{ny},B_{nu},B_{nn}) \longleftarrow ApproxFormula(F_2,B_{n*})$\\
$(B_{uy},B_{uu},B_{un}) \longleftarrow ApproxFormula(F_2,B_{u*})$\\
$B_y \longleftarrow B_{y*}+B_{ny}+B_{uy}$\\
$B_u \longleftarrow B_{nu}+B_{un}+B_{uu}$\\
$B_n \longleftarrow B_{nn}$
\item[(4)] $F \equiv \lnot F$\\
$(B_{y*},B_{u*},B_{n*}) \longleftarrow ApproxFormula(F_1,B)$\\
$B_y \longleftarrow B_{n*}$\\
$B_u \longleftarrow B_{u*}$\\
$B_n \longleftarrow B_{y*}$
\item[(5)] $F \equiv F_1 \Longrightarrow F_2$\\
$(B_{y*},B_{u*},B_{n*}) \longleftarrow ApproxFormula(F_1,B)$\\
$(B_{yy},B_{yu},B_{yn}) \longleftarrow ApproxFormula(F_2,B_{y*})$\\
$(B_{uy},B_{uu},B_{un}) \longleftarrow ApproxFormula(F_2,B_{u*})$\\
$B_y \longleftarrow B_{n*}+B_{yy}+B_{uy}$\\
$B_u \longleftarrow B_{yu}+B_{un}+B_{uu}$\\
$B_n \longleftarrow B_{yn}$
\item[(6)] $F \equiv F_1 \Longleftrightarrow F_2$\\
$(B_{y*},B_{u*},B_{n*}) \longleftarrow ApproxFormula(F_1,B)$\\
$(B_{yy},B_{yu},B_{yn}) \longleftarrow ApproxFormula(F_2,B_{y*})$\\
$(B_{ny},B_{nu},B_{nn}) \longleftarrow ApproxFormula(F_2,B_{n*})$\\
$B_y \longleftarrow B_{yy}+B_{nn}$\\
$B_u \longleftarrow B_{u*}+B_{yu}+B_{nu}$\\
$B_n \longleftarrow B_{yn}+B_{ny}$
\item[(7)] $F \equiv \forall x F_1$ where $x$ is a vector of $n$ variables\\
$(B_{y*},B_{u*},B_{n*}) \longleftarrow 
ApproxFormula(F_1,B\times\mathbb{R}^n)$\\
$B_y \longleftarrow \forall x B_{y*}$\\
$B_n \longleftarrow \exists x B_{n*}$\\
$B_u \longleftarrow B-(B_y+B_n)$
\item[(8)] $F \equiv \exists x F_1$ where $x$ is a vector of $n$ variables\\
$(B_{y*},B_{u*},B_{n*}) \longleftarrow 
ApproxFormula(F_1,B\times\mathbb{R}^n)$\\
$B_y \longleftarrow \exists x B_{y*}$\\
$B_n \longleftarrow \forall x B_{n*}$\\
$B_u \longleftarrow B-(B_y+B_n)$
\end{itemize}
\vskip -0.4cm
\noindent
\hrulefill

\noindent
In the cases (7) and (8), the notation ``$B\times\mathbb{R}^n$'' means to
embed each box in $B$ into the $m+n$ dimensional space by letting the $n$
new variables to range over $(-\infty,\infty)$.

Correctness of this algorithm is not difficult to prove and is left to the
reader. Now we give brief and informal descriptions of the sub-algorithms:

\noindent
\begin{tabular}{ll}
$\bullet ApproxAtomic$ & approximation of atomic formula\\
$\bullet -$ & subtraction of box-sets\\
$\bullet \lor$ & disjunciton of box-sets\\
$\bullet \land$ & conjuncton of box-sets\\
$\bullet \exists$ & existential quantification of box-sets\\
$\bullet \forall$ & universal quantification of box-sets\\
\end{tabular}

\noindent
In order to keep the presentation of the ideas simple, we will
intentionally avoid optimizations, except straightforward ones. But
currently various optimizations, improvements and different methods
are being researched.

\vskip 0.5cm
\noindent
{\bf Approximating Atomic Formula}:
Consider an atomic formula $P > 0$ and a box-set $B$. Without losing
generality let us assume that $B$ consists of just one box. If not, we
can apply the method described below to each box in the box-set (or
just one box from it). Our goal is to partition $B$ into three box-sets
$B_y$, $B_u$, $B_n$ such that every point of $B_y$ satisfies $P > 0$ and no
point of $B_n$ satisfies $P > 0$.

We first check if the whole $B$ satisfies the atomic formula. This can
be done by computing an interval $I$ that bounds the range of the
polynomial $P$ on $B$, that is, $P(B) \subseteq I$. The range estimation can
be done by using various methods from interval mathematics. If $I > 0$,
then we can set $B_y$ to be $B$ and the others empty. If $I \le 0$, then we
can set $B_n$ to be $B$ and the other empty. Otherwise, we split the box $B$
into several boxes and do the same on each box.

But this method can be very inefficient because in general it will
result in numerous small boxes (exponential in the number of
variables). A better approach is to adapt the {\sl tightening} method of
Hong\cite{Hong94}. The method roughly works as follows. Let 
$P\in\mathbb{R}[x_1,\ldots,x_r]$
and $B$ is a box in the $r$-dimensional space. We choose a variable, say
$x_k$. We view $P$ as a univariate polynomial in $x_k$ . Then the
coefficients are polynomials in the other variables. We, using
interval methods, compute intervals that bound the coefficients on the
box\\ 
\[B_1\times\ldots\times B_{k-1}\times B_{k+1}\times\ldots\times B_r\]
As a result, we obtain
a univariate polynomial in $x_k$ with interval coefficients. Next, we
compute the ``root intervals'' of the polynomial, say $R_1,\ldots,R_\ell$.
Let $C_1,\ldots,C_u$ be the intervals that form the complement of
the root intervals with respect to $B_k$. Let $B^i$ be the box:
\[B_1\times\ldots\times B_{k-1}\times C_i\times B_{k+1}\times\ldots\times B_r\]
Then the sign of $P$ is constant
throughout each $B^i$, and the sign is either positive or negative. If
$P$ is positive on $B^i$, then we put $B^i$ into the box-set $B_y$, and if
negative, into the box-set $B_n$. We put the remaining parts of the box
$B$ into $B_u$. Specifically we put into $B_u$ the boxes 
\[B_1\times\ldots\times B_{k-1}\times R_i\times B_{k+1}\times\ldots\times B_r\]

\vskip 0.5cm
\noindent
{\bf Subtraction of Box-sets}:
Let $B$ and $C$ be two box-sets. We would like to compute a box-set $D$
such that $D \Longleftrightarrow B-C$, that is, $B \land \lnot C$.

If $B$ is empty then $D$ is empty. If $C$ is empty then $D=B$. Thus assume
that $B$ is not empty and $C$ is not empty. Let $B=B^1+B^*$ and
$C=C^1+C^*$. Observe:
\[\begin{array}{rcl}
B-C & \Longleftrightarrow & (B^1 + B^*) - C\\
& \Longleftrightarrow & (B^1 - C) + (B^*-C)\\
& \Longleftrightarrow & (B^1 - (C^1+C^*)) + (B^*-C)\\
& \Longleftrightarrow & ((B^1-C^1)-C^*)+(B^*-C)
\end{array}\]
Thus, the subtraction between two box-sets is reduced to the subtraction
between two boxes: $B^1-C^1$.

So let us now find out how to do subtraction between two boxes. Let $B$
consist of only one box and $C$ also consist of only one box.
Let $B=B_1\times B_*$ and let $C=C_1\times C_*$. Observe:
\[\begin{array}{rcl}
B-C & \Longleftrightarrow & (B_1\times B_*) - (C_1\times C_*)\\
& \Longleftrightarrow & (B_1\times B_*) \land \lnot (C_1\times C_*)\\
& \Longleftrightarrow & (B_1\times B_*) \land (\lnot C_1 \lor \lnot C_*)\\
& \Longleftrightarrow & (B_1\times B_*) \land 
(\lnot C_1 + (C_1 \land \lnot C_*))\\
& \Longleftrightarrow & ((B_1 \land \lnot C_1)\times B_*) +
((B_1 \land C_1) \times (B_* \land \lnot C_*))\\
& \Longleftrightarrow & ((B_1-C_1)\times B_*)+
((B_1 \land C_1)\times (B_* -C_*))
\end{array}\]
Thus the subtraction between two boxes is reduced to the subtraction
of two intervals $(B_1 - C_1)$ and the conjunction of two intervals 
$(B_1 \land C_1)$. These are trivial to do.

\vskip 0.5cm
\noindent
{\bf Disjunction of Box-sets}: Let $B$ and $C$ be two box-sets. We would
like to compute a box-set such that $D \Longleftrightarrow B \lor C$.
We can immediately reduce this to the subtraction problem as:
\[B \lor C \Longleftrightarrow B + (C - B)\]

\vskip 0.5cm
\noindent
{\bf Conjunction of Box-sets} Let $B$ and $C$ be two box-sets. We would
like to compute a box-set $D$ such that $D \Longleftrightarrow B \land C$.
Let $B=B^1+\cdots+B^\mu$ and let $C=C^1+\cdots+C^\nu$. We have
\[\begin{array}{rcl}
B \land C & \Longleftrightarrow & 
(+_{i=1}^\mu B^i) \land (+_{j=1}^\nu C^j)\\
& \Longleftrightarrow & 
+_{i=1}^\mu +_{j=1}^\nu (B^i \land C^j)\\
& \Longleftrightarrow & 
+_{i=1}^\mu +_{j=1}^\nu
(\times_{k=1}^n B_k^i \land \times_{k=1}^n C_k^j)\\
& \Longleftrightarrow & 
+_{i=1}^\mu +_{j=1}^\nu \times_{k=1}^n (B_k^i \land C_k^j)\\
\end{array}\]
Thus the conjunction of box-sets is reduced to the conjuntions of
intervals, which are trivial to do.

\vskip 0.5cm
\noindent
{\bf Existential Quantifier Elimination on Box-sets}:
Consider $F \equiv \exists x B$ where $x$ is a vector of $n$ variables
and $B$ is a box-set in the $\mathbb{R}^{m+n}$ dimensional space. First
note that $B$ is a disjunction of boxes, say $B^1,\ldots,B^\ell$.
Since the disjunction commutes with the existential quantification, we have
\[F \Longleftrightarrow \bigvee_{i=1}^\ell \exists x B^i\]
Now let $B^i=B_1^i \land\cdots\land B_{m+n}^i$ where $B_j^i$ is an interval in
the $x_j$-line. Let $\underline{B^i}$ %be the conjunction of the $m$
intervals corresponding to the free variables and $\overline{B^i}$ be the
conjunction of the $n$ intervals corresponding to the bound variables.
Note that
\[\exists x B^i \Longleftrightarrow \exists x\underline{B^i}\land\overline{B^i}
\Longleftrightarrow \underline{B^i} \land \exists x \overline{B^i}
\Longleftrightarrow \underline{B^i}\]
So we trivially have
\[F \Longleftrightarrow \bigvee_{i=1}^\ell \underline{B^i}\]
Thus the problem is reduced to the disjunction problem.

\vskip 0.5cm
\noindent
{\bf Universal Quantifier Elimination on Box-sets}:
Consider $F \equiv \forall x B$. 
One way is to turn this into existential problem
through double negation. But there is a better way. Let $B^1$ be a box
in the box-set $B$ and let $B^*$ be the box-set consisting of the
remaining boxes. Now observe:
\[\begin{array}{rcl}
F & \equiv & \forall x B\\
& \Longleftrightarrow & \forall x [B^1 \lor B^*]\\
& \Longleftrightarrow & \forall x [(\underline{B^1}\land\overline{B^1})
\lor B^*]\\
& \Longleftrightarrow & \forall x [(\underline{B^1} \lor B^*) \land
(\overline{B^1} \lor B^*)]\\
& \Longleftrightarrow & \forall x [\underline{B^1} \lor B^*] \land
\forall x [\overline{B^1} \lor B^*]\\
& \Longleftrightarrow & (\underline{B^1} \lor \forall x B^*) \land
\forall x [\overline{B^1} \lor B^*]\\
& \Longleftrightarrow & (\underline{B^1} \land 
\forall x [\overline{B^1} \lor B^*]) \lor 
(\forall x B^* \land \forall x [\overline{B^1} \lor B^*])\\
& \Longleftrightarrow & (\underline{B^1} \land 
\forall x [\overline{B^1} \lor B^*]) \lor \forall x B^*
\end{array}\]
Note that we have divided the formula $F$ into two smaller
formulas. This suggests a recursive-divide-conquer algorithm. For this
goal, let us rewrite the above observation in a slightly more general
way (so that we can do recursion):
\[\begin{array}{rl}
& L \land \forall x [U \lor B]\\
\Longleftrightarrow & (L \land \underline{B^1} \land
\forall x [U \lor \overline{B^1} \lor B^*]) \lor
(L \land \forall x [U \lor B^*])
\end{array}\]
where $L$ is a box in the free variable space. and $U$ is a box-set in the
bound variable space. We rewrite it again to make the recursion even
more explicit:
\[\begin{array}{rl}
& L \land \forall x [U \lor B]\\
\Longleftrightarrow & L^* \land \forall x [U^* \lor B^*] \quad\lor\quad
L \land \forall x [U \lor B^*]
\end{array}\]
where $L^* \equiv L \land \underline{B^1}$ and 
$U^* \equiv U \lor \overline{B^1}$.
The recursion terminates since the number of boxes in $B$ decreases.

Now we have the following obvious recursive algorithm. For efficiency,
we do some easy check in the step (1) in order to detect trivial cases
for which the recursion is not necessary.

\vskip 0.5cm
\noindent
{\bf Algorithm 7}

\vskip -0.2cm
\noindent
\hrulefill
\begin{center}
$D \longleftarrow UnivQE(x,B,L,U)$
\end{center}

\noindent
\begin{tabular}{ll}
{\sl Input}: & $x$ is the vector of variables.\\
& $B$ is a box-set.\\
& $L$ is a box in the free variable space.\\
& $U$ is a box-set in the bound variable space.\\
{\sl Output}: & $D$ is a box-set in the free variable space such that
$D \Longleftrightarrow L \land \forall x [U \lor B]$
\end{tabular}

\begin{itemize}
\item[(1)] [Prune/Recursion base]\\
If $L$ is empty then set $D$ to be empty and return\\
If $U$ is the whole bound variable space then set $D$ to be $L$ and return\\
If $B$ is empty then set $D$ to be empty and return
\item[(2)] [Recursion]\\
Choose a box $B^1$ from $B$\\
Let $B^*$ be the box-set of the remaining boxes.\\
$L^* \longleftarrow L \land \underline{B^1}$\\
$U^* \longleftarrow U \lor \overline{B^1}$\\
$D \longleftarrow UnivQE(x,B^*,L^*,U^*) \lor UnivQE(x,B^*,L,U)
\quad\blacksquare$
\end{itemize}
\vskip -0.4cm
\noindent
\hrulefill

On the top-level, this algorithm is called with $L$ being the whole
free-variable space and $U$ being empty, so that the output is a box-set
equivalent to $\forall x B$. Some optimization can be done.
\begin{itemize}
\item First, the line in Step (2)
\[U^* \longleftarrow U \lor \overline{B^1}\]
can be replaced by the easier operation
\[U^* \longleftarrow U + \overline{B^1}\]
even though $U$ and $\overline{B^1}$ might overlap.\\
The reason is as follows. Whenever $U$ and $\overline{B^1}$ overlap,
$L$ and $\underline{B^1}$ are disjoint (making $L^*$ empty), and
thus, the recursive call will not use $U^*$. So, it is safe to carry
out the ``illegal'' operation.
\item Next, the line in Step (2)
\[D \longleftarrow UnivQE(x,B^*,L^*,U^*) \lor UnivQE(x,B^*,L,U)\]
can be replaced by the easier one
\[D \longleftarrow UnivQE(x,B^*,L^*,U^*) + UnivQE(x,B^*,L,U)\]
It is because the outputs of the two calls to {\sl UnivQE} are disjoint, as
we show now. When $U$ and $\overline{B^1}$ overlap, 
$L$ and $\underline{B^1}$ are disjoint, and thus
the output of the first call is empty. So the disjunction can be
trivially replaced by $+$. Thus, now assume that $U$ and $\overline{B^1}$ 
do not overlap. Let us assume the two outputs overlap, then there exists a
point, say $v$, which satisfies both outputs. 
Then, $v\in \underline{B^1}$. Let $w\in \overline{B^1}$.
Then $(v,w)\in B^1$. Since $B^1$ is disjoint from $U$ and $B^*$, $(v,w)$
is not in $U\lor B^*$. This contradicts the fact that $\forall x[U\lor B^*]$
is true on $v$. Thus, the two outputs do not overlap.

\end{itemize}

\subsubsection{Termination}

Does the algorithm ({\sl VolumeApproxQE}) terminate always? The answer is
{\sl almost always}. Now we will discuss a few causes of non-termination and
solutions for them.

\vskip 0.5cm
\noindent
{\bf Non-termination 1}

Assume that the solution set of a formula 
$F(x,y)$ is $\{(x,y)|0 \le x-y \le 1\}$
By plotting the solution set and trying to approximate it with
a finite number of boxes, one will discover that there is no way to
make the volume-error finite. In general, the method might not
terminate if the solution set is un-bounded.

We can avoid such difficulty if we restrict the input to a box-set
(finite boxes). This can be done by modifying the algorithm
{\sl VolumeApproxQE} so that it has one more input argument, say a box-set
$B$, and that it initializes $B_u$ to be $B$. The outputs will be
necessary/sufficient conditions for $F \land B$.

For the same reason, we also restrict the range of the bound
variables. Thus, instead of $\forall x$, we have
$\forall x \in C$ where $C$ is a finite box.

For the same reason, we also restrict the range of the bound
variables. Thus, instead of $\forall x$, we have
$\forall x \in C$ where $C$ is a finite box. Likewise, instead of
$\exists x$, we have $\exists x\in C$. One can easily make
necessary modifications on the algorithms.

\vskip 0.5cm
\noindent
{\bf Non-termination 2} There is one more cause for non-termination.
Consider the formula $F$ and the initial box $B$:
\[\begin{array}{rcl}
F & : & \forall y \in [0,1]~x\ne y\\
B & : & [0,1]
\end{array}\]

By plotting the solution set, one will discover that the
volume-difference will not shrink, no matter how accurately the atomic
formula $x\ne y$ is approximated. Thus, the algorithm will loop
forever. We can avoid such difficulty in several ways.
\begin{itemize}
\item In $\forall x F(x,y)$, 
we restrict that the solution set (in the $x$-space) of $F$
is a closed set for all values of $y$.  Likewise in
$\exists x F(x,y)$, we restrict that the solution set (in the
$x$-space) of $F$ is an open set for all values of $y$.
Then, the algorithm will terminate.
\item Replace the quantifiers with the generic quantifiers (as defined the
section on Generic Quantifier Elim- ination) and use minimal root
separation theorems to decide when to abort looping.
\item Replace the quantifiers with the measure-quantifier $\mathcal{M}^p$,
defined as the sentence
\[\mathcal{M}^p x \in C~F(x)\]
means that $V(F)/V(C)\ge p$. If $0 < p < 1$, then the algorithm terminates.
\item Use locally more powerful methods such as: resultants, quadratic
forms, Krawszyk's operator, etc
\end{itemize}
Among them, the method of the measure-quantifier seems to be the most
useful in practice.

\chapter{Potential Future Algebra}
{\center{\includegraphics[scale=0.70]{ps/v101toe.eps}}}
\chapter{Groebner Basis by Siddharth Bhat}

Quoting Philip Zucker \cite{Zuck19}, the Groebner basis algorithm is: 
\begin{quote}
The algorithm churns on a set of multivariate polynomials and spits
out a new set that is equivalent in the sense that the new set is
equal to zero if and only if the original set was. However, now (if
you ask for the appropriate term ordering) the polynomials are
organized in such a way that they have an increasing number of
variables in them. So you solve the 1-variables equations (easy), and
substitute into the 2-variable equation. Then that is a 1-variable
equation, which you solve (easy) and then you substitute into the
three variable equation, and so on. It's analogous to gaussian
elimination. 
\end{quote}

Siddharth Bhat \cite{Bhat19}
wrote a couple blog posts on groebner basis which we quote here.

\section{A Grobner Basis example}

Here's a fun little problem, whose only solution I know involves a
fair bit of math and computer algebra.

We are given the grammar for a language $L$:
\begin{verbatim}
   E = T +_mod8 T | T = -_mod8 T
   T = V | V ^ V | V ^ V ^ V
   V = 'a1' | 'a2' | ...
\end{verbatim}

where \verb|+_mod8| is addition modulo 8, \verb|-_mod8| is subtraction
modulo 8, and \verb|^| is XOR.

This language is equipped with the obvious evaluation rules,
corresponding to those of arithmetic. We are guaranteed that during
evaluation, the variables \verb|a_i| will only have values 0 and 1.
Since we have addition, we can perform multiplication by a constant by
repeated addition. So we can perform \verb|3*a| as \verb|a+a+a|.

We are then given the input expression
\begin{verbatim}
   (a0 ^ a1 ^ a2 ^ a3)
\end{verbatim}
We wish t find an equivalent expression in terms of the above language
$L$. 

We think of $E$ as some set of logic gates we are allowed to use, and
we are trying to express the above operation in terms of these gates.

The first idea that I thought was that of employing a grobner basis,
since they essentially embody rewrite rules modulo polynomial
equalities, which is precisely our setting here. 

In this blog post, I'm going to describe what a grobner basis is and
why it's natural to reach for them to solve this problem, the code,
and eventual solution. 

As a spoiler, the solution is:
\begin{verbatim}
   a^b^c^d =
      -a - b + c + 3*d - 3*axorb - axorc
      + axord - bxorc + bxord + 3*cxord
      - 3*axorbxorc - axorbxord
      + axorcxord + bxorcxord
\end{verbatim}

Clearly, this contains only additions/subtractions and multiplication
by a constant.

\subsection{What the hell is Grobner Basis?}

The nutshell is that a grobner basis is a way to construct rewrite
rules which also understand arithmetic (I learnt this viewpoint from
the book ``Term Rewriting and All That''. Fantastic book in
general). Expanding on the nutshell, assuming we have a term rewriting
system:
\begin{verbatim}
  A -> -1*B -- (1)
  C -> B^2  -- (2)
\end{verbatim}
over an alphabet \verb|(A, B, C)|.

Now, given the string \verb|C + AB|, we wish to find out if it can be
rewritten to 0 or not. Let's try to substitute and see what happens.
\begin{verbatim}
   C + AB -2-> B^2 + AB -1-> B^2 + (-1*B)B
\end{verbatim}

At this point, we're stuck! We don't have rewrite rules to allow us to
rewrite \verb|(-1*B)B| into \verb|-B^2|. Indeed, creating such a list
would be infinitely long. But if we are willing to accept that we
somehow have the rewrite rules that correspond to polynomial
arithmetic, where we view \verb|A,B,C| as variables, then we can
rewrite the above string to 0.
\begin{verbatim}
   B^2 + (-1*B)B -> B^2 - B^2 -> 0
\end{verbatim}

A Grobner basis is the algorithmic / mathematical machine that allows
us to perform this kind of sumbstitution.

In this example, this might appear stupid; what is so special? We
simply substituted variables and arrived at 0 by using
arithmetic. What's so complicated about that? To understand why this
is not always so easy, let's consider a pathologicial, specially
constructed example.

\subsection{A complicated example that shattters dreams}

Here's the patheologicial example:
\begin{verbatim}
   A -> 1     -- (1)
   AB -> -B^2 -- (2)
\end{verbatim}

And we consider the string \verb|S = AB + B^2|. If we blindly apply
the first rule, we arrive at
\begin{verbatim}
   S = AB + B^2 -1-> 1B + B^2 = B + B^2 (STUCK)
\end{verbatim}

However, if we apply (2) and then (1)
\begin{verbatim}
   S = AB + B^2 -2-> -B^2 + B^2 -> 0
\end{verbatim}

This tells us that we can't just apply the rewrite rules
willy-nilly. It's sensitive to the order of the rewrites! That is, the
rewrite system is not {\bf confluent}.

The grobner basis is a function from rewrite systems to rewrite
systems. When given a rewrite system $R$, it produces a new rewrite
system $R^\prime$ that is {\bf confluent}. So, we can apply the
rewrite rules of $R^\prime$ in any order, and we are guaranteed that
we will only get a 0 from $R^\prime$ if and only if we could have
gotten a 0 from $R$ for all strings.

We can then go on to phrase this whole rewriting setup in the language
of ideals from ring theory, and that is the language in which it is
most often described. 

Now that we have a handle on what a grobner basis is, let's go on to
solve the original problem.

\subsection{An explanation through a slightly simpler problem}

I'll first demonstrate the idea of how to solve the original problem
by solving a slightly simpler problem:

Rewrite \verb|a^b^c| in terms of \verb|a^b|, \verb|b^c|, \verb|c^a|
and the same \verb|+_mod8| instruction set as the original
problem. The only difference this time is that we do not have 
\begin{verbatim}
   T -> V ^ V ^ V
\end{verbatim}

The idea is to construct the polynomial ring over \verb|Z/8Z|
(integers modulo 8) with variables a, b, c, axorb, bxorc, axorc. Now,
we know that 
\begin{verbatim}
   a^b = a + b - 2ab
\end{verbatim}
So, we setup rewrite rules such that 
\begin{verbatim}
   a + b - 2ab -> axorb
   b + c - 2bc -> bxorc
   c + a - 2ca -> cxora
\end{verbatim}

We construct the polynomial
\begin{verbatim}
   f(a, b, c) = a^b^c
\end{verbatim}
which has been written in terms of addition and multiplication,
defined as
\begin{verbatim}
   f_orig(a, b, c) = 4*a*b*c - 2*a*b - 2*a*c - 2*b*c + a + b + c
\end{verbatim}
We then rewrite \verb|f_orig| with respect to our rewrite
rules. Hopefully, the rewrite rules should give us a clean expression
in terms of one variable and two-variable xor's. There is the danger
that we may have some term such as \verb|a * bxorc|, and we do get
such a term \verb|(2*b*axorc)| in this case, but it does not appear in
the original problem.

Create a ring with variables a, b, c, axorb, bxorc, axorc.
\begin{verbatim}
   R = IntegerModRing(8)['a, b, c, axorb, bxorc, axorc']
   (a, b, c, axorb, bxorc, axorc) = R.gens()
\end{verbatim}

XOR in terms of polynomials
\begin{verbatim}
   def xor2(x, y): return x + y - 2*x*y
\end{verbatim}

Define the ideal which allows us to rewrite 
\verb|xor2(a,b)->axorb|, and so on we also add the relation
\verb|a^2-a=0 => a=0| or \verb|a=1| in case this helps the solver.
\begin{verbatim}
   I = ideal((axorb - xor2(a,b), 
              bxorc - xors(b,c), 
              axorc - xor2(a,c), 
              a*a-a, b*b-b, c*c-c))
\end{verbatim}

The polynomial representing \verb|a^b^c| we wish to reduce
\begin{verbatim}
   f_orig = xor2(a, b, c)
\end{verbatim}

We take the groebner basis of the ring to reduce the polynomial $f$
\begin{verbatim}
   IG = I.groebner_basis()
\end{verbatim}

We reduce \verb|a^b^c| with respect to the groebner basis.
\begin{verbatim}
   f_reduced = f_orig.reduce(IG)
\end{verbatim}

\begin{verbatim}
   print("value of a^b^c:\n\t%s\n\treduced: %s" % (f_orig, f_reduced))
\end{verbatim}

Code to evaluate the function $f$ on all inputs to check correctness.
\begin{verbatim}
   def evalxor2(f)
     for (i, j, k) in [(i, j, k) 
       for i in [0, 1] 
         for j in [0, 1] 
           for k in [0, 1]]:
             ref = i^^j^^k
             eval = f.substitute(a=i, b=j, 
                                 axorb=i^^j, bxorc=j^^k, axorc=i^^k)
             print("%s^%s^%s: ref(%s) =?= f(%s): %s" %
               (i, j, k, ref, eval, ref == eval))
\end{verbatim}

Check original formulation is correct
\begin{verbatim}
   print("evaluating original f for sanity check")
   evalxor2(f_orig)
\end{verbatim}

Check reduced formulation is correct
\begin{verbatim}
   print("evaluating reduced f:")
   evalxor2(f_reduced)
\end{verbatim}

Running the code gives us the redued polynomial
\verb|-2*b*axorc+b+axorc| which unfortunately contains a term that is 
\verb|b*axorc|. So, this approach does not work, and I was informed by
my friend that she is unaware of a solution to this problem (writing
\verb|a^b^c| in terms of smaller xors and sums).

The full code output is:
\begin{verbatim}
   value of a^b^c:
     4*a*b*c - 2*a*b - 2*a*c - 2*b*c + a + b + c
     reduced: -2*b*axorc + b + axorc
   evaluating original f for sanity check:
\end{verbatim}

\begin{verbatim}
   0^0^0: ref(0) =?= f(0): True
   0^0^1: ref(1) =?= f(1): True
   0^1^0: ref(1) =?= f(1): True
   0^1^1: ref(0) =?= f(0): True
   1^0^0: ref(1) =?= f(1): True
   1^0^1: ref(0) =?= f(0): True
   1^1^0: ref(0) =?= f(0): True
   1^1^1: ref(1) =?= f(1): True
\end{verbatim}

\begin{verbatim}
   evaluating reduced f:
   0^0^0: ref(0) =?= f(0): True
   0^0^1: ref(1) =?= f(1): True
   0^1^0: ref(1) =?= f(1): True
   0^1^1: ref(0) =?= f(0): True
   1^0^0: ref(1) =?= f(1): True
   1^0^1: ref(0) =?= f(0): True
   1^1^0: ref(0) =?= f(0): True
   1^1^1: ref(1) =?= f(1): True
\end{verbatim}

That is, both the original polynomial and the reduced polynomial match
the expected results. But the reduced polynomial is not in our
language $L$, since it has a term that is a product of $b$ with
$axorc$.
\begin{verbatim}
   -a - b + c + 3*d - 3*axorb - axorc
   + axord - bxorc + bxord + 3*cxord
   - 3*axorbxorc - axorbxord
   + axorcxord + bxorcxord
\end{verbatim}

which happily has no products between terms! It also passes our sanity
check, so we've now found the answer.

The full output is:
\begin{verbatim}
   value of a^b^c^d:
     4*a*b*c + 4*a*b*d + 4*a*c*d + 4*b*c*d - 2*a*b -2*a*c - 2*b*c
     - 2*a*d - 2*b*d - 2*c*d + a + b + c + d
    reduced:
      -a - b + c + 3*d - 3*axorb - axorc + axord - bxorc + bxord
      + 3*cxord - 3*axorbxorc - axorbxord + axorcxord + bxorcxord
 
\end{verbatim}

\begin{verbatim}
   evaluating original a^b^c^d
   0^0^0^0: ref(0) =?= f(0): True
   0^0^0^1: ref(1) =?= f(1): True
   0^0^1^0: ref(1) =?= f(1): True
   0^0^1^1: ref(0) =?= f(0): True
   0^1^0^0: ref(1) =?= f(1): True
   0^1^0^1: ref(0) =?= f(0): True
   0^1^1^0: ref(0) =?= f(0): True
   0^1^1^1: ref(1) =?= f(1): True
   1^0^0^0: ref(1) =?= f(1): True
   1^0^0^1: ref(0) =?= f(0): True
   1^0^1^0: ref(0) =?= f(0): True
   1^0^1^1: ref(1) =?= f(1): True
   1^1^0^0: ref(0) =?= f(0): True
   1^1^0^1: ref(1) =?= f(1): True
   1^1^1^0: ref(1) =?= f(1): True
   1^1^1^1: ref(0) =?= f(0): True
\end{verbatim}

\begin{verbatim}
   evaluating reduced a^b^c^d
   0^0^0^0: ref(0) =?= f(0): True
   0^0^0^1: ref(1) =?= f(1): True
   0^0^1^0: ref(1) =?= f(1): True
   0^0^1^1: ref(0) =?= f(0): True
   0^1^0^0: ref(1) =?= f(1): True
   0^1^0^1: ref(0) =?= f(0): True
   0^1^1^0: ref(0) =?= f(0): True
   0^1^1^1: ref(1) =?= f(1): True
   1^0^0^0: ref(1) =?= f(1): True
   1^0^0^1: ref(0) =?= f(0): True
   1^0^1^0: ref(0) =?= f(0): True
   1^0^1^1: ref(1) =?= f(1): True
   1^1^0^0: ref(0) =?= f(0): True
   1^1^0^1: ref(1) =?= f(1): True
   1^1^1^0: ref(1) =?= f(1): True
   1^1^1^1: ref(0) =?= f(0): True
\end{verbatim}

Code for \verb|a^b^c^d|
\begin{verbatim}
def xor3(x, y, z): return xor2(x, xor2(y, z))

R = IntegerModRing(8)['a, b, c, d, axorb, axorc, axord,
                       bxorc, bxord, cxord, axorbxorc,
                       axorbxord, axorcxord, bxorcxord']
(a, b, c, d, axorb, axorc, axord, bxorc, bxord, cxord, 
  axorbxorc, axorbxord, axorcxord, bxorcxord) = R.gens()
I = ideal((axorb - xor2(a, b),
           axorc - xor2(a, c),
           axord - xor2(a, d),
           bxorc - xor2(b, c),
           bxord - xor2(b, d),
           cxord - xor2(c, d),
           axorbxorc - xor3(a, b, c),
           axorbxord - xor3(a, b, d),
           axorcxord - xor3(a, c, d),
           bxorcxord - xor3(b, c, d),
           a*a-a,
           b*b-b,
           c*c-c,
           d*d-d))
IG = I.groebner_basis()
f_orig = (xor2(a, xor2(b, xor2(c, d))))
f_reduced = f_orig.reduce(IG)
print("value of a^b^c^d:\n\t%s\n\treduced: %s" % (f_orig, f_reduced))

def evalxor3(f):
  for (i, j, k, l) in [(i, j, k, l)
    for i in [0, 1]
      for j in [0, 1]
        for k in [0, 1]
          for l in [0, 1]]:
    ref = i^^j^^k^^l
    eval = f.substitute(a=i, b=j, c=k, d=l,
                        axorb=i^^j, axorc=i^^k, axord=i^^l,
                        bxorc=j^^k, bxord=j^^l,
                        cxord=k^^l, axorbxorc=i^^j^^k,
                        axorbxord=i^^j^^l, axorcxord=i^^k^^l,
                        bxorcxord=j^^k^^l)
    print("%s^%s^%s^%s: ref(%s) =?= f(%s): %s" %
          (i, j, k, l, ref, eval, ref == eval))

print("evaluating original a^b^c^d")
evalxor3(f_orig)

print("evaluating reduced a^b^c^d")
evalxor3(f_reduced)

\end{verbatim}

\section{Ideals as Rewrite Systems}

\subsection{A motivating example}

The question a Grobner basis allows us to answer is this: can the
polynomial
\[p(x,y)=xy^2+y\]
be factorized in terms of
\[a(x,y)=xy+1,\quad b(x,y)=y^2-1\]
such that
\[p(x,y)=f(x,y)a(x,y)+g(x,y)b(x,y)\]
for some {\sl arbitrary} polynomials
\[f(x,y), g(x,y) \in R[x,y]\]

One might imagine, ``well, I'll divide and see what happens!''. Now,
there are two routes to do down:
\[xy^2+y=y(xy+1)=ya(x,y)+0b(x,y)\]
Well, problem solved?
\[xy^2+y=xy^2-x+x+y=x(y^2-1)+x+y=xb(x,y)+(x+y)\]
Now what? We're stuck, and we can't apply $a(x,y)$.

So, clearly, the {\sl order} in which we perform the factorization /
division starts to matter! Ideally, we want an algorithm which is 
{\sl not sensitive} to the order in which we choose to apply these
changes. $x^2+1$

\subsection{The rewrite rule perspective}

An alternative viewpoint of asking ``can this be factorized'', is to
ask ``can we look at the factorization as a rewrite rule?''. For this
perspective, notice that ``factorizing'' in terms of $xy+1$ is the
same as being able to set $xy=-1$, and then have the polynomial
collapse to zero. For the more algebraic minded, this relates to the
fact that
\[R[x]/p(x)\approx R(\text{roots of p})\]
The intuition behind this is that when we ``divide by $xy+1$'', really
what we are doing is we are setting $xy+1=0$, and then seeing what
remains. But 
\[xy+1=0 \Leftrightarrow xy=-1\].
Thus, we can look at the original question as:

How can we apply the rewrite rules $xy\rightarrow -1$,
$y^2\rightarrow 1$, along with the regular rewrite rules of polynomial
arithmetic to the polynomial $p(x,y)=xy^2+y$, such that we end with
the value 0?

Our two derivations above correspond to the application of the rules:
\[xy+y \xrightarrow{xy=-1} -y+y=0\]
\[xy^2+y \xrightarrow{y^2=1} x+y \not\to \text{stuck!}\]

That is, our rewrite rules are not confluent.

The grobner basis is a mathematical object, which is a 
{\sl confluent set of rewrite rules} for the above problem. That is,
it's a set of polynomials which manage to find the rewrite
\[p(x,y)\overset{*}{\rightarrow} 0\]
regardless of the order in which we apply them. It's also 
{\sl correct}, in that it only rewrites to 0 if the original system
has {\sl some way} to rewrite to 0.

\subsection{Buchberger's algorithm}

We need to identify {\sl critical pairs}, which in this setting are
called S-polynomials.

Let
\[f_i=H(f_i)+R(f_i)\]
\[f_j=H(f_j)+R(f_j)\].
\[m=lcm(H(f_i),H(f_j))\]
and let $m_i$, $m_j$ be monomials such that
\[m_i * H(f_i) = m = m_j * H(f_j)\]

The S-polynomial induced by $f_i$, $f_j$ is defined as
\[S(f_i,f_j)=m_if_i - m_if_j\]


\chapter{Greatest Common Divisor}
Greatest Common Divisor
\chapter{Polynomial Factorization}
Polynomial Factorization


\chapter{Differential Forms}
This is quoted from Wheeler \cite{Whee12}.

\section{From differentials to differential forms}

In a formal sense, we may define differentials as the vector space of
linear mappings from curves to the reals, that is, given a
differential $df$ we may use it to map any curve, C $ \in \mathit{C}$
to a real number simply by integrating:
\[df:C \rightarrow R\]
\[ x = \int_C{df}\]
This suggests a generalization, since we know how to integrate over
surfaces and volumes as well as curves. In higher dimensions we also
have higher order multiple integrals. We now consider the integrands
of arbitrary multiple integrals
\[\int{f(x)}dl,\quad\int\int{f(x)}dS,\quad\int\int\int{f(x)}dV\]
Much of their importance lies in the coordinate invariance of the
resulting integrals.

One of the important properties of integrands is that they can all be
regarded as oriented. If we integrate a line integral along a curve
from $A$ to $B$ we get a number, while if we integrate from $B$ to $A$
we get minus the same number,
\[\int_A^B{f(x)}dl= -\int_B^A{f(x)}dl\]
We can also demand oriented surface integrals, so the surface integral
\[\int\int{\bf A\cdot n}~dS\]
changes sign if we reverse the direction of the normal to the surface.
This normal can be thought of as the cross product of two basis
vectors within the surface. If these basis vectors' cross product is
taken in one order, {\bf n} has one sign. If the opposite order is
taken then {\bf -n} results. Similarly, volume integrals change sign
if we change from a right- or left-handed coordinate system.

\subsection{The wedge product}

We can build this alternating sign into our convention for writing
differential forms by introducing a formal antisymmetric product,
called the {\sl wedge} product, symbolized by $\wedge$, which is
defined to give these differential elements the proper signs. Thus,
surface integrals will be written as integrals over the products
\[{\bf dx} \wedge {\bf dy},
  {\bf dy} \wedge {\bf dz},
  {\bf dz} \wedge {\bf dx}\]
with the convention that $\wedge$ is antisymmetric:
\[{\bf dx} \wedge {\bf dy} = -{\bf dy} \wedge {\bf dx}\]
under the interchange of any two basis forms. This automatically gives
the right orientation of the surface. Similarly, the volume element
becomes
\[{\bf V} = {\bf dx} \wedge {\bf dy} \wedge {\bf dz}\]
which changes sign if any pair of the basis elements are switched.

We can go further than this by formalizing the full integrand. For
a line integral, the general form of the integrand is a linear
combination of the basis differentials,
\[{\bf A}_x{\bf dx} + {\bf A}_y{\bf dy} + {\bf A}_z{\bf dz}\]
Notice that we simply add the different parts. Similary, a general
surface integrand is
\[{\bf A}_z {\bf dx \wedge dy} + 
  {\bf A}_y {\bf dz \wedge dx} + 
  {\bf A}_x {\bf dy \wedge dz }\]
while the volume integrand is
\[f(x)~{\bf dx \wedge dy \wedge dz}\]
These objects are called {\sl differential forms}.

Clearly, differential forms come in severaly types. Functions are
called 0-forms, line elements 1-forms, surface elements 2-forms, and
volume elements are called 3-forms. These are all the types that exist
in 3-dimensions, but in more than three dimensions we can have
$p$-forms with $p$ ranging from zero to the dimension, $d$, of the
space. Since we can take arbitrary linear combinations of $p$-forms,
they form a vector space, $\Lambda_p$.

We can always wedge together any two forms. We assume this wedge
product is associative, and obeys the usual distributive laws. The
wedge product of a $p$-form with a $q$-form is a $(p+q)$-form.

Notice that the antisymmetry is all we need to rearrange any
combination of forms. In general, wedge products of even order forms
with any other forms commute while wedge products of pairs of
odd-order forms anticommute. In particular, functions (0-forms)
commute with all $p$-forms. Using this, we may interchange the order
of a line element and a surface area, for if
\[{\bf l} = A~{\bf dx}\]
\[{\bf S} = B~{\bf dy \wedge dz}\]
then
\[\begin{array}{rcl}
{\bf l \wedge S}&=& (A~{\bf dx}) \wedge (B~{\bf dy \wedge dz})\\
&=&A~{\bf dx} \wedge B~{\bf dy \wedge dz}\\
&=&AB~{\bf dx \wedge dy \wedge dz}\\
&=&-AB~{\bf dy \wedge dx \wedge dz}\\
&=&AB~{\bf dy \wedge dz \wedge dx}\\
&=&{\bf S \wedge l}
\end{array}\]
but the wedge product of two line elements changes sign, for if
\[{\bf l}_1 = A~{\bf dx}\]
\[{\bf l}_2 = B~{\bf dy} + C~{\bf dz}\]
then
\[\begin{array}{rcl}
{\bf l}_1 \wedge {\bf l}_2&=&(A~{\bf dx}) \wedge(B~{\bf dy}+C~{\bf dz})\\
&=&A~{\bf dx} \wedge B~{\bf dy} + A~{\bf dx} \wedge C~{\bf dz}\\
&+&AB~{\bf dx \wedge dy} + AC~{\bf dx \wedge dz}\\
&=&-AB~{\bf dy \wedge dx} - AC~{\bf dz \wedge dz}\\
&=&-B~{\bf dy} \wedge A~{\bf dx} - C~{\bf dz} \wedge A~{\bf dx}\\
&=&-{\bf l}_2 \wedge {\bf l}_1
\end{array}\]
For any odd-order form, $\omega$, we immediately have
\[\omega \wedge\omega = -\omega \wedge\omega = 0\] In 3-dimensions there
are no 4-forms because anything we try to construct must contain a
repeated basis form. For example,
\[\begin{array}{rcl}
{\bf l} \wedge {\bf V}&=&(A~{\bf dx}) \wedge(B~{\bf dx \wedge dy \wedge dz})\\
&=&AB~{\bf dx \wedge dx \wedge dy \wedge dz}\\
&=&0
\end{array}\]
since ${\bf dx \wedge dx}=0$. The same occurs for anything we try. Of
course, if we have more dimensions then there are more independent
directions and we can find nonzero 4-forms. In general, in
$d$-dimensions we can find $d$-forms, but no $(d+1)$-forms.

Now suppose we want to change coordinates. How does an integrand change?
Suppose Cartesian coordinates (x,y) in the plane are given as some
functions of new coordinates (u,v). Then we already know that
differentials change according to 
\[{\bf dx} = {\bf dx}(u,v) = 
  \frac{\partial x}{\partial u}{\bf du} +
  \frac{\partial x}{\partial v}{\bf dv}\]
and similarly for ${\bf dy}$, applying the usual rules for partial
differentiation. Notice what happens when we use the wedge
product to calculate the new area element:
\[\begin{array}{rcl}
{\bf dx} \wedge{\bf dy}&=&
\displaystyle\left(\frac{\partial x}{\partial u}{\bf du}+
      \frac{\partial x}{\partial v}{\bf dv}\right) \wedge
\displaystyle\left(\frac{\partial y}{\partial u}{\bf du}+
      \frac{\partial y}{\partial v}{\bf dv}\right)\\
&&\\
&=&\displaystyle\frac{\partial x}{\partial v}\frac{\partial y}{\partial u}
   {\bf dv \wedge du} +
   \displaystyle\frac{\partial x}{\partial u}\frac{\partial y}{\partial v}
   {\bf du \wedge dv} \\
&&\\
&=&\left(
\displaystyle\frac{\partial x}{\partial u}\frac{\partial y}{\partial v}-
\displaystyle\frac{\partial x}{\partial v}\frac{\partial y}{\partial u}
\right) {\bf du \wedge dv}\\
&&\\
&=&\mathit{J}~{\bf du \wedge dv}
\end{array}\]
where
\[J=\textrm{det}\left(
\begin{array}{rcl}
\displaystyle\frac{\partial x}{\partial u} & 
\displaystyle\frac{\partial x}{\partial v}\\
&\\
\displaystyle\frac{\partial y}{\partial u} & 
\displaystyle\frac{\partial y}{\partial v}
\end{array}
\right)\]
is the Jacobian of the coordinate transformation. This is exactly the
way that an area element changes when we change coordinates. Notice
the Jacobian coming out automatically. We couldn't ask for more -
the wedge product not only gives us the right signs for oriented
areas and volumes, but gives us the right transformation to new
coordinates. Of course the volume change works, too.

Under a coordinate transformation
\[ x \rightarrow x(u,v,w)\]
\[ y \rightarrow y(u,v,w)\]
\[ z \rightarrow z(u,v,w)\]
the new volume element is the full Jacobian times the new volume form,
\[{\bf dx \wedge dy \wedge dz} = J(xyz;uvw)~{\bf du \wedge dv \wedge dw}\]

So the wedge product successfully keesp track of $p$-dim volumes and
their orientations in a coordinate invariant way. Now any time we have
an integral, we can regard the integrand as being a differential form.
But all of this can go much further. Recall our proof that 1-forms form
a vector space. Thus, the differential, ${\bf dx}$ of $x(u,v)$ given
above is just a gradient. It vanishes along surfaces where $x$ is
constant, and the components of the vector
\[\displaystyle\left(
\frac{\partial x}{\partial u},\frac{\partial x}{\partial v}\right)
\]
point in a direction normal to those surfaces. So symbols like
${\bf dx}$ or ${\bf du}$ contain directional information. Writing
them with a boldface {\bf d} indicates this vector character. Thus,
we write
\[{\bf A} = A_i{\bf dx^i}\]

Let
\[f(x,y)=axy\]
The vector with components
\[\displaystyle\left(
\frac{\partial f}{\partial u},\frac{\partial f}{\partial v}\right)
\]
is perpendicular to the surfaces of constant $f$.

We have defined forms, have written down their formal properties, and have
used those properties to write them in components. Then, we define the
wedge product, which enables us to write $p$-dimensional integrands as
$p$-forms in such a way that the orientation and coordinate transformation
properties of the integrals emerges automatically.

Though it is 1-forms, $A_i{\bf dx^i}$ that corresponding to vectors,
we have defined a product of basis forms that we can generalize to
more complicated objects. Many of these objects are already
familiar. Consider the product of two 1-forms.
\[\begin{array}{rcl}
{\bf A} \wedge {\bf B}
&=&A_i~{\bf dx}^i \wedge B_j~{\bf dx}^j\\
&=&A_iB_j~{\bf dx}^i \wedge {\bf dx}^j\\
&=&\displaystyle\frac{1}{2}
A_iB_j~({\bf dx}^i \wedge {\bf dx}^j-{\bf dx}^j \wedge {\bf dx}^i)\\
&&\\
&=&\displaystyle\frac{1}{2}
(A_iB_j~{\bf dx}^i \wedge {\bf dx}^j-A_iB_j~{\bf dx}^j \wedge {\bf dx}^i)\\
&&\\
&=&\displaystyle\frac{1}{2}
(A_iB_j~{\bf dx}^i \wedge {\bf dx}^j-A_jB_i~{\bf dx}^i \wedge {\bf dx}^j)\\
&&\\
&=&\displaystyle\frac{1}{2}(A_iB_j-A_jB_i)~{\bf dx}^i \wedge {\bf dx}^j
\end{array}\]
The coefficients
\[A_iB_j-A_jB_i\]
are essentially the components of the cross product. We will see this in
more detail below when we discuss the curl.

\subsection{The exterior derivative}

We may regard the differential of any function, say $f(x,y,z)$, as the
1-form:
\[\begin{array}{rcl}
{\bf d}f&=&
\displaystyle\frac{\partial f}{\partial x}{\bf d}x+
\displaystyle\frac{\partial f}{\partial y}{\bf d}y+
\displaystyle\frac{\partial f}{\partial z}{\bf d}z\\
&&\\
&=&\displaystyle\frac{\partial f}{\partial x^i}{\bf d}x^i
\end{array}\]

Since a fnction is a 0-form then we can imagine an operator {\bf d} that
differentiates any 0-form to give a 1-form. In Cartesian coordinates,
the coefficients of this 1-form are just the Cartesian components of the
gradient.

The operator {\bf d} is called the {\sl exterior derivative}, and we may
apply it to any $p$-form to get a $(p+1)$-form. The extension is defined
as follows. First consider a 1-form
\[{\bf A}=A_i~{\bf dx}^i\]
We define
\[{\bf dA}={\bf d}A_i \wedge {\bf dx}^i\]
Similarly, since an arbitrary $p$-form in $n$-dimensions may be written as
\[\omega=A_{i_1,i_2,\cdots,i_p} \wedge {\bf dx}^{i_1} \wedge {\bf dx}^{i_2}
\cdots \wedge {\bf dx}^{i_p}\]
we define the exterior derivative of $\omega$ to be a $(p+1)$-form
\[{\bf d}\omega=
{\bf d}A_{i_1,i_2,\cdots,i_p} \wedge {\bf dx}^{i_1} \wedge {\bf dx}^{i_2}
\cdots \wedge {\bf dx}^{i_p}\]

Let's see what happens if we apply ${\bf d}$ twice to the Cartesian
coordinate, $x$ regarded as a function of $x,y$ and $z$:
\[\begin{array}{rcl}
{\bf d}^2x&=&{\bf d}({\bf d}x)\\
&=&{\bf d}(1{\bf d}x)\\
&=&{\bf d}(1) \wedge{\bf d}x\\
&=&0
\end{array}\]
since all derivatives of the constant function $f=1$ are zero. The
same applies if we apply {\bf d} twice to {\sl any} function:
\[\begin{array}{rcl}
{\bf d}^2f &=&{\bf d}({\bf d}f)\\
&=&\displaystyle{\bf d}
\left(\frac{\partial f}{\partial x^i}{\bf d}x^i\right)\\
&&\\
&=&\displaystyle{\bf d}
\left(\frac{\partial f}{\partial x^i} \wedge {\bf d}x^i\right)\\
&&\\
&=&\displaystyle\left(\frac{\partial^2 f}{\partial x^j\partial x^i}
{\bf d}x^j\right) \wedge {\bf d}x^i\\
&&\\
&=&\displaystyle
\frac{\partial^2 f}{\partial x^j\partial x^i}{\bf d}x^j \wedge{\bf d}x^i
\end{array}\]
By the same argument we used to get the components of the curl, we may
write this as
\[\begin{array}{rcl}
{\bf d}^2f&=&\displaystyle\frac{1}{2}\left(
\displaystyle\frac{\partial^2f}{\partial x^j\partial x^i}-
\displaystyle\frac{\partial^2f}{\partial x^i\partial x^j}\right)
{\bf d}x^j \wedge{\bf d}x^i\\
&=&0
\end{array}\]
since partial derivatives commute.

Poincar\'e Lemma: ${\bf d}^2\omega=0$ where $\omega$ is an arbitrary $p$-form.
\index{Poincar\'e Lemma}

Next, consider the effect on {\bf d} on an arbitrary 1-form. We have
\[\begin{array}{rcl}
{\bf dA}&=&{\bf d}(A_i{\bf d}x^i)\\
&&\\
&=&\displaystyle\left(\frac{\partial A_i}{\partial x^j}{\bf d}x^j\right)
 \wedge{\bf d}x^i\\
&&\\
&=&\displaystyle\frac{1}{2}\left(
\frac{\partial A_i}{\partial x^j}-\frac{\partial A_j}{\partial x^i}\right)
{\bf d}x^j \wedge{\bf d}x^i
\end{array}\]
We have the components of the curl of the vector {\bf A}. We must be
careful here, however, because these are the components of the curl
only in Cartesian coordinates. Later we will see how these components
relate to those in a general coordinate system. Also, recall that the
components $A_i$ are distinct from the usual vector components $A^i$.
These differences will be resolved when we give a detailed discussion
of the metric. Ultimately, the action of {\bf d} on a 1-form gives us
a coordinate invariant way to calculate the curl.

Finally, suppose we have a 2-form expressed as
\[{\bf S}=A_z~{\bf d}x \wedge {\bf d}y+A_y~{\bf d}z \wedge {\bf d}x+
A_x~{\bf d}y \wedge {\bf d}z\]
Then apply the exterior derivative gives
\[\begin{array}{rcl}
{\bf d}S&=&{\bf d}A_z \wedge{\bf d}x \wedge{\bf d}y+
{\bf d}A_y \wedge{\bf d}z \wedge{\bf d}x+
{\bf d}A_x \wedge{\bf d}y \wedge{\bf d}z\\
&&\\
&=&\displaystyle\frac{\partial A_z}{\partial z}{\bf d}z \wedge{\bf d}x
 \wedge{\bf d}y+
\displaystyle\frac{\partial A_y}{\partial y}{\bf d}y \wedge{\bf d}z
 \wedge{\bf d}x+
\displaystyle\frac{\partial A_x}{\partial x}{\bf d}x \wedge{\bf d}y
\wedge{\bf d}z\\
&&\\
&=&\displaystyle\left(
\frac{\partial A_z}{\partial z}+
\frac{\partial A_y}{\partial y}+
\frac{\partial A_x}{\partial x}\right)~{\bf d}x \wedge{\bf d}y \wedge{\bf d}z
\end{array}\]
so that the exterior derivative can also reproduce the divergence.

\subsection{The Hodge dual}

To truly have the curl we need a way to turn a 2-form into a vector, i.e.,
a 1-form and a way to turn a 3-form into a 0-form. This leads us to 
introduce the Hodge dual
\index{Hodge dual}, or star, operator $\star$.

Notice that in 3-dim, both 1-forms and 2-forms have three independent
components, while both 0- and 3-forms have one component. This suggests
that we can define an invertible mapping between these pairs. In
Cartesian coordinates, suppose we set
\[\begin{array}{rcl}
\star({\bf dx} \wedge {\bf dy})&=&{\bf dz}\\
\star({\bf dy} \wedge {\bf dz})&=&{\bf dx}\\
\star({\bf dz} \wedge {\bf dx})&=&{\bf dy}\\
\star({\bf dx} \wedge {\bf dy} \wedge {\bf dz})&=&1
\end{array}\]
and further require that the star be its own inverse
\[\star\star = 1\]
With these rules we can find the Hodge dual of any form in 3-dim.

The dual of the general 1-form
\[{\bf A} = A_i{\bf dx}^i\]
is the 2-form
\[S=A_z~{\bf dx} \wedge {\bf dy} + A_y~{\bf dz} \wedge {\bf dx} +
A_x~{\bf dy} \wedge {\bf dz}\]

For an arbitrary (Cartesian) 1-form
\[{\bf A} = A_i{\bf dx}^i\]
that
\[\star{\bf d}\star{\bf A} = div {\bf A}\]

The curl of {\bf A} 
\[curl({\bf A}) =
\displaystyle\left(
\frac{\partial A_y}{\partial z}-
\frac{\partial A_z}{\partial y}\right){\bf dx}+
\displaystyle\left(
\frac{\partial A_z}{\partial x}-
\frac{\partial A_x}{\partial z}\right){\bf dy}
\displaystyle\left(\frac{\partial A_x}{\partial y}-
\frac{\partial A_y}{\partial x}\right){\bf dz}\]

Three operations - the wedge product $\wedge$, the exterior derivative 
{\bf d}, and the Hodge dual $\star$ - together encompass the usual dot
and cross products as well as the divergence, curl and gradient. In fact,
they do much more - they extend all of these operations to arbitrary
coordinates and arbitrary numbers of dimensions. To explore these
generalizations, we must first explore properties of the metric and
look at coordinate transformations. This will allow us to define the
Hodge dula in arbitrary coordinates.

\chapter{Pade approximant}
Pade approximant
\chapter[Schwartz-Zippel lemma]
{Schwartz-Zippel lemma and testing polynomial identities}
Schwartz-Zippel lemma and testing polynomial identities
\chapter{Chinese Remainder Theorem}
Chinese Remainder Theorem
\chapter{Gaussian Elimination}
Gaussian Elimination
\chapter{Diophantine Equations}
Diophantine Equations

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Bibliography}
We have made every effort to consult and reference primary
sources. (See \cite{Rekd14}).
\bibliographystyle{axiom}
\bibliography{axiom}

\newpage
\phantomsection
{\huge Signatures Index}
\addcontentsline{toc}{chapter}{Signatures Index}
\immediate\closeout\sigfile
\vskip 1cm
\input{signatures.sort}

\newpage
\phantomsection
\addcontentsline{toc}{chapter}{Figure Index}
\renewcommand{\indexname}{Figure Index}
\printindex[fig]

\newpage
\phantomsection
\addcontentsline{toc}{chapter}{Documentation Index}
\renewcommand{\indexname}{Documentation Index}
\printindex[doc]

\newpage
\phantomsection
\addcontentsline{toc}{chapter}{Code Index}
\renewcommand{\indexname}{Code Index}
\printindex[code]

\newpage
\phantomsection
\addcontentsline{toc}{chapter}{Error Index}
\renewcommand{\indexname}{Error Index}
\printindex[err]

\newpage
\phantomsection
\addcontentsline{toc}{chapter}{Category Index}
\renewcommand{\indexname}{Category Index}
\printindex[cat]

\newpage
\phantomsection
\addcontentsline{toc}{chapter}{Domain Index}
\renewcommand{\indexname}{Domain Index}
\printindex[dom]

\newpage
\phantomsection
\addcontentsline{toc}{chapter}{Package Index}
\renewcommand{\indexname}{Package Index}
\printindex[pkg]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
